<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-11-07T13:11:06-05:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Anna Rosengart</title><subtitle>An academic and research website.</subtitle><entry><title type="html">Deriving A Variance Component Score Test - Poisson</title><link href="http://localhost:4000/posts/2024/11/06/derive-score-test-poisson.html" rel="alternate" type="text/html" title="Deriving A Variance Component Score Test - Poisson" /><published>2024-11-06T00:00:00-05:00</published><updated>2024-11-06T00:00:00-05:00</updated><id>http://localhost:4000/posts/2024/11/06/derive-score-test-poisson</id><content type="html" xml:base="http://localhost:4000/posts/2024/11/06/derive-score-test-poisson.html"><![CDATA[<h2 id="set-up">Set-Up</h2>

<p>The set-up is the standard generalized linear mixed model setting. Suppose we have $n$ samples and $k$ targets (or phenotypes, if you’re dealing with GWAS).</p>

<p>Our data will include a count table, $Y$, that is $n \times k$, some fixed covariates, $X$, that is $n \times p$, and some random covariates, $Z$, that is $n \times q$.</p>

<p>We’ll assume that the counts are Poisson, so our link function is $\log(\cdot)$. Let $\mu_{j, i}$ denote the mean of $Y_{j, i}$ (the count for target $j$ of sample $i$). We further assume:</p>

\[\log(\mu_{j,i}) = X \alpha + Z\beta\]

<p>for $p \times k$ fixed effects matrix $\alpha$ and $q \times k$ random effects matrix $\beta$. Conditional on the random effects, we assume the counts are independent and identically distributed.</p>

<p>Let $\text{vec}(\cdot)$ denote vectorization of the input where we take the matrix of interest and concanate its columns into one long vector. As is standard with mixed models, we assume:</p>

\[\text{vec}(\beta) \sim \mathcal{N}(\vec{0}, \tau^2 \left( \Sigma_Z \otimes \Sigma_T \right))\]

<p>where $\tau^2$ is a variance parameter, $\Sigma_Z$ is an $q \times q$ covariance matrix associated with the random effects, and $\Sigma_T$ is a $k \times k$ covariance matrix associated with the targets.</p>

<p>Our interest lies in testing whether the columns of $\beta$ are $\vec{0}$ (i.e. $\beta_1 = \beta_2 = \dots = \beta_k = \vec{0}$). Equivalently, we can just test whether $\tau = 0$. If $\tau = 0$, then $\text{vec}(\beta)$ has $0$ variance, and all of the random effects are $0$. Thus, our null hypothesis will be $H_0: \tau = 0$.</p>

<h2 id="score-test">Score Test</h2>

<p>We will need three ingredients to write out the score test:</p>

<ol>
  <li>The log likelihood function</li>
  <li>The score, $U(\tau)$, which is the gradient of the log-likelihood with respect to $\tau$</li>
  <li>The Fisher Information, $\mathcal{I}(\tau)$, which is the variance of the score</li>
</ol>

<p>Once we have the above pieces, the test statistic for a null hypothesis $H_0: \tau = \tau_0$ is given by:</p>

\[S(\tau_0) = U^\top(\hat{\tau}_0) \mathcal{I}^{-1}(\hat{\tau}_0) U(\hat{\tau}_0)\]

<p>where $\hat{\tau}_0$ is the maximum likelihood estimate of $\tau$ under $H_0$.</p>

<h2 id="case-1">Case 1</h2>

<p>For simplicity, let’s assume that $\Sigma_T = \mathbb{I}_{k \times k}$, the $k$-dimensional identity matrix and $q = 1$. This implies that the random effects are independent and that $\beta$ and $Z$ are $k$-dimensional vectors. Let’s assume they are also transposed to they are column vectors to make notation more consistent.</p>

<h4 id="likelihood">Likelihood</h4>

<p>Let $y_i$ ($1 \times k$), $x_i$ ($1 \times p$), and $z_i$ ($1 \times q$) denote the $i$-th rows of $Y$, $X$, and $Z$, respectively. Let $\alpha_j$ ($p \times 1$) and $\beta_j$ ($q \times 1$) denote the $j$-th columns of $\alpha$ and $\beta$, respectively. Recall that since we assumed $q = 1$, $\beta_j$ and $z_i$ are scalar-valued.</p>

<p>The likelihood is given by:</p>

\[\begin{aligned}
\mathcal{L}(\alpha, \tau; Y, X, Z) &amp;= \int \mathcal{L}(\alpha, \tau, \beta; Y, X, Z) d\beta \\
&amp;= \int \mathcal{L}(\alpha, \tau; Y, X, Z \rvert \beta) \mathcal{L}(\beta) d\beta \\
&amp;= \int \exp \left(  \ell(\alpha, \tau; Y, X, Z\rvert \beta\right) \left( \frac{1}{\sqrt{(2\pi)^k \rvert \tau^2 \Sigma_T \rvert }} \exp \left(- \frac{1}{2}\beta^\top \left( \tau^2 \Sigma_T \right)^{-1} \beta \right) \right) d\beta\\
&amp;= \frac{1}{\sqrt{(2\pi)^k \rvert \tau^2 \Sigma_T \rvert }} \int \exp \left( \ell(\alpha, \tau; Y, X, Z \rvert \beta) - \frac{1}{2} \beta^\top \left( \tau^2 \Sigma_T \right)^{-1} \beta \right) d\beta\\
\end{aligned}
\label{eq:lik}\]

<p>Since Eq. \eqref{eq:lik} is of the form $c \rvert \tau^2 \Sigma_T \rvert^{-1/2} \int \exp(f(\beta)) d\beta$ for some constant $c$ and function $f$, we can use Laplace’s method to approximate the integral part.</p>

<p>We first expand $f(\beta) = \ell(\alpha, \tau; Y, X, Z \rvert \beta) - \frac{1}{2}\beta^\top \left( \tau^2 \Sigma_T \right)^{-1} \beta$ about $\tilde{\beta}$, which requires the first- and second-order partial derivatives of $f$ with respect to $\beta$:</p>

\[\begin{aligned}
\frac{\partial}{\partial \beta} f(\beta) &amp;= \begin{bmatrix}
\sum_{i = 1}^n [y_{i, 1}z_i - \exp(x_i \alpha_1 + z_i \beta_1) z_i] \\
\vdots \\
\sum_{i = 1}^n [y_{i, k}z_i - \exp(x_i \alpha_k + z_i \beta_k) z_i]
\end{bmatrix} 
- (\tau^2 \Sigma_T)^{-1}\beta \\
\frac{\partial^2}{\partial \beta \partial \beta^\top} f(\beta) &amp;= \begin{bmatrix}
- \sum_{i = 1}^n \exp(x_i \alpha_1 + z_i \beta_1)z_i^2 &amp; \dots &amp; 0 \\
0 &amp; \ddots &amp; 0 \\
0 &amp; \dots &amp; - \sum_{i = 1}^n \exp(x_i \alpha_k + z_i \beta_k)z_i^2
\end{bmatrix} 
- (\tau^2 \Sigma_T)^{-1}
\end{aligned}\]

<hr />
<details>
<summary>Derivation Of First-Order Partial Derivatives.</summary>

The conditional log-likelihood for the complete data is the sum of the conditional log-likelihoods for each observation. Furthermore, since the elements of $\beta$ are also independent, we can just sum over the targets as well:

$$
\begin{aligned}
\ell(\alpha, \tau; Y, X, Z \rvert \beta) &amp;=  \sum_{i = 1}^n \ell(\alpha, \tau; y_i, x_i, z_i \rvert \beta) \\
&amp;= \sum_{i = 1}^n \sum_{j = 1}^k \left[ y_{i,j} \left( x_i \alpha_j + z_i \beta_j \right) - \exp\left( x_i \alpha_j + z_i \beta_j \right) - \log(y_{i,j}!) \right]
\end{aligned}
$$

The $m$-th element of the gradient vector with respect to $\beta$ is given by:

$$
\begin{aligned}
\frac{\partial}{\partial \beta_m} \ell(\alpha, \tau; Y, X, Z \rvert \beta)&amp;= \frac{\partial}{\partial \beta_m} \left[ \sum_{i = 1}^n \ell(\alpha, \tau; y_i, x_i, z_i \rvert \beta)\right]  \\
&amp;=  \sum_{i = 1}^n \sum_{j = 1}^k \frac{\partial}{\partial \beta_m} \left[ y_{i,j} \left( x_i \alpha_j + z_i \beta_j \right) - \exp\left( x_i \alpha_j + z_i \beta_j \right) - \log(y_{i,j}!) \right]  \\
&amp;= \sum_{i = 1}^n [y_{i, m}z_i - \exp(x_i \alpha_m + z_i \beta_m) z_i]
\end{aligned}
$$

The gradient of $\frac{1}{2}\beta^\top (\tau^2 \Sigma_T)^{-1} \beta$ is:

$$
\frac{\partial}{\partial \beta} \left[ \frac{1}{2}\beta^\top (\tau^2 \Sigma_T)^{-1} \beta \right] = (\tau^2 \Sigma_T)^{-1} \beta 
$$

Thus, the $k \times 1$ first-order partial derivative vector is:

$$
\begin{bmatrix}
\sum_{i = 1}^n [y_{i, 1}z_i - \exp(x_i \alpha_1 + z_i \beta_1) z_i] \\
\vdots \\
\sum_{i = 1}^n [y_{i, k}z_i - \exp(x_i \alpha_k + z_i \beta_k) z_i]
\end{bmatrix} 
- (\tau^2 \Sigma_T)^{-1}\beta
$$
</details>
<hr />
<details>
<summary>Derivation Of Second-Order Partial Derivatives.</summary>
To calculate the second-order partial derivatives, we just find the partial derivatives of the first-order partial derivative vector. 

We have:

$$
\begin{aligned}
\frac{\partial^2}{\partial \beta_m^2} \ell(\alpha, \tau; Y, X, Z \rvert \beta) &amp;= \frac{\partial}{\partial \beta_m} \left[ \sum_{i = 1}^n [y_{i, 1}z_i - \exp(x_i \alpha_m + z_i \beta_m) z_i]
\right] \\
&amp;= - \sum_{i = 1}^n \exp(x_i \alpha_m + z_i \beta_m)z_i^2 \\
\frac{\partial^2}{\partial \beta_l \partial \beta_m} \ell(\alpha, \tau; Y, X, Z \rvert \beta) &amp;= \frac{\partial}{\partial \beta_l} \left[ \sum_{i = 1}^n [y_{i, 1}z_i - \exp(x_i \alpha_m + z_i \beta_m) z_i]
\right] \\
&amp;= 0
\end{aligned}
\label{eq:second-a}
$$

Furthermore, we have that:
$$
\frac{\partial^2}{\partial \beta \partial \beta^\top} \beta^\top (\tau^2\Sigma_T)^{-1} \beta = (\tau^2 \Sigma_T)^{-1}
\label{eq:second-b}
$$

Putting Eqs. \eqref{eq:second-a} and \eqref{eq:second-b} yields the matrix of partial derivatives:

$$
\begin{bmatrix}
- \sum_{i = 1}^n \exp(x_i \alpha_1 + z_i \beta_1)z_i^2 &amp; \dots &amp; 0 \\
0 &amp; \ddots &amp; 0 \\
0 &amp; \dots &amp; - \sum_{i = 1}^n \exp(x_i \alpha_k + z_i \beta_k)z_i^2
\end{bmatrix} 
- (\tau^2 \Sigma_T)^{-1}
$$
</details>
<hr />

<p>We choose $\tilde{\beta}$ to be the solution to $\frac{\partial}{\partial \beta} f(\beta) = 0$. Our expansion is then:</p>

\[\begin{aligned}
f(\beta) &amp;= f(\tilde{\beta}) + (\beta - \tilde{\beta})^\top \frac{\partial}{\partial \beta} f(\tilde{\beta}) + \frac{1}{2}(\beta - \tilde{\beta})^\top \frac{\partial^2}{\partial \beta \partial \beta^\top} f(\tilde{\beta}) (\beta - \tilde{\beta}) + R \\
&amp;= \sum_{i = 1}^n \sum_{j = 1}^k \left[ y_{i,j}(x_i \alpha_j + z_i \tilde{\beta_j}) - \exp(x_i \alpha_j + z_i \tilde{\beta}_j) - \log(y_{i,j}!)\right] \\
&amp;\hspace{6mm} -\frac{1}{2}
\sum_{j = 1}^k
\beta_j^2 \left[ \sum_{i = 1}^n \exp(x_i \alpha_j + z_i \tilde{\beta}_j)z_i^2\right]
- \frac{1}{2} \sum_{j = 1}^k \left[ \tilde{\beta}_j^2 \sum_{i = 1}^n \exp(x_i \alpha_j + z_i \tilde{\beta}_j)z_i^2 \right]
- \frac{1}{2}\beta^\top  (\tau^2 \Sigma_T)^{-1} \beta + R
\end{aligned}\]

<p>where $R$ contains the higher order terms of the expansion.</p>

<hr />
<details>
<summary>Taylor Expansion Simplification.</summary>
$$
\begin{aligned}
f(\beta) &amp;= f(\tilde{\beta}) + (\beta - \tilde{\beta})^\top \frac{\partial}{\partial \beta} f(\tilde{\beta}) + \frac{1}{2}(\beta - \tilde{\beta})^\top \frac{\partial^2}{\partial \beta \partial \beta^\top} f(\tilde{\beta}) (\beta - \tilde{\beta}) + R \\
&amp;= \sum_{i = 1}^n \sum_{j = 1}^k \left[ y_{i,j}(x_i \alpha_j + z_i \tilde{\beta_j}) - \exp(x_i \alpha_j + z_i \tilde{\beta}_j) - \log(y_{i,j}!)\right] - \frac{1}{2}\tilde{\beta}^\top \left( \tau^2 \Sigma_T \right)^{-1} \tilde{\beta} \\
&amp;\hspace{6mm} + \frac{1}{2}(\beta - \tilde{\beta})^\top 
\left(
\begin{bmatrix}
- \sum_{i = 1}^n \exp(x_i \alpha_1 + z_i \tilde{\beta}_1)z_i^2 &amp; \dots &amp; 0 \\
0 &amp; \ddots &amp; 0 \\
0 &amp; \dots &amp; - \sum_{i = 1}^n \exp(x_i \alpha_k + z_i \tilde{\beta}_k)z_i^2
\end{bmatrix} 
- (\tau^2 \Sigma_T)^{-1}\right)
(\beta - \tilde{\beta}) + R \\
&amp;= \sum_{i = 1}^n \sum_{j = 1}^k \left[ y_{i,j}(x_i \alpha_j + z_i \tilde{\beta_j}) - \exp(x_i \alpha_j + z_i \tilde{\beta}_j) - \log(y_{i,j}!)\right] - \frac{1}{2}\tilde{\beta}^\top \left( \tau^2 \Sigma_T \right)^{-1} \tilde{\beta} \\
&amp;\hspace{6mm} + 
\frac{1}{2}(\beta - \tilde{\beta})^\top  \begin{bmatrix}
- \sum_{i = 1}^n \exp(x_i \alpha_1 + z_i \tilde{\beta}_1)z_i^2 &amp; \dots &amp; 0 \\
0 &amp; \ddots &amp; 0 \\
0 &amp; \dots &amp; - \sum_{i = 1}^n \exp(x_i \alpha_k + z_i \tilde{\beta}_k)z_i^2
\end{bmatrix} (\beta - \tilde{\beta})
- \frac{1}{2}(\beta - \tilde{\beta})^\top  (\tau^2 \Sigma_T)^{-1} (\beta - \tilde{\beta}) 
 + R \\
&amp;= \sum_{i = 1}^n \sum_{j = 1}^k \left[ y_{i,j}(x_i \alpha_j + z_i \tilde{\beta_j}) - \exp(x_i \alpha_j + z_i \tilde{\beta}_j) - \log(y_{i,j}!)\right] \\
&amp;\hspace{6mm} + 
\frac{1}{2}\beta^\top  \begin{bmatrix}
- \sum_{i = 1}^n \exp(x_i \alpha_1 + z_i \tilde{\beta}_1)z_i^2 &amp; \dots &amp; 0 \\
0 &amp; \ddots &amp; 0 \\
0 &amp; \dots &amp; - \sum_{i = 1}^n \exp(x_i \alpha_k + z_i \tilde{\beta}_k)z_i^2
\end{bmatrix} \beta \\
&amp;\hspace{6mm} + 
\frac{1}{2} (-\tilde{\beta})^\top  \begin{bmatrix}
- \sum_{i = 1}^n \exp(x_i \alpha_1 + z_i \tilde{\beta}_1)z_i^2 &amp; \dots &amp; 0 \\
0 &amp; \ddots &amp; 0 \\
0 &amp; \dots &amp; - \sum_{i = 1}^n \exp(x_i \alpha_k + z_i \tilde{\beta}_k)z_i^2
\end{bmatrix} (-\tilde{\beta})
- \frac{1}{2}\beta^\top  (\tau^2 \Sigma_T)^{-1} \beta 
+ R \\
&amp;= \sum_{i = 1}^n \sum_{j = 1}^k \left[ y_{i,j}(x_i \alpha_j + z_i \tilde{\beta_j}) - \exp(x_i \alpha_j + z_i \tilde{\beta}_j) - \log(y_{i,j}!)\right] \\
&amp;\hspace{6mm} + 
\begin{bmatrix}
- \frac{1}{2}\beta_1 \sum_{i = 1}^n \exp(x_i \alpha_1 + z_i \tilde{\beta}_1)z_i^2 \\
0 &amp; \ddots &amp; 0 \\
0 &amp; \dots &amp; - \frac{1}{2}\beta_k\sum_{i = 1}^n \exp(x_i \alpha_k + z_i \tilde{\beta}_k)z_i^2
\end{bmatrix} \beta \\
&amp;\hspace{6mm} + 
\begin{bmatrix}
\frac{1}{2} \tilde{\beta}_1\sum_{i = 1}^n \exp(x_i \alpha_1 + z_i \tilde{\beta}_1)z_i^2 &amp; \dots &amp; 0 \\
0 &amp; \ddots &amp; 0 \\
0 &amp; \dots &amp; - \frac{1}{2} \tilde{\beta}_k\sum_{i = 1}^n \exp(x_i \alpha_k + z_i \tilde{\beta}_k)z_i^2
\end{bmatrix} (-\tilde{\beta})
- \frac{1}{2}\beta^\top  (\tau^2 \Sigma_T)^{-1} \beta + R \\
&amp;= \sum_{i = 1}^n \sum_{j = 1}^k \left[ y_{i,j}(x_i \alpha_j + z_i \tilde{\beta_j}) - \exp(x_i \alpha_j + z_i \tilde{\beta}_j) - \log(y_{i,j}!)\right] \\
&amp;\hspace{6mm} -\frac{1}{2}
\sum_{j = 1}^k
\beta_j^2 \left[ \sum_{i = 1}^n \exp(x_i \alpha_j + z_i \tilde{\beta}_j)z_i^2\right]
- \frac{1}{2} \sum_{j = 1}^k \left[ \tilde{\beta}_j^2 \sum_{i = 1}^n \exp(x_i \alpha_j + z_i \tilde{\beta}_j)z_i^2 \right]
- \frac{1}{2}\beta^\top  (\tau^2 \Sigma_T)^{-1} \beta + R
\end{aligned}
$$
</details>
<hr />

<p>Supposing that $R$ is negligible, we can <button class="bland-button" onclick="SnackbarFunc()">approximate the integral</button> in Eq. \eqref{eq:lik} as:</p>

<div id="snackbar"> 😮‍💨 Phew, that was a lot of work for just the first step! 😮‍💨</div>

\[\begin{aligned}
\mathcal{L}(\alpha, \tau; Y, X, Z) &amp;= \frac{1}{\sqrt{(2\pi)^k \rvert \tau^2 \Sigma_T \rvert }} \int \exp \left( \ell(\alpha, \tau; Y, X, Z \rvert \beta) - \frac{1}{2} \beta^\top \left( \tau^2 \Sigma_T \right)^{-1} \beta \right) d\beta \\
&amp;\approx 
\frac{1}{\sqrt{(2\pi)^k \rvert \tau^2 \Sigma_T \rvert}} 
\sum_{i = 1}^n \sum_{j = 1}^k \left[ y_{i,j}(x_i \alpha_j + z_i \tilde{\beta_j}) - \exp(x_i \alpha_j + z_i \tilde{\beta}_j) - \log(y_{i,j}!)\right] \\
&amp;\hspace{6mm} - \frac{1}{2\sqrt{(2\pi)^k \rvert \tau^2 \Sigma_T \rvert}} 
\sum_{j = 1}^k
\beta_j^2 \left[ \sum_{i = 1}^n \exp(x_i \alpha_j + z_i \tilde{\beta}_j)z_i^2\right] \\
&amp;\hspace{6mm} -\frac{1}{2\sqrt{(2\pi)^k \rvert \tau^2 \Sigma_T \rvert}} \sum_{j = 1}^k \left[ \tilde{\beta}_j^2 \sum_{i = 1}^n \exp(x_i \alpha_j + z_i \tilde{\beta}_j)z_i^2 \right] \\
&amp;\hspace{6mm} -\frac{1}{2\sqrt{(2\pi)^k \rvert \tau^2 \Sigma_T \rvert}}\beta^\top  (\tau^2 \Sigma_T)^{-1} \beta 
\end{aligned}\]

<hr />

<h2 id="references">References</h2>

<p>Lin, X. “Variance component testing in generalised linear models with random effects”. Biometrika, Volume 84, Issue 2, June 1997. Pages 309–326. <a href="https://doi.org/10.1093/biomet/84.2.309">doi:10.1093/biomet/84.2.309</a>.</p>

<!-- q > 1 -->
<!-- #### Log-Likelihood

The log-likelihood is given by:

$$
\begin{aligned}
\mathcal{L}(\alpha, \tau; Y, X, Z) &= \int \cdots \int \mathcal{L}(\alpha, \tau, \beta; Y, X, Z) d\beta_1 \dots d\beta_k \\
&= \int \cdots \int \mathcal{L}(\alpha, \tau; Y, X, Z \rvert \beta) \mathcal{L}(\beta) d\beta_1 \dots d\beta_k \\
&= \int \cdots \int \exp \left(  \ell(\alpha, \tau; Y, X, Z\rvert \beta\right) \prod_{j = 1}^k \left( \frac{1}{\sqrt{(2\pi)^q \rvert \tau^2 \Sigma_T \rvert }} \exp \left(- \frac{1}{2}\beta_j^\top \left( \tau^2 \Sigma_T \right)^{-1} \beta_j \right) \right) d\beta_1 \dots d\beta_k\\
&= \frac{1}{\sqrt{(2\pi)^q \rvert \tau^2 \Sigma_T \rvert }} \int \cdots \int \exp \left( \ell(\alpha, \tau; Y, X, Z \rvert \beta) - \frac{1}{2} \sum_{j = 1}^k \beta_j^\top \left( \tau^2 \Sigma_T \right)^{-1} \beta_j \right) d\beta_1 \dots d\beta_k \\
\end{aligned}
\label{eq:lik}
$$

Since Eq. \eqref{eq:lik} is of the form $c \rvert \tau^2 \Sigma_T \rvert^{-1/2} \int \exp(f(\beta)) d\beta$ for some constant $c$ and function $f$, we can use Laplace's method to approximate the integral part. 

We first expand $f(\beta) = \ell(\alpha, \tau; Y, X, Z \rvert \beta) - \frac{1}{2}\beta^\top \left( \tau^2 \Sigma_T \right)^{-1} \beta$ about $\tilde{\beta}$, which requires the first- and second-order partial derivatives of $f$ with respect to $\beta$:

$$
\begin{aligned}
\frac{\partial}{\partial \beta} f(\beta) &= ?? \\
\frac{\partial^2}{\partial \beta \partial \beta^\top} f(\beta) &= ??
\end{aligned}
$$ 


---
<details>
<summary>First-Order Partial Derivatives.</summary>

Let $y_i$ ($1 \times k$), $x_i^\top$ ($1 \times p$), and $z_i^\top$ ($1 \times q$) denote the $i$-th rows of $Y$, $X^\top$, and $Z^\top$, respectively. Let $\alpha_j$ ($p \times 1$) and $\beta_j$ ($q \times 1$) denote the $j$-th columns of $\alpha$ and $\beta$, respectively. 

Since we only have a single fixed effect and a single random effect, $x_i$, $z_i$, $\alpha_j$, and $\beta_j$ are all scalars, so we can drop the transpositions. Let $\mathbf{1}_k$ denote a $k$-dimensional vector of ones, and let $\exp(v)$ for vector $v$ be the element-wise exponentiation. Note that:

$$
\begin{aligned}
\ell(\alpha, \tau; Y, X, Z \rvert \beta) &=  \sum_{i = 1}^n \ell(\alpha, \tau; y_i, x_i^\top, z_i^\top \rvert \beta) \\
&=  \sum_{i = 1}^n \sum_{j = 1}^k \left[ y_{i,j} \left( x_i^\top \alpha_j + z_i^\top \beta_j \right) - \exp\left( x_i^\top \alpha_j + z_i^\top \beta_j \right) - \log(y_{i,j}!) \right]
\end{aligned}
$$

The $q$-dimensional gradient vector with respect to $\beta_m$ is given by:

$$
\begin{aligned}
\frac{\partial}{\partial \beta_m} \ell(\alpha, \tau; Y, X, Z \rvert \beta)&= \frac{\partial}{\partial \beta_m} \left[ \sum_{i = 1}^n \ell(\alpha, \tau; y_i, x_i, z_i \rvert \beta)\right]  \\
&= \frac{\partial}{\partial \beta_m} \left[ \sum_{i = 1}^n \sum_{j = 1}^k \left[ y_{i,j} \left( x_i^\top \alpha_j + z_i^\top \beta_j \right) - \exp\left( x_i^\top \alpha_j + z_i^\top \beta_j \right) - \log(y_{i,j}!) \right] \right] \\
&= \sum_{i = 1}^n \left[ y_{i,m} z_i^\top - \exp(x_i^\top \alpha_m + z_i^\top \beta_m)z_i^\top \right] \\
&= \sum_{i = 1}^n \left(y_{i,m} - \exp(x_i^\top \alpha_m + z_i^\top \beta_m) \right)z_i^\top
\end{aligned}
$$

The gradient of $\frac{1}{2}\beta^\top (\tau^2 \Sigma_T)^{-1} \beta$ is $(\tau^2 \Sigma_T)^{-1} \beta_m$, so the first-order partial derivative vector is:

$$
\begin{bmatrix}
\left( \sum_{i = 1}^n \left(y_{i,1} - \exp(x_i^\top \alpha_1 + z_i^\top \beta_1) \right)z_i^\top \right)^\top & \dots &
\left( \sum_{i = 1}^n \left(y_{i,k} - \exp(x_i^\top \alpha_k + z_i^\top \beta_k) \right)z_i^\top \right)^\top
\end{bmatrix} - (\tau^2 \Sigma_T)^{-1}\beta
$$
</details>
---


<details>
<summary>Second-Order Partial Derivatives.</summary>
The second-order partial derivatives will be a $k \times k$ matrix. First we will do the conditional log-likelihood to get its diagonal components:

$$
\begin{aligned}
\frac{\partial^2}{\partial \beta_m^2} \ell(\alpha, \tau; Y, X, Z \rvert \beta) &= \frac{\partial}{\partial \beta_m} \left[ \sum_{i = 1}^n [y_{i, m}z_i - \exp(x_i \alpha_m + z_i \beta_m)z_i]\right] \\
&= -\sum_{i = 1}^m \exp(x_i \alpha_m + z_i \beta_m)z_i^2
\end{aligned}
$$


And the off-diagonal elements will be given by:

$$
\begin{aligned}
\frac{\partial^2}{\partial \beta_m \partial \beta_l} \ell(\alpha, \tau; Y, X, Z \rvert \beta) &= \frac{\partial}{\partial \beta_l} \left[ \sum_{i = 1}^n [y_{i, m}z_i - \exp(x_i \alpha_m + z_i \beta_m)z_i] \right] \\
&= - 
\end{aligned}
$$

</details>
---


#### Score

Taking the derivative (or gradient in multi-dimensional settings) of Eq. \eqref{eq:lik} with respect to $\tau$ can be difficult because the integral is cumbersome. As in (Lin, 1997), we will do a Taylor expansion of $\mathcal{L}(\alpha, \tau; Y, X, Z \rvert \beta)$ about $\beta = \vec{0}$. 

---
<details>
  <summary>Taylor Expansion Details.</summary>
  A Taylor expansion about $\beta = \vec{0}$ is given by:

  $$
  \mathcal{L}(\alpha, \tau; Y, X, Z \rvert \beta) = \mathcal{L}(\alpha, \tau; Y, X, Z \rvert \beta) \bigg \rvert_{\beta = \vec{0}} + \beta^\top \left[ \frac{\partial}{\partial \beta} \mathcal{L}(\alpha, \tau; Y, X, Z \rvert \vec{0}) \bigg \rvert_{\beta = \vec{0}} \right]  + \frac{1}{2} \beta^\top \left[ \frac{\partial^2}{\partial \beta \beta^\top} \mathcal{L}(\alpha, \tau; Y, X, Z \rvert \beta) \bigg \rvert_{\beta = \vec{0}} \right] \beta + e
  $$

  where $e$ contains all higher order partial derivative terms. To write the above equation, we need to find the first- and second-order partial derivatives of $\mathcal{L}(\alpha, \tau; Y, X, Z \rvert \beta)$ with respect to $\beta$. Let $y_i$, $x_i^\top$, and $z_i^\top$ denote the $i$-th rows of $Y$, $X$, and $Z$, respectively. Let $\alpha_j$ and $\beta_j$ denote the $j$-th columns of $\alpha$ and $\beta$, respectively. Since we only have a single fixed effect and a single random effect, $x_i$, $z_i$, $\alpha_j$, and $\beta_j$ are all scalars, so we can drop the transpositions. We can write our likelihood as:

  $$
  \begin{aligned}
  \mathcal{L}(\alpha, \tau; Y, X, Z \rvert \beta) &= \exp \left( \sum_{i = 1}^n \ell(\alpha, \tau; y_i, x_i, z_i \rvert \beta) \right) \\
  &= \exp \left( \sum_{i = 1}^n \sum_{j = 1}^k \left[ y_{i,j} \left( x_i \alpha_j + z_i \beta_j \right) - \exp\left( x_i \alpha_j + z_i \beta_j \right) - \log(y_{i,j}!) \right] \right)
  \end{aligned}
  \label{eq:split-lik}
  $$

  Evaluating Eq. \eqref{eq:split-lik} at $\beta = \vec{0}$ yields:

  $$
  \mathcal{L}(\alpha, \tau; Y, X, Z \rvert \beta) \bigg \rvert_{\beta = \vec{0}} = \exp \left( \sum_{i = 1}^n \sum_{j = 1}^k \left[ y_{i,j} \left( x_i \alpha_j\right) - \exp\left( x_i \alpha_j \right) - \log(y_{i,j}!) \right] \right)
  $$

  The elements of the gradient are given by:

  $$
  \begin{aligned}
  \frac{\partial}{\partial \beta_j} \mathcal{L}(\alpha, \tau; Y, X, Z \rvert \beta) &= \frac{\partial}{\partial \beta_j} \left[ \exp \left( \sum_{i = 1}^n \ell(\alpha, \tau; y_i, x_i, z_i \rvert \beta) \right) \right] \\
  &= \exp \left( \sum_{i = 1}^n \ell(\alpha, \tau; y_i, x_i, z_i \rvert \beta) \right) \left( \frac{\partial}{\partial \beta_j} \left[ \sum_{i = 1}^n \ell(\alpha, \tau; y_i, x_i, z_i \rvert \beta) \right] \right) \\
  &= \exp \left( \sum_{i = 1}^n \ell(\alpha, \tau; y_i, x_i, z_i \rvert \beta) \right) \left( \sum_{i = 1}^n \frac{\partial}{\partial \beta_j} \left[ \sum_{j = 1}^k \left[ y_{i,j} \left( x_i \alpha_j + z_i \beta_j \right) - \exp\left( x_i \alpha_j + z_i \beta_j \right) - \log(y_{i,j}!) \right]  \right] \right) \\
  &= \exp \left( \sum_{i = 1}^n \ell(\alpha, \tau; y_i, x_i, z_i \rvert \beta) \right) \left( \sum_{i = 1}^n \frac{\partial}{\partial \beta_j} \left[ y_{i,j} \left( x_i \alpha_j + z_i \beta_j \right) - \exp\left( x_i \alpha_j + z_i \beta_j \right) - \log(y_{i,j}!) \right] \right) \\
  &= \exp \left( \sum_{i = 1}^n \ell(\alpha, \tau; y_i, x_i, z_i \rvert \beta) \right) \left( \sum_{i = 1}^n \left[ y_{i,j} z_i - \exp\left( x_i \alpha_j + z_i \beta_j \right)z_i \right] \right) \\
  &= \exp \left( \sum_{i = 1}^n \ell(\alpha, \tau; y_i, x_i, z_i \rvert \beta) \right) \left( \sum_{i = 1}^n (y_{i,j} - \mu_{i,j}) z_i\right) \\
  \implies  \frac{\partial}{\partial \beta_j} \mathcal{L}(\alpha, \tau; Y, X, Z \rvert \beta) \bigg \rvert_{\beta = \vec{0}} &= \exp \left( \sum_{i = 1}^n \sum_{j = 1}^k \left[ y_{i,j} \left( x_i \alpha_j\right) - \exp\left( x_i \alpha_j \right) - \log(y_{i,j}!) \right] \right) \left( \sum_{i = 1}^n \left[ y_{i,j} z_i - \exp(x_i \alpha_j) z_i \right]\right)
  \end{aligned}
  \label{eq:first-order}
  $$

  The diagonal elements of the Hessian are given by:

  $$
  \begin{aligned}
  \frac{\partial^2}{\partial \beta_j^2} \mathcal{L}(\alpha, \tau; Y, X, Z \rvert \beta) &= \frac{\partial^2}{\partial \beta_j^2} \left[ \exp \left( \sum_{i = 1}^n \ell(\alpha, \tau; y_i, x_i, z_i \rvert \beta) \right) \right] \\
  &=  \frac{\partial}{\partial \beta_j} \left[ \exp \left( \sum_{i = 1}^n \ell(\alpha, \tau; y_i, x_i, z_i \rvert \beta) \right) \left( \sum_{i = 1}^n \left[ y_{i,j} z_i - \exp\left( x_i \alpha_j + z_i \beta_j \right)z_i \right] \right) \right] \\
  &=  \exp \left( \sum_{i = 1}^n \ell(\alpha, \tau; y_i, x_i, z_i \rvert \beta) \right) \frac{\partial}{\partial \beta_j} \left[ \sum_{i = 1}^n \left[ y_{i,j} z_i - \exp\left( x_i \alpha_j + z_i \beta_j \right)z_i \right] \right] + \frac{\partial}{\partial \beta_j} \left[ \exp \left( \sum_{i = 1}^n \ell(\alpha, \tau; y_i, x_i, z_i \rvert \beta) \right) \right] \left( \sum_{i = 1}^n \left[ y_{i,j} z_i - \exp\left( x_i \alpha_j + z_i \beta_j \right)z_i \right] \right)  \\
  &=  \exp \left( \sum_{i = 1}^n \ell(\alpha, \tau; y_i, x_i, z_i \rvert \beta) \right)\left( \sum_{i = 1}^n - \exp(x_i \alpha_j + z_i \beta_j) z_i^2 \right) + \exp \left( \sum_{i = 1}^n \ell(\alpha, \tau; y_i, x_i, z_i \rvert \beta) \right) \left(\sum_{i = 1}^n \left[ y_{i,j} z_i - \exp\left( x_i \alpha_j + z_i \beta_j \right)z_i \right] \right)^2 \\
  &= \exp \left( \sum_{i = 1}^n \ell(\alpha, \tau; y_i, x_i, z_i \rvert \beta) \right) \left[ \left(\sum_{i = 1}^n \left[ y_{i,j} z_i - \exp\left( x_i \alpha_j + z_i \beta_j \right)z_i \right] \right)^2 -\sum_{i = 1}^n \exp(x_i \alpha_j + z_i \beta_j) z_i^2 \right]  \\
  \implies  \frac{\partial^2}{\partial \beta_j^2} \mathcal{L}(\alpha, \tau; Y, X, Z, \rvert \beta) \bigg \rvert_{\beta = \vec{0}} &= \exp \left( \sum_{i = 1}^n \sum_{j = 1}^k \left[ y_{i,j} \left( x_i \alpha_j\right) - \exp\left( x_i \alpha_j \right) - \log(y_{i,j}!) \right] \right) \left[ \left( y_{i,j} z_i - \exp(x_i \alpha_j) z_i \right)^2  - \sum_{i = 1}^n \exp(x_i \alpha_j)z_i^2 \right] 
  \end{aligned}
  \label{eq:second-order-diag}
  $$

  The off-diagonal elements of the Hessian are:

  $$
  \begin{aligned}
  \frac{\partial^2}{\partial \beta_j \partial \beta_m} \mathcal{L}(\alpha, \tau; Y, X, Z \rvert \beta) &= \frac{\partial^2}{\partial \beta_j \partial \beta_m} \left[ \exp \left( \sum_{i = 1}^n \ell(\alpha, \tau; y_i, x_i, z_i \rvert \beta) \right) \right] \\
  &= \frac{\partial}{\partial \beta_m} \left[ \exp \left( \sum_{i = 1}^n \ell(\alpha, \tau; y_i, x_i, z_i \rvert \beta) \right) \left( \sum_{i = 1}^n \left[ y_{i,j} z_i - \exp\left( x_i \alpha_j + z_i \beta_j \right)z_i \right] \right) \right] \\
  &= \exp \left( \sum_{i = 1}^n \ell(\alpha, \tau; y_i, x_i, z_i \rvert \beta) \right) \left( \sum_{i = 1}^n \left[ y_{i,m} z_i - \exp\left( x_i \alpha_m + z_i \beta_m \right)z_i \right] \right)\left( \sum_{i = 1}^n \left[ y_{i,j} z_i - \exp\left( x_i \alpha_j + z_i \beta_j \right)z_i \right] \right) \\
  \implies \frac{\partial^2}{\partial \beta_j \partial \beta_m} \mathcal{L}(\alpha, \tau; Y, X, Z, \rvert \beta) \bigg \rvert_{\beta = \vec{0}} &=  \exp \left( \sum_{i = 1}^n \sum_{j = 1}^k \left[ y_{i,j} \left( x_i \alpha_j\right) - \exp\left( x_i \alpha_j \right) - \log(y_{i,j}!) \right] \right) \left( \sum_{i = 1}^n[y_{i,m}z_i - \exp(x_i \alpha_m) z_i]\right) \left( \sum_{i = 1}^n [y_{i,j}z_i - \exp(x_i \alpha_j)z_i ]\right)
  \end{aligned}
  \label{eq:second-order-off-diag}
  $$
</details>
---

If we assume that terms of higher order than two are small enough, then we can use a Taylor approximation by truncating the expansion to:

$$
\mathcal{L}(\alpha, \tau; Y, X, Z \rvert \beta) \approx \mathcal{L}(\alpha, \tau; Y, X, Z \rvert \beta) \bigg \rvert_{\beta = \vec{0}} + \beta^\top \left[ \frac{\partial}{\partial \beta} \mathcal{L}(\alpha, \tau; Y, X, Z \rvert \beta) \bigg \rvert_{\beta = \vec{0}} \right]  + \frac{1}{2} \beta^\top \left[ \frac{\partial^2}{\partial \beta \beta^\top} \mathcal{L}(\alpha, \tau; Y, X, Z \rvert \beta) \bigg \rvert_{\beta = \vec{0}} \right] \beta
\label{eq:taylor}
$$

Denote the right-hand side of Eq. \eqref{eq:taylor} with $\mathcal{L}_T(\alpha, \tau; Y, X, Z \rvert \beta)$. Notice that Eq. \eqref{eq:lik} is the expectation of the conditional likelihood of $(\alpha, \tau)$ given $\beta$ (subject to the probability function used for $\beta$). We can then get around the complicated integral by taking the expectation of our Taylor approximation.

$$
\begin{aligned}
\mathcal{L}(\alpha, \theta; Y, X, Z) &= \int \mathcal{L}(\alpha, \tau; Y, X, Z \rvert \beta) \mathcal{L}(\beta) d \beta \\
&\approx \int \mathcal{L}_T(\alpha, \tau; Y, X, Z \rvert \beta) \mathcal{L}(\beta) d\beta \\
&= \mathbb{E}\left[ \mathcal{L}_T(\alpha, \tau; Y, X, Z \rvert \beta)  \right] \\
&= \underbrace{\mathbb{E}\left[ \mathcal{L}(\alpha, \tau; Y, X, Z \rvert \beta) \bigg \rvert_{\beta = \vec{0}} \right]}_{(a)} + 
\underbrace{\mathbb{E} \left[ \beta^\top \left[ \frac{\partial}{\partial \beta} \mathcal{L}(\alpha, \tau; Y, X, Z \rvert \vec{0}) \bigg \rvert_{\beta = \vec{0}} \right] \right]}_{(b)} + 
\frac{1}{2} \underbrace{\mathbb{E} \left[ \beta^\top \left[ \frac{\partial^2}{\partial \beta \beta^\top} \mathcal{L}(\alpha, \tau; Y, X, Z \rvert \beta) \bigg \rvert_{\beta = \vec{0}} \right] \beta \right]}_{(c)} \\
&= \mathcal{L}(\alpha, \tau; Y, X, Z \rvert \beta) \bigg \rvert_{\beta = \vec{0}}
+ 0 + \frac{1}{2} \text{tr}\left[ \left[ \frac{\partial^2}{\partial \beta \beta^\top} \mathcal{L}(\alpha, \tau; Y, X, Z \rvert \beta) \bigg \rvert_{\beta = \vec{0}} \right] \tau^2 \Sigma_T \right] \\
&\overset{(i)}{=} \mathcal{L}(\alpha, \tau; Y, X, Z \rvert \beta) \bigg \rvert_{\beta = \vec{0}}
+ \frac{\tau^2}{2} \text{tr}\left[ \left[ \frac{\partial^2}{\partial \beta \beta^\top} \mathcal{L}(\alpha, \tau; Y, X, Z \rvert \beta) \bigg \rvert_{\beta = \vec{0}} \right] \right] \\
&= \mathcal{L}(\alpha, \tau; Y, X, Z \rvert \beta) \bigg \rvert_{\beta = \vec{0}}
+ \frac{\tau^2}{2} \sum_{j = 1}^k \left[ \left[ \frac{\partial^2}{\partial \beta \beta^\top} \mathcal{L}(\alpha, \tau; Y, X, Z \rvert \beta) \bigg \rvert_{\beta = \vec{0}} \right] \right]_{(j, j)} \\
&= \exp \left( \sum_{i = 1}^n \sum_{j = 1}^k \left[ y_{i,j} \left( x_i \alpha_j\right) - \exp\left( x_i \alpha_j \right) - \log(y_{i,j}!) \right] \right) + \frac{1}{2} \sum_{m = 1}^k  \exp \left( \sum_{i = 1}^n \sum_{j = 1}^k \left[ y_{i,j} \left( x_i \alpha_j\right) - \exp\left( x_i \alpha_j \right) - \log(y_{i,j}!) \right] \right) \left[ \left( y_{i,m} z_i - \exp(x_i \alpha_m) z_i \right)^2  - \sum_{i = 1}^n \exp(x_i \alpha_m)z_i^2 \right]  \\
&= \exp \left( \sum_{i = 1}^n \sum_{j = 1}^k \left[ y_{i,j} \left( x_i \alpha_j\right) - \exp\left( x_i \alpha_j \right) - \log(y_{i,j}!) \right] \right)  \left(1 + \frac{\tau^2}{2} \sum_{m = 1}^k  \left( y_{i,m} z_i - \exp(x_i \alpha_m) z_i \right)^2  - \sum_{i = 1}^n \exp(x_i \alpha_m)z_i^2  \right) 
\end{aligned}
\label{eq:approx-marg-lik}
$$

where $(i)$ follows from the fact that we take $\Sigma_T = \mathbb{I}_{k \times k}$. 

---
<details>
  <summary>Expectation Calculations - $(a)$</summary>
  $$
  \begin{aligned}
  \mathbb{E}\left[ \mathcal{L}(\alpha, \tau; Y, X, Z \rvert \beta) \bigg \rvert_{\beta = \vec{0}} \right] &= \mathcal{L}(\alpha, \tau; Y, X, Z \rvert \beta) \bigg \rvert_{\beta = \vec{0}}
  \end{aligned}
  $$
</details>
---
<details>
  <summary>Expectation Calculations - $(b)$</summary>
  $$
  \begin{aligned}
  \mathbb{E} \left[ \beta^\top \left[ \frac{\partial}{\partial \beta} \mathcal{L}(\alpha, \tau; Y, X, Z \rvert \vec{0}) \bigg \rvert_{\beta = \vec{0}} \right] \right] = \vec{0}^\top \frac{\partial}{\partial \beta} \mathcal{L}(\alpha, \tau; Y, X, Z \rvert \vec{0}) \bigg \rvert_{\beta = \vec{0}} = 0
  \end{aligned}
  $$
</details>
---
<details>
  <summary>Expectation Calculations - $(c)$</summary>
  $$
  \begin{aligned}
  \mathbb{E} \left[ \beta^\top \left[ \frac{\partial^2}{\partial \beta \beta^\top} \mathcal{L}(\alpha, \tau; Y, X, Z \rvert \beta) \bigg \rvert_{\beta = \vec{0}} \right] \beta \right] 
  &= \mathbb{E}\left[\sum_{i = 1}^k \sum_{j = 1}^k  \left[ \frac{\partial^2}{\partial \beta \beta^\top} \mathcal{L}(\alpha, \tau; Y, X, Z \rvert \beta) \bigg \rvert_{\beta = \vec{0}} \right]_{i,j} \beta_i \beta_j \right] \\
  &= \sum_{i =1}^k \sum_{j = 1}^k \left[ \frac{\partial^2}{\partial \beta \beta^\top} \mathcal{L}(\alpha, \tau; Y, X, Z \rvert \beta) \bigg \rvert_{\beta = \vec{0}} \right]_{i,j} \mathbb{E}\left[ \beta_i \beta_j \right] \\
  &\overset{(i)}{=} \sum_{i =1}^k \sum_{j = 1}^k \left[ \frac{\partial^2}{\partial \beta \beta^\top} \mathcal{L}(\alpha, \tau; Y, X, Z \rvert \beta) \bigg \rvert_{\beta = \vec{0}} \right]_{i,j} \text{Cov}(\beta_i, \beta_j) \\
  &= \sum_{i = 1}^k \sum_{j = 1}^k \left[ \frac{\partial^2}{\partial \beta \beta^\top} \mathcal{L}(\alpha, \tau; Y, X, Z \rvert \beta) \bigg \rvert_{\beta = \vec{0}} \right]_{i,j} (\tau^2 \Sigma_T)_{i,j} \\
  &= \text{tr}\left[ \left[ \frac{\partial^2}{\partial \beta \beta^\top} \mathcal{L}(\alpha, \tau; Y, X, Z \rvert \beta) \bigg \rvert_{\beta = \vec{0}} \right] \tau^2 \Sigma_T \right]
  \end{aligned}
  $$

  $(i)$ is due to the fact that $\mathbb{E}[\beta] = \vec{0}$. 
</details>
---

We can then approximate the score with the derivative of Eq. \eqref{eq:approx-marg-lik} with respect to $\tau$:

$$
\begin{aligned}
U(\tau) &= \frac{d}{d\tau} \left[ \mathcal{L}(\alpha, \theta; Y, X, Z) \right] \\
&\approx \frac{d}{d\tau} \left[ \exp \left( \sum_{i = 1}^n \sum_{j = 1}^k \left[ y_{i,j} \left( x_i \alpha_j\right) - \exp\left( x_i \alpha_j \right) - \log(y_{i,j}!) \right] \right)  \left(1 + \frac{\tau^2}{2} \sum_{m = 1}^k  \left( y_{i,m} z_i - \exp(x_i \alpha_m) z_i \right)^2  - \sum_{i = 1}^n \exp(x_i \alpha_m)z_i^2  \right) \right]  \\
&= \tau \exp \left( \sum_{i = 1}^n \sum_{j = 1}^k \left[ y_{i,j} \left( x_i \alpha_j\right) - \exp\left( x_i \alpha_j \right) - \log(y_{i,j}!) \right] \right) \left( \sum_{m = 1}^k  \left( y_{i,m} z_i - \exp(x_i \alpha_m) z_i \right)^2  - \sum_{i = 1}^n \exp(x_i \alpha_m)z_i^2  \right) 
\end{aligned}
$$

-->]]></content><author><name></name></author><category term="posts" /><summary type="html"><![CDATA[Set-Up]]></summary></entry><entry><title type="html">Spatio-Temporal Variability of the Pepper Mild Mottle Virus Biomarker in Wastewater</title><link href="http://localhost:4000/research/2024/08/21/pmmov-preprint.html" rel="alternate" type="text/html" title="Spatio-Temporal Variability of the Pepper Mild Mottle Virus Biomarker in Wastewater" /><published>2024-08-21T00:00:00-04:00</published><updated>2024-08-21T00:00:00-04:00</updated><id>http://localhost:4000/research/2024/08/21/pmmov-preprint</id><content type="html" xml:base="http://localhost:4000/research/2024/08/21/pmmov-preprint.html"><![CDATA[<p><em>Since the start of the coronavirus-19 pandemic, the use of wastewater-based epidemiology (WBE) for disease surveillance has increased throughout the world. Because wastewater measurements are affected by external factors, processing WBE data typically includes a normalization step in order to adjust wastewater measurements (e.g. viral RNA concentrations) to account for variation due to dynamic population changes, sewer travel effects, or laboratory methods. Pepper mild mottle virus (PMMoV), a plant RNA virus abundant in human feces and wastewater, has been used as a fecal contamination indicator and has been used to normalize wastewater measurements extensively. However, there has been little work to characterize the spatio-temporal variability of PMMoV in wastewater, which may influence the effectiveness of PMMoV for adjusting or normalizing WBE measurements. Here, we investigate its variability across space and time using data collected over a two-year period from sewage treatment plants across the United States. We find that most variation in PMMoV measurements can be attributed to longitude and latitude followed by site-specific variables. Further research into cross-geographical and -temporal comparability of PMMoV-normalized pathogen concentrations would strengthen the utility of PMMoV in WBE.</em></p>

<p><strong>Rosengart A</strong>, Bidwell AL, Wolfe MK, Boehm AB, Townes FW. Spatio-Temporal Variability of the Pepper Mild Mottle Virus Biomarker in Wastewater. <em>arXiv</em> (2024). <a href="https://doi.org/10.48550/arXiv.2408.12012">doi:10.48550/arXiv.2408.12012</a>.</p>

<hr />

<p>That’s just the abstract! Read the full preprint on <a href="https://doi.org/10.48550/arXiv.2408.12012">arXiv</a>.</p>]]></content><author><name></name></author><category term="research" /><summary type="html"><![CDATA[Since the start of the coronavirus-19 pandemic, the use of wastewater-based epidemiology (WBE) for disease surveillance has increased throughout the world. Because wastewater measurements are affected by external factors, processing WBE data typically includes a normalization step in order to adjust wastewater measurements (e.g. viral RNA concentrations) to account for variation due to dynamic population changes, sewer travel effects, or laboratory methods. Pepper mild mottle virus (PMMoV), a plant RNA virus abundant in human feces and wastewater, has been used as a fecal contamination indicator and has been used to normalize wastewater measurements extensively. However, there has been little work to characterize the spatio-temporal variability of PMMoV in wastewater, which may influence the effectiveness of PMMoV for adjusting or normalizing WBE measurements. Here, we investigate its variability across space and time using data collected over a two-year period from sewage treatment plants across the United States. We find that most variation in PMMoV measurements can be attributed to longitude and latitude followed by site-specific variables. Further research into cross-geographical and -temporal comparability of PMMoV-normalized pathogen concentrations would strengthen the utility of PMMoV in WBE.]]></summary></entry><entry><title type="html">Informing policy via dynamic models: Cholera in Haiti</title><link href="http://localhost:4000/research/2024/04/29/haiti-cholera.html" rel="alternate" type="text/html" title="Informing policy via dynamic models: Cholera in Haiti" /><published>2024-04-29T00:00:00-04:00</published><updated>2024-04-29T00:00:00-04:00</updated><id>http://localhost:4000/research/2024/04/29/haiti-cholera</id><content type="html" xml:base="http://localhost:4000/research/2024/04/29/haiti-cholera.html"><![CDATA[<p><em>Public health decisions must be made about when and how to implement interventions to control an infectious disease epidemic. These decisions should be informed by data on the epidemic as well as current understanding about the transmission dynamics. Such decisions can be posed as statistical questions about scientifically motivated dynamic models. Thus, we encounter the methodological task of building credible, data-informed decisions based on stochastic, partially observed, nonlinear dynamic models. This necessitates addressing the tradeoff between biological fidelity and model simplicity, and the reality of misspecification for models at all levels of complexity. We assess current methodological approaches to these issues via a case study of the 2010-2019 cholera epidemic in Haiti. We consider three dynamic models developed by expert teams to advise on vaccination policies. We evaluate previous methods used for fitting these models, and we demonstrate modified data analysis strategies leading to improved statistical fit. Specifically, we present approaches for diagnosing model misspecification and the consequent development of improved models. Additionally, we demonstrate the utility of recent advances in likelihood maximization for high-dimensional nonlinear dynamic models, enabling likelihood-based inference for spatiotemporal incidence data using this class of models. Our workflow is reproducible and extendable, facilitating future investigations of this disease system.</em></p>

<p>Wheeler J, <strong>Rosengart A</strong>, Jiang Z, Tan K, Treutle N, Ionides EL. Informing policy via dynamic models: Cholera in Haiti. <em>PLOS Computational Biology</em> (2024). <a href="https://doi.org/10.1371/journal.pcbi.1012032">doi:10.1371/journal.pcbi.1012032</a>.</p>

<hr />

<p>That’s just the abstract! Read the full journal article at <a href="https://doi.org/10.1371/journal.pcbi.1012032">PLOS Comp Bio</a>.</p>]]></content><author><name></name></author><category term="research" /><summary type="html"><![CDATA[Public health decisions must be made about when and how to implement interventions to control an infectious disease epidemic. These decisions should be informed by data on the epidemic as well as current understanding about the transmission dynamics. Such decisions can be posed as statistical questions about scientifically motivated dynamic models. Thus, we encounter the methodological task of building credible, data-informed decisions based on stochastic, partially observed, nonlinear dynamic models. This necessitates addressing the tradeoff between biological fidelity and model simplicity, and the reality of misspecification for models at all levels of complexity. We assess current methodological approaches to these issues via a case study of the 2010-2019 cholera epidemic in Haiti. We consider three dynamic models developed by expert teams to advise on vaccination policies. We evaluate previous methods used for fitting these models, and we demonstrate modified data analysis strategies leading to improved statistical fit. Specifically, we present approaches for diagnosing model misspecification and the consequent development of improved models. Additionally, we demonstrate the utility of recent advances in likelihood maximization for high-dimensional nonlinear dynamic models, enabling likelihood-based inference for spatiotemporal incidence data using this class of models. Our workflow is reproducible and extendable, facilitating future investigations of this disease system.]]></summary></entry><entry><title type="html">SLIDE: Significant Latent Factor Interaction Discovery and Exploration across biological domains</title><link href="http://localhost:4000/research/2024/02/19/slide.html" rel="alternate" type="text/html" title="SLIDE: Significant Latent Factor Interaction Discovery and Exploration across biological domains" /><published>2024-02-19T00:00:00-05:00</published><updated>2024-02-19T00:00:00-05:00</updated><id>http://localhost:4000/research/2024/02/19/slide</id><content type="html" xml:base="http://localhost:4000/research/2024/02/19/slide.html"><![CDATA[<p><em>Modern multiomic technologies can generate deep multiscale profiles. However, differences in data modalities, multicollinearity of the data, and large numbers of irrelevant features make analyses and integration of high-dimensional omic datasets challenging. Here we present Significant Latent Factor Interaction Discovery and Exploration (SLIDE), a first-in-class interpretable machine learning technique for identifying significant interacting latent factors underlying outcomes of interest from high-dimensional omic datasets. SLIDE makes no assumptions regarding data-generating mechanisms, comes with theoretical guarantees regarding identifiability of the latent factors/corresponding inference, and has rigorous false discovery rate control. Using SLIDE on single-cell and spatial omic datasets, we uncovered significant interacting latent factors underlying a range of molecular, cellular and organismal phenotypes. SLIDE outperforms/performs at least as well as a wide range of state-of-the-art approaches, including other latent factor approaches. More importantly, it provides biological inference beyond prediction that other methods do not afford. Thus, SLIDE is a versatile engine for biological discovery from modern multiomic datasets.</em></p>

<p>Rahimikollu J, Xiao H, <strong>Rosengart A</strong>, Rosen ABI, Tabib T, Zdinak PM, He K, Bing X, Bunea F, Wegkamp M, Poholek A, Joglekar A, Lafyatis R, Das J. SLIDE: Significant Latent Factor Interaction Discovery and Exploration across biological domains. <em>Nature Methods</em> (2024). <a href="https://doi.org/10.1038/s41592-024-02175-z">doi:10.1038/s41592-024-02175-z</a>.</p>

<hr />

<p>That’s just the abstract! Read the full journal article at <a href="https://doi.org/10.1038/s41592-024-02175-z">Nature Methods</a>.</p>]]></content><author><name></name></author><category term="research" /><summary type="html"><![CDATA[Modern multiomic technologies can generate deep multiscale profiles. However, differences in data modalities, multicollinearity of the data, and large numbers of irrelevant features make analyses and integration of high-dimensional omic datasets challenging. Here we present Significant Latent Factor Interaction Discovery and Exploration (SLIDE), a first-in-class interpretable machine learning technique for identifying significant interacting latent factors underlying outcomes of interest from high-dimensional omic datasets. SLIDE makes no assumptions regarding data-generating mechanisms, comes with theoretical guarantees regarding identifiability of the latent factors/corresponding inference, and has rigorous false discovery rate control. Using SLIDE on single-cell and spatial omic datasets, we uncovered significant interacting latent factors underlying a range of molecular, cellular and organismal phenotypes. SLIDE outperforms/performs at least as well as a wide range of state-of-the-art approaches, including other latent factor approaches. More importantly, it provides biological inference beyond prediction that other methods do not afford. Thus, SLIDE is a versatile engine for biological discovery from modern multiomic datasets.]]></summary></entry><entry><title type="html">High-dimensional proteomics identifies organ injury patterns associated with outcomes in human trauma</title><link href="http://localhost:4000/research/2023/06/01/proteomics.html" rel="alternate" type="text/html" title="High-dimensional proteomics identifies organ injury patterns associated with outcomes in human trauma" /><published>2023-06-01T00:00:00-04:00</published><updated>2023-06-01T00:00:00-04:00</updated><id>http://localhost:4000/research/2023/06/01/proteomics</id><content type="html" xml:base="http://localhost:4000/research/2023/06/01/proteomics.html"><![CDATA[<p><em>Severe traumatic injury with shock can lead to direct and indirect organ injury; however, tissue-specific biomarkers are limited in clinical panels. We used proteomic and metabolomic databases to identify organ injury patterns after severe injury in humans.</em></p>

<p>Li SR, Moheimani H, Herzig B, Kail M, Krishnamoorthi N, Wu J, Abdelhamid S, Scioscia J, Sung E, Rosengart A, Bonaroti J, Johansson PI, Stensballe J, Neal MD, Das J, Kar U, Sperry J, Billiar TR. High-dimensional proteomics identifies organ injury patterns associated with outcomes in human trauma. <em>Journal of Trauma and Acute Care Surgery</em> (2023). <a href="https://doi.org/10.1097/TA.0000000000003880">doi:10.1097/TA.0000000000003880</a>.</p>

<hr />

<p>That’s just the introduction! Read the full journal article at the <a href="https://doi.org/10.1097/TA.0000000000003880">Journal of Trauma and Acute Care Surgery</a>.</p>]]></content><author><name></name></author><category term="research" /><summary type="html"><![CDATA[Severe traumatic injury with shock can lead to direct and indirect organ injury; however, tissue-specific biomarkers are limited in clinical panels. We used proteomic and metabolomic databases to identify organ injury patterns after severe injury in humans.]]></summary></entry><entry><title type="html">Multi-Omic Admission-Based Prognostic Biomarkers Identified by Machine Learning Algorithms Predict Patient Recovery and 30-Day Survival in Trauma Patients</title><link href="http://localhost:4000/research/2022/08/23/multiomics.html" rel="alternate" type="text/html" title="Multi-Omic Admission-Based Prognostic Biomarkers Identified by Machine Learning Algorithms Predict Patient Recovery and 30-Day Survival in Trauma Patients" /><published>2022-08-23T00:00:00-04:00</published><updated>2022-08-23T00:00:00-04:00</updated><id>http://localhost:4000/research/2022/08/23/multiomics</id><content type="html" xml:base="http://localhost:4000/research/2022/08/23/multiomics.html"><![CDATA[<p><em>Admission-based circulating biomarkers for the prediction of outcomes in trauma patients could be useful for clinical decision support. It is unknown which molecular classes of biomolecules can contribute biomarkers to predictive modeling. Here, we analyzed a large multi-omic database of over 8500 markers (proteomics, metabolomics, and lipidomics) to identify prognostic biomarkers in the circulating compartment for adverse outcomes, including mortality and slow recovery, in severely injured trauma patients. Admission plasma samples from patients (n = 129) enrolled in the Prehospital Air Medical Plasma (PAMPer) trial were analyzed using mass spectrometry (metabolomics and lipidomics) and aptamer-based (proteomics) assays. Biomarkers were selected via Least Absolute Shrinkage and Selection Operator (LASSO) regression modeling and machine learning analysis. A combination of five proteins from the proteomic layer was best at discriminating resolvers from non-resolvers from critical illness with an Area Under the Receiver Operating Characteristic curve (AUC) of 0.74, while 26 multi-omic features predicted 30-day survival with an AUC of 0.77. Patients with traumatic brain injury as part of their injury complex had a unique subset of features that predicted 30-day survival. Our findings indicate that multi-omic analyses can identify novel admission-based prognostic biomarkers for outcomes in trauma patients. Unique biomarker discovery also has the potential to provide biologic insights.</em></p>

<p>Abdelhamid SS, Scioscia J, Vodovotz Y, Wu J, <strong>Rosengart A</strong>, Sung E, Rahman S, Voinchet R, Bonaroti J, Li S, Darby JL, Kar U, Neal MD, Sperry J, Das J, Billiar TR. Multi-Omic Admission-Based Prognostic Biomarkers Identified by Machine Learning Algorithms Predict Patient Recovery and 30-Day Survival in Trauma Patients. <em>Metabolites</em> (2022). (doi:10.3390/metabo12090774)[https://doi.org/10.3390/metabo12090774].</p>

<hr />

<p>That’s just the abstract! Read the full journal article at the <a href="https://doi.org/10.3390/metabo12090774">MDPI</a>.</p>]]></content><author><name></name></author><category term="research" /><summary type="html"><![CDATA[Admission-based circulating biomarkers for the prediction of outcomes in trauma patients could be useful for clinical decision support. It is unknown which molecular classes of biomolecules can contribute biomarkers to predictive modeling. Here, we analyzed a large multi-omic database of over 8500 markers (proteomics, metabolomics, and lipidomics) to identify prognostic biomarkers in the circulating compartment for adverse outcomes, including mortality and slow recovery, in severely injured trauma patients. Admission plasma samples from patients (n = 129) enrolled in the Prehospital Air Medical Plasma (PAMPer) trial were analyzed using mass spectrometry (metabolomics and lipidomics) and aptamer-based (proteomics) assays. Biomarkers were selected via Least Absolute Shrinkage and Selection Operator (LASSO) regression modeling and machine learning analysis. A combination of five proteins from the proteomic layer was best at discriminating resolvers from non-resolvers from critical illness with an Area Under the Receiver Operating Characteristic curve (AUC) of 0.74, while 26 multi-omic features predicted 30-day survival with an AUC of 0.77. Patients with traumatic brain injury as part of their injury complex had a unique subset of features that predicted 30-day survival. Our findings indicate that multi-omic analyses can identify novel admission-based prognostic biomarkers for outcomes in trauma patients. Unique biomarker discovery also has the potential to provide biologic insights.]]></summary></entry></feed>