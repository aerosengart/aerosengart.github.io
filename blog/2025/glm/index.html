<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Generalized Linear Models | Anna Rosengart </title> <meta name="author" content="Anna Rosengart"> <meta name="description" content="A Primer"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://aerosengart.github.io/blog/2025/glm/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Generalized Linear Models",
            "description": "A Primer",
            "published": "June 03, 2025",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Anna Rosengart </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">curriculum vitae </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/music/">music </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Generalized Linear Models</h1> <p>A Primer</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#set-up">Set-Up</a> </div> <div> <a href="#linear-models">Linear Models</a> </div> <ul> <li> <a href="#least-squares">Least Squares</a> </li> <li> <a href="#likelihood">Likelihood</a> </li> <li> <a href="#uncertainty-quantification">Uncertainty Quantification</a> </li> </ul> <div> <a href="#generalized-linear-models">Generalized Linear Models</a> </div> <ul> <li> <a href="#model">Model</a> </li> <li> <a href="#estimation">Estimation</a> </li> <li> <a href="#uncertainty-quantification">Uncertainty Quantification</a> </li> </ul> </nav> </d-contents> <p>This post is a primer on generalized linear models and their associated estimation procedures.</p> <h2 id="set-up">Set-Up</h2> <p>Let’s assume we have covariate matrix, $\mathbf{X}$, response vector $\mathbf{y}$, and parameter (coefficient) vector, $\beta$, and error vector, $\epsilon$, given by:</p> \[\mathbf{X} = \begin{bmatrix} 1 &amp; x_{1,1} &amp; \dots &amp; x_{1, m} \\ 1 &amp; x_{2, 1} &amp; \dots &amp; x_{2, m} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 1 &amp; x_{n, 1} &amp; \dots &amp; x_{n, m} \end{bmatrix}, \hspace{8mm} \mathbf{y} = \begin{bmatrix} y_1 \\ \vdots \\ y_n \end{bmatrix}, \hspace{8mm} \beta = \begin{bmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_m \end{bmatrix}, \hspace{8mm} \epsilon = \begin{bmatrix} \epsilon_1 \\ \vdots \\ \epsilon_n \end{bmatrix}\] <p>Assume that the following holds:</p> \[\begin{equation} \label{eq:assumption-1} g(\mathbf{y}) = \mathbf{X} \beta + \epsilon; \hspace{5mm} \epsilon_i \overset{iid}{\sim} F \end{equation}\] <p>where $g(\cdot)$ is some function (called a <i>link</i> function) and $F$ is some noise distribution. Equivalently, we can assume:</p> \[\begin{equation} \label{eq:assumption-2} \mathbb{E}\left[ g(\mathbf{y}) \rvert \mathbf{X} \right] = \mathbf{X}\beta + \mathbb{E}[\epsilon] \end{equation}\] <p>For the rest of the post, we’ll use $\mathbf{x}_i$ to denote:</p> \[(1, x_{i, 1}, \dots, x_{i, m})^\top\] <hr> <h2 id="linear-models">Linear Models</h2> <p>Let’s first cover the basics of linear regression, which will be a good base upon which to build the theory of generalized linear models. For basic linear regression, we assume that $g(\cdot)$ is thhe identity function (i.e. $g(\mathbf{y}) = \mathbf{y}$) and that, conditional on $\mathbf{X}$, $F = \mathcal{N}(0, \sigma^2)$.</p> <h3 id="least-squares">Least Squares</h3> <p>We usually estimate $\beta$ via <i>ordinary least squares (OLS)</i>, which provides a closed form solution. The least squares objective is to minimize the <i>residual sum of squares</i>:</p> \[RSS(\beta) = \sum_{i = 1}^n (y_i - \mathbf{x}_i^\top \beta )^2 = (\mathbf{y} - \mathbf{X} \beta)^\top (\mathbf{y} - \mathbf{X} \beta)\] <p>The above function is quadratic in $\beta$, so we can differentiate with respect to $\beta$, set that equal to $0$, and solve for $\beta$ to find a minimizer:</p> \[\begin{aligned} &amp;\mathbf{X}^\top(\mathbf{y} - \mathbf{X} \beta) = 0 \\ \implies &amp;\mathbf{X}^\top \mathbf{y} - \mathbf{X}^\top \mathbf{X} \beta = 0 \\ \implies &amp;\hat{\beta}_{OLS} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y} \end{aligned}\] <p>This solution is unique if $\mathbf{X}^\top \mathbf{X}$ is non-singular.</p> <h4 id="generalized-least-squares">Generalized Least Squares</h4> <p><a href="https://en.wikipedia.org/w/index.php?title=Generalized_least_squares&amp;oldid=1292245629" rel="external nofollow noopener" target="_blank">Generalized least squares (GLS)</a> is a method to estimate the parameters in a linear regression model when there is correlation between the errors. Suppose we are in the linear regression setting as before except we assume that we have a <i>known</i> and non-singular conditional covariance matrix, $\Omega$, for the errors. That is:</p> \[\text{Cov}(\epsilon \rvert \mathbf{X}) = \Omega\] <p>GLS estimates $\beta$ through the following optimization problem:</p> \[\begin{aligned} \hat{\beta}_{GLS} &amp;= \underset{\beta}{\arg \min} \left\{ (\mathbf{y} - \mathbf{X} \beta)^\top \Omega^{-1} (\mathbf{y} - \mathbf{X}\beta) \right\} \\ &amp;= \underset{\beta}{\arg \min} \left\{ -2 \beta^\top \mathbf{X}^\top \Omega^{-1} \mathbf{y} + \beta^\top \mathbf{X}^\top \Omega^{-1} \mathbf{X} \beta \right\} \end{aligned}\] <p>The above expression is quadratic in $\beta$, so we take the gradient, set it equal to zero, and solve for $\beta$:</p> \[\begin{aligned} 0 &amp;= -2\mathbf{X}^\top \Omega^{-1} \mathbf{y} + 2\mathbf{X}^\top \Omega^{-1} \mathbf{X} \beta \\ \implies \hat{\beta}_{GLS} &amp;= (\mathbf{X}^\top \Omega{-1} \mathbf{X})^{-1} \mathbf{X}^\top \Omega{-1} \mathbf{y} \end{aligned}\] <p>Just like in OLS, the GLS estimator of $\beta$ is unbiased, and its conditional covariance matrix also has a similiar form:</p> \[\mathbb{E}[\hat{\beta}_{GLS} \rvert \mathbf{X}] = \beta, \hspace{8mm} \text{Cov}(\hat{\beta}_{GLS} \rvert \mathbf{X}) = (\mathbf{X}^\top \Omega^{-1} \mathbf{X})^{-1}\] <h4 id="weighted-least-squares">Weighted Least Squares</h4> <p><a href="https://en.wikipedia.org/w/index.php?title=Weighted_least_squares&amp;oldid=1279139018" rel="external nofollow noopener" target="_blank">Weighted least squares (WLS)</a> is equivalent to generalized least squares when $\Omega$ is a diagonal matrix. However, we’ll discuss it in more detail here for completeness.</p> <p>The idea behind WLS is that we may have more or less confidence in the reliability of our observations, so we want to weight their contributions to the objective function during estimation. Let $\mathbf{W}$ be an $n \times n$ diagonal matrix of weights. The WLS goal is to find:</p> \[\hat{\beta}_{WLS} = \underset{\beta}{\arg \min} \left\{ (\mathbf{y} - \mathbf{X} \beta)^\top \mathbf{W} (\mathbf{y} - \mathbf{X} \beta) \right\}\] <p>Since this is quadratic in $\beta$, we take the derivative with respect to $\beta$. Solving the following for $\beta$ will give us our estimate:</p> \[\begin{equation} \label{eq:wls-obj} \frac{\partial}{\partial \beta} \left[ (\mathbf{y} - \mathbf{X} \beta)^\top \mathbf{W} (\mathbf{y} - \mathbf{X} \beta) \right] = -2 \mathbf{X}^\top \mathbf{W} (\mathbf{y} - \mathbf{X}\beta) = 0 \end{equation}\] <p>As we showed in the <a href="#generalized-least-squares">generalized least squares section</a>, the solution to this problem is to set:</p> \[\hat{\beta}_{WLS} = (\mathbf{X}^\top \mathbf{W} \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{W} \mathbf{y}\] <p>However, this solution isn’t finished because we don’t know what $\mathbf{W}$ is! By the same argument in the OLS case, $\hat{\beta}_{WLS}$ is unbiased for $\beta$ for any $\mathbf{W}$, so how can we determine what the best weight matrix is?</p> <p>One way is to minimize the (weighted) mean squared error. For any vector of weights $\lambda = (\lambda_0, \lambda_1, \dots, \lambda_m)^\top$, we want to find the $\mathbf{W}$ such that $\hat{\beta}_{WLS}$ minimizes the expression:</p> \[\mathbb{E}\left[ (\lambda^\top(\hat{\beta}_{WLS} - \beta))^2 \right] = \text{Var}(\lambda^\top \hat{\beta}_{WLS})\] <p>where the equality follows from the unbiasedness of the WLS estimator.</p> <h3 id="likelihood">Likelihood</h3> <p>Since we have assumed a particular distribution for the errors, we can also estimate the coefficient vector with maximum likelihood (see <a href="/posts/2025/02/03/likelihood-theory.html">my likelihood post</a> for details). Using Eq. \eqref{eq:assumption-2}, our assumption is:</p> \[\mathbb{E}\left[ \mathbf{y} \rvert \mathbf{X} \right] = \mathbf{X}\beta\] <p>Since our observations are i.i.d., the (full) log-likelihood (technically conditional on $\mathbf{X}$…) is:</p> \[\begin{aligned} \ell(\mathbf{y}; \mathbf{X}, \beta) &amp;= \log\left( \prod_{i = 1}^n \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(- \frac{(y_i - \mathbf{x}_i \beta)^2}{2 \sigma^2} \right)\right) \\ &amp;= \frac{n}{\sqrt{2\pi \sigma^2}} - \frac{1}{2 \sigma^2}\sum_{i = 1}^n (y_i - \mathbf{x}_i \beta)^2 \end{aligned}\] <p>Taking the derivative with respect to $\beta$, setting this equal to zero, and solving for $\beta$ gives us our maximum likelihood estimate:</p> \[\begin{aligned} \frac{\partial \ell(\mathbf{y}; \mathbf{X}, \beta)}{\partial \beta} &amp;= - \frac{2}{2 \sigma^2}\sum_{i = 1}^n \mathbf{x}_i^\top (y_i - \mathbf{x}_i \beta) \\ &amp;= 0 \\ \implies \mathbf{X}^\top (\mathbf{y} - \mathbf{X} \beta) &amp;= 0 \\ \implies \hat{\beta}_{MLE} &amp;= (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y} \end{aligned}\] <p>We’ve just shown that the OLS and ML estimates of the parameters are equivalent!</p> <h3 id="uncertainty-quantification">Uncertainty Quantification</h3> <p>Often a simple estimate of $\beta$ is not enough; we want to be able to say how confident we are in our estimation. This is usually done through confidence intervals, which require estimates of the mean and variance of our estimates.</p> <p>Let $\hat{\beta}$ denote the OLS (and ML since they are equivalent) estimator for $\beta$. The mean vector of $\hat{\beta}$ is given by:</p> \[\begin{aligned} \mathbb{E}\left[ \hat{\beta} \rvert \mathbf{X} \right] &amp;= (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y} \\ &amp;= (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbb{E}\left[ \mathbf{y} \rvert \mathbf{X} \right] \\ &amp;= (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{X} \beta \\ &amp;= \beta \end{aligned}\] <div class="theorem"> <strong>Claim (Estimator Covariance).</strong> <ul id="covar-claim" class="tab" data-tab="71c85216-ba69-45a1-a54c-5534c6d860c0" data-name="covar-claim"> <li class="active" id="covar-claim-statement"> <a href="#">statement </a> </li> <li id="covar-claim-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="71c85216-ba69-45a1-a54c-5534c6d860c0" data-name="covar-claim"> <li class="active"> <p>We can similarly derive the covariance matrix of $\hat{\beta}$ as: \(\text{Cov}(\hat{\beta} \rvert \mathbf{X}) = \sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1}\)</p> </li> <li> <p>Let $\mathbf{X}$ be fixed, so we can drop the conditioning. Using the fact that $\mathbb{E}[\hat{\beta}] = \beta$, we see that:</p> \[\begin{aligned} \text{Cov}(\hat{\beta}) &amp;= \mathbb{E}\left[ (\hat{\beta} - \beta) (\hat{\beta} - \beta)^\top \right] \\ &amp;= \mathbb{E}\left[(\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y} \mathbf{y}^\top \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1} \right] - 2 \mathbb{E}\left[ \hat{\beta} \right]\beta^\top + \beta \beta^\top\\ &amp;= (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbb{E}\left[ \mathbf{y} \mathbf{y}^\top \right] \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1} - \beta \beta^\top \\ &amp;= (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbb{E}\left[ (\mathbf{X} \beta + \epsilon)(\mathbf{X} \beta + \epsilon)^\top \right] \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1} - \beta \beta^\top\\ &amp;= (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{X} \beta \beta^\top \mathbf{X}^\top \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1} - 2 (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{X} \beta \mathbb{E}\left[ \epsilon \right] \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1} + (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbb{E}\left[ \epsilon \epsilon^\top \right]\mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1} - \beta \beta^\top \\ &amp;= \beta \beta^\top + (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top (\sigma^2 \mathbb{I}_{n \times n}) \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1} - \beta \beta^\top \\ &amp;= \sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1} \end{aligned}\] </li> </ul> </div> <p>Putting the above results together, we see that the estimator has a Gaussian distribution:</p> \[\hat{\beta} \sim \mathcal{N}(\beta, \sigma^2 (\mathbf{X}^\top \mathbf{X}){-1})\] <p>We can use this information for additional inference about the parameters (e.g. hypothesis testing).</p> <hr> <h2 id="generalized-linear-models">Generalized Linear Models</h2> <p>Generalized linear models are not that different from classical linear regression models. In this case, we assume that $\mathbf{y}$ follow a distribution from an overdispersed exponential family (e.g. Poisson, Binomial, exponential, Gaussian, etc.).<d-cite key="mccullagh1989"></d-cite></p> <div id="exponential-family"></div> <div class="definition"> <strong>Definition (Overdispersed Exponential Family).</strong> <br> Let $\theta \in \mathbb{R}^{m \times 1}$ be a parameter vector. We say that the distribution of random vector $\mathbf{y} \in \mathbb{R}^{n \times 1}$ comes from an <i>overdispersed exponential family</i> if the probability density/mass function can be written as: $$ \begin{aligned} \label{eq:exp-fam} f_Y(\mathbf{y} \rvert \theta, \tau) = \prod_{i = 1}^n h(\mathbf{y}_i, \tau_i) \exp\left(\frac{\mathbf{b}(\theta_i) \mathbf{T}(\mathbf{y}_i) - A(\theta_i)}{d(\tau_i)} \right) \end{aligned} $$ <ul> <li>$\tau$ is the <i>dispersion parameter</i>, usually related to the variance somehow.</li> <li>$d(\tau_i)$ is some function of $\tau_i$ used to introduce the overdispersion, often taken to be $\frac{\tau}{\omega_i}$.</li> <li>$h(\mathbf{y}, \tau)$ is a non-negative function. </li> <li>The vector $\mathbf{b}(\theta)$ is a function of the parameter vector. If $\mathbf{b}(\theta) = \theta$, then we say the distribution is in <i>canonical form</i>. Any distribution can be written in canonical form by letting $\theta' = \theta$ and then using $\mathbf{b}(\theta') = \theta$.</li> <li>The vector $\mathbf{T}(\mathbf{y})$ is a sufficient statistic, meaning $f_Y(\mathbf{f} \rvert \mathbf{T}(\mathbf{y}))$ is independent of $\theta$.</li> <li>The scalar $A(\theta)$ is the <i>log-partition function</i> and is the natural logarithm of the normalization constant needed so that the above function integrates to one.</li> </ul> All $h(\mathbf{y}, \tau)$, $\mathbf{b}(\theta)$, $\mathbf{T}(\mathbf{y})$, $A(\theta)$, and $d(\tau)$ are assumed to be known. However, these functions are non-unique! For example, you can easily scale $\mathbf{T}(\theta)$ one by a constant and scale $\mathbf{b}(\theta)$ by the reciprocal. </div> <h3 id="model">Model</h3> <p>We’ll assume the response, $\mathbf{y}$, comes from an exponential family distribution so that the likelihood has the form in Eq. \eqref{eq:exp-fam}. We’ll also take $\mathbf{T}(\mathbf{y}) = \mathbf{y}$, $\mathbf{b}(\theta) = \theta$, and we’ll rewrite $h(\mathbf{y}, \tau)$ as $\exp(\log(h(\mathbf{y}, \tau)))$ so we can bring it inside the exponential function. Taking the natural logarithm gives us the log-likelihood:</p> \[\begin{equation} \label{eq:log-lik-glm} \begin{aligned} \ell(\mathbf{y}; \theta, \tau) &amp;= \log(f_Y(\mathbf{y} \rvert \theta, \tau)) \\ &amp;= \sum_{i = 1}^n \left[ \frac{\theta_i \mathbf{y}_i - A(\theta_i)}{d(\tau_i)} + \log(h(\mathbf{y}_i, \tau_i))\right] \end{aligned} \end{equation}\] <p>Taking the first- and second-order derivatives of the log-likelihood with respect to the parameter $\theta$ gives us the gradient and Hessian:</p> \[\begin{equation} \label{eq:grad-hessian} \begin{aligned} \frac{\partial \ell(\mathbf{y}; \theta, \tau)}{\partial \theta} &amp;= \begin{bmatrix} \frac{\mathbf{y}_1 - \frac{d A(\theta_1)}{d \theta_1}}{d(\tau_1)} \\ \vdots \\ \frac{\mathbf{y}_n - \frac{d A(\theta_n)}{d \theta_1}}{d(\tau_n)} \\ \end{bmatrix} \\ &amp;= \text{diag}\left( \frac{1}{d(\tau)} \right) \left[ \mathbf{y} - \frac{\partial A(\theta)}{\partial \theta}\right] \\ \frac{\partial^2 \ell(\mathbf{y}; \theta, \tau)}{\partial \theta \partial \theta^\top} &amp;= \text{diag}\left( \frac{1}{d(\tau)} \right) \begin{bmatrix} - \frac{d^2(A(\theta_1))}{d \theta_1 d \theta_1} &amp; \dots &amp; - \frac{d^2(A(\theta_1))}{d \theta_1 d \theta_n} \\ \vdots &amp; \ddots &amp; \vdots \\ - \frac{d^2(A(\theta_n))}{d \theta_n d \theta_1} &amp; \dots &amp; - \frac{d^2(A(\theta_n))}{d \theta_n d \theta_n} \end{bmatrix} \\ &amp;= -\text{diag}\left( \frac{1}{d(\tau)} \right)\frac{\partial^2 A(\theta)}{\partial \theta \partial \theta^\top} \end{aligned} \end{equation}\] <p>where we let $\tau = (\tau_1, \dots, \tau_n)^\top$.</p> <div class="theorem"> <strong>Claim (Mean and Variance).</strong> <ul id="mean-var-glm" class="tab" data-tab="285006e7-27ef-4606-81bd-7a4f12a55a94" data-name="mean-var-glm"> <li class="active" id="mean-var-glm-statement"> <a href="#">statement </a> </li> <li id="mean-var-glm-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="285006e7-27ef-4606-81bd-7a4f12a55a94" data-name="mean-var-glm"> <li class="active"> <p>Using properties of the likelihood function, we can derive the mean (denoted by $\mu$) and variance of $\mathbf{y}$ as:</p> \[\begin{equation} \label{eq:mean-var-glm} \begin{aligned} \mu &amp;= \mathbb{E}[\mathbf{y}] = \frac{\partial A(\theta)}{\partial \theta} \\ \text{Cov}(\mathbf{y}) &amp;= \text{diag}\left( d(\tau) \right) \underbrace{\frac{\partial^2 A(\theta)}{\partial \theta \partial \theta^\top}}_{= V(\mu)} \end{aligned} \end{equation}\] </li> <li> <p>Under certain regularity conditions, which we’ll assume hold here, the expectation of the score function (the gradient of the log-likelihood) should equal zero (see <a href="/blog/2025/likelihood-theory">my likelihood post</a> for a discussion on this). Thus:</p> \[\begin{aligned} &amp;\mathbb{E}\left[ \frac{\partial \ell(\mathbf{y}; \theta, \tau)}{\partial \theta} \right] = \mathbf{0} \\ \implies &amp;\text{diag}\left( \frac{1}{d(\tau)} \right) \left(\mathbb{E}[\mathbf{y}] - \frac{\partial A(\theta)}{\partial \theta} \right) = \mathbf{0} \\ \implies &amp;\mathbb{E}[\mathbf{y}] = \frac{\partial A(\theta)}{\partial \theta} \end{aligned}\] <p>Under these conditions, we also have that the Fisher information (the variance of the score) is the negative expectation of the Hessian. Thus:</p> \[\begin{aligned} &amp;\mathbb{E}\left[ \frac{\partial \ell(\mathbf{y}; \theta, \tau)}{\partial \theta} \frac{\partial \ell(\mathbf{y}; \theta, \tau)}{\partial \theta^\top} \right] = -\mathbb{E}\left[\frac{\partial^2 \ell(\mathbf{y}; \theta, \tau)}{\partial \theta \partial \theta^\top} \right] \\ \implies &amp;\mathbb{E}\left[ \text{diag}\left( \frac{1}{d^2(\tau)} \right) \left(\mathbf{y} - \frac{\partial A(\theta)}{\partial \theta}\right)\left(\mathbf{y} - \frac{\partial A(\theta)}{\partial \theta}\right)^\top\right] = - \mathbb{E}\left[ -\text{diag}\left( \frac{1}{d(\tau)} \right)\frac{\partial^2 A(\theta)}{\partial \theta \partial \theta^\top} \right] \\ \implies &amp; \text{diag}\left( \frac{1}{d^2(\tau)} \right)\mathbb{E}\left[ \left(\mathbf{y} -\mathbb{E}[\mathbf{y}]\right)\left(\mathbf{y} - \mathbb{E}[\mathbf{y}]\right)^\top\right] = \text{diag}\left( \frac{1}{d(\tau)} \right)\frac{\partial^2 A(\theta)}{\partial \theta \partial \theta^\top} \\ \implies &amp;\text{Cov}(\mathbf{y}) = \text{diag}\left(d(\tau)\right) \frac{\partial^2 A(\theta)}{\partial \theta \partial \theta^\top} \end{aligned}\] </li> </ul> </div> <p>Eq. \eqref{eq:mean-var-glm} shows that the covariance matrix of $\mathbf{y}$ is the product of a function of the dispersion parameter and a function of the parameter vector $\theta$. In the literature, $\frac{\partial^2 A(\theta)}{\partial \theta \partial \theta^\top}$ is often referred to as the <i>variance function</i> and denoted by $V(\mu)$ (where $\mu$ is the mean $\mathbb{E}[\mathbf{y}]$) since a function of the parameters is just a function of the mean as $\mathbf{X}$ is fixed.</p> <p>Arguably more importantly, the righthand side of $\mu = \frac{\partial A(\theta)}{\partial \theta}$ is some function of $\theta$, which we’ll denote by $s^{-1}(\theta)$. This lets us link the mean with the canonical parameter through the expression:</p> \[\begin{equation} \label{eq:link-mean-param} \theta = s(\mu) \end{equation}\] <p>Recall that we assumed that $\mathbb{E}\left[ g(\mathbf{y}) \rvert \mathbf{X} \right] = \mathbf{X} \beta$, which is equivalent to the assumption that $\mu = g^{-1}(\mathbf{X}\beta)$. Thus, we can relate the likelihood parameter, $\theta$, to the regression parameters, $\beta$, by:</p> \[\begin{equation} \label{eq:link-param-beta} \theta = s(g^{-1}(\mathbf{X}\beta)) \end{equation}\] <p>Now we have everything we need for our generalized linear model!</p> <h3 id="estimation">Estimation</h3> <p>The parameter estimates for generalized linear models are generally found through maximum likelihood methods. Since we assume a distribution from an exponential family, we can (usually) write a likelihood function as in Eq. \eqref{eq:log-lik-glm}:</p> \[\ell(\mathbf{y}; \theta, \tau) = \sum_{i = 1}^n \left[ \frac{\theta_i^\top \mathbf{y}_i - A(\theta_i)}{d(\tau_i)} + \log(h(\mathbf{y}_i, \tau_i)) \right]\] <p>As we usually do, we want to take the gradient with respect to the parameters of interest, $\beta$. However, this expression is not in terms of $\beta$, so we use the chain rule. The maximum likelihood estimate of $\beta$ is a solution to:</p> \[\begin{equation} \label{eq:score-beta} \begin{aligned} \frac{\partial \ell(\mathbf{y}; \theta, \tau)}{\partial \beta} &amp;= \frac{\partial \theta}{\partial \beta} \frac{\partial \ell(\mathbf{y}; \theta, \tau)}{\partial \theta} = \mathbf{0} \\ \implies \frac{\partial \ell(\mathbf{y}; \theta, \tau)}{\partial \beta} &amp;= \text{diag}\left( d(\tau) \right)\frac{\partial \theta}{\partial \beta} \left(\mathbf{y} - \frac{\partial A(\theta)}{\partial \theta}\right) = \mathbf{0} &amp; \left(\text{Eq. }\eqref{eq:grad-hessian} \right) \\ \implies \frac{\partial \ell(\mathbf{y}; \theta, \tau)}{\partial \beta} &amp;= \text{diag}\left( d(\tau) \right)\frac{\partial \theta}{\partial \beta} \left(\mathbf{y} - g^{-1}(\mathbf{X} \beta) \right) = \mathbf{0} &amp; \left(\text{Eq. }\eqref{eq:mean-var-glm} \right) \end{aligned} \end{equation}\] <div class="theorem"> <strong>Claim (Derivatives).</strong> <ul id="d-theta-d-beta" class="tab" data-tab="fc2a12eb-60eb-4d66-af30-a054f6eefc75" data-name="d-theta-d-beta"> <li class="active" id="d-theta-d-beta-statement"> <a href="#">statement </a> </li> <li id="d-theta-d-beta-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="fc2a12eb-60eb-4d66-af30-a054f6eefc75" data-name="d-theta-d-beta"> <li class="active"> <p>To find $\frac{\partial \theta}{\partial \beta}$, we use the chain rule again:</p> \[\begin{equation} \label{eq:d-theta-d-beta} \frac{\partial \theta}{\partial \beta} = \frac{\partial \eta}{\partial \beta} \frac{\partial \theta}{\partial \eta} = \text{diag}\left( \frac{1}{d(\tau)} \right)\mathbf{X}^\top \text{Cov}^{-1}(\mathbf{y})\left[ \frac{\partial g(\mu)}{\partial \mu} \right]^{-1} \end{equation}\] </li> <li> <p>The first derivative is easy:</p> \[\frac{\partial \eta}{\partial \beta} = \frac{\partial}{\partial \beta} [\mathbf{X} \beta] = \mathbf{X}^\top\] <p>The second is a bit trickier. By the chain rule:</p> \[\frac{\partial \theta}{\partial \eta} = \frac{\partial \theta}{\partial \mu} \frac{\partial \mu}{\partial \eta}\] <p>Using the fact that $\theta = s(\mu)$ and $\mu = \frac{\partial A(\theta)}{\partial \theta}$, we get:</p> \[\begin{aligned} &amp;\theta = s(\mu) \\ \implies &amp;\frac{\partial \theta}{\partial \theta} = \frac{\partial s(\mu)}{\partial \theta} = \frac{\partial s(\mu)}{\partial \mu} \frac{\partial \mu}{\partial \theta} \\ \implies &amp;\mathbb{I}_{n \times n} = \frac{\partial s(\mu)}{\partial \mu} \frac{\partial}{\partial \theta} \left[ \frac{\partial A(\theta)}{\partial \theta} \right] = \frac{\partial s(\mu)}{\partial \mu} \frac{\partial^2 A(\theta)}{\partial \theta \partial \theta^\top} \end{aligned}\] <p>Using Eq. \eqref{eq:mean-var-glm}, we get:</p> \[\begin{aligned} &amp;\mathbb{I}_{n \times n} = \frac{\partial s(\mu)}{\partial \mu} \\ \implies &amp;\text{diag}\left( \frac{1}{d(\tau)} \right) \text{Cov}(\mathbf{y}) \\ \implies &amp;\text{diag}\left( d(\tau) \right) \text{Cov}^{-1}(\mathbf{y}) = \frac{\partial s(\mu)}{\partial \mu} = \frac{\partial \theta}{\partial \mu} \end{aligned}\] <p>We can do the same sort of trick for $\frac{\partial \mu}{\partial \eta}$:</p> \[\begin{aligned} &amp;\mu = g^{-1}(\eta) \\ \implies &amp;\eta = g(\mu) \\ \implies &amp;\frac{\partial \eta}{\partial \eta} = \frac{\partial g(\mu)}{\partial \eta} = \frac{\partial g(\mu)}{\partial \mu} \frac{\partial \mu}{\partial \eta} \\ \implies &amp;\mathbb{I}_{n \times n} = \frac{\partial g(\mu)}{\partial \mu} \frac{\partial \mu}{\partial \eta} \\ \implies &amp;\frac{\partial \mu}{\partial \eta} = \left[ \frac{\partial g(\mu)}{\partial \mu} \right]^{-1} \end{aligned}\] <p>Putting the two results together, we get: \(\frac{\partial \theta}{\partial \eta} = \text{diag}\left( d(\tau) \right) \text{Cov}^{-1}(\mathbf{y}) \left[ \frac{\partial g(\mu)}{\mu} \right]^{-1}\)</p> </li> </ul> </div> <p>Using the fact that each component of $g(\mu)$ uses only the corresponding component in $\mu$, we see that $\left[ \frac{\partial g(\mu)}{\partial \mu} \right]^{-1}$ is diagonal, so the inverse is the matrix where we take the reciprocal of the diagonal elements. The same goes for $\text{Cov}^{-1}(\mathbf{y})$ since we assumed independent data (i.i.d. errors).</p> <p>Plugging Eq. \eqref{eq:d-theta-d-beta} into Eq. \eqref{eq:score-beta}, our objective becomes solving:</p> \[\begin{equation} \label{eq:new-obj} \frac{\partial \ell(\mathbf{y}; \theta, \tau)}{\partial \beta} = \mathbf{X}^\top \text{Cov}^{-1}(\mathbf{y})\left[ \frac{\partial g(\mu)}{\partial \mu} \right]^{-1} \left(\mathbf{y} - g^{-1}(\mathbf{X} \beta) \right) = \mathbf{0} \end{equation}\] <p>Unfortunately, there may not be a closed form solution to the above equation since $g^{-1}(\cdot)$ may be non-linear. To get around this problem, we do a linear approximation of $g^{-1}(\mathbf{X}\beta)$ and optimize this instead.</p> <p>Let $\hat{\beta}^{(t)}$ be a guess for the optimal value of $\beta$. Also let \(\eta^{(t)} = \mathbf{X}\beta^{(t)}\) be the linear predictor evaluated at this value. We can then write the first order Taylor approximation of \(g^{-1}(\eta)\) as:</p> \[\begin{equation} \label{eq:taylor-mu} g^{-1}(\eta) \approx g^{-1}(\eta^{(t)}) + \frac{\partial g^{-1}(\eta)}{\partial \eta} \bigg\rvert_{\eta = \eta^{(t)}} (\eta - \eta^{(t)}) = g^{-1}(\eta^{(t)}) + \mathbf{W}^{(t)}(\eta - \eta^{(t)}) \end{equation}\] <p>where $\mathbf{W}^{(t)} = \frac{\partial g^{-1}(\eta)}{\partial \eta} \bigg\rvert_{\eta = \eta^{(t)}}$ is the matrix of first order partial derivatives of $\mu$ with respect to $\eta$ evaluated at our guess for $\eta$, $\eta^{(t)}$. This matrix is diagonal since any component, $\mu_i$, of the mean vector is only a function of one corresponding component of the linear predictor $\eta_i$.</p> <p>We now use this approximation into Eq. \eqref{eq:new-obj}:</p> \[\begin{aligned} \frac{\partial \ell(\mathbf{y}; \theta, \tau)}{\partial \beta} &amp;\approx \mathbf{X}^\top \text{Cov}^{-1}(\mathbf{y})\left[ \frac{\partial g(\mu)}{\partial \mu} \right]^{-1}\bigg\rvert_{\eta = \eta^{(t)}} \left(\mathbf{y} - g^{-1}(\eta^{(t)}) + \mathbf{W}^{(t)} \left( \eta^{(t)} - \eta \right) \right) \\ &amp;= \mathbf{X}^\top \text{Cov}^{-1}(\mathbf{y})\left[ \frac{\partial g(\mu)}{\partial \mu} \right]^{-1}\bigg\rvert_{\eta = \eta^{(t)}} \mathbf{W}^{(t)} \left([\mathbf{W}^{(t)}]^{-1} (\mathbf{y} - g^{-1}(\eta^{(t)})) + \eta^{(t)} - \eta \right) \\ &amp;= \mathbf{X}^\top \text{Cov}^{-1}(\mathbf{y})\left[ \frac{\partial g(\mu)}{\partial \mu} \right]^{-2} \bigg\rvert_{\eta = \eta^{(t)}} \left(\frac{\partial g(\mu)}{\partial \mu}\bigg\rvert_{\eta = \eta^{(t)}} (\mathbf{y} - g^{-1}(\eta^{(t)})) + \eta^{(t)} - \eta \right) &amp; \left(\frac{\partial \mu}{\partial \eta} \bigg\rvert_{\eta = \eta^{(t)}} = \frac{\partial g^{-1}(\eta)}{\partial \eta} \bigg\rvert_{\eta = \eta^{(t)}} = \mathbf{W}^{(t)} = \left[\frac{\partial g(\mu)}{\partial \mu} \right]^{-1} \bigg\rvert_{\eta = \eta^{(t)}} \right) \end{aligned}\] <p>Letting $\mathbf{z}^{(t)} = \frac{\partial g(\mu)}{\partial \mu} \bigg\rvert_{\eta = \eta^{(t)}} (\mathbf{y} - g^{-1}(\eta^{(t)})) + \eta^{(t)}$, and defining $\tilde{\mathbf{W}}^{(t)} = \text{Cov}^{-1}(\mathbf{y})\left[ \frac{\partial g(\mu)}{\partial \mu} \right]^{-2}\bigg\rvert_{\eta = \eta^{(t)}}$, we can rewrite this as:</p> \[\frac{\partial \ell(\mathbf{y}; \theta, \tau)}{\partial \beta} \approx \mathbf{X}^\top \tilde{\mathbf{W}}^{(t)} \left(\mathbf{z}^{(t)} - \eta \right)\] <p>Notice that setting the above equal to zero is exactly Eq. \eqref{eq:wls-obj} with $\mathbf{z}^{(t)}$ instead of $\mathbf{y}$, so we can use weighted least squares. Though we have a closed form solution, since we are using an approximation, we need to iterate the above process until convergence. So at iteration $t + 1$, we will set:</p> \[\begin{aligned} \eta^{(t+1)} &amp;= \mathbf{X} \beta^{(t)} \\ \mu^{(t+1)} &amp;= g^{-1} \eta^{(t+1)} \\ \mathbf{W}^{(t+1)} &amp;= \text{Cov}^{-1}(\mathbf{y}) \left[ \frac{\partial g(\mu)}{\partial \mu} \right]^{-2} \bigg\rvert_{\eta = \eta^{(t+1)}} \\ \mathbf{z}^{(t+1)} &amp;= \eta^{(t+1)} + \frac{ \partial g(\mu)}{\partial \mu}\bigg\rvert_{\eta = \eta^{(t+1)}}(\mathbf{y} - g^{-1}(\eta^{(t+1)})) \\ \beta^{(t+1)} &amp;= (\mathbf{X}^\top \tilde{\mathbf{W}}^{(t+1)} \mathbf{X})^{-1} \mathbf{X}^\top \tilde{\mathbf{W}}^{(t+1)} \mathbf{z}^{(t+1)} \end{aligned}\] <p>This process is called <a href="https://en.wikipedia.org/w/index.php?title=Iteratively_reweighted_least_squares&amp;oldid=1279139010" rel="external nofollow noopener" target="_blank">iteratively reweighted least squares</a>.</p> <h3 id="uncertainty-quantification-1">Uncertainty Quantification</h3> <p>Due to the iterative nature of the estimation procedure, it is not so clear how to quantify our uncertainty in our estimates. Since there is no closed form solution to the MLEs, <a href="https://statisticaloddsandends.wordpress.com/2020/11/20/variance-of-coefficients-for-linear-models-and-generalized-linear-models/" rel="external nofollow noopener" target="_blank">there is also no closed form for the variance of the MLEs</a>.</p> <p>One can, instead, derive the asymptotic variance of the estimator using <a href="/posts/2025/02/03/likelihood-theory.html">likelihood theory</a>. As a maximum likelihood estimator, it should be asymptotically normal with mean $\beta$ and covariance matrix $-\left[ \mathbb{E}\left[ \frac{\partial^2 \ell(\mathbf{y}; \theta, \tau)}{\partial \beta \beta^\top} \right] \right]^{-1}$ <i>if the model is correct</i>.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2025-06-03-glm.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/score-info/">Score and Information</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/quasi-likelihood/">Quasi-Likelihood</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/score-test-one-sided/">One-Sided Score Test For Variance Components</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Anna Rosengart. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: January 06, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?d4c3ed73337d78e34b10d24890d1fc56"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/tabs.min.js?b8748955e1076bbe0dabcf28f2549fdc"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>