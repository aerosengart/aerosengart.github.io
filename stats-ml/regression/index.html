<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Linear Regression | Anna Rosengart </title> <meta name="author" content="Anna Rosengart"> <meta name="description" content=""> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://aerosengart.github.io/stats-ml/regression/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Linear Regression",
            "description": "",
            "published": "January 19, 2026",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Anna Rosengart </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">curriculum vitae </a> </li> <li class="nav-item "> <a class="nav-link" href="/music/">music </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/stats-ml/">stats &amp; ml</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/measure-theory/">measure theory</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/paper-notes/">paper notes</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Linear Regression</h1> <p></p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#background">Background</a> </div> <div> <a href="#estimation">Estimation</a> </div> <div> <a href="#prediction">Prediction</a> </div> <div> <a href="#inference">Inference</a> </div> <div> <a href="#best-linear-unbiased-estimator">Best Linear Unbiased Estimator</a> </div> </nav> </d-contents> <p>Linear regression is a method for predicting some outcome using information from other quantitative variables. It’s one of the most important models in statistics/machine learning due to its ubiquity. Most of this post follows Chapter 3 in <i>The Elements of Statistical Learning</i>.<d-cite key="hastie2017"></d-cite></p> <hr> <h2 id="background">Background</h2> <p>Let $X = (X_1, X_2, \dots, X_p)^\top$ be an input vector of $p$ <i>predictors</i>, also called <i>regressors</i> or <i>features</i>, and let $Y$ be a real-valued outcome variable. The goal of linear regression is to predict $Y$ using the information in $X$ by assuming that the mean of $Y$ is a function of the predictors with the form:</p> \[\begin{equation} \label{eq:lin-reg} f(X) = \beta_0 + \sum_{j = 1}^p X_j \beta_j \end{equation}\] <p>The $\beta = (\beta_0, \beta_1, \dots, \beta_p)^\top$ are the unknown model parameters (called <i>coefficients</i>) which we must estimate by minimizing a chosen loss function over a sample (i.e. a <i>training set</i>). We’ll denote a sample of $n$ observations of the predictors and outcome with ordered pairs $(\mathbf{x}_1, y_1), \dots, (\mathbf{x}_n, y_n)$ where $\mathbf{x}_i = (x_1, \dots, x_p)^\top$.</p> <p>We’ll also use $\mathbf{X} = (\mathbf{1}_n, \mathbf{x}_1, \dots, \mathbf{x}_n)^\top$ to denote the $n \times (p + 1)$ of regressors (plus a prepended $n$-vector of ones) and $\mathbf{y} = (y_1, \dots, y_n)^\top$ to denote the $n$-vector of the sample observations.</p> <hr> <h2 id="estimation">Estimation</h2> <p>Usually, when someone refers to linear regression, they have estimated their parameters using <i>least squares</i>, which uses the <i>residual sum of squares (RSS)</i> as its loss function:</p> \[\begin{equation} \label{eq:rss} \begin{aligned} RSS(\beta) &amp;= \sum_{i = 1}^n \left(y_i - f(\mathbf{x}_i)\right)^2 \\ &amp;= \sum_{i = 1}^n \left(y_i - \beta_0 - \sum_{j = 1}^p \mathbf{x}_{i,j} \beta_j \right)^2 \\ &amp;= (\mathbf{y} - \mathbf{X} \beta)^\top (\mathbf{y} - \mathbf{X} \beta) \end{aligned} \end{equation}\] <p>Least squares makes no assumptions about the data distributions; it simply tries to minimize the average squared difference between the predicted and true values where the predicted values <i>must be</i> linear in the regressors. Since Eq. \eqref{eq:rss} is quadratic in $\beta$, we can (under certain conditions) minimize it by taking the gradient, setting that equal to zero, and solving for $\beta$:</p> \[\begin{equation} \begin{aligned} \frac{\partial}{\partial \beta} \left[ RSS(\beta) \right] &amp;= - 2 (\mathbf{y} - \mathbf{X} \beta)^\top \mathbf{X} = -2 \mathbf{X}^\top (\mathbf{y} - \mathbf{X} \beta) \\ \frac{\partial^2}{\partial \beta \partial \beta^\top} \left[ RSS(\beta) \right] &amp;= 2 \mathbf{X}^\top \mathbf{X} \end{aligned} \end{equation}\] <p>Assuming that $\mathbf{X}^\top \mathbf{X}$ is invertible (i.e. the Hessian is positive-definite):</p> \[\begin{aligned} &amp; &amp;\frac{\partial}{\partial \beta} \left[ RSS(\beta) \right] &amp;= \mathbf{0} \\ &amp;\implies &amp;-2 \mathbf{X}^\top (\mathbf{y} - \mathbf{X} \beta) &amp;= \mathbf{0} \\ &amp;\implies &amp;\mathbf{X}^\top (\mathbf{y} - \mathbf{X} \beta) &amp;= \mathbf{0} \\ &amp;\implies &amp;\mathbf{X}^\top \mathbf{y} &amp;= \mathbf{X}^\top \mathbf{X} \beta \\ &amp;\implies &amp;(\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y} &amp;= \beta \end{aligned}\] <hr> <h2 id="prediction">Prediction</h2> <p>The predicted values are then given by:</p> \[\hat{\mathbf{y}} = \mathbf{X} \hat{\beta} = \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}\] <p>The matrix $\mathbf{H} = \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top$ is often called the <i>hat matrix</i> because it puts a “hat” on $\mathbf{y}$ to create the predicted value.</p> <p>If we also notice that the least squares objective yields a predicted value that is the <i>orthogonal projection</i> of the outcome vector onto the <i>column space</i> of the predictors (i.e. the span of the columns of $\mathbf{X}$), we can see why $\mathbf{H}$ is also called the <i>projection matrix</i>.</p> <hr> <h2 id="inference">Inference</h2> <p>It’s important to keep in mind that the least squares parameter estimates, $\hat{\beta}$, are functions of the sample, and we can therefore try to characterize its sampling distribution.</p> <p>We will make the following assumptions:</p> <ul> <li>$\mathbf{x}_1, \dots, \mathbf{x}_n$ are fixed</li> <li> <strong>Independence</strong>: $y_1, \dots, y_n$ are uncorrelated</li> <li> <strong>Homoscedasticity</strong>: $y_1, \dots, y_n$ have constant variance, $\sigma^2$</li> <li> <strong>Linearity</strong>: $\mathbb{E}[Y \rvert X] = X\beta$</li> <li> <strong>Gaussianity</strong>: $Y = \mathbb{E}[Y \rvert X] + \epsilon$ with $\epsilon \sim \mathcal{N}(0, \sigma^2)$</li> </ul> <p>We can then derive the mean of $\hat{\beta}$:</p> <div class="theorem"> <strong>Claim (Mean of Least Squares Estimate).</strong> <br> <ul id="mean-ls" class="tab" data-tab="004322c2-f8e0-4e49-a564-67cac813d5b1" data-name="mean-ls"> <li class="active" id="mean-ls-statement"> <a href="#">statement </a> </li> <li id="mean-ls-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="004322c2-f8e0-4e49-a564-67cac813d5b1" data-name="mean-ls"> <li class="active"> \[\mathbb{E}[\hat{\beta}] = \beta\] </li> <li> <p>Here, let $\epsilon = (\epsilon_1, \dots, \epsilon_n)^\top$ with $\epsilon_i \overset{iid}{\sim} \mathcal{N}(0, \sigma^2)$. The expectations below are taken conditional on $\mathbf{X}$ (i.e. with $\mathbf{X}$ fixed):</p> \[\begin{aligned} \mathbb{E}\left[ \hat{\beta} \right] &amp;= \mathbb{E} \left[ (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y} \right] \\ &amp;= (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbb{E}\left[ \mathbf{y} \right] &amp; \left(\mathbf{X} \text{ fixed}\right) \\ &amp;= (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbb{E}\left[ \mathbf{X} \beta + \epsilon \right] \\ &amp;= (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \left( \mathbf{X} \beta + \mathbb{E}\left[ \epsilon \right] \right) &amp;= (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{X}\beta &amp; \left(\mathbb{E}[\epsilon] = 0 \right) \\ &amp;= \beta \end{aligned}\] </li> </ul> </div> <p>We can also derive its variance-covariance matrix:</p> <div class="theorem"> <strong>Claim (Covariance of Least Squares Estimate).</strong> <br> <ul id="var-ls" class="tab" data-tab="6a47d741-35b4-4330-817f-07182e6ebf8d" data-name="var-ls"> <li class="active" id="var-ls-statement"> <a href="#">statement </a> </li> <li id="var-ls-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="6a47d741-35b4-4330-817f-07182e6ebf8d" data-name="var-ls"> <li class="active"> \[\text{Var}(\hat{\beta}) = \sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1}\] </li> <li> <p>Here, let $\epsilon = (\epsilon_1, \dots, \epsilon_n)^\top$ with $\epsilon_i \overset{iid}{\sim} \mathcal{N}(0, \sigma^2)$. The expectations below are taken conditional on $\mathbf{X}$ (i.e. with $\mathbf{X}$ fixed):</p> \[\begin{aligned} \text{Var} \left( \hat{\beta} \right) &amp;= \mathbb{E} \left[ \left(\hat{\beta} - \mathbb{E}[\hat{\beta}]\right) \left( \hat{\beta} - \mathbb{E}[\hat{\beta}]\right)^\top \right] \\ &amp;= \mathbb{E} \left[ \left(\hat{\beta} - \beta \right)\left(\hat{\beta} - \beta\right)^\top \right] \\ &amp;= \mathbb{E}\left[ \hat{\beta} \hat{\beta}^\top - 2 \beta\hat{\beta}^\top - \beta \beta^\top \right] \\ &amp;= \mathbb{E}\left[ \left( (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \mathbf{y} \right) \left( (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \mathbf{y} \right)^\top \right] -2 \beta \mathbb{E}\left[\hat{\beta}^\top\right] - \beta\beta^\top &amp; \left(\text{linearity of expectation}\right) \\ &amp;= \mathbb{E}\left[ (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y} \mathbf{y}^\top \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1} \right] - 2 \beta \beta^\top + \beta \beta^\top &amp; \left(\text{previous proof}\right) \\ &amp;= \mathbb{E}\left[ (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top (\mathbf{X} \beta + \epsilon) (\mathbf{X} \beta + \epsilon)^\top \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \right] - \beta\beta^\top \\ &amp;= \mathbb{E}\left[ (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \left(\mathbf{X} \beta \beta^\top \mathbf{X}^\top -2 \epsilon \beta^\top \mathbf{X}^\top + \epsilon \epsilon^\top \right) \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \right] - \beta \beta^\top \\ &amp;= (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \left(\mathbf{X} \beta \beta^\top \mathbf{X}^\top - 2 \mathbb{E}\left[ \epsilon \right] \beta^\top \mathbf{X}^\top + \mathbb{E}\left[ \epsilon \epsilon^\top \right] \right) \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top - \beta \beta^\top \\ &amp;= (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \left(\mathbf{X} \beta \beta^\top \mathbf{X}^\top + \sigma^2 \mathbb{I}_{n \times n} \right) \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top - \beta^\top \beta \\ &amp;= (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{X} \beta \beta^\top \mathbf{X}^\top \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top + \sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbb{I}_{n \times n} \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top - \beta\beta^\top \\ &amp;= \beta \beta^\top + \sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1} - \beta \beta^\top \\ &amp;= \sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1} \end{aligned}\] </li> </ul> </div> <p>Under the above assumptions and given the above derivations, we conclude that:</p> \[\hat{\beta} \sim \mathcal{N}\left(\beta, \sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1}\right)\] <p>We can also form an estimate of $\sigma^2$ as:</p> \[\hat{\sigma}^2 = \frac{1}{n - p - 1} \sum_{i = 1}^n (y_i - \hat{y}_i)^2\] <p>which is unbiased (shown below).</p> <div class="theorem"> <strong>Claim (Mean of Least Squares Estimate).</strong> <br> <ul id="var-est" class="tab" data-tab="03975669-eac8-44dd-acd5-0eb2ab90c2d0" data-name="var-est"> <li class="active" id="var-est-statement"> <a href="#">statement </a> </li> <li id="var-est-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="03975669-eac8-44dd-acd5-0eb2ab90c2d0" data-name="var-est"> <li class="active"> \[\mathbb{E}[\hat{\sigma}^2] = \sigma^2\] </li> <li> <p>Here, let $\epsilon = (\epsilon_1, \dots, \epsilon_n)^\top$ with $\epsilon_i \overset{iid}{\sim} \mathcal{N}(0, \sigma^2)$. Let $\tilde{\mathbf{x}}_i = (1, \mathbf{x}_i^\top)^\top$. The expectations below are taken conditional on $\mathbf{X}$ (i.e. with $\mathbf{X}$ fixed):</p> \[\begin{aligned} \mathbb{E}\left[ \hat{\sigma}^2 \right] &amp;= \mathbb{E}\left[ \frac{1}{n - p - 1} \sum_{i = 1}^n (y_i - \hat{y}_i)^2 \right] \\ &amp;= \frac{1}{n - p - 1} \mathbb{E}\left[(\mathbf{y} - \hat{\mathbf{y}})^\top(\mathbf{y} - \hat{\mathbf{y}}) \right] \\ &amp;= \frac{1}{n - p - 1} \mathbb{E}\left[\mathbf{y}^\top \mathbf{y} - 2\mathbf{y}^\top \hat{\mathbf{y}} + \hat{\mathbf{y}}^\top \hat{\mathbf{y}} \right] \\ &amp;= \frac{1}{n - p - 1} \left( \mathbb{E}\left[\mathbf{y}^\top \mathbf{y} \right] - 2 \mathbb{E}\left[ \mathbf{y}^\top \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \mathbf{y} \right] + \mathbb{E}\left[ (\mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \mathbf{y})^\top \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \mathbf{y} \right] \right) \\ &amp;= \frac{1}{n - p - 1} \left( \mathbb{E}\left[\mathbf{y}^\top \mathbf{y} \right] - 2 \mathbb{E}\left[ \mathbf{y}^\top \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \mathbf{y} \right] + \mathbb{E}\left[ \mathbf{y}^\top \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \mathbf{y} \right] \right) \\ &amp;= \frac{1}{n - p - 1} \left( \mathbb{E}\left[\mathbf{y}^\top \mathbf{y} \right] - 2 \mathbb{E}\left[ \mathbf{y}^\top \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \mathbf{y} \right] + \mathbb{E}\left[ \mathbf{y}^\top \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \mathbf{y} \right] \right) \\ &amp;= \frac{1}{n - p - 1} \left( \mathbb{E}\left[\mathbf{y}^\top \mathbf{y} \right] - \mathbb{E}\left[ \mathbf{y}^\top \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \mathbf{y} \right] \right) \\ &amp;= \frac{1}{n - p - 1} \left( \mathbb{E}\left[(\mathbf{X} \beta + \epsilon)^\top (\mathbf{X}\beta + \epsilon) \right] -\mathbb{E}\left[ (\mathbf{X} \beta + \epsilon)^\top \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top (\mathbf{X} \beta + \epsilon) \right] \right) \\ &amp;= \frac{1}{n - p - 1} \mathbb{E}\left[ (\mathbf{X} \beta + \epsilon)^\top\left[ \mathbb{I}_{n \times n} - \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top\right] (\mathbf{X} \beta + \epsilon) \right] \\ &amp;= \frac{1}{n - p - 1} \left( \mathbb{E}\left[ (\mathbf{X} \beta)^\top \left[ \mathbb{I}_{n \times n} - \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top\right] \mathbf{X} \beta \right] + 2 \mathbb{E}\left[ \epsilon^\top \left[ \mathbb{I}_{n \times n} - \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top\right] \mathbf{X} \beta \right] + \mathbb{E}\left[ \epsilon^\top \left[ \mathbb{I}_{n \times n} - \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top\right] \epsilon \right] \right) \\ &amp;= \frac{1}{n - p - 1} \left( \beta^\top \mathbf{X}^\top \left[ \mathbb{I}_{n \times n} - \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top\right] \mathbf{X} \beta + \mathbb{E}\left[ \text{tr}\left[ \epsilon^\top \left[ \mathbb{I}_{n \times n} - \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top\right] \epsilon \right] \right] \right) \\ &amp;= \frac{1}{n - p - 1} \left( \beta^\top \mathbf{X}^\top \left[ \mathbb{I}_{n \times n} - \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top\right] \mathbf{X} \beta + \mathbb{E}\left[ \text{tr}\left[ \left[ \mathbb{I}_{n \times n} - \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top\right] \epsilon \epsilon^\top \right] \right] \right) \\ &amp;= \frac{1}{n - p - 1} \left( \beta^\top \mathbf{X}^\top \mathbf{X} \beta - \beta^\top \mathbf{X}^\top \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \mathbf{X} \beta + \text{tr}\left[ \mathbb{I}_{n \times n} - \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \mathbb{E}\left[ \epsilon \epsilon^\top \right] \right] \right) \\ &amp;= \frac{1}{n - p - 1} \left( \beta^\top \mathbf{X}^\top \mathbf{X} \beta - \beta^\top \mathbf{X}^\top \mathbf{X} \beta + \text{tr}\left[ \mathbb{I}_{n \times n} - \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top (\sigma^2 \mathbb{I}_{n \times n}) \right] \right) &amp;= \frac{1}{n - p - 1} \text{tr}\left[ \mathbb{I}_{n \times n} - \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top (\sigma^2 \mathbb{I}_{n \times n}) \right] &amp;= \frac{1}{n - p - 1} \text{tr}\left[ \sigma^2 \mathbb{I}_{n \times n} - \sigma^2 \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top\right] \\ &amp;= \frac{1}{n - p - 1} \left( \sigma^2 \text{tr}\left[ \mathbb{I}_{n \times n} \right] - \sigma^2 \text{tr}\left[ \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top\right] \right)\\ &amp;= \frac{1}{n - p - 1} \left( n \sigma^2 - \sigma^2 \text{tr}\left[ (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top\mathbf{X} \right] \right)\\ &amp;= \frac{1}{n - p - 1} \left( n \sigma^2 - \sigma^2 \text{tr}\left[\mathbb{I}_{p + 1}\right] \right)\\ &amp;= \frac{1}{n - p - 1} \left( n \sigma^2 - (p + 1)\sigma^2 \right) \\ &amp;= \sigma^2 \end{aligned}\] </li> </ul> </div> <hr> <h2 id="best-linear-unbiased-estimator">Best Linear Unbiased Estimator</h2> <p>Recall that an unbiased estimator will have mean equal to the true parameter value, and a linear estimator will be a linear combination of the sample responses, $\mathbf{y}$.</p> <p>The least squares estimator is a linear, unbiased estimator. As we showed in the previous section, $\mathbb{E}[\hat{\beta}] = \beta$ (unbiased), and it is a linear estimator because, if we let $\mathbf{A} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top$, we have:</p> \[\begin{aligned} \hat{\beta} &amp;= (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y} = \mathbf{A}\mathbf{y} \end{aligned}\] <p>The least squares estimate is considered the <i>best linear unbiased estimator (BLUE)</i> because it has the smallest variance of all linear unbiased estimates. This result is summarized in the <i>Gauss-Markov Theorem</i>.</p> <div class="theorem"> <strong>Gauss-Markov Theorem.</strong><d-cite key="gm2025"></d-cite> <br> <ul id="gm-theorem" class="tab" data-tab="a9d2cd39-5aa2-453a-be9f-800370083d3b" data-name="gm-theorem"> <li class="active" id="gm-theorem-statement"> <a href="#">statement </a> </li> <li id="gm-theorem-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="a9d2cd39-5aa2-453a-be9f-800370083d3b" data-name="gm-theorem"> <li class="active"> <p>For $\mathbf{y}, \epsilon \in \mathbb{R}^n$, $\mathbf{X} \in \mathbb{R}^{n \times (p + 1)}$, and $\beta \in \mathbb{R}^{p + 1}$, assume $\mathbf{y} = \mathbf{X} \beta + \epsilon$ where all $\epsilon_i$ are independent with mean $0$ and variance $\sigma^2$ (but are not necessarily Gaussian). <br> Let \(\hat{\beta}_{OLS}\) be the ordinary least squares estimator, and let \(\tilde{\beta} = \mathbf{A} \mathbf{y}\) be some other unbiased linear estimator. The Gauss-Markov Theorem states that the OLS estimator minimizes the mean squared error criterion. That is, for any set of coefficients \(\lambda_1, \dots, \lambda_{p+1}\):</p> \[\underset{\hat{\beta}}{\arg \min} \left[ \mathbb{E}\left[ \left( \sum_{j = 1}^{p + 1} \lambda_j (\hat{\beta}_j - \beta_j) \right)^2 \right] \right] = \hat{\beta}_{OLS}\] <p>which is equivalent to:</p> \[\text{Var}(\hat{\beta}) - \text{Var}(\hat{\beta}_{OLS})\] <p>being positive semi-definite for all other linear unbiased estimators, $\hat{\beta}$.</p> <details> <summary>Proof.</summary> $$ \begin{aligned} \mathbb{E}\left[ \left( \sum_{j = 1}^{p + 1} \lambda_j (\hat{\beta}_j - \beta_j) \right)^2\right] &amp;= \mathbb{E}\left[ \left((\hat{\beta} - \beta)^\top \lambda \right)^2 \right] \\ &amp;= \mathbb{E}\left[ \lambda^\top\left( \hat{\beta} - \mathbb{E}\left[ \hat{\beta} \right]\right)\left( \hat{\beta} - \mathbb{E}\left[ \hat{\beta} \right]\right)^\top \lambda \right] \\ &amp;= \lambda^\top \mathbb{E}\left[ \left( \hat{\beta} - \mathbb{E}\left[ \hat{\beta} \right]\right)\left( \hat{\beta} - \mathbb{E}\left[ \hat{\beta} \right]\right)^\top \right] \lambda \\ &amp;= \lambda^\top \text{Var}(\hat{\beta}) \lambda \\ &amp;= \text{Var}(\lambda^\top \hat{\beta}) \end{aligned} $$ </details> </li> <li> <p>Note that $\tilde{\beta}$ can be rewritten as:</p> \[\mathbf{A} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top + \mathbf{D}\] <p>for some $(p + 1) \times n$ matrix, $\mathbf{D}$. Deriving the mean of $\tilde{\beta}$, which we know to be $\mathbf{0}_{p + 1}$:</p> \[\begin{aligned} \mathbb{E}\left[ \tilde{\beta} \right] &amp;= \mathbb{E}\left[ \mathbf{A} \mathbf{y} \right] \\ &amp;= \mathbb{E}\left[ \left((\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top + \mathbf{D} \right) (\mathbf{X} \beta = \epsilon) \right] \\ &amp;= \mathbb{E}\left[ (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{X} \beta + (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top\epsilon + \mathbf{D} \mathbf{X} \beta + \mathbf{D} \epsilon \right] \\ &amp;= \mathbb{I}_{(p + 1) \times (p + 1)} \beta + (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbb{E}\left[ \epsilon \right] + \mathbf{D} \mathbf{X} \beta + \mathbf{D} \mathbb{E}\left[ \epsilon \right] \\ &amp;= \beta + \mathbf{D} \mathbf{X} \beta \\ &amp;= (\mathbb{I}_{(p + 1) \times (p + 1)} + \mathbf{D}\mathbf{X}) \beta \end{aligned}\] <p>Since we assumed that $\tilde{\beta}$ is unbiased:</p> \[\begin{aligned} &amp;\mathbb{E}\left[ \tilde{\beta} \right] = \mathbf{0}_{p + 1} \\ \implies &amp;(\mathbb{I}_{(p + 1) \times (p + 1)} + \mathbf{D}\mathbf{X}) = \beta \\ \implies &amp;\mathbf{D}\mathbf{X} = \mathbb{0}_{(p + 1) \times (p + 1)} \end{aligned}\] <p>Now, consider the variance of $\tilde{\beta}$:</p> \[\begin{aligned} \text{Var}(\tilde{\beta}) &amp;= \text{Var}(\mathbf{A}\mathbf{y}) \\ &amp;= \mathbf{A} \text{Var}(\mathbf{y}) \mathbf{A}^\top \\ &amp;= \mathbf{A} \left[\sigma^2 \mathbb{I}_{n \times n} \right] \mathbf{A}^\top \\ &amp;= \sigma^2 \mathbf{A}\mathbf{A}^\top \\ &amp;= \sigma^2 \left((\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top + \mathbf{D}\right)\left((\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top + \mathbf{D}\right)^\top \\ &amp;= \sigma^2 \left[ \left((\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top + \mathbf{D}\right) \mathbf{X}(\mathbf{X}^\top \mathbf{X})^{-1} + \left((\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top + \mathbf{D}\right)\mathbf{D}^{-1} \right] \\ &amp;= \sigma^2 \left[(\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{X}(\mathbf{X}^\top \mathbf{X})^{-1} + \mathbf{D}\mathbf{X}(\mathbf{X}^\top \mathbf{X})^{-1} + (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \mathbf{D}^\top + \mathbf{D}\mathbf{D}^\top \right] \\ &amp;= \sigma^2 \left[(\mathbf{X}^\top \mathbf{X})^{-1} + \mathbf{0}_{(p + 1) \times (p + 1)}(\mathbf{X}^\top \mathbf{X})^{-1} + (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{0}_{(p + 1) \times (p + 1)} + \mathbf{D}\mathbf{D}^\top \right] \\ &amp;= \sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1} + \sigma^2 \mathbf{D}\mathbf{D}^\top \\ &amp;= \text{Var}(\hat{\beta}_{OLS}) + \sigma^2 \mathbf{D} \mathbf{D}^\top \end{aligned}\] <p>Because $\sigma^2 &gt; 0$ and $\mathbf{D} \mathbf{D}^\top$ is positive semi-definite, we have the desired result.</p> </li> </ul> </div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/stats-ml.bib"></d-bibliography> <d-article> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Anna Rosengart. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: February 16, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?d4c3ed73337d78e34b10d24890d1fc56"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/tabs.min.js?b8748955e1076bbe0dabcf28f2549fdc"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>