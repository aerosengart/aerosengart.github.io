<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Quadrature | Anna Rosengart </title> <meta name="author" content="Anna Rosengart"> <meta name="description" content="A Primer"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://aerosengart.github.io/stats-ml/quadrature/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Quadrature",
            "description": "A Primer",
            "published": "February 05, 2026",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Anna Rosengart </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">curriculum vitae </a> </li> <li class="nav-item "> <a class="nav-link" href="/music/">music </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/stats-ml/">stats &amp; ml</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/measure-theory/">measure theory</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/paper-notes/">paper notes</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Quadrature</h1> <p>A Primer</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#step-function-quadrature">Step Function Quadrature</a> </div> <ul> <li> <a href="#left-and-right-rules">Left and Right Rules</a> </li> <li> <a href="#midpoint-rule">Midpoint Rule</a> </li> <li> <a href="#trapezoidal-rule">Trapezoidal Rule</a> </li> </ul> <div> <a href="#interpolating-function-quadrature">Interpolating Function Quadrature</a> </div> <ul> <li> <a href="#newton-cotes-rules">Newton-Cotes Rules</a> </li> <li> <a href="#simpson-s-rules">Simpson's Rules</a> </li> </ul> <div> <a href="#gaussian-quadrature">Gaussian Quadrature</a> </div> <ul> <li> <a href="#gauss-legendre-quadrature">Gauss-Legendre Quadrature</a> </li> <li> <a href="#gauss-jacobi-quadrature">Gauss-Jacobi Quadrature</a> </li> <li> <a href="#gauss-laguerre-quadrature">Gauss-Laguerre Quadrature</a> </li> <li> <a href="#gauss-hermite-quadrature">Gauss-Hermite Quadrature</a> </li> </ul> <div> <a href="#adaptive-quadrature">Adaptive Quadrature</a> </div> <div> <a href="#example">Example</a> </div> </nav> </d-contents> <p>Since I’ve been working on integral approximations via marginal quasi-likelihood (and some penalized quasi-likelihood), it’s probably a good idea to review <i>numerical integration</i> techniques to see how good the approximations that the more analytical techniques provide are. I’ll focus on one-dimensional cases for now.</p> <p>In essence, numerical integration<d-cite key="numericalintegration2025"></d-cite> is a way to compute an integral without evaluating it analytically. In one-dimensional cases, it is fairly simple to approximate definite integrals (under some niceness conditions) by evaluating the function at a finite number of points and adding together the results in a weighted sum. The way this sum is defined is called a <strong>quadrature rule</strong>, and using these rules to approximate an integral is called <strong>quadrature</strong>.</p> <aside><p>That is, a Riemann sum. As such, we will assume that all integrands are Riemann integrable.</p></aside> <p>In what follows, we will be considering approximations of the following one-dimensional integral with $f(x)$ continuous on $[a, b]$:</p> \[\int_a^b f(x) dx\] <p>We will devide the domain of integration into $n \in \mathbb{N}$ sub-intervals each of length $\delta_x = \frac{b - a}{n}$. We will let $P = { x_0, x_1, \dots, x_n }$ be the set of endpoints of these sub-intervals.</p> <aside><p>If $n &gt; 2$, then these are called <strong>composite</strong> rules.</p></aside> <hr> <h2 id="step-function-quadrature">Step Function Quadrature</h2> <h3 id="left-and-right-rules">Left and Right Rules</h3> <p>The <strong>left rule</strong> and <strong>right rule</strong> are quadratures rules that uses the function’s output at the left and right, respectively, endpoints of sub-intervals defined on the domain of integration as the weights for the summation.<d-cite key="riemann2025"></d-cite></p> <p>The left rule is given by:</p> \[\begin{equation} \label{eq:left} L_n = \sum_{i = 1}^n \delta_x f(x_{i - 1}) \end{equation}\] <p>Similarly, the right rule is:</p> \[\begin{equation} \label{eq:right} R_n = \sum_{i = 1}^n \delta_x f(x_{i}) \end{equation}\] <p>In words, these rules approximate the integral using the sum of the areas of the rectangles that have widths equal to $\delta_x$ and heights equal to the function’s output at the left or right endpoints of the sub-intervals.</p> <p>The left rule and right rule can both result in overestimation or underestimation in certain cases. If the integrand is monotonically decreasing on $[a, b]$, then the left rule will overestimate while the right rule will underestimation. If it is monotonically increasing, then the left rule will underestimate while the right rule will over estimate.</p> <p>If we let $S_n$ be either $L_n$ or $R_n$, the error of the approximation is bounded by:</p> \[\left\rvert \int_a^b f(x) dx - S_n \right\rvert \leq \frac{(b - a)^2}{2n} \underset{x \in [a, b]}{\max} \left\{ \rvert f'(x) \rvert \right\}\] <h3 id="midpoint-rule">Midpoint Rule</h3> <p>The <strong>midpoint rule</strong><d-cite key="riemann2025"></d-cite> is essentially splits the difference between the left and right rules by using the points halfway between the left and right endpoints of the sub-intervals as the weights.</p> <p>The midpoint rule is given by:</p> \[\begin{equation} \label{eq:midpoint} M_n = \sum_{i = 1}^n f(m_i) \delta_x \end{equation}\] <p>The midpoint rule’s error can be bounded as:</p> \[\left\rvert \int_a^b f(x) dx - M_n \right\rvert \leq \frac{(b - a)^3}{24n^2} \underset{x \in [a, b]}{\max} \left\{ \rvert f''(x) \rvert \right\}\] <aside><p>The midpoint rule can be generalized to obtain a better approximation error, but it is kind of complicated so we omit the details here.</p></aside> <hr> <h2 id="interpolating-function-quadrature">Interpolating Function Quadrature</h2> <h3 id="trapezoidal-rule">Trapezoidal Rule</h3> <p>The previous three rules all defined <i>step functions</i> on the sub-intervals over the domain of integration. We can also extend our approximations to more complex functions.</p> <p>The <strong>trapezoidal rule</strong> is similar to the midpoint rule but uses trapezoids rather than rectangles. This is a fairly simple interpolating function; rather than a step function, we have extended to general affine functions (i.e. linear functions, or polynomials of degree $1$)</p> <p>Recall that the area of a trapezoid with height, $h$, and base lengths $b_1$ and $b_2$ is given by $\frac{1}{2} h(b_1 + b_2)$. The trapezoids we define will have heights equal to the length of the sub-intervals, and the base lengths will be the function’s outputs at the two endpoints of each sub-interval.</p> <p>We have:</p> \[\begin{equation} \label{eq:trapezoid} T_n = \sum_{i = 1}^{n} \frac{1}{2} \delta_x (f(x_{i - 1}) + f(x_{i})) \end{equation}\] <p>By construction, the trapezoidal rule’s approximation can also be obtained as the average of the left and right rules’. The trapezoidal rule will also tend to overestimate the integral on sub-intervals where the function is concave up and underestimate on sub-intervals where the function is concave down. Its error can be bounded as:</p> \[\left\rvert \int_a^b f(x) dx - T_n \right\rvert \leq \frac{(b - a)^3}{12n^2} \underset{x \in [a, b]}{\max} \left\{ \rvert f''(x) \rvert \right\}\] <p>This is exactly double the error bound for the midpoint rule’s approximation.</p> <h3 id="newton-cotes-rules">Newton-Cotes Rules</h3> <p>To get a little more complicated, we can use a <strong>Newton-Cotes rule</strong><d-cite key="newtoncotes2026"></d-cite>, which uses the integral of Langrange basis polynomials as the weights.</p> <aside><p>Some of the other rules are specific types of Newton-Cotes rules!</p></aside> <p>Assuming the endpoints in $P$ are distinct (which should be true), the <strong>Lagrange basis</strong> for polynomials of degree $\leq n$ (for $P$) is the set of polynomials of degree $n$, ${ l_0(x), l_1(x), \dots, l_n(x) }$, where each polynomial is defined as:</p> \[l_i(x) = \prod_{0 \leq m \leq n}{m \neq i} \frac{x - x_m}{x_i - x_m}\] <p>The <strong>Lagrange interpolating polynomial</strong>, also called the <strong>Lagrange basis polynomial</strong> is the unique polynomial that interpolated the points in $P$. It is defined as:</p> \[L(x) = \sum_{i = 0}^n x_i l_i(x)\] <p>A Newton-Cotes rule is then defined as:</p> \[\begin{equation} \label{eq:newton-cotes} C_n = \sum_{i = 0}^n f(x_i) \int_a^b l_i(x) dx \end{equation}\] <p>which arises from the approximation of the function with some Lagrange interpolating polynomial.</p> <h3 id="simpsons-rules">Simpson’s Rules</h3> <p>Simpson’s rules<d-cite key="simpson2026"></d-cite> are a few different types of interpolating quadrature rules. If $n &gt; 2$ is even, then <strong>Simpson’s 1/3 rule</strong> is given by:</p> \[\begin{equation} \label{eq:simpson-1-3} S^{1/3}_n = \frac{1}{3} \delta_x \left[ f(x_) + 4 \sum_{i = 1}^{\frac{n}{2}} f(x_{2i - 1}) + 2 \sum_{i = 1}^{\frac{n}{2} - 1} f(x_{2i}) + f(x_n) \right] \end{equation}\] <p>We can bound the approximation error as:</p> \[\begin{aligned} \left\rvert \int_a^b f(x) dx - S^{1/3}_n \right\rvert \leq \frac{\delta_x^3(b - a)}{180} \underset{x \in [a, b]}{\max} \left\{ \rvert f^(4)(x) \rvert \right\} \end{aligned}\] <p>If $n$ is a multiple of $3$, we can use <strong>Simpson’s 3/8 rule</strong>, which is defined as:</p> \[\begin{equation} \label{eq:simpson-3-8} S^{3/8}_n = \frac{3}{8} \delta_x \left[ f(x_) + 3 \sum_{i = 1; i \nmid 3}^{n - 1} f(x_i) + 2 \sum_{i = 1}^{\frac{n}{3} - 1} f(x_{3i}) + f(x_n) \right] \end{equation}\] <aside><p>$i \nmid 3$ means $i$ is not divisible by $3$.</p></aside> <p>Simpson’s rules are examples of <i>closed</i> Newton-Cotes rules.</p> <aside><p>A <strong>closed</strong> Newton-Cotes rule is such that $x_0 = a$ and $x_n = b$. If $x_0 &gt; a$ and $x_n &lt; b$, then the rule is <strong>open</strong>.</p></aside> <hr> <h2 id="gaussian-quadrature">Gaussian Quadrature</h2> <p>A <strong>Gaussian quadrature rule</strong><d-cite key="gaussquad2025"></d-cite> using interpolating functions as well. In general, a Gaussian quadrature rule is an approximation of the form:</p> \[\int_{a}^b \omega(x) f(x) dx \approx \sum_{i = 1}^n w_i f(x_i)\] <p>where ${ x_1, \dots, x_n }$ are the roots of some specially chosen polynomial of degree $n$ from a family of orthogonal polynomials.</p> <h3 id="gauss-legendre-quadrature">Gauss-Legendre Quadrature</h3> <p>In the simplest setting, we have $[a, b] = [-1, 1]$ and $\omega(x) = 1$. The <a href="https://en.wikipedia.org/wiki/Legendre_polynomials" rel="external nofollow noopener" target="_blank"><strong>Legendre polynomials</strong></a> are the system of polynomials satisfying, for $k = 0, 1, 2, \dots, n$:</p> \[\begin{aligned} P_k(1) &amp;= 1 \\ \int_{-1}^1 P_m(x) P_k(x) dx = 0 \text{ if } k \neq m \end{aligned}\] <p>This implies $P_0(x) = 1$, $P_1(x)$, etc. <strong>Gauss-Legendre quadrature</strong><d-cite key="gausslegendre2025"></d-cite> is then:</p> \[\begin{equation} \label{eq:legendre} G_n = \sum_{i = 1}^n \frac{2}{(1 - x_i)^2 \left[ P'_n(x_i) \right]^2} f(x_i) \end{equation}\] <p>where ${ x_1, \dots, x_n } are the roots of the $n$-th degree Legendre polynomial.</p> <aside><p>For polynomials of degree $2n - 1$, this approximation will be exact.</p></aside> <h3 id="gauss-jacobi-quadrature">Gauss-Jacobi Quadrature</h3> <p>Slightly more complicated is <strong>Gauss-Jacobi quadrature</strong>,<d-cite key="gaussjacobi2025"></d-cite> which supposes $[a, b] = [-1, 1]$ but that $\omega(x) = (1 - x)^\alpha(1 + x)^\beta$ for $\alpha, \beta &gt; -1$. It is useful over Gauss-Legendre quadrature because it can be uses when $f$ is not defined at the endpoints, $a$ and/or $b$.</p> <p>It uses <a href="https://en.wikipedia.org/wiki/Jacobi_polynomials" rel="external nofollow noopener" target="_blank"><strong>Jacobi polynomials</strong></a>, which are polynomials of the form:</p> \[\begin{aligned} P_n^{(\alpha, \beta)}(z) &amp;= \frac{(\alpha+1)^{(n)}}{n!} _2F_1 \left(-n, 1 + \alpha + \beta + n; \alpha + 1; \frac{1 - z}{2}\right) \\ &amp;= \frac{\Gamma(\alpha + n + 1)}{n! \Gamma(\alpha + \beta + n + 1)} \sum_{m = 0}^n {n \choose m} \frac{\Gamma(\alpha + \beta + n + m + 1)}{\Gamma(\alpha + m + 1)} \left(\frac{z - 1}{2} \right)^m \end{aligned}\] <p>where:</p> \[\begin{aligned} (q)^{(n)} &amp;= \prod_{k = 1}^n (q + k - 1) &amp; \left(\text{rising factorial}\right) \\ _2 F_1 (a, b; c; z) &amp;= \sum_{n = 0}^\infty \frac{(a)^{(n)} (b)^{(n)}}{(c)^{(n)}} \frac{z^n}{n!} \text{ for } \rvert z \rvert &lt; 1 &amp; \left(\text{hypergeometric function}\right) \end{aligned}\] <p>and $\Gamma(\cdot)$ denotes the <a href="https://en.wikipedia.org/wiki/Gamma_function" rel="external nofollow noopener" target="_blank">Gamma function</a>.</p> <p>Letting ${ x_1, \dots x_n }$ be the set of roots of the $n$-th Jacobi polynomial, the Gauss-Jacobi quadrature approximation on $n$ points is:</p> \[\begin{equation} \label{eq:jacobi} \begin{aligned} GJ_n &amp;= \sum_{i = 1}^n \lambda_i f(x_i) \\ \lambda_i &amp;= - \left(\frac{2n + \alpha + \beta + 2}{n + \alpha + \beta + 1}\right)\left(\frac{\Gamma(n + \alpha + 1) \Gamma(n + \beta + 1)}{\Gamma(n + \alpha + \beta + 1)(n + 1)!} \right)\left(\frac{2^{\alpha + \beta}}{\left(P_n^{(\alpha,\beta)}(x_i)\right)' P_{n + 1}^{(\alpha, \beta)}(x_i)}\right) \end{aligned} \end{equation}\] <p>Its error can be bounded as:</p> \[\begin{aligned} \left\rvert \int_{-1}^1 f(x) (1 - x)^\alpha (1 + x)^\beta dx - GJ_n \right\rvert \leq \left(\frac{\Gamma(n + \alpha + 1)\Gamma(n + \beta + 1) \Gamma(n + \alpha + \beta + 1)}{(2n + \alpha + \beta + 1)\left[ \Gamma(2n + \alpha + \beta + 1) \right]^2}\right) \left(\frac{2^{2 + \alpha + \beta + 1}}{(2n)!}\right) \underset{x \in (-1, 1)}{\max} \left\{ \rvert f^(2n)(x) \rvert \right\} \end{aligned}\] <h3 id="gauss-laguerre-quadrature">Gauss-Laguerre Quadrature</h3> <p>If we instead have $[a, b) = [0, +\infty)$ and $\omega(x) = \exp(-x)$, we can use <strong>Gauss-Laguerre quadrature</strong><d-cite key="gausslaguerre2025"></d-cite>. It uses the roots of the $n$-th <a href="https://en.wikipedia.org/wiki/Laguerre_polynomials" rel="external nofollow noopener" target="_blank"><strong>Laguerre polynomial</strong></a>. These polynomials are defined to satisfy:</p> \[\begin{aligned} L_0(x) &amp;= 1 \\ L_1(x) &amp;= 1 - x \\ L_{k + 1}(x) = \frac{(2k + 1 - x)L_k(x) - kL_{k - 1}(x)}{k + !} \text{ for } k \geq 1 \end{aligned}\] <p>Thus, the $n$-th Laguerre polynomial is given by:</p> \[L_n(x) = \sum_{k = 0}^n {n \choose k} \frac{(-1)^k}{k!} x^k \text{ for } n = 0, 1, \dots\] <p>The Gauss-Laguerre approximation is then:</p> \[\begin{equation} \label{eq:laguerre} GL_n(x) = \sum_{i = 1}^n \left( \frac{x_i}{(n + 1)^2} \left[ L_{n + 1}(x_i) \right]^2 \right) f(x_i) \end{equation}\] <p>Note that this quadrature rule can be used for general function, $g(x)$, by defining $f(x) = \exp(x) g(x)$, yielding:</p> \[g(x) = g(x) \exp(x) \exp(-x) = \exp(-x) f(x)\] <h3 id="gauss-hermite-quadrature">Gauss-Hermite Quadrature</h3> <p>Finally, suppose we have an indefinite integral with $\omega(x) = \exp\left(-x^2\right)$. We can then choose to use the (physicist’s) <a href="https://en.wikipedia.org/wiki/Hermite_polynomials" rel="external nofollow noopener" target="_blank"><strong>Hermite polynomials</strong></a>, which are defined as:</p> \[H_n(x) = (-1)^n \exp\left( x^2\right) \frac{d^n}{dx^n} \left[ \exp\left(- x^2 \right) \right] \text{ for } n = 0, 1, \dots\] <p>The <strong>Gauss-Hermite quadrature</strong><d-cite key="gausshermite2025"></d-cite> approximation is then:</p> \[GH_n(x) = \sum_{i = 1}^n \frac{2^{n-1} n! \sqrt{\pi}}{n^2 \left[ H_{n-1}(x_i)\right]^2} f(x_i)\] <p>where ${ x_1, \dots, x_n }$ are the roots of the $n$-th Hermite polynomial.</p> <p>This approximation has the following error bound:</p> \[\left\rvert \int_{-\infty}^{+\infty} f(x) \exp(-x^2) dx - GH_n \right\rvert \leq \frac{n! \sqrt{\pi}}{2^n (2n)!}\underset{x \in (-\infty, +\infty)}{\max} \left\{ \rvert f^(2n)(x) \rvert \right\}\] <hr> <h2 id="adaptive-quadrature">Adaptive Quadrature</h2> <p>The basic idea behind <strong>adaptive quadrature</strong><d-cite key="adaptquad2025"></d-cite> is to repeatedly approximate an integral using some other quadrature rule by defining smaller and smaller sub-intervals.</p> <ol> <li>Approximate integral with quadrature.</li> <li>Compute absolute error of approximation.</li> <li>If error is too large, split domain of integration in half.</li> <li>Repeat for both halves of the domain.</li> </ol> <hr> <h2 id="example">Example</h2> <p>To illustrate its use, let’s use Gauss-Hermite quadrature to approximate the derivation of the marginal log-likelihood in a <a href="/stats-ml/glmm">generalized linear mixed model</a> with just a random intercept. We’ll assume our $N$ datapoints come from $K$ clusters, and the $i$-th response in the $k$-th cluster will be denoted by $y_i^k$. For simplicity, we’ll assume the clusters are all of size $n$.</p> <p>We’ll further assume an explicit distribution for our responses (Poisson) and the random effects (Gaussian), so our set-up is:</p> \[\begin{aligned} \mathbb{E}\left[ y_i^k \rvert \beta_k \right] &amp;= \mu_i^k \\ \log(\mu_i^k) &amp;= \alpha + \beta_k \\ \beta_k &amp;\overset{iid}{\sim} \mathcal{N}(0, \tau^2) \end{aligned}\] <p>We’ll let $\mathbf{y} = (y_1^1, \dots, y_n^1, \dots, y_1^K, \dots, y_n^K)^\top$ and $\beta = (\beta_1, \dots, \beta_K)^\top$. To obtain the marginal likelihood for each cluster, we multiply by the likelihood of its random intercept and then integrate that parameter out:</p> \[\mathcal{L}(\mathbf{y}^k; \alpha, \tau^2) = \int \mathcal{L}(\mathbf{y}^k; \alpha, \tau^2 \rvert \beta_k) \mathcal{L}(\beta_k) d \beta_k\] <p>Since we assumed Poisson responses and Gaussian random intercepts, we have:</p> \[\begin{aligned} \mathcal{L}(\mathbf{y}^k; \alpha, \tau^2) &amp;= \int \left[ \mathcal{L}(\mathbf{y}^k; \alpha, \tau^2 \rvert \beta_k) \mathcal{L}(\beta_k) \right] d \beta_k \\ &amp;= \int \left[\mathcal{L}(\mathbf{y}^k; \alpha, \tau^2 \rvert \beta_k) \left( \frac{1}{\sqrt{2 \pi \tau^2}} \exp\left(- \frac{\beta_k^2}{2 \tau^2} \right) \right) \right] d \beta_k \\ &amp;= \frac{1}{\sqrt{2 \pi \tau^2}} \int \left[ \mathcal{L}(\mathbf{y}^k; \alpha, \tau^2 \rvert \beta_k) \exp\left(- \frac{\beta_k^2}{2 \tau^2} \right) \right] d \beta_k \end{aligned}\] <p>Let $x_k^2 = \frac{\beta_k^2}{2 \tau^2}$, which implies $\beta_k = \sqrt{2 \tau^2 x_k^2}$. We can then define:</p> \[\begin{aligned} \mathcal{L}(\mathbf{y}^k; \alpha, \tau^2 \rvert \beta_k) &amp;= \prod_{i = 1}^n \frac{(\mu_i^k)^{y_i^k} \exp(- \mu_i^k)}{y_i^k!} \\ &amp;= \prod_{i = 1}^n \frac{(\exp(\alpha + \beta_k))^{y_i^k} \exp\left( - \exp(\alpha + \beta_k)\right)}{y_i^k!} \\ &amp;= \prod_{i = 1}^n \frac{(\exp(\alpha + \sqrt{2 \tau^2 x_k^2}))^{y_i^k} \exp\left( - \exp(\alpha + \sqrt{2 \tau^2 x_k^2})\right)}{y_i^k!} \\ &amp;= \frac{1}{\prod_{i = 1}^n y_i^k!} \left[ \exp(\alpha + \sqrt{2 \tau^2 x_k^2})\right]^{\sum_{i = 1}^n y_i^k} \left[ \exp\left( - \exp( \alpha + \sqrt{2 \tau^2 x_k^2}) \right) \right]^n \end{aligned}\] <p>which we can call $f(x_k)$. Then, via $u$-substitution, we have:</p> \[\begin{aligned} \mathcal{L}(\mathbf{y}^k; \alpha, \tau^2) &amp;= \frac{1}{\sqrt{2 \pi \tau^2}} \int f(x_k) \exp(- x_k^2) \left(\sqrt{2 \tau^2} \right) dx_k \\ &amp;= \frac{1}{\sqrt{\pi}} \int f(x_k) \exp(- x_k^2) dx_k \end{aligned}\] <p>This is the form we need for Gauss-Hermite quadrature, and the order $m$ approximation is:</p> \[\begin{aligned} \mathcal{L}(\mathbf{y}; \alpha, \tau^2) &amp;= \prod_{k = 1}^K \mathcal{L}(\mathbf{y}^k; \alpha, \tau^2) \\ &amp;= \prod_{k = 1}^K \left(\frac{1}{\sqrt{\pi}} \right) \int f(x_k) \exp(- x_k^2) dx_k \\ &amp;\approx \frac{1}{\pi^{\frac{K}{2}}} \prod_{k = 1}^K \left( \sum_{i = 1}^m \frac{2^{m - 1} m! \sqrt{\pi}}{m^2 \left[ H_{m - 1} (\bar{x}_i) \right]^2} f(\bar{x}_i) \right) \\ \implies \ell(\mathbf{y}; \alpha, \tau^2) &amp;= \sum_{k = 1}^K \left[ -\frac{1}{K} \log(\pi) + \log\left( \int f(x_k) \exp(- x_k^2) dx_k\right) \right] \\ &amp;\approx - \frac{K}{2} \log(\pi) + \sum_{k = 1}^K \log\left( \sum_{i = 1}^m \frac{2^{m - 1} m! \sqrt{\pi}}{m^2 \left[ H_{m - 1} (\bar{x}_i) \right]^2} f(\bar{x}_i) \right) \end{aligned}\] <p>where $m$ is the desired polynomial degree for the approximation and $\bar{x}_i$ are the roots of the $m$-th Hermite polynomial.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/stats-ml.bib"></d-bibliography> <d-article> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Anna Rosengart. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: February 17, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?d4c3ed73337d78e34b10d24890d1fc56"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/tabs.min.js?b8748955e1076bbe0dabcf28f2549fdc"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>