<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Quasi-Likelihood | Anna Rosengart </title> <meta name="author" content="Anna Rosengart"> <meta name="description" content="A Primer"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://aerosengart.github.io/stats-ml/quasi-likelihood/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Quasi-Likelihood",
            "description": "A Primer",
            "published": "May 30, 2025",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Anna Rosengart </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">curriculum vitae </a> </li> <li class="nav-item "> <a class="nav-link" href="/music/">music </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/stats-ml/">stats &amp; ml</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/measure-theory/">measure theory</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/paper-notes/">paper notes</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Quasi-Likelihood</h1> <p>A Primer</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#set-up">Set-Up</a> </div> <div> <a href="#connections-to-likelihood">Connections To Likelihood</a> </div> <div> <a href="#estimation">Estimation</a> </div> </nav> </d-contents> <p>This post is a review of quasi-likelihood theory and mostly relies upon Wedderburn<d-cite key="wedderburn1974"></d-cite> and Breslow &amp; Clayton<d-cite key="breslow1993"></d-cite>. Quasi-likelihood functions provide an alternative and less demanding way of characterizing the distribution of observations compared to specifying a true likelihood function. In essence, we simply assume a particular relationship between the mean and the variance rather than a particular distributional family. Then a so-called <i>quasi-likelihood</i> function can be defined and used for parameter estimation.</p> <hr> <h2 id="set-up">Set-Up</h2> <p>Let $x_1, \dots, x_n$ denote our (independent) observations. Denote the expectation and variance of an arbitrary $x_i$ with $\mathbb{E}[x_i] = \mu_i$ and $\text{var}(x_i) = \phi V(\mu_i)$ where $\phi &gt; 0$ is some scale parameter. $V(\mu_i)$ is some (known) function of the mean.</p> <p>It’s important to note that the expectations and variances need not be identical, but we enforce that the variances are proportional to some (shared) function of the expectations. We assume that $\mu_i = g(\beta_1, \dots, \beta_m)$. That is, the expectations are some (known) function of parameters $\beta_1, \dots, \beta_m$.</p> <p>The quasi-likelihood is easier to explain after defining the <i>quasi-score</i> function.</p> <div class="definition"> <strong>Definition (Quasi-Score).</strong> <br> The <i>quasi-score</i> of $x_i$ is given by: $$ \begin{equation} \label{eq:quasi-score} U(x_i; \mu_i) = \frac{x_i - \mu_i}{\phi V(\mu_i)} \end{equation} $$ </div> <p>In likelihood theory, the score function is the gradient of the log-likelihood function with respect to the parameters. In a similar fashion, the quasi-score is the gradient of the quasi-likelihood function with respect to the mean.</p> <div class="definition"> <strong>Definition (Quasi-Likelihood).</strong> <br> The <i>quasi-likelihood</i> (or, more precisely, the <i>quasi log-likelihood</i>) of $x_i$ is given by: $$ \begin{equation} \label{eq:quasi-likelihood} \begin{aligned} \ell_q(x_i; \mu_i) &amp;= \int_{x_i}^{\mu_i} \frac{x_i - z}{\phi V(z)} dz \\ &amp;\iff \\ \frac{\partial}{\partial \mu_i} [\ell_q(x_i; \mu_i)] &amp;= U(x_i; \mu_i) \end{aligned} \end{equation} $$ </div> <p>The quasi-score satisfies several of the properties that the score in likelihood theory satisfies, which justifies its name as a <i>quasi</i> score.</p> <div id="theorem-1"></div> <div class="theorem"> <strong>Theorem 1.<d-cite key="wedderburn1974"></d-cite></strong> <ul id="theorem-1-wedderburn" class="tab" data-tab="e8c10730-f6e6-40fb-b5cb-672f60d4590d" data-name="theorem-1-wedderburn"> <li class="active" id="theorem-1-wedderburn-theorem"> <a href="#">theorem </a> </li> <li id="theorem-1-wedderburn-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="e8c10730-f6e6-40fb-b5cb-672f60d4590d" data-name="theorem-1-wedderburn"> <li class="active"> <p>Let $x$ be some observation with expectation $\mu$ and variance $\phi V(\mu)$ for some $\phi &gt; 0$. Suppose $\mu = g(\beta_1, \dots, \beta_m)$ for some (continuous and differentiable) funciton $g(\cdot)$. The quasi-score and quasi-likelihood, as defined in Eqs. \eqref{eq:quasi-score} and \eqref{eq:quasi-likelihood}, satisfy the following properties:</p> <ol> <li>$\mathbb{E}\left[ U(x; \mu) \right] = 0$</li> <li>$\mathbb{E}\left[ \frac{\partial \ell_q(x; \mu) }{\partial \beta_i} \right] = 0$ for all $i \in [m]$</li> <li>$\text{Var}(U(x; \mu)) = - \mathbb{E}\left[ \frac{\partial^2 \ell_q(x;\mu)}{\partial \mu^2}\right] = \frac{1}{\phi V(\mu)}$</li> <li>$\mathbb{E}\left[ \frac{\partial \ell_q(x; \mu)}{\partial \beta_i} \frac{\partial \ell_q(x; \mu)}{\partial \beta_j}\right] = - \mathbb{E}\left[ \frac{\partial^2 \ell_q(x; \mu)}{\partial \beta_i \partial \beta_j} \right] = \frac{1}{\phi V(\mu)} \frac{\partial \mu}{\partial \beta_i} \frac{\partial \mu}{\partial \beta_j}$</li> </ol> </li> <li> <p>Proving (1) is simple:</p> \[\mathbb{E}\left[ U(x; \mu) \right] = \mathbb{E}\left[ \frac{x - \mu}{\phi V(\mu)} \right] = \frac{1}{\phi V(\mu)} \mathbb{E}\left[ x - \mu\right] = \frac{1}{\phi V(\mu)} (\mu - \mu) = 0\] <p>To show (2), we note that:</p> \[\frac{\partial \ell_q(x; \mu)}{\partial \beta_i} = \frac{\partial \ell_q(x; \mu)}{\mu} \frac{\partial \mu}{\beta_j} \hspace{5mm} \implies \hspace{5mm} \mathbb{E}\left[ \frac{\partial \ell_q(x; \mu) }{\partial \beta_i} \right] = \mathbb{E}\left[ \frac{\partial \ell_q(x; \mu)}{\partial \mu} \frac{\partial \mu}{\beta_i} \right] = \frac{\partial \mu}{\beta_i} \mathbb{E}\left[ U(x; \mu) \right] = 0 \nonumber\] <p>Showing (3) is also relatively easy. We first show that $\text{Var}(U(x;\mu)) = \frac{1}{\phi V(\mu)}$:</p> \[\text{Var}(U(x; \mu)) = \text{Var}\left(\frac{x - \mu}{\phi V(\mu)}\right) = \frac{1}{\phi^2 V^2(\mu)} \text{Var}(x - \mu) = \frac{1}{\phi^2 V^2(\mu)} \text{Var}(x) = \frac{\phi V(\mu)}{\phi^2 V^2(\mu)} = \frac{1}{\phi V(\mu)}\] <p>Next, we show that $- \mathbb{E}\left[ \frac{\partial^2 \ell_q(x;\mu)}{\partial \mu^2}\right] = \frac{1}{\phi V(\mu)}$:</p> \[- \mathbb{E}\left[ \frac{\partial^2 \ell_q(x;\mu)}{\partial \mu^2}\right] = - \mathbb{E}\left[ \frac{\partial U(x; \mu)}{\partial \mu} \right] = - \mathbb{E}\left[ \frac{\partial}{\partial \mu} \left[ \frac{x - \mu}{\phi V(\mu)}\right] \right] = - \mathbb{E}\left[ \frac{\phi V(\mu)(-1) - (x- \mu)(\phi V'(\mu))}{\phi^2 V^2(\mu)} \right] = \frac{1}{\phi V(\mu)} + \frac{(\mathbb{E}[x]- \mu)(\phi V'(\mu))}{\phi^2 V^2(\mu)} = \frac{1}{\phi V(\mu)}\] <p>For (4), we first show that $\mathbb{E}\left[ \frac{\partial \ell_q(x; \mu)}{\partial \beta_i} \frac{\partial \ell_q(x; \mu)}{\partial \beta_j}\right] = \frac{1}{\phi V(\mu)} \frac{\partial \mu}{\partial \beta_i} \frac{\partial \mu}{\partial \beta_j}$:</p> \[\begin{aligned} \mathbb{E}\left[ \frac{\partial \ell_q(x; \mu)}{\partial \beta_i} \frac{\partial \ell_q(x; \mu)}{\partial \beta_j}\right] &amp;= \mathbb{E}\left[ \frac{\partial \ell_q(x;\mu)}{\partial \mu} \frac{\partial \mu}{\partial \beta_i} \frac{\partial \ell_q(x;\mu)}{\partial \mu} \frac{\partial \mu}{\partial \beta_j}\right] \\ &amp;= \mathbb{E}\left[ \left( \frac{\partial \ell_q(x;\mu)}{\partial \mu} \right)^2 \right] \frac{\partial \mu}{\partial \beta_i} \frac{\partial \mu}{\partial \beta_j} \\ &amp;= \mathbb{E}\left[ U^2(x; \mu) \right] \frac{\partial \mu}{\partial \beta_i} \frac{\partial \mu}{\partial \beta_j} \\ &amp;= \mathbb{E}\left[ \frac{(x - \mu)^2}{\phi^2 V^2(\mu)} \right] \frac{\partial \mu}{\partial \beta_i} \frac{\partial \mu}{\partial \beta_j} \\ &amp;= \frac{\phi V(\mu)}{\phi^2 V(\mu)} \frac{\partial \mu}{\partial \beta_i} \frac{\partial \mu}{\partial \beta_j} \\ &amp;= \frac{1}{\phi V(\mu)} \frac{\partial \mu}{\partial \beta_i} \frac{\partial \mu}{\partial \beta_j} \end{aligned}\] <p>Then we show that $- \mathbb{E}\left[ \frac{\partial^2 \ell_q(x; \mu)}{\partial \beta_i \partial \beta_j} \right] = \frac{1}{\phi V(\mu)} \frac{\partial \mu}{\partial \beta_i} \frac{\partial \mu}{\partial \beta_j}$:</p> \[\begin{aligned} - \mathbb{E}\left[ \frac{\partial^2 \ell_q(x; \mu)}{\partial \beta_i \partial \beta_j} \right] &amp;= - \mathbb{E}\left[ \frac{\partial}{\partial \beta_j} \left[ \frac{\partial \ell_q(x; \mu)}{\partial \mu} \frac{\partial \mu}{\partial \beta_i}\right] \right] \\ &amp;= - \mathbb{E}\left[ \frac{\partial}{\partial \beta_j} \left[ U(x; \mu) \frac{\partial \mu}{\partial \beta_i}\right] \right] \\ &amp;= - \mathbb{E}\left[ \frac{\partial}{\partial \beta_j} \left[ \frac{x - \mu}{\phi V(\mu)} \frac{\partial \mu}{\partial \beta_i}\right] \right] \\ &amp;= - \mathbb{E}\left[ \frac{\partial}{\partial \beta_j} \left[\frac{x-\mu}{\phi V(\mu)} \right] \frac{\partial \mu}{\partial \beta_i} \right] + \underbrace{\mathbb{E}\left[ \frac{x - \mu}{\phi V(\mu)}\frac{\partial}{\partial \beta_j} \left[\frac{\partial \mu}{\partial \beta_i} \right] \right]}_{=0} \\ &amp;= - \mathbb{E}\left[ \frac{\phi V(\mu) \frac{\partial}{\partial \beta_j}[x - \mu] - (x-\mu) \frac{\partial}{\partial \beta_j}[\phi V(\mu)]}{\phi^2 V^(\mu)}\right] \frac{\partial \mu}{\partial \beta_i} \\ &amp;= \left(- \mathbb{E}\left[ - \frac{1}{\phi V(\mu)} \frac{\partial \mu}{\partial \beta_j} \right] + \underbrace{\mathbb{E}\left[ x- \mu\right]}_{=0} \frac{\frac{\partial}{\partial \beta_j}[\phi V(\mu)]}{\phi^2 V^2(\mu)} \right) \frac{\partial \mu}{\partial \beta_i} \\ &amp;= \frac{1}{\phi V(\mu)} \frac{\partial \mu}{\partial \beta_j} \frac{\partial \mu}{\partial \beta_i} \end{aligned}\] </li> </ul> </div> <hr> <h2 id="connections-to-likelihood">Connections To Likelihood</h2> <p>Suppose that the distribution of $x$ is a function of $\mu$ such that a log-likelihood can be explicitly written. Let $\ell(z; \mu)$ denote this log-likelihood. The following property is due to the above theorem:</p> <div class="theorem"> <strong>Claim.</strong> <ul id="claim-lik1" class="tab" data-tab="99b37a5b-0fa9-4a87-b698-7c3d3ebe1b47" data-name="claim-lik1"> <li class="active" id="claim-lik1-statement"> <a href="#">statement </a> </li> <li id="claim-lik1-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="99b37a5b-0fa9-4a87-b698-7c3d3ebe1b47" data-name="claim-lik1"> <li class="active"> \[\begin{equation} \label{eq:corollary-1} - \mathbb{E}\left[ \frac{\partial^2 \ell_q(x; \mu)}{\partial \mu^2} \right] \leq - \mathbb{E}\left[ \frac{\partial^2 \ell(x; \mu)}{\partial \mu^2} \right] \end{equation}\] </li> <li> <p>By (4) of <a href="#theorem-1">Theorem 1</a>:</p> \[- \mathbb{E}\left[ \frac{\partial^2 \ell_q(x; \mu)}{\partial \mu^2}\right] = \frac{1}{\phi V(\mu)}\] <p>Our problem then becomes showing that:</p> \[\frac{1}{\phi V(\mu)} \leq - \mathbb{E}\left[\frac{\partial^2 \ell(x; \mu)}{\partial \mu^2} \right] \hspace{5mm} \iff \hspace{5mm} \phi V(\mu) \geq - \frac{1}{\mathbb{E}\left[\frac{\partial^2 \ell(x; \mu)}{\partial \mu^2} \right]}\] <p>Under certain regularity conditions (see <a href="/posts/2025/02/03/likelihood-theory.html">my likelihood post</a>), we have that $-\mathbb{E}\left[\frac{\partial^2 \ell(x; \mu)}{\partial \mu^2} \right]$ is the Fisher information. The result follows directly from the <a href="https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound" rel="external nofollow noopener" target="_blank">Cramér-Rao bound</a>.</p> </li> </ul> </div> <p>Wedderburn provides an additional connection between quasi-likelihood and likelihood functions for one-parameter distributions specified by the mean.</p> <div id="theorem-2"></div> <div class="theorem"> <strong>Theorem 2.<d-cite key="wedderburn1974"></d-cite></strong> <ul id="theorem-2-wedderburn" class="tab" data-tab="0d43aaec-7362-43d8-be64-d81e65e6d803" data-name="theorem-2-wedderburn"> <li class="active" id="theorem-2-wedderburn-statement"> <a href="#">statement </a> </li> <li id="theorem-2-wedderburn-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="0d43aaec-7362-43d8-be64-d81e65e6d803" data-name="theorem-2-wedderburn"> <li class="active"> <p>Let $x$ be some observation with expectation $\mu$ and variance $\phi V(\mu)$ for some $\phi &gt; 0$. Suppose $\mu = g(\beta_1, \dots, \beta_m)$ for some (continuous and differentiable) function $g(\cdot)$. The log-likelihood function, $\ell(x; \mu)$, for $x$ satisfies:</p> \[\begin{equation} \label{eq:ll-condition} \frac{\partial}{\partial \mu} \left[ \ell(x; \mu) \right] = \frac{x - \mu}{\phi V(\mu)} \end{equation}\] <p>if and only if the density function of $x$ can be written, with respect to some measure, as:</p> \[f_x = \exp\left( x \theta - h(\theta) \right)\] </li> <li> <p>We first prove the forwards direction. Assume the log-likelihood satisfies Eq. \eqref{eq:ll-condition}. We integrate with respect to $\mu$:</p> \[\begin{aligned} &amp;\int \frac{\partial}{\partial \mu} \left[ \ell(x; \mu) \right] d\mu = \int \frac{x - \mu}{\phi V(\mu)} d\mu \\ \implies &amp;\ell(x; \mu) = \frac{x}{\phi} \int \frac{1}{V(\mu)} d \mu - \frac{1}{\phi}\int \frac{\mu}{V(\mu)} d\mu \end{aligned}\] <p>Substituting in $\theta = \frac{1}{\phi} \int \frac{1}{V(\mu)} d\mu$:</p> \[\ell(x; \mu) = x \theta - ?\] </li> </ul> </div> <p>The theorem can be summarized quite nicely: the quasi-likelihood function will equal the log-likelihood function <i>if and only if</i> the distribution comes from an exponential family.</p> <p>Extending the previous corollary, we see that for a one-parameter exponential family, Eq. \eqref{eq:corollary-1} obtains equality. Under certain regularity conditions (see <a href="/stats-ml/likelihood-theory">my likelihood post</a>), $-\mathbb{E}\left[\frac{\partial^2 \ell(x; \mu)}{\partial \mu^2} \right]$ is the Fisher information, which describes the amount of information about $\mu$ that is held in $x$.</p> <p>Since equality is obtained, we can also think of $-\mathbb{E}\left[\frac{\partial^2 \ell_q(x; \mu)}{\partial \mu^2} \right]$ as describing the amount of information about $\mu$ that is held in $x$. In addition, the difference between the former and the latter can be thought of as the amount of information gained by knowing, specifically, the distribution of $z$.</p> <hr> <h2 id="estimation">Estimation</h2> <p>Let $x_{1:n}= (x_1, \dots, x_n)$ for independent observations $x_1, \dots, x_n$, and let $\mu_{1:n} = (\mu_1, \dots, \mu_n)$. We’ll denote the gradient of the (full) quasi-likelihood with respect to the parameters $\beta_1, \dots, \beta_m$ with:</p> \[\mathbf{u} = \frac{\partial \ell_q(x_{1:n}; \mu_{1:n})}{\partial \beta} = \begin{bmatrix} \frac{\partial \ell_q(x_{1:n}; \mu_{1:n})}{\partial \beta_1} \\ \vdots \\ \frac{\partial \ell_q(x_{1:n}; \mu_{1:n})}{\partial \beta_m} \end{bmatrix} \label{eq:gradient-ql}\] <p>By <a href="#theorem-1">Theorem 1</a>, $\mathbf{u}$ has mean vector:</p> \[\begin{equation} \label{eq:u-mean} \begin{aligned} \mathbb{E}[\mathbf{u}] &amp;= \mathbb{E}\left[ \frac{\partial \ell_q(x_{1:n}; \mu_{1:n})}{\partial \beta} \right] \\ &amp;= \mathbb{E}\left[ \begin{bmatrix} \sum_{i = 1}^n \frac{\partial \ell_q(x_i; \mu_i)}{\partial \beta_1} \\ \vdots \\ \sum_{i = 1}^n \frac{\partial \ell_q(x_i; \mu_i)}{\partial \beta_m} \\ \end{bmatrix} \right] \\ &amp;= \sum_{i = 1}^n \mathbb{E}\left[ \begin{bmatrix} \frac{\partial \ell_q(x_i; \mu_i)}{\partial \beta_1} \\ \vdots \\ \frac{\partial \ell_q(x_i; \mu_i)}{\partial \beta_m} \\ \end{bmatrix} \right]\\ &amp;= \mathbf{0} \end{aligned} \end{equation}\] <p>and covariance matrix:</p> \[\begin{equation} \label{eq:u-cov} \begin{aligned} \text{Cov}(\mathbf{u}) &amp;= \mathbb{E}\left[ \frac{\partial \ell_q(x_{1:n}; \mu_{1:n})}{\partial \beta} \frac{\partial \ell_q(x_{1:n}; \mu_{1:n})}{\partial \beta^\top}\right] \\ &amp;= - \mathbb{E}\left[\frac{\partial^2 \ell_q(x_{1:n}; \mu_{1:n})}{\partial \beta \partial \beta^\top}\right] \end{aligned} \end{equation}\] <p>The <i>maximum quasi-likelihood estimates</i> of $\beta$, denoted by $\hat{\beta}$, are found by setting $\mathbf{u}$ equal to $\mathbf{0}$ and solving for $\beta$, just like we would do for maximum likelihood estimation.</p> <div id="theorem-3"></div> <div class="theorem"> <strong>Theorem 3.<d-cite key="wedderburn1974"></d-cite></strong> <ul id="theorem-3-wedderburn" class="tab" data-tab="3b8958bc-e92f-4556-b37c-4960e3030e3f" data-name="theorem-3-wedderburn"> <li class="active" id="theorem-3-wedderburn-theorem"> <a href="#">theorem </a> </li> <li id="theorem-3-wedderburn-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="3b8958bc-e92f-4556-b37c-4960e3030e3f" data-name="theorem-3-wedderburn"> <li class="active"> <p>Let $x$ be some observation with expectation $\mu$ and variance $\phi V(\mu)$ for some $\phi &gt; 0$. Suppose $\mu = g(\beta_1, \dots, \beta_m)$ for some (continuous and differentiable) function $g(\cdot)$.</p> <p>Denote the gradient of the (full) quasi-likelihood with respect to the parameters $\beta_1, \dots, \beta_m$ with $\mathbf{u}$, and let $\hat{\beta}$ be the maximum quasi-likelihood estimates of $\beta$. The mean of $\hat{\beta}$ is approximately $\mathbf{0}$, and the covariance of $\hat{\beta}$ is approximately:</p> \[\text{Cov}(\hat{\beta}) \approx \text{Cov}^{-1}(\mathbf{u}) = \left[ -\mathbb{E}\left[ \frac{\partial^2 \ell_q(x_{1:n}; \mu_{1:n})}{\partial \beta \partial \beta^\top} \right] \right]^{-1}\] <p>if $\phi = 1$.</p> </li> <li> <p>Let $\hat{\mathbf{u}}$ denote the gradient vector evaluated at the maximum quasi-likelihood estimate, $\hat{\beta}$. Since $\hat{\beta}$ is the value of $\beta$ such that $\mathbf{u}$ equals $\mathbf{0}$, a first-order Taylor approximation of $\mathbf{u}$ gives us:</p> \[\begin{aligned} \mathbf{u} &amp;\approx \hat{\mathbf{u}} + \frac{\partial \mathbf{u}}{\partial \beta} (\beta - \hat{\beta}) \\ &amp;= \frac{\partial^2 \ell_q(x_{1:n}; \mu_{1:n})}{\partial \beta \partial \beta^\top} (\beta - \hat{\beta}) \\ \implies \beta - \hat{\beta} &amp;\approx \left[ \frac{\partial^2 \ell_q(x_{1:n}; \mu_{1:n})}{\partial \beta \partial \beta^\top} \right]^{-1} \mathbf{u} \end{aligned}\] <p>If we approximate the inverted matrix by its expectation, whose elements are given in <a href="#theorem-1">Theorem 1</a>, we get:</p> \[\begin{aligned} &amp;\beta - \hat{\beta} \approx -\text{Cov}^{-1}(\mathbf{u})\mathbf{u} \\ \implies &amp;\hat{\beta} \approx \beta + \text{Cov}^{-1}(\mathbf{u})\mathbf{u} \end{aligned}\] <p>Since $\mathbf{u}$ has expectation zero, it is clear that $\mathbb{E}[\hat{\beta}] \approx \mathbf{0}$ as well.</p> <p>The first term on the right-hand side of the above expression is fixed (the true parameter value), so the <i>approximate</i> covariance matrix of $\hat{\beta}$ is:</p> \[\begin{aligned} \text{Cov}(\hat{\beta}) &amp;\approx \text{Cov}\left( \beta + \text{Cov}^{-1}(\mathbf{u})\mathbf{u} \right) \\ &amp;= \mathbb{E}\left[ \text{Cov}^{-1}(\mathbf{u})\mathbf{u} \left(\text{Cov}^{-1}(\mathbf{u})\mathbf{u}\right)^\top \right] \\ &amp;= \text{Cov}^{-1}(\mathbf{u})\mathbb{E}\left[ \mathbf{u} \mathbf{u}^\top\right] \text{Cov}^{-1}(\mathbf{u}) \\ &amp;= \text{Cov}^{-1}(\mathbf{u}) \end{aligned}\] </li> </ul> </div> <p>The above theorem holds only for a scale parameter $\phi = 1$. If we relax this assumption to $\phi &gt; 0$, the expectation does not change, but we need to estimate $\phi$ before we can approximate the covariance of the maximum quasi-likelihood estimates.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2025-05-30-quasi-likelihood.bib"></d-bibliography> <d-article> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Anna Rosengart. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: February 18, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?d4c3ed73337d78e34b10d24890d1fc56"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/tabs.min.js?b8748955e1076bbe0dabcf28f2549fdc"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>