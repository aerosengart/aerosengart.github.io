<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Shrinkage In Regression | Anna Rosengart </title> <meta name="author" content="Anna Rosengart"> <meta name="description" content=""> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://aerosengart.github.io/stats-ml/shrinkage/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Shrinkage In Regression",
            "description": "",
            "published": "February 16, 2026",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Anna Rosengart </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">curriculum vitae </a> </li> <li class="nav-item "> <a class="nav-link" href="/music/">music </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/stats-ml/">stats &amp; ml</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/measure-theory/">measure theory</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/paper-notes/">paper notes</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Shrinkage In Regression</h1> <p></p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#background">Background</a> </div> <div> <a href="#ridge-regression">Ridge Regression</a> </div> <ul> <li> <a href="#linear-algebra-perspective">Linear Algebra Perspective</a> </li> <li> <a href="#bayesian-perspective">Bayesian Perspective</a> </li> </ul> <div> <a href="#lasso-regression">LASSO Regression</a> </div> <ul> <li> <a href="#bayesian-perspective">Bayesian Perspective</a> </li> <li> <a href="#grouped-lasso">Grouped LASSO</a> </li> </ul> <div> <a href="#elastic-net-regression">Elastic-Net Regression</a> </div> <div> <a href="#others">Others</a> </div> </nav> </d-contents> <p>Sometimes when doing linear regression, we want to balance model complexity with performance to achieve a model that is useful but more interpretable.</p> <aside><p>That is, we favor <i>parsimony</i>.</p></aside> <p>One way to achieve a smaller model is with <strong>shrinkage</strong>, which is the practice of attenuating the effect of certain regressors through their coefficients. This is usually performed by adding a penalty to the objective function that <i>shrinks</i> the model coefficients towards zero.<d-cite key="hastie2017"></d-cite></p> <hr> <h2 id="background">Background</h2> <p>Let’s first consider the standard linear regression objective function (see <a href="/stats-ml/regression/">this post</a> for a more in-depth discussion of linear regression). Suppose our inputs are centered and scaled (so we do not have $\beta_0$). We choose the coefficient vector, $\hat{\beta}$, as the one that minimizes the residual sum of squares (i.e. the $\ell_2$-norm of the residuals):</p> \[\begin{equation} \label{eq:ols} \hat{\beta} = \underset{\beta}{\arg\min} \left\{ \rvert \rvert \mathbf{y} - \mathbf{X}\beta \rvert \rvert_2^2 \right\} \end{equation}\] <p>If we want to penalize large coordinates in $\beta$, we can add a term on:</p> \[\begin{equation} \label{eq:shrink} \hat{\beta}_{S} = \underset{\beta}{\arg\min} \left\{ \rvert \rvert \mathbf{y} - \mathbf{X}\beta \rvert \rvert_2^2 + \lambda \rvert \rvert \beta \rvert \rvert \right\} \end{equation}\] <p>In Eq. \eqref{eq:shrink}, $\lambda \geq 0$ is a tuning parameter, usually called the <strong>complexity</strong>, that controls how much shrinkage we desire. It is usually chosen by cross-validation (or something similar) because the optimal value is unknown. The term $\rvert \rvert \beta \rvert \rvert$ is some norm of $\beta$ and measures the length/magnitude of the vector. The choice of norm dictates what type of shrinkage we are performing.</p> <aside><p>It's important to note that the predictions will be dependent upon the scaling and centering of $\mathbf{y}$ and $\mathbf{X}$, which is why we have standardized the inputs.</p></aside> <hr> <h2 id="ridge-regression">Ridge Regression</h2> <p>Suppose we choose the (squared) $\ell_2$-norm for our penalty:</p> \[\begin{equation} \label{eq:ridge} \hat{\beta}_{R} = \underset{\beta}{\arg\min} \left\{ \rvert \rvert \mathbf{y} - \mathbf{X}\beta \rvert \rvert_2^2 + \lambda \rvert \rvert \beta \rvert \rvert_2^2 \right\} \end{equation}\] <p>This is called <strong>ridge regression</strong> and shrinks coefficients towards zero and each other.<d-cite key="hastie2017"></d-cite></p> <aside><p>This is also called <strong>Tikhonov regularization</strong>.</p></aside> <h3 id="linear-algebra-perspective">Linear Algebra Perspective</h3> <p>The objective can be rewritten in a way that makes the restriction on the size of $\beta$ explicit:</p> \[\begin{equation} \label{eq:ridge-2} \hat{\beta}_{R} = \underset{\beta}{\arg\min} \left\{ \rvert \rvert \mathbf{y} - \mathbf{X}\beta \rvert \rvert_2^2 + \lambda \left( \rvert \rvert \beta \rvert \rvert_2^2 - c\right) \right\} \end{equation}\] <p>where $c \geq 0$ is the constraint value. Taking the derivative of Eq. \eqref{eq:ridge-2} with respect to $\beta$, setting equal to zero, and solving yields the closed form solution:</p> \[\begin{aligned} \frac{\partial}{\partial \beta} \left[ \rvert \rvert \mathbf{y} - \mathbf{X}\beta \rvert \rvert_2^2 + \lambda \left( \rvert \rvert \beta \rvert \rvert_2^2 - c\right) \right] &amp;= \frac{\partial}{\partial \beta} \left[ (\mathbf{y} - \mathbf{X} \beta)^\top (\mathbf{y} - \mathbf{X} \beta) + \lambda \left( \beta^\top \beta - c \right) \right] \\ &amp;= -2\mathbf{X}^\top(\mathbf{y} - \mathbf{X} \beta) + 2 \lambda \beta \\ \implies \mathbf{0}_p &amp;= -2\mathbf{X}^\top(\mathbf{y} - \mathbf{X} \beta) + 2 \lambda \beta \\ \implies \lambda \beta &amp;= \mathbf{X}^\top\mathbf{y} - \mathbf{X}^\top \mathbf{X} \beta \\ \implies \mathbf{X}^\top \mathbf{y} &amp;= \left( \lambda \mathbb{I}_{p \times p} + \mathbf{X}^\top \mathbf{X}\right) \beta \\ \implies \hat{\beta}_R &amp;= \left(\mathbf{X}^\top \mathbf{X} + \lambda \mathbb{I}_{p \times p}\right)^{-1} \mathbf{X}^\top \mathbf{y} \end{aligned}\] <p>Now, consider the ridge regression estimate of the model coefficients:</p> \[\hat{\beta}_R = \left(\mathbf{X}^\top \mathbf{X} + \lambda \mathbb{I}_{p \times p}\right)^{-1} \mathbf{X}^\top \mathbf{y}\] <p>This can be interpreted as a perturbation of the matrix $\mathbf{X}^\top \mathbf{X}$. In the event that this matrix is ill-conditioned, computing the OLS solution may be problematic because it involves $(\mathbf{X}^\top \mathbf{X})^{-1}$. By adding a positive constant to its main diagonal, we make it non-singular and thus avoid issues with inversion.</p> <h3 id="bayesian-perspective">Bayesian Perspective</h3> <p>The ridge regression solution can alternatively be derived as the maximum a posteriori estimate when a Gaussian prior is assumed for $\beta$. Let’s assume the following:</p> \[\begin{aligned} \mathbf{y}_i &amp;\overset{iid}{\sim} \mathcal{N}\left(\mathbf{x}_i^\top \beta, \sigma^2 \right) \\ \beta_j &amp;\overset{iid}{\sim} \mathcal{N}\left(0, \tau^2\right) \end{aligned}\] <p>Assuming $\tau^2$ and $\sigma^2$ are known, the posterior density of $\beta$ can be derived as:</p> \[\begin{aligned} f(\beta \rvert \mathbf{y}) &amp;= \frac{f(\mathbf{y} \rvert \beta) f(\beta)}{f(\mathbf{y})} \\ &amp;\propto \left[ \prod_{i = 1}^n \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(- \frac{\left( \mathbf{y}_i - \mathbf{x}_i^\top \beta \right)^2}{2 \sigma^2} \right) \right] \left[ \prod_{j = 1}^p \frac{1}{\sqrt{2 \pi \tau^2}} \exp\left(- \frac{\beta_j^2}{2 \tau^2} \right) \right] \\ &amp;= \frac{1}{(2 \pi \sigma^2)^{\frac{n}{2}} (2 \pi \tau^2)^{\frac{p}{2}}} \exp\left( - \frac{1}{2 \sigma^2} \sum_{i = 1}^n \left(\mathbf{y}_i - \mathbf{x}_i^\top \beta \right)^2 - \frac{1}{2 \tau^2} \sum_{j = 1}^p \beta_j^2 \right) \end{aligned}\] <p>We can then take the derivative with respect to $\beta$ of the log-posterior density, we get:</p> \[\begin{aligned} \frac{\partial}{ \partial \beta} \left[ \log f(\beta \rvert \mathbf{y}) \right] &amp;\propto \frac{\partial}{\partial \beta} \left[ - \log\left( (2 \pi \sigma^2)^{\frac{n}{2}} (2 \pi \tau^2)^{\frac{p}{2}} \right) - \left[ \frac{1}{2 \sigma^2} \sum_{i = 1}^n \left(\mathbf{y}_i - \mathbf{x}_i^\top \beta \right)^2 + \frac{1}{2 \tau^2} \sum_{j = 1}^p \beta_j^2 \right]\right] \\ &amp;= - \left(- \frac{2}{2\sigma^2} \sum_{i = 1}^n \mathbf{x}_i^\top \left(\mathbf{y}_i - \mathbf{x}_i^\top \beta\right) + \frac{2}{2\tau^2} \beta\right) \\ &amp;= \frac{1}{\sigma^2} \mathbf{X}^\top \left(\mathbf{y} - \mathbf{X} \beta \right) - \frac{1}{\tau^2} \beta \end{aligned}\] <p>Setting this equal to $\mathbf{0}_p$ and solvng for $\beta$ yields the posterior mode:</p> \[\begin{aligned} \mathbf{0}_p &amp;= \frac{1}{\sigma^2} \mathbf{X}^\top \left(\mathbf{y} - \mathbf{X} \beta \right) - \frac{1}{\tau^2} \beta \\ \implies - \frac{1}{\tau^2} \beta &amp;= - \frac{1}{\sigma^2} \mathbf{X}^\top \left(\mathbf{y} - \mathbf{X} \beta \right) \\ \implies -\frac{\sigma^2}{\tau^2} \beta &amp;= -\mathbf{X}^\top \mathbf{y} + \mathbf{X}^\top \mathbf{X} \beta \\ \implies \mathbf{X}^\top \mathbf{y} &amp;= \left(\mathbf{X}^\top \mathbf{X} + \frac{\sigma^2}{\tau^2} \right) \beta \\ \implies \hat{\beta}_R &amp;= \left(\mathbf{X}^\top \mathbf{X} + \frac{\sigma^2}{\tau^2} \right)^{-1}\mathbf{X}^\top \mathbf{y} \end{aligned}\] <p>We see that this is the same as the solution in Eq. \eqref{eq:ridge} but with $\lambda = \frac{\sigma^2}{\tau^2}$. Since the posterior distribution is also Gaussian, the mode equals the mean, so this is also the maximum a posteriori estimate.</p> <hr> <h2 id="lasso-regression">LASSO Regression</h2> <p>We can alternatively choose the $\ell_1$-norm for our penalty:</p> \[\begin{equation} \label{eq:lasso} \hat{\beta}_{L} = \underset{\beta}{\arg\min} \left\{ \rvert \rvert \mathbf{y} - \mathbf{X}\beta \rvert \rvert_2^2 + \lambda \rvert \rvert \beta \rvert \rvert_1 \right\} \end{equation}\] <p>This is called <strong>LASSO regression</strong>, which stands for <i>least absolute shrinkage and selection operator</i>.<d-cite key="hastie2017"></d-cite> Eq. \eqref{eq:lasso} is equivalent to:</p> \[\begin{equation} \label{eq:lasso-2} \begin{aligned} \hat{\beta}_{L} &amp;= \underset{\beta}{\arg\min} \left\{ \rvert \rvert \mathbf{y} - \mathbf{X}\beta \rvert \rvert_2^2 \right\} \\ &amp;\text{subject to} \rvert \rvert \beta \rvert \rvert_1 \leq c \end{aligned} \end{equation}\] <p>If $c$ is chosen to be small enough, then some coordinates of $\beta$ with be <i>shrunken to exactly zero</i>. This is in contrast to ridge regression which does not fully eliminate any coordinates. Because of this feature, LASSO can be used as a feature selection method. However, there is no closed form solution.</p> <h3 id="bayesian-perspective-1">Bayesian Perspective</h3> <p>The LASSO can also be though of from a Bayesian perspective. We have the same setting as with ridge regression but with a Laplace prior with location parameter $\mu = 0$ and scale parameter $\tau &gt; 0$:</p> \[f(\beta_j) = \frac{1}{2 \tau} \exp\left(- \frac{\rvert \beta_j \rvert}{\tau}\right)\] <p>The LASSO estimate can then be derived as the posterior mode (but not necessarily the posterior mean because that requires a symmetric, unimodal distribution).</p> <h3 id="grouped-lasso">Grouped LASSO</h3> <p>LASSO regression has also been extended to penalties that affect groups of predictors in the same way. Suppose we have $Q$ groups of predictors with $m_q$ predictors in group $q$. The <strong>grouped LASSO</strong> is the solution to:</p> \[\begin{equation} \label{eq:group-lasso} \hat{\beta}_{GL} = \underset{\beta}{\arg \min} \left\{ \rvert \rvert \mathbf{y} - \sum_{q = 1}^Q \mathbf{X}_q \beta_q \rvert \rvert_2^2 = \lambda \sum_{q = 1}^Q \sqrt{m_q} \rvert \rvert \beta_q \rvert \rvert_2 \right\} \end{equation}\] <p>where $\mathbf{X}_q$ is the matrix containing the observed values for all samples of the predictors in group $q$. Since $\rvert \rvert \beta_q \rvert \rvert_2 = 0$ if, and only if, $\beta_q = \mathbf{0}$, this shrinkage method aims for group-wise sparsity in addition to shrinking individual coordinates towards $0$.<d-cite key="hastie2017"></d-cite></p> <hr> <h2 id="elastic-net-regression">Elastic-Net Regression</h2> <p>The <strong>elastic-net</strong> estimate is a compromise between ridge and LASSO regression:</p> \[\begin{equation} \hat{\beta}_{EN} = \underset{\beta}{\arg\min} \left\{ \rvert \rvert \mathbf{y} - \mathbf{X}\beta \rvert \rvert_2^2 + \lambda \left(\alpha \rvert \rvert \beta \rvert \rvert_2^2 + (1 - \alpha) \rvert \rvert \beta \rvert \rvert_1 \right)\right\} \end{equation}\] <p>Here, $\alpha$ is a tuning parameter that controls how much we lean towards ridge regression. That is, $\alpha = 1$ will yield $\hat{\beta}_R$ and $\alpha = 0$ will yield $\hat{\beta}_L$.<d-cite key="hastie2017"></d-cite></p> <hr> <h2 id="others">Others</h2> <p>There are many other alternative shrinkage programs. Ridge and LASSO regression are both types of <strong>$\ell_p$ regularization</strong>, where the penalty term is some $\ell_p$ norm of $\beta$ (for $p &gt; 0$). Another alternative is <a href="https://en.wikipedia.org/wiki/Least-angle_regression" rel="external nofollow noopener" target="_blank"><strong>least angle regression</strong></a>.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/stats-ml.bib"></d-bibliography> <d-article> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Anna Rosengart. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: February 16, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?d4c3ed73337d78e34b10d24890d1fc56"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/tabs.min.js?b8748955e1076bbe0dabcf28f2549fdc"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>