<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Likelihood and Large-Sample Theory | Anna Rosengart </title> <meta name="author" content="Anna Rosengart"> <meta name="description" content="A Primer"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://aerosengart.github.io/stats-ml/likelihood-theory/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Likelihood and Large-Sample Theory",
            "description": "A Primer",
            "published": "February 02, 2025",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Anna Rosengart </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">curriculum vitae </a> </li> <li class="nav-item "> <a class="nav-link" href="/music/">music </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/stats-ml/">stats &amp; ml</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/measure-theory/">measure theory</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/paper-notes/">paper notes</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Likelihood and Large-Sample Theory</h1> <p>A Primer</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#background">Background</a> </div> <div> <a href="#score-and-information">Score and Information</a> </div> <ul> <li> <a href="#fisher-information-conditions">Fisher Information Conditions</a> </li> <li> <a href="#implications">Implications</a> </li> </ul> <div> <a href="#maximum-likelihood">Maximum Likelihood</a> </div> <ul> <li> <a href="#finding-mles">Finding MLEs</a> </li> <li> <a href="#asymptotic-normality">Asymptotic Normality</a> </li> </ul> </nav> </d-contents> <p>The score test in non-standard conditions has been the motivation for much of my reading these past few months. However, it has led me to wonder about the small details of the test in standard conditions. What <i>exactly</i> are the regularity conditions and when are they usually satisfied? When can we appeal to large-sample theory for the score test? It is slightly more challenging than anticipated to get a straight answer to these questions, and this is the purpose of this post.</p> <p>This post relies on some measure theory, which I’ve covered in <a href="/measure-theory">a series of posts</a>. Most of the content comes from Moran (1971)<d-cite key="moran1971"></d-cite> and Schervish (1995)<d-cite key="schervish1995"></d-cite>.</p> <hr> <h2 id="background">Background</h2> <p>Suppose we have some probability space $(S, \mathcal{A}, \mu)$. A random variable is some function $X: S \rightarrow \mathcal{X}$ where $\mathcal{X}$ is the sample space (which we only require to be a Borel space but is usually a subset of Euclidean space) with $\sigma$-field $\mathcal{B}$. Individual elements of $\mathcal{X}$ are denoted with $x$.</p> <p>Let \(\mathcal{P}_\Theta\) be a family of distributions for $X$ parametrized by $\Theta: S \rightarrow \Omega$ where $\Omega$ is the parameter space with $\sigma$-field $\tau$. Individual elements of $\Omega$ are denoted with $\theta$. Denote the conditional distribution of $X$ given $\Theta = \theta$ with $P_\theta$ (which is a distribution on $(\mathcal{X},\mathcal{B})$).</p> <p>To make our notation match less measure theoretical texts, we’ll use $X$ to denote a random variable with realizations denoted with the lowercase $x$. A parameter will be denoted with $\Theta$ with particular values denoted by $\theta$ and its true value by $\theta^*$. The density of $X$ given parameter $\Theta$ evaluated at a particular $x$ and $\theta$ will be denoted by $f_{X \rvert \Theta}(x; \theta)$ or, more compactly, $f(x; \theta)$.</p> <hr> <h2 id="score-and-information">Score and Information</h2> <p>We’ll first define a <strong>very</strong> important quantity in likelihood theory: the <i>score function</i>.</p> <div class="definition"> <strong>Definition (Score).</strong> <br> Suppose $\Theta$ is $k$-dimensional, let $f_{X \rvert \Theta}(x; \theta)$ denote the density of $X$ given $\Theta = \theta$. The <i>score function</i> (or <i>score statistic</i>) is given by: $$ \begin{equation} \label{eq:score} U_\Theta(x; \theta) = \frac{\partial \log f_{X \rvert \Theta}(x; \Theta)}{\partial \Theta} \bigg\rvert_{\Theta = \theta} = \frac{\partial \log f_{X \rvert \Theta}(x; \theta)}{\partial \theta} \end{equation} $$ </div> <p>The score function is the gradient of the log density of the data with respect to the parameter. It describes the curvature of the log density at a particular value of the parameter $\Theta$.</p> <div class="definition"> <strong>Definition (Fisher Information).</strong> <br> Suppose $\Theta$ is $k$-dimensional, let $f_{X \rvert \Theta}(x; \theta)$ denote the density of $X$ given $\Theta = \theta$. The <i>Fisher information</i> is the expectation of the squared gradient of the log density of the data: $$ \begin{equation} \label{eq:information} \mathcal{I}_X(\theta) = \mathbb{E}_\Theta \left[ U_\Theta(x; \theta) U_\theta(x; \theta)^\top \right] \end{equation} $$ </div> <p>The Fisher information describes the amount of information about $\Theta$ held by $X$. It is also the variance of the score function under conditions when the score has expectation zero.</p> <h3 id="fisher-information-conditions">Fisher Information Conditions</h3> <p>Schervish outlines several regularity conditions, which he terms the <i>Fisher Information (FI) conditions</i>, that are needed for the definition of the Fisher information and some nice results about the properties of the score.</p> <h4 id="condition-1">Condition 1</h4> <blockquote> <p>There exists a subset of the sample space, $B$, with measure $0$ (i.e. $\mu(B) = 0$) such that $\frac{\partial f_{X \rvert \Theta}(x; \theta)}{\partial \theta_i}$ exists for any $x \notin B$ and all values (and coordinates) of $\theta$.</p> </blockquote> <p>Condition 1 requires that the partial derivatives with respect to all coordinates of $\theta$ (for all values of $\theta$) exists almost surely.</p> <p>Intuitively (and a bit hand-wavily), this means that the derivatives must exist for all possible values of $\theta$ for pretty much any sample. This implies that log-likelihood functions that have cusps or points will not be differentiable at the particular value of $\theta$ where the feature occurs, implying a violation of this condition.</p> <div class="example"> <strong>Example.</strong> Suppose we have $n$ i.i.d. samples $x_1, \dots, x_n \sim Unif(0, \theta)$ for some value $\theta$. The log-likelihood is given by: $$ \begin{aligned} \ell(\theta; x_1, \dots, x_n) &amp;= \log \left( \prod_{i = 1}^n \frac{1}{\theta} \mathbb{I}(0 \leq x_i \leq \theta) \right) \\ &amp;= \log \left( \frac{1}{\theta^n} \mathbb{I}(0 \leq x_i \leq \theta; \forall i) \right) \\ &amp;= \log \left(\frac{1}{\theta^n}\mathbb{I}(\min_i x_i \geq 0) \mathbb{I}(\max_i x_i \leq \theta) \right) \end{aligned} $$ The MLE is $\max_i x_i$. Clearly, the above is not differentiable at this point. </div> <h4 id="condition-2">Condition 2</h4> <blockquote> <p>The order of integration and differentiation can be exchanged for all coordinates of $\theta$. That is: \(\frac{\partial}{\partial \theta_i} \int f_{X \rvert \Theta}(x; \theta) d\mu(x) = \int \frac{\partial f_X(x; \theta)}{\partial \theta_i} d \mu(x)\)</p> </blockquote> <p>Condition 2 states the order of integration of differentiation can be exchanged. Since differentiation is basically just a particular limit, we can use results about the interchanging of the integral and limit to get results about interchaing the integral with differentiation.</p> <div class="theorem"> <strong>Theorem 1 (Dominated Convergence Theorem).</strong> <ul id="dom-conv" class="tab" data-tab="8883c49f-02b8-47dc-8aaf-d2fdd860d1d5" data-name="dom-conv"> <li class="active" id="dom-conv-statement"> <a href="#">statement </a> </li> <li id="dom-conv-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="8883c49f-02b8-47dc-8aaf-d2fdd860d1d5" data-name="dom-conv"> <li class="active"> <p>For a sequence of measurable functions ${f_n}_{n = 1}^\infty$ and measurable functions $f$ and $g$ satisfying $f_n(x) \rightarrow f(x)$ almost everywhere, $\rvert f_n(x) \rvert \leq g(x)$ almost everywhere, and $\int g(x) d\mu(x) &lt; \infty$:</p> \[\underset{n \rightarrow \infty}{\lim} \int f_n(x) d\mu(x) = \int f(x) d\mu(x)\] <p>The dominated convergence theorem states that the integral of the limit of a sequence of measurable functions equals the limit of the integral of each element in the sequence.</p> </li> <li> <p>Proof to be completed.</p> </li> </ul> </div> <p>The Dominated Convergence Theorem states that we can interchange the order of limits and integrals for (certain) functions that are always smaller than (in absolute value) some other function with finite integral. If we define a function that mimics the form of the derivative as a limit (something along the lines of $h(x) = \frac{f(x + \delta) - f(x)}{\delta}$), then we can use this theorem to get results for derivatives and integrals.</p> <p>This is the basic idea of the Leibniz integral rule:</p> <div class="theorem"> <strong>Theorem 2 (Leibniz Integral Rule).</strong> <ul id="leibniz" class="tab" data-tab="01b8f389-32b4-4339-82a4-06ab9ae8a468" data-name="leibniz"> <li class="active" id="leibniz-statement"> <a href="#">statement </a> </li> <li id="leibniz-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="01b8f389-32b4-4339-82a4-06ab9ae8a468" data-name="leibniz"> <li class="active"> <p>Let $\Omega$ be an open subset of $\mathbb{R}$ and $(S, \mathcal{A}, \mu) =: \mathcal{M}$ be a measure space. Let $f: \Omega \times \mathcal{M} \rightarrow \mathbb{R}$ be a function that satisfies:</p> <ul> <li>$f(x, \theta)$ is Lebesgue-integrable in $x$ for all $\theta \in \Omega$</li> <li>$\frac{\partial f(x, \theta)}{\partial \theta}$ exists for all $\theta \in \Omega$ and for <i>almost all</i> $x \in \mathcal{M}$</li> <li>There exists integrable function $g: \mathcal{M} \rightarrow \mathbb{R}$ that is integrable and satisfies $\big\rvert \frac{\partial f(x, \theta)}{\partial \theta} \big\rvert \leq g(x)$ for all $\theta \in \Omega$ and almost every $x \in \mathcal{M}$</li> </ul> <p>Then for all $\theta \in \Omega$: \(\frac{d}{d\theta} \int_\mathcal{M} f(x, \theta) dx = \int_{\mathcal{M}} \frac{\partial}{\partial \theta} f(x, \theta) dx\)</p> </li> <li> <p>Proof to be completed.</p> </li> </ul> </div> <p>In summary, if our log-likelihood/density satisfies the (Lebesgue version of the) Leibniz Rule conditions, then it will satisfy Condition 2.</p> <h4 id="condition-3">Condition 3</h4> <blockquote> <p>The set \(C = \{ x: f_X(x \rvert \theta) &gt; 0 \}\) is the same $\forall \theta$.</p> </blockquote> <p>Condition 3 states that the support of $f_X(x; \theta)$ should not depend on $\theta$. This is fairly easy to verify because we usually assume we know the family of distributions that our data are drawn from. I won’t go into any more details than this.</p> <h3 id="implications">Implications</h3> <p>The FI conditions allow us to obtain the following results that are pretty fundamental for later likelihood theory.</p> <div class="theorem"> <strong>Claim (Score Expectation).</strong> <ul id="score-exp-0" class="tab" data-tab="35f36aa0-4a8d-4453-ae6f-356593b96b63" data-name="score-exp-0"> <li class="active" id="score-exp-0-statement"> <a href="#">statement </a> </li> <li id="score-exp-0-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="35f36aa0-4a8d-4453-ae6f-356593b96b63" data-name="score-exp-0"> <li class="active"> <p>When these conditions hold, the score has expectation $0$.</p> </li> <li> \[\begin{aligned} \mathbb{E}_{\Theta}\left[ U_\Theta(x; \theta) \right] &amp;= \int f_{X \rvert \Theta}(x; \theta) \frac{\partial \log f_{X \rvert \Theta}(x; \theta)}{\partial \Theta} d x \\ &amp;= \int \frac{\partial f_{X \rvert \Theta}(x; \theta)}{\partial \Theta} d x \\ &amp;= \frac{\partial}{\partial \Theta} \int f_{X \rvert \Theta}(x; \theta) d x \\ &amp;= \frac{\partial}{\partial \Theta} [1]\\ &amp;= 0 \end{aligned}\] </li> </ul> </div> <p>Furthermore, we have the following result under additional constraints.</p> <div class="theorem"> <strong>Claim (Fisher Information).</strong> <ul id="fisher-info-1" class="tab" data-tab="7e56b5e7-b3ff-43d3-93a0-b591581f421a" data-name="fisher-info-1"> <li class="active" id="fisher-info-1-statement"> <a href="#">statement </a> </li> <li id="fisher-info-1-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="7e56b5e7-b3ff-43d3-93a0-b591581f421a" data-name="fisher-info-1"> <li class="active"> <p>If the log-likelihood is twice differentiable with respect to $\theta$, then the Fisher information is equal to:</p> \[\mathcal{I}_X(\theta) = - \mathbb{E}_\Theta \left[ \frac{\partial^2 \log f_{X \rvert \Theta}(x; \Theta)}{\partial \Theta \partial \Theta^\top} \bigg\rvert \theta \right]\] </li> <li> \[\begin{aligned} \frac{\partial^2}{\partial \Theta \partial \Theta^\top} \left[ \log f_{X \rvert \Theta}(x; \Theta)\right] &amp;= \frac{\partial}{\partial \Theta} \left[ \frac{1}{f_{X \rvert \Theta}(x; \Theta)} \frac{\partial}{\partial \Theta^\top} \left[ f_{X \rvert \Theta}(x; \Theta) \right] \right] \\ &amp;= \frac{\partial}{\partial \Theta} \left[ \frac{1}{f_{X\rvert \Theta}(x; \Theta)} \right] \frac{\partial}{\partial \Theta^\top} \left[ f_{X \rvert \Theta}(x; \Theta) \right] + \frac{1}{f_{X\rvert \Theta}(x; \Theta)} \frac{\partial^2}{\partial \Theta \partial \Theta^\top} \left[ f_{X \rvert \Theta}(x; \Theta) \right] \\ &amp;= - \frac{\frac{\partial}{\partial \Theta} \left[ f_{X \rvert \Theta}(x; \Theta) \right] \frac{\partial}{\partial \Theta^\top} \left[ f_{X \rvert \Theta}(x; \Theta) \right]}{f^2_{X \rvert \Theta}(x; \Theta)} + \frac{\frac{\partial^2}{\partial \Theta \partial \Theta^\top}[ f_{X \rvert \Theta}(x; \Theta)]}{f_{X \rvert \Theta}(x; \Theta)} \\ &amp;= \frac{\frac{\partial^2}{\partial \Theta \partial \Theta^\top}[ f_{X \rvert \Theta}(x; \Theta)]}{f_{X \rvert \Theta}(x; \Theta)} - \left(\frac{\partial}{\partial \Theta} [ \log f_{X \rvert \Theta}(x; \Theta)]\right)^2 \end{aligned}\] <p>Taking the expected value:</p> \[\begin{aligned} \mathbb{E}_\Theta \left[ \frac{\partial^2}{\partial \Theta \partial \Theta^\top} \left[ \log f_{X \rvert \Theta}(x; \Theta)\right] \bigg\rvert \theta \right] &amp;= \mathbb{E}_\Theta \left[ \frac{\frac{\partial^2}{\partial \theta \partial \theta^\top}[ f_{X \rvert \Theta}(x; \theta)]}{f_{X \rvert \Theta}(x; \theta)} - \left(\frac{\partial}{\partial \theta} [ \log f_{X \rvert \Theta}(x; \theta)]\right)^2 \right] \\ &amp;= \mathbb{E}_\Theta \left[ \frac{\frac{\partial^2}{\partial \theta \partial \theta^\top}[ f_{X \rvert \Theta}(x; \theta)]}{f_{X \rvert \Theta}(x; \theta)} \right] - \mathbb{E}_\Theta \left[ \frac{\partial}{\partial \theta} [ \log f_{X \rvert \Theta}(x; \theta)] \right] \\ &amp;= \mathbb{E}_\Theta \left[ \frac{\frac{\partial^2}{\partial \theta \partial \theta^\top}[ f_{X \rvert \Theta}(x; \theta)]}{f_{X \rvert \Theta}(x; \theta)} \right] - \mathcal{I}_X(\theta) \\ &amp;= \int_\mathbb{R} \left( \frac{\frac{\partial^2}{\partial \theta \partial \theta^\top}[ f_{X \rvert \Theta}(x; \theta)]}{f_{X \rvert \Theta}(x; \theta)} \right) f(x; \theta) dx - \mathcal{I}_X(\theta) \\ &amp;= \int_\mathbb{R} \left( \frac{\partial^2}{\partial \theta \partial \theta^\top}[ f_{X \rvert \Theta}(x; \theta)] \right) dx - \mathcal{I}_X(\theta) \\ &amp;\overset{(i)}{=} \frac{\partial^2}{\partial \theta \partial \theta^\top} \left[ \underbrace{\int_\mathbb{R} f_{X \rvert \Theta}(x; \theta) dx}_{= 1} \right] - \mathcal{I}_X \\ &amp;= - \mathcal{I}_X \end{aligned}\] <p>In $(i)$, we rely on the regularity conditions (specifically number 2 above) so we can interchange the order of differentiation and integration.</p> </li> </ul> </div> <hr> <h2 id="maximum-likelihood">Maximum Likelihood</h2> <p>If we consider the observations $x$ as fixed, then we can define the <i>likelihood function</i> as a function of $\Theta$:</p> \[\begin{equation} \label{eq:lik-func} \mathcal{L}(\theta; x) = f_{X \rvert \Theta}(x; \theta) \end{equation}\] <p>One of the most common settings in which the likelihood function will be useful is in statistical inference. A good starting point is in point estimation. Intuitively, it seems reasonable to judge the quality of a parameter estimate by how probable it is one would observe the sample at hand under the assumption that the estimate is the true parameter value. Or, in another way, we might think that the best estimate we could come up with is the one that is most likely to result in the observations we have. Thus, maximum likelihood estimation is born.</p> <div class="definition"> <strong>Definition (Maximum Likelihood Estimator).</strong> <br> Let $X$ be a random variable with density $f_{X \rvert \Theta}(x; \theta)$, and let $x$ be some realization of $X$. A <i>maximum likelihood estimator (MLE)</i> is any random quantity: $$ \hat{\theta} = \underset{\theta \in \Omega}{\arg\max}\left\{ f_{X \rvert \Theta}(x; \theta) \right\} = \underset{\theta \in \Omega}{\arg \max}\left\{ \mathcal{L}(\theta; x) \right\} $$ The MLE is a function $\hat{\theta}: \mathcal{X} \rightarrow \Omega$ mapping from the sample space to the parameter space. </div> <p>If the parameter space $\Omega$ is compact and the likelihood function is continuous over $\Omega$, then maximum likelihood estimate will exist for a given sample (i.e. the supremum of the maximum likelihood estimator will be achieved in $\Omega$). If the parameter space is open, then the likelihood may increase and never reach a supremum.</p> <div class="example"> <strong>Example.</strong> Consider $x$ distributed uniformly on the open interval $(0, \theta)$. The likelihood is: $$ \mathcal{L}(\theta; x) = \frac{1}{\theta} \mathbb{I}(x &gt; 0) \mathbb{I}(x &lt; \theta) $$ This function is decreasing on the interval $(0, \theta)$, and the maximum is never achieved. </div> <p>MLEs exhibit the <i>invariance property</i>, which is, in words, that a function of an MLE is the MLE of that function.</p> <div class="theorem"> <strong>Invariance Property of Maximum Likelihood Estimators.</strong> <ul id="invariance-prop" class="tab" data-tab="a4c8aa6c-5466-49d2-a92c-e127ce929c3a" data-name="invariance-prop"> <li class="active" id="invariance-prop-statement"> <a href="#">statement </a> </li> <li id="invariance-prop-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="a4c8aa6c-5466-49d2-a92c-e127ce929c3a" data-name="invariance-prop"> <li class="active"> <p>Let $\hat{\theta}$ be an MLE of $\Theta$, and let $g$ be some function of $\theta$. Then $g(\hat{\theta})$ is an MLE of $g(\Theta)$.</p> </li> <li> <p>Define the <i>induced likelihood function</i>:</p> \[\mathcal{L}^*(\eta; x) = \underset{\theta: g(\theta) = \eta}{\sup} \left\{ \mathcal{L}(\theta; x) \right\}\] <p>which is a function of $\eta$ equal to the maximum value of the likelihood function over all values of $\theta$ such that $g(\theta) = \eta$. Let:</p> \[\hat{\eta} = \underset{\eta}{\arg\sup}\left\{ \mathcal{L}^*(\eta; x) \right\}; \hspace{5mm} \hat{\theta} = \underset{\theta}{\arg\sup}\left\{ \mathcal{L}(\theta; x) \right\}\] <p>We have:</p> \[\begin{aligned} \mathcal{L}^*(\hat{\eta}; x) &amp;= \underset{\eta}{\sup}\left\{ \mathcal{L}^*(\eta; x) \right\} \\ &amp;= \underset{\eta}{\sup}\left\{ \underset{\theta: g(\theta) = \eta}{\sup} \left\{ \mathcal{L}(\theta; x) \right\} \right\} \\ &amp;= \underset{\theta}{\sup}\left\{ \mathcal{L}(\theta; x) \right\} \\ &amp;= \mathcal{L}(\hat{\theta}; x) \\ &amp;= \underset{\theta: g(\theta) = g(\hat{\theta})}{\sup} \left\{ \mathcal{L}(\theta; x) \right\} \\ &amp;= \mathcal{L}^*(\hat{\theta}; x) \end{aligned}\] </li> </ul> </div> <h3 id="finding-mles">Finding MLEs</h3> <p>The easiest way to find an MLE is to use set the log-likelihood equal to zero (since monotonic transformations will not affect the $\arg \max$ or $\arg \min$).</p> <div class="theorem"> <strong>Theorem 3 (Fermat's Interior Extremem Theorem).</strong> <ul id="fermat" class="tab" data-tab="7dd54a33-9560-4c16-b4c7-b614f803c3c8" data-name="fermat"> <li class="active" id="fermat-statement"> <a href="#">statement </a> </li> <li id="fermat-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="7dd54a33-9560-4c16-b4c7-b614f803c3c8" data-name="fermat"> <li class="active"> <p>For function $f: A \rightarrow \mathbb{R}$, let $x_0 \in A$ be a local extremum of $f$. If $f$ is differentiable at $x_0$, then $f’(x_0) = 0$.</p> <p>A simple corollary states that a global extremum $x_0$ of $f$ must fall into one of the following cases: (1) $x_0$ is on the boundary of $A$; (2) $f$ is not differentiable at $x_0$; (3) $x_0$ is a stationary point of $f$.</p> </li> <li> <p>Proof to be completed.</p> </li> </ul> </div> <p>For Fermat’s Theorem to apply, the log-likelihood must be differentiable in $\Omega$. The previous example is one such where this is not the case. This theorem also implies that this $x_0$ occurs on the <em>interior</em> of the domain of $f$. We would have to also check the boundary points if $A$ were closed.</p> <h3 id="asymptotic-normality">Asymptotic Normality</h3> <p>The MLE is asymptotically normal under suitable conditions.</p> <div class="theorem"> <strong>Theorem 4 (Asymptotic Normality of MLEs).</strong> <ul id="asymp-norm-mle" class="tab" data-tab="c0378a45-f834-4551-8ced-9e05b848314f" data-name="asymp-norm-mle"> <li class="active" id="asymp-norm-mle-statement"> <a href="#">statement </a> </li> <li id="asymp-norm-mle-"> <a href="#"></a> </li> </ul> <ul class="tab-content" id="c0378a45-f834-4551-8ced-9e05b848314f" data-name="asymp-norm-mle"> <li class="active"> <p>Let $\Omega \subseteq \mathbb{R}^p$ and ${ X_n }<em>{i = 1}^\infty$ be conditionally i.i.d. random variables given $\Theta = \theta^*$ with distribution $P</em>{\theta^<em>}$. Let $\hat{\theta}_n$ be an MLE for $\Theta$ and assume $\hat{\theta}_n \overset{p}{\rightarrow} \theta^</em>$.</p> <p>Further assume that the second partial derivatives with respect to $\Theta$ of the densities are continuous and that the order of differentiation and integration can be exchanged. Suppose there exists function $H_r(x, \theta)$ such that, for $\theta^<em>$ in the interior of $\Omega$ and each $k,j$, the following is satisfied with $\underset{r \rightarrow 0}{\lim} \mathbb{E}_{\theta^</em>} \left[ H_r(X, \theta^*) \right] = 0$:</p> \[\underset{\rvert\rvert \theta - \theta^* \rvert \rvert \leq r}{\sup} \left\{ \frac{\partial^2 \ell(\theta^*; x)}{\partial \theta_k \partial \theta_j} - \frac{\partial^2 \ell(\theta; x)}{\partial \theta_k \partial \theta_j} \right\} \leq H_r(x, \theta^*)\] <p>And finally, assume the Fisher information for a single data point, $\mathcal{I}<em>X(\theta)$, is finite and non-singular. Then, under $P</em>{\theta^*}$:</p> \[\sqrt{n}\left( \hat{\theta}_n - \theta^* \right) \rightsquigarrow \mathcal{N}\left(0, \mathcal{I}^{-1}_{X}(\theta^*) \right)\] <p>In words, this theorem states that the MLE (suitably centered and scaled) is asymptotically normal.</p> </li> <li> <p>Proof to be completed.</p> </li> </ul> </div> <p>The requirements are that the the true parameter value is on the interior of the parameter space (if it is restricted); the MLE is consistent; the density is nice enough (continuous second derivatives); the order of integration and differentiation can be exchanged; and that there is a function with finite mean that bounds the difference between the second derivatives of the log-likelihoods for two values of $\Theta$. This last condition is a uniform law of large numbers.</p> <p>We often supply \(\hat{\theta}_n\) for (usually unknown) \(\theta^*\) in the Fisher information, which yields the <em>expected Fisher information</em>. The expected Fisher information converges in probability to the Fisher Information given \(\Theta = \theta^*\).</p> <p>We could also instead use the scaled matrix of second partial derivaties of the log-likelihood function near to $\hat{\theta}_n$:</p> \[- \frac{1}{n} \left\{ \frac{\partial^2 \ell(\Theta; x)}{\partial \Theta_i \Theta_j} \bigg\rvert_{\Theta = \hat{\theta}_n} \right\}\] <p>which is called the <i>observed Fisher information</i>. It’s basically the sample analog of the expected information.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2025-02-02-likelihood-theory.bib"></d-bibliography> <d-article> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Anna Rosengart. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: February 02, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?d4c3ed73337d78e34b10d24890d1fc56"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/tabs.min.js?b8748955e1076bbe0dabcf28f2549fdc"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>