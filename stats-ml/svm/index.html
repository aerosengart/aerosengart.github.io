<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Support Vector Machines | Anna Rosengart </title> <meta name="author" content="Anna Rosengart"> <meta name="description" content="A Primer"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://aerosengart.github.io/stats-ml/svm/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Support Vector Machines",
            "description": "A Primer",
            "published": "February 17, 2026",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Anna Rosengart </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">curriculum vitae </a> </li> <li class="nav-item "> <a class="nav-link" href="/music/">music </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/stats-ml/">stats &amp; ml</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/measure-theory/">measure theory</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/paper-notes/">paper notes</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Support Vector Machines</h1> <p>A Primer</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#hyperplanes">Hyperplanes</a> </div> <div> <a href="#support-vector-classifier">Support Vector Classifier</a> </div> <ul> <li> <a href="#optimization">Optimization</a> </li> <li> <a href="#kkt-conditions">KKT Conditions</a> </li> </ul> <div> <a href="#support-vector-machine">Support Vector Machine</a> </div> <ul> <li> <a href="#extensions">Extensions</a> </li> </ul> </nav> </d-contents> <p>There are a multitude of ways to perform classification. One of the foundational techniques for this is the <strong>support vector machine</strong> (<strong>SVM</strong>). SVMs are extensions of separating hyperplanes to classes that cannot be perfectly linearly separated.<d-cite key="hastie2017"></d-cite></p> <hr> <h2 id="hyperplanes">Hyperplanes</h2> <p>For any $\beta \in \mathbb{R}^p$, we can represent all hyperplanes in $\mathbb{R}^p$ as the set of vectors satisfying:</p> \[\{\mathbf{x} \mid \mathbf{x}^\top \beta = 0 \}\] <aside><p>Hyperplanes are also called <strong>affine sets</strong>.</p></aside> <p>We can then consider the hyperplane $L$ defined by $\beta_L = (\beta_0, \beta^\top)^\top$:</p> \[L = \{ \mathbf{x} \mid \mathbf{x}^\top \beta + \beta_0 = 0 \}\] <p>Note that, by definition, $\mathbf{x}^\top \beta = -\beta_0$ for any $\mathbf{x} \in L$. Furthermore, for any $\mathbf{x}_1, \mathbf{x}_2 \in L$, we have that:</p> \[(\mathbf{x}_1 - \mathbf{x}_2)^\top \beta = \mathbf{x}_1^\top \beta - \mathbf{x}_2^\top \beta = - \beta_0 + \beta_0 = 0\] <p>This implies that $\beta$ is orthogonal to the surface of $L$, and $\beta^* = \frac{\beta}{\rvert \rvert \beta \rvert \rvert_2}$ is the vector normal to $L$.</p> <aside><p>This also follows from the definition of the <a href="https://en.wikipedia.org/wiki/Normal_(geometry)#Normal_to_planes_and_polygons" rel="external nofollow noopener" target="_blank">normal</a>.</p></aside> <p>Let $\mathbf{x}$ be an arbitrary point in $L$ and consider, now, the (signed) distance between $\mathbf{x}_0 \in \mathbb{R}^{p}$ and $L$:</p> \[\begin{aligned} \frac{\beta^* \cdot (-(\mathbf{x} - \mathbf{x}_0))}{\rvert \rvert \beta^* \rvert \rvert_2} &amp;= \frac{1}{\rvert \rvert \frac{\beta}{\rvert \rvert \beta \rvert \rvert_2} \rvert \rvert_2} (\beta^*)^\top (\mathbf{x}_0 - \mathbf{x}) \\ &amp;= (\beta^*)^\top \mathbf{x}_0 - (\beta^*)^\top \mathbf{x} \\ &amp;= \frac{1}{\rvert \rvert \beta \rvert \rvert_2} \left[ \beta^\top (\mathbf{x}_0 - \mathbf{x}) \right]\\ &amp;= \frac{1}{\rvert \rvert \beta \rvert \rvert_2} \left[ \beta^\top \mathbf{x}_0 - (- \beta_0) \right]\\ &amp;= \frac{1}{\rvert \rvert \beta \rvert \rvert_2} \left[ \beta^\top \mathbf{x}_0 + \beta_0 \right] \end{aligned}\] <p>This shows that the function $f(\mathbf{x}) = \beta^\top \mathbf{x} + \beta_0 = 0$ that defines $L$ above is proportional to the signed distance between $\mathbf{x}$ and $L$.</p> <hr> <h2 id="support-vector-classifier">Support Vector Classifier</h2> <p>Now, we’ll discuss the <strong>support vector classifier</strong>, which is a classifier that constructs the optimal separating hyperplane between two perfectly linearly separable classes. By <i>optimal</i>, we mean that the distance between the closest point to the hyperplane and the hyperplane is maximized.</p> <aside><p>This distance is called the <strong>margin</strong>.</p></aside> <p>Let’s say we have a training set of $N$ observations with class labels $y_i \in \{ -1, 1 \}$ and $p$-dimensional predictors $\mathbf{x}_i \in \mathbb{R}^{p}$. Let $\mathbf{X}$ be the $n \times p$ matrix with rows equal to the $\mathbf{x}_i$ vectors.</p> <p>Suppose we have the hyperplane $L$ again, and we classify points with positive distance to $L$ to one class and those with negative distance to $L$ to another. We can represent this binary classifier as:</p> \[g(\mathbf{x}) = \text{sign}\left(\mathbf{x}^\top \beta + \beta_0 \right) = \begin{cases} 1 &amp; \text{ if } \mathbf{x}^\top \beta + \beta_0 \geq 0 \\ -1 &amp; \text{ else } \end{cases}\] <aside><p>For $0-1$ classes, this classifier is called a <strong>perceptron</strong> in the machine learning literature.</p></aside> <h3 id="optimization">Optimization</h3> <p>To find the optimal hyperplane, we want to maximize the distance between the points and $L$. Notice that if $y_i$ is correctly classified, then:</p> \[y_i(\mathbf{x}_i^\top \beta + \beta_0) \geq 0\] <p>Thus, optimization consists of solving:</p> \[\begin{equation} \label{eq:first-opt} \begin{aligned} &amp;\underset{\beta, \beta_0, \rvert \rvert \beta \rvert \rvert_2 = 1}{\max} \left\{ M \right \} \\ &amp;\text{ subject to } y_i(\mathbf{x}_i^\top \beta + \beta_0) \geq M \text{ } \forall i = 1, \dots, N \end{aligned} \end{equation}\] <p>Via some rescaling and manipulation, this is equivalent to:</p> \[\begin{aligned} &amp;\underset{\beta, \beta_0}{\min} \left\{ \frac{1}{2}\rvert \rvert \beta \rvert \rvert_2^2 \right \} \\ &amp;\text{ subject to } 0 \geq 1 - y_i(\mathbf{x}_i^\top \beta + \beta_0) \text{ } \forall i = 1, \dots, N \\ \end{aligned}\] <details> <summary>Proof.</summary> Moving the norm requirement into the constraint (and thus rescaling $\beta_0$), this is equivalent to: $$ \begin{equation} \label{eq:second-opt} \begin{aligned} &amp;\underset{\beta, \beta_0}{\max} \left\{ M \right \} \\ &amp;\text{ subject to } \frac{1}{\rvert \rvert \beta \rvert\rvert_2} \left[ y_i(\mathbf{x}_i^\top \beta + \beta_0)\right] \geq M \text{ } \forall i = 1, \dots, N \\ \iff &amp;\underset{\beta, \beta_0}{\max} \left\{ M \right \} \\ &amp;\text{ subject to } y_i(\mathbf{x}_i^\top \beta + \beta_0) \geq \rvert \rvert \beta \rvert\rvert_2 M \text{ } \forall i = 1, \dots, N \\ \end{aligned} \end{equation} $$ Again rescaling (since any scalar multiple of the optimal $\beta$ and $\beta_0$ will also satisfy the constraints) by setting $\rvert \rvert \beta \rvert \rvert_2 = \frac{1}{M}$, we can write: $$ \begin{equation} \label{eq:third-opt} \begin{aligned} &amp;\underset{\beta, \beta_0}{\max} \left\{ \frac{1}{\rvert \rvert \beta \rvert \rvert_2} \right \} \\ &amp;\text{ subject to } y_i(\mathbf{x}_i^\top \beta + \beta_0) \geq 1 \text{ } \forall i = 1, \dots, N \\ \iff &amp;\underset{\beta, \beta_0}{\min} \left\{ \rvert \rvert \beta \rvert \rvert_2 \right \} \\ &amp;\text{ subject to } y_i(\mathbf{x}_i^\top \beta + \beta_0) \geq 1 \text{ } \forall i = 1, \dots, N \\ \iff &amp;\underset{\beta, \beta_0}{\min} \left\{ \frac{1}{2}\rvert \rvert \beta \rvert \rvert_2^2 \right \} \\ &amp;\text{ subject to } y_i(\mathbf{x}_i^\top \beta + \beta_0) \geq 1 \text{ } \forall i = 1, \dots, N \\ \end{aligned} \end{equation} $$ The margin is now encoded in the norm of $\beta$ (inversely).We'll rewrite the last line of Eq. \eqref{eq:third-opt} as: $$ \begin{aligned} &amp;\underset{\beta, \beta_0}{\min} \left\{ \frac{1}{2}\rvert \rvert \beta \rvert \rvert_2^2 \right \} \\ &amp;\text{ subject to } 0 \geq 1 - y_i(\mathbf{x}_i^\top \beta + \beta_0) \text{ } \forall i = 1, \dots, N \\ \end{aligned} $$ </details> <p>Note that is a convex optimization problem (quadratic objective with linear inequality constraints), so we can use <a href="/stats-ml/constrained-optim/">duality</a> to rewrite this as:</p> \[\begin{equation} \label{eq:lagrange-dual} \begin{aligned} &amp;\underset{\lambda}{\max} \left\{ \underset{\beta, \beta_0}{\inf} \left\{ \frac{1}{2} \rvert \rvert\beta \rvert \rvert_2^2 + \sum_{i = 1}^N \lambda_i \left(1 - y_i (\mathbf{x}_i^\top \beta + \beta_0) \right) \right\} \right\} \\ &amp;\text{ subject to } \lambda_i \geq 0 \text{ } \forall i = 1, \dots, N \end{aligned} \end{equation}\] <aside><p>This is called the <strong>primal form</strong>.</p></aside> <p>Assuming the objective and all of the constraints are convex and continuously differentiable functions, we can recast the problem in terms of the gradients. We have:</p> \[\begin{aligned} \frac{\partial }{\partial \beta} \left[ \frac{1}{2} \rvert \rvert\beta \rvert \rvert_2^2 + \sum_{i = 1}^N \lambda_i \left(1 - y_i (\mathbf{x}_i^\top \beta + \beta_0) \right) \right] &amp;= \beta - \sum_{i= 1}^N \lambda_i y_i \mathbf{x}_i \\ \implies \beta &amp;= \sum_{i = 1}^N \lambda_i y_i \mathbf{x}_i \\ \frac{\partial}{\partial \beta_0} \left[ \frac{1}{2} \rvert \rvert\beta \rvert \rvert_2^2 + \sum_{i = 1}^N \lambda_i \left(1 - y_i (\mathbf{x}_i^\top \beta + \beta_0) \right) \right] &amp;= - \sum_{i = 1}^N \lambda_i y_i \\ \implies 0 &amp;= \sum_{i = 1}^N \lambda_i y_i \end{aligned}\] <p>We substitute these into the objective function in Eq. \eqref{eq:lagrange-dual} to get:</p> \[\begin{equation} \label{eq:wolfe-dual} \begin{aligned} &amp;\underset{\lambda}{\max} \left\{ \sum_{i = 1}^N \lambda_i - \frac{1}{2} \sum_{i = 1}^N \sum_{j = 1}^N \lambda_i \lambda_j y_i y_j \mathbf{x}_i^\top \mathbf{x}_j \right\} \\ &amp;\text{ subject to } \sum_{i = 1}^N \lambda_i y_i = 0 \text{ and } \lambda_i \geq 0 \text{ } \forall i = 1, \dots, N \end{aligned} \end{equation}\] <aside><p>This is called the <strong>dual form</strong>.</p></aside> <details> <summary>Proof.</summary> $$ \begin{aligned} \mathcal{L}_D &amp;= \frac{1}{2} \rvert \rvert\beta \rvert \rvert_2^2 + \sum_{i = 1}^N \lambda_i \left(1 - y_i (\mathbf{x}_i^\top \beta + \beta_0) \right) \\ &amp;= \frac{1}{2} \left\rvert\left\rvert \sum_{i = 1}^N \lambda_i y_i \mathbf{x}_i \right\rvert\right\rvert_2^2 + \sum_{i = 1}^N \lambda_i \left(1 - y_i\left(\mathbf{x}_i^\top \sum_{j = 1}^N \lambda_j y_j \mathbf{x}_j + \beta_0 \right) \right) \\ &amp;= \frac{1}{2}\left(\sum_{i = 1}^N \lambda_i y_i \mathbf{x}_i \right)^\top \left(\sum_{i = 1}^N \lambda_i y_i \mathbf{x}_i \right) + \sum_{i = 1}^N \lambda_i \left(1 - \sum_{j = 1}^N \lambda_j y_i y_j \mathbf{x}_i^\top \mathbf{x}_j - y_i \beta_0 \right) \\ &amp;= \frac{1}{2}\sum_{i = 1}^N \sum_{j = 1}^N \lambda_i \lambda_j y_i y_j \mathbf{x}_i^\top \mathbf{x}_j - \sum_{i = 1}^N \sum_{j = 1}^N \lambda_i \lambda_j y_i y_j \mathbf{x}_i^\top \mathbf{x}_j + \sum_{i = 1}^N \lambda_i y_i \beta_0 + \sum_{i = 1}^N \lambda_i \\ &amp;= \sum_{i = 1}^N \lambda_i - \frac{1}{2} \sum_{i = 1}^N \sum_{j = 1}^N \lambda_i \lambda_j y_i y_j \mathbf{x}_i^\top \mathbf{x}_j - \beta_0 \sum_{i = 1}^N \lambda_i y_i \end{aligned} $$ Under the constraint that $0 = \sum_{i = 1}^N \lambda_i y_i$, the objective then becomes: $$ \mathcal{L}_D = \sum_{i = 1}^N \lambda_i - \frac{1}{2} \sum_{i = 1}^N \sum_{j = 1}^N \lambda_i \lambda_j y_i y_j \mathbf{x}_i^\top \mathbf{x}_j $$ </details> <p>This is a quadratic optimization problem with a linear constraint and a cone (the non-negative orthant) constraint, and it can be solved with standard packages (e.g. <code class="language-plaintext highlighter-rouge">cvxopt</code>).</p> <h3 id="kkt-conditions">KKT Conditions</h3> <p>In order to be optimal, the solution must satisfy the <a href="https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions" rel="external nofollow noopener" target="_blank">Karush-Kuhn-Tucker conditions</a>. That is, they must satisfy:</p> <ol> <li> <strong>Stationarity</strong>: $$\frac{\partial}{\partial \beta_L} \left[ \frac{1}{2} \rvert\rvert \beta \rvert \rvert_2^2 + \sum_{i = 1}^N \lambda_i \left(1 - y_i(\mathbf{x}_i^\top \beta + \beta_0)\right) \right] = \mathbf{0}_{p + 1}$$</li> <li> <strong>Primal Feasibility</strong>: $$y_i(\mathbf{x}_i^\top \beta + \beta_0) - 1 \geq 0 \text{ } \forall i = 1, \dots, N$$</li> <li> <strong>Dual Feasibility</strong>: $$\lambda_i \geq 0 \text{ } \forall i = 1, \dots, N$$</li> <li> <strong>Complementary Slackness</strong>: $$\lambda_i \left( y_i(\mathbf{x}_i^\top \beta + \beta_0) - 1\right) = 0 \text{ } \forall i = 1, \dots, N$$</li> </ol> <p>The stationarity condition states that:</p> \[\beta = \sum_{i = 1}^N \lambda_i y_i \mathbf{x}_i\] <p>which shows that the optimal $\beta$ must be a linear combination of the $\mathbf{x}_i$ vectors. The complementary slackness condition implies that if $\lambda_i &gt; 0$, then $y_i(\mathbf{x}_i^\top \beta + \beta_0) = 1$. On the other hand, if $\lambda_i = 0$, then the $\mathbf{x}_i$ does not contribute to $\beta$. Thus, the optimal $\beta$ is a linear combination of some of the $\mathbf{x}_i$ vectors, which are called <strong>support vectors</strong>.</p> <aside><p>Looking at the earlier iterations of our problem, we see that points that lie exactly on the boundary of the margin will be these points.</p></aside> <p>Thus, the optimal separating hyperplane (i.e. the support vector classifier) is the one represented by:</p> \[\begin{aligned} G(\mathbf{x}) &amp;= \text{sign}\left(\mathbf{x}^\top \hat{\beta} + \hat{\beta}_0 \right) \\ &amp;= \text{sign}\left( \mathbf{x}^\top \sum_{i = 1}^N \lambda_i y_i \mathbf{x}_i + \hat{\beta}_0 \right) \\ &amp;= \text{sign}\left( \sum_{i = 1}^N \lambda_i y_i \mathbf{x}^\top \mathbf{x}_i + \hat{\beta}_0 \right) \\ \end{aligned}\] <p>where $\hat{\beta}$ and $\hat{\beta}_0$ are the solutions to the dual problem in Eq. \eqref{eq:wolfe-dual}.</p> <hr> <h2 id="support-vector-machine">Support Vector Machine</h2> <p>The <strong>support vector machine</strong> extends the support vector classifier to classes that are not linearly separable (in the feature space). In this case, we must adjust the optimization problem because there is no longer a separating hyperplane (let alone an optimal one).</p> <p>To get around this fact, we introduce <strong>slack variables</strong> $\xi = (\xi_1, xi_2, \dots, \xi_N)^\top$. We adjust the constraint to include these slack variables:</p> \[\begin{equation} \label{eq:svm-1} y_i(\mathbf{x}_i^\top \beta + \beta_0) \geq M(1 - \xi_i) \end{equation}\] <p>where $\xi_i \geq 0$ for all $i = 1, \dots, N$ and $\sum_{i = 1}^N \xi_i \leq c$ for some constant $c$. By incorporating $\xi$ in this way, we permit each observation to cross the appropriate margin by an amount proportional to $\xi_i$ (i.e. only by $M \xi_i$). The constraint that the sum of slack variables does not exceed $c$ forces the total amount of “crossing” to only be so large. Since misclassifications occur if $\xi_i &gt; 1$, the number of misclassifications cannot exceed $c$.</p> <p>We can write the optimization problem as:</p> \[\begin{equation} \label{eq:svm-2} \begin{aligned} &amp;\underbrace{\lambda, \mu}{\max} \left\{ \sum_{i = 1}^N \lambda_i -\frac{1}{2} \sum_{i = 1}^N \sum_{j = 1}^N \lambda_i \lambda_j y_i y_j \mathbf{x}_i \mathbf{x}_j \right\} \\ &amp;\text{ subject to } \sum_{i = 1}^N \lambda_i y_i = 0 \text{ and } 0 \leq \lambda_i \leq C \text{ } \forall i = 1, \dots, N \end{aligned} \end{equation}\] <details> <summary>Proof.</summary> As we showed in the SVC case, we can rescale (since any scalar multiple of the optimal $\beta$ and $\beta_0$ will also satisfy the constraints) by setting $\rvert \rvert \beta \rvert \rvert_2 = \frac{1}{M}$ to get: $$ \begin{aligned} &amp;\underset{\beta, \beta_0}{\min} \left\{ \frac{1}{2}\rvert \rvert \beta \rvert \rvert_2^2 \right \} &amp;\\ &amp;\text{ subject to } &amp;y_i(\mathbf{x}_i^\top \beta + \beta_0) \geq 1 - \xi_i \\ &amp; &amp;\sum_{i = 1}^N \xi_i \leq c \\ &amp; &amp;\xi_i \geq 0 \text{ } \forall i = 1, \dots, N \\ \end{aligned} $$ and we've just added in the constraints on the slack variables. We can reparametrize the constraint on the sum of the slack variables with $C$ and put it in the objective function to get the equivalent form: $$ \begin{aligned} &amp;\underset{\beta, \beta_0}{\min} \left\{ \frac{1}{2}\rvert \rvert \beta \rvert \rvert_2^2 + C \sum_{i = 1}^N \xi_i \right \} &amp;\\ &amp;\text{ subject to } &amp;y_i(\mathbf{x}_i^\top \beta + \beta_0) \geq 1 - \xi_i \\ &amp; &amp;\xi_i \geq 0 \text{ } \forall i = 1, \dots, N \\ \end{aligned} $$ Similar to the SVC case, we rewrite this as the Lagrangian dual problem: $$ \begin{equation} \label{eq:svm-lagrange} \begin{aligned} &amp;\underset{\lambda, \mu}{\max} \left\{ \underset{\beta, \beta_0}{\inf} \left\{ \frac{1}{2} \rvert \rvert\beta \rvert \rvert_2^2 + C \sum_{i = 1}^N \xi_i + \sum_{i = 1}^N \lambda_i \left(1 - \xi_i - y_i (\mathbf{x}_i^\top \beta + \beta_0) \right) - \sum_{i = 1}^N \mu_i \xi_i \right\} \right\}\\ &amp;\text{ subject to } \lambda_i, \mu_i \geq 0 \text{ } \forall i = 1, \dots, N \end{aligned} \end{equation} $$ Taking the derivative with respect to $\beta$, $\beta_0$, and $\xi$, and setting equal to zero, we get: $$ \begin{aligned} \frac{\partial}{\partial \beta} \left[ \frac{1}{2} \rvert \rvert\beta \rvert \rvert_2^2 + C \sum_{i = 1}^N \xi_i + \sum_{i = 1}^N \lambda_i \left(1 - \xi_i - y_i (\mathbf{x}_i^\top \beta + \beta_0) \right) - \sum_{i = 1}^N \mu_i \xi_i \right] &amp;= \beta - \sum_{i = 1}^N \lambda_i y_i \mathbf{x}_i \\ \implies \beta &amp;= \sum_{i = 1}^N \lambda_i y_i \mathbf{x} \\ \frac{\partial}{\partial \beta_0}\left[ \frac{1}{2} \rvert \rvert\beta \rvert \rvert_2^2 + C \sum_{i = 1}^N \xi_i + \sum_{i = 1}^N \lambda_i \left(1 - \xi_i - y_i (\mathbf{x}_i^\top \beta + \beta_0) \right) - \sum_{i = 1}^N \mu_i \xi_i\right] &amp;= - \sum_{i = 1}^N \lambda_i y_i \\ \implies 0 &amp;= \sum_{i = 1}^N \lambda_i y_i \\ \frac{\partial}{\partial \xi_i} \left[ \frac{1}{2} \rvert \rvert\beta \rvert \rvert_2^2 + C \sum_{i = 1}^N \xi_i + \sum_{i = 1}^N \lambda_i \left(1 - \xi_i - y_i (\mathbf{x}_i^\top \beta + \beta_0) \right) - \sum_{i = 1}^N \mu_i \xi_i\right] &amp;= C - \lambda_i + \mu_i \\ \implies \lambda_i &amp;= C - \mu_i \end{aligned} $$ Plugging these into the objective funciton of Eq. \eqref{eq:svm-lagrange}, we get: $$ \begin{aligned} \mathcal{L}_D &amp;= \frac{1}{2} \rvert \rvert \beta \rvert \rvert_2^2 + C \sum_{i = 1}^N \xi_i + \sum_{i = 1}^N \lambda_i \left(1 - \xi_i - y_i (\mathbf{x}_i^\top \beta + \beta_0) \right) - \sum_{i = 1}^N \mu_i \xi_i \\ &amp;= \frac{1}{2} \left(\sum_{i = 1}^N \lambda_i y_i \mathbf{x}_i \right)^2 + C \sum_{i = 1}^N \xi_i - \sum_{i = 1}^N \lambda_i \left(\xi_i + y_i \left( \mathbf{x}_i^\top \sum_{j = 1}^N \lambda_j y_j \mathbf{x}_j + \beta_0 \right) - 1\right) - \sum_{i = 1}^N \mu_i \xi_i \\ &amp;= \frac{1}{2} \sum_{i = 1}^N \sum_{j = 1}^N \lambda_i \lambda_j y_i y_j \mathbf{x}_i \mathbf{x}_j + C \sum_{i = 1}^N \xi_i - \sum_{i = 1}^N (C - \mu_i) \xi_i + \sum_{i = 1}^N \lambda_i - \sum_{i = 1}^N \sum_{j = 1}^N \lambda_i \lambda_j y_i y_j \mathbf{x}_i^\top \mathbf{x}_j - \sum_{i = 1}^N \mu_i \xi_i \\ &amp;= \sum_{i = 1}^N \lambda_i -\frac{1}{2} \sum_{i = 1}^N \sum_{j = 1}^N \lambda_i \lambda_j y_i y_j \mathbf{x}_i \mathbf{x}_j \end{aligned} $$ The optimization problem is then: $$ \begin{aligned} &amp;\underbrace{\lambda, \mu}{\max} \left\{ \sum_{i = 1}^N \lambda_i -\frac{1}{2} \sum_{i = 1}^N \sum_{j = 1}^N \lambda_i \lambda_j y_i y_j \mathbf{x}_i \mathbf{x}_j \right\} \\ &amp;\text{ subject to } \sum_{i = 1}^N \lambda_i y_i = 0 \text{ and } 0 \leq \lambda_i \leq C \text{ } \forall i = 1, \dots, N \end{aligned} $$ </details> <p>The KKT conditions are:</p> <ol> <li> <strong>Stationarity</strong>: $$ \begin{aligned} &amp;\frac{\partial}{\partial \beta_L} \left[ \frac{1}{2} \rvert \rvert\beta \rvert \rvert_2^2 + C \sum_{i = 1}^N \xi_i + \sum_{i = 1}^N \lambda_i \left(1 - \xi_i - y_i (\mathbf{x}_i^\top \beta + \beta_0) \right) - \sum_{i = 1}^N \mu_i \xi_i \right] = \mathbf{0}_{p + 1} \\ &amp;\frac{\partial}{\partial \xi} \left[ \frac{1}{2} \rvert \rvert\beta \rvert \rvert_2^2 + C \sum_{i = 1}^N \xi_i + \sum_{i = 1}^N \lambda_i \left(1 - \xi_i - y_i (\mathbf{x}_i^\top \beta + \beta_0) \right) - \sum_{i = 1}^N \mu_i \xi_i \right] = C\mathbf{1}_N - \mu \end{aligned} $$ </li> <li> <strong>Primal Feasibility</strong>: $$y_i(\mathbf{x}_i^\top \beta + \beta_0) - ( 1- \xi_i ) \geq 0 \text{ } \forall i = 1, \dots, N$$</li> <li> <strong>Dual Feasibility</strong>: $$\mu_i \xi_i \geq 0 \text{ } \forall i = 1, \dots, N$$</li> <li> <strong>Complementary Slackness</strong>: $$\lambda_i \left( y_i(\mathbf{x}_i^\top \beta + \beta_0) - (1 - \xi_i)\right) = 0 \text{ } \forall i = 1, \dots, N$$</li> </ol> <p>By a similar argument to the SVC case, the optimal $\beta$ is given by:</p> \[\hat{\beta} = \sum_{i = 1}^N \lambda_i y_i \mathbf{x}_i\] <p>By the complementary slackness requirement, $\lambda_i &gt; 0$ only for those points $\mathbf{x}_i$ that satisfy:</p> \[y_i(\mathbf{x}_i^\top \beta + \beta_0) = 1 - \xi_i\] <p>These are called <strong>support vectors</strong>, and those with $\xi_i = 0$ will lie exactly on the margin. If $\lambda_i = C$, then if $\xi_i \leq 1$, the point will lie within the margin but be classified correctly. Otherwise, it is incorrectly classified.</p> <p>The support vector machine is the classifier:</p> \[\begin{aligned} G(\mathbf{x}) &amp;= \text{sign} \left( \mathbf{x}^\top \hat{\beta} + \hat{\beta}_0 \right) \\ &amp;= \text{sign} \left( \mathbf{x}^\top \sum_{i = 1}^N \lambda_i y_i\mathbf{x}_i + \hat{\beta}_0 \right) \\ &amp;= \text{sign} \left( \sum_{i = 1}^N \lambda_i y_i \mathbf{x}^\top \mathbf{x}_i + \hat{\beta}_0 \right) \\ \end{aligned}\] <p>where $C$ is left as a tuning parameter, and $\hat{\beta}$ and $\hat{\beta}_0$ are the solutions to Eq. \eqref{eq:svm-2}.</p> <h3 id="extensions">Extensions</h3> <p>The SVM can be extended beyond the inner product with respect to the Euclidean norm via the <a href="/stats-ml/kernel.md">kernel trick</a>. The classifier then becomes:</p> \[G(\mathbf{x}) = \text{sign} \left( \sum_{i = 1}^N \lambda_i y_i \mathcal{K}(h(\mathbf{x}), h(\mathbf{x}_i)) + \beta_0 \right)\] <p>for some choice of symmetric positive semi-definite kernel function $\mathcal{K}(\cdot, \cdot)$.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/stats-ml.bib"></d-bibliography> <d-article> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Anna Rosengart. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: February 18, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?d4c3ed73337d78e34b10d24890d1fc56"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/tabs.min.js?b8748955e1076bbe0dabcf28f2549fdc"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>