<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Clustering | Anna Rosengart </title> <meta name="author" content="Anna Rosengart"> <meta name="description" content="An Axiomatic Approach"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://aerosengart.github.io/paper-notes/impossibility-clustering/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Clustering",
            "description": "An Axiomatic Approach",
            "published": "January 13, 2025",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Anna Rosengart </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">curriculum vitae </a> </li> <li class="nav-item "> <a class="nav-link" href="/music/">music </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/stats-ml/">stats &amp; ml</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/measure-theory/">measure theory</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/paper-notes/">paper notes</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Clustering</h1> <p>An Axiomatic Approach</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#background">Background</a> </div> <div> <a href="#the-impossibility-theorem">The Impossibility Theorem</a> </div> <div> <a href="#relaxations">Relaxations</a> </div> <ul> <li> <a href="#relaxing-richness">Relaxing Richness</a> </li> <li> <a href="#relaxing-consistency">Relaxing Consistency</a> </li> <li> <a href="#relaxing-scale-invariance">Relaxing Scale-Invariance</a> </li> </ul> <div> <a href="#the-consistency-theorem">The Consistency Theorem</a> </div> <div> <a href="#examples-and-extensions">Examples and Extensions</a> </div> <ul> <li> <a href="#weakest-link">Weakest Link</a> </li> <li> <a href="#additive-margin">Additive Margin</a> </li> <li> <a href="#functions-of-cqms">Functions of CQMS</a> </li> <li> <a href="#"></a> </li> </ul> </nav> </d-contents> <p>Though the journey to this point is a bit confusing, I have recently become interesting in clustering metrics and evaluation. In this post, I’ll work through a couple papers on describing how good a clustering function is based upon a set of axioms. These include Kleinberg’s <i>An Impossibility Theorem for Clustering</i><d-cite key="kleinberg2002"></d-cite> and Ben-David and Ackerman’s <i>Measure of Clustering Quality</i><d-cite key="bendavid2008"></d-cite>.</p> <h2 id="background">Background</h2> <p>First, let’s more rigorously define what we mean by <i>clustering</i> and our problem setting as defined by Kleinberg.</p> <div id="clustering"></div> <div class="definition"> <strong>Definition (Clustering).</strong> <br> Suppose we have a set $S$ of $n \geq 2$ observations, which we notate as $S = \{ 1, 2, \dots, n\}$. We'll define a <i>distance function</i> $d: S \times S \rightarrow \mathbb{R}$ as a function satisfying, for all $i,j,k \in S$: <ul> <li>Non-Negativity: $d(i,j) \geq 0$</li> <li>Symmetry: $d(i,j) = d(j,i)$</li> <li>Identity of Indiscernibles: $d(i,j) = 0$ if, and only if, $i = j$</li> <li>Triangle Inequality: $d(i,k) \leq d(i,j) + d(j,k)$</li> </ul> The last condition (Triangle Inequality) is not necessary for the following discussion, but it is a nice property and requiring it makes a choice of $d$ a metric. <br> A <i>clustering function</i>, $f$, is a function operating on a distance function $d$ on $S$ that outputs a partition $\Gamma$ of $S$. A partition is called <i>trivial</i> if each cluster contains one point or if it has only one cluster. </div> <p>We can evaluate a clustering function with what the author terms a <i>clustering quality metric</i>.</p> <div id="cqm"></div> <div class="definition"> <strong>Definition (Clustering Quality Metric).</strong> <br> A <i>clustering-quality measure (CQM)</i> is a function operating on a partition $\Gamma$ of a set $S$ with respect to distance $d$ that outputs a non-negative real number which represents the "goodness" of the clustering $\Gamma$. That is, a CQM $m$ is the function $m: (\Gamma, S, d) \rightarrow \mathbb{R}^+_0$. </div> <p>Finally, we’ll define some helpful terms and quantities that will be used later. Let $\alpha \cdot d$ denote scaling the distance function $d$ by $\alpha$. That is $\alpha \cdot d$ is the distance function who assigns distance $\alpha d(i,j)$ between points $i$ and $j$. We’ll denote all possible output partitions of $f$ as $\text{Range}(f)$. We denote the fact points $i$ and $j$ are in the same cluster in partition $\Gamma$ with $i \underset{\Gamma}{\sim} j$, and we use the notation $i \underset{\Gamma}{\not \sim} j$ if they are not in the same cluster.</p> <div id="rep-set"></div> <div class="definition"> <strong>Definition (Representative Set).</strong> <br> For $\Gamma = \{ \gamma_1, \dots, \gamma_g\}$, $G$ is a <i>representative set</i> of $\Gamma$ if $\rvert G \rvert = g$ and $G\cap \gamma_i \neq \emptyset$ \forall i$. </div> <p>We call a set $G$ a <i>representative set</i> of $\Gamma$ if it contains a single observation from each cluster in $\Gamma$.</p> <div id="isomorphism"></div> <div class="definition"> <strong>Definition (Isomorphism).</strong> <br> Let $\Gamma$ and $\Gamma'$ be partitions of $S$ with $d$. We call $\Gamma$ and $\Gamma'$ <i>isomorphic</i>, denoted by $\Gamma \underset{d}{\approx} \Gamma'$ if there is a <i>distance-preserving isomorphism</i> $\phi: S \rightarrow S$ such that $\forall i,j \in S$, we have $i \underset{\Gamma}{\sim} j$ if, and only if, $\phi(i) \underset{\Gamma'}{\sim} \phi(j)$. </div> <p>A <i>distance-preserving isomorphism</i> can be thought of as a mapping between our source set, $S$, and some target set $\phi(S)$ such that for any $i \in S$, there exists $\phi(i) \in \phi(S)$; for any $i, j \in S$, $d(i, j) = d(\phi(i), \phi(j))$; and there exists an inverse mapping, $\phi’$, such that $\phi’(\phi(i)) = i$.</p> <div id="gamma-transformation"></div> <div class="definition"> <strong>Definition ($\Gamma$-Transformation).</strong> <br> Let $d$ and $d'$ be distance functions on $S$, and let $\Gamma$ be some partition of $S$. We call $d'$ a <i>$\Gamma$-transformation</i> of $d$ if: $$ d'(i,j) \leq d(i,j) \hspace{2mm} \forall \hspace{1mm} i,j \in S \hspace{10mm} \text{s.t. } i \underset{\Gamma}{\sim} j $$ And: $$ d'(i,j) \geq d(i,j) \hspace{2mm} \forall \hspace{1mm} i,j \in S \hspace{10mm} \text{s.t. } i \underset{\Gamma}{\not \sim} j $$ </div> <p>A $\Gamma$-transformation of a distance function $d$ will assign smaller distances to points in the same cluster and larger distances to points in different clusters.</p> <div id="refinement"></div> <div class="definition"> <strong>Definition (Refinement).</strong> <br> Let $\Gamma$ be some partition of $S$. We call another partition $\Gamma'$ of $S$ a <i>refinement</i> of $\Gamma$ if, for every cluster $C' \in \Gamma'$, there is a cluster $C \in \Gamma$ such that $C' \subseteq C$. </div> <p>More intuitively, a refinement is just a finer partition, which means each cluster in $\Gamma$ is either also in $\Gamma’$ or is split into multiple smaller clusters in $\Gamma’$. We can also define the partial order $\Gamma’ \preccurlyeq \Gamma$ if $\Gamma’$ is a refinement of $\Gamma$.</p> <div id="antichain"></div> <div class="definition"> <strong>Definition (Antichain).</strong> <br> An <i>antichain</i> is a collection $\mathcal{A}$ of partitions of $S$ such that for all $\Gamma, \Gamma' \in \mathcal{A}$ where neither $\Gamma$ is a refinement of $\Gamma'$ nor is $\Gamma'$ a refinement of $\Gamma$. We also require that $\Gamma$ and $\Gamma'$ be different partitions. </div> <p>One way I like to conceptualize an antichain is like a collection of subsets where no subset is a subset of any other…but instead of subsets we have partitions.</p> <div id="ab-conforming"></div> <div class="definition"> <strong>Definition ($(a,b)$-Conforming).</strong> <br> Let $\Gamma$ be a partition of $S$. A distance function $d$ is said to <i>$(a,b)$-conform</i> to $\Gamma$ if: $$ d(i,j) \leq a \hspace{2mm} \forall \hspace{1mm} i,j \in S \hspace{10mm} \text{s.t. } i \underset{\Gamma}{\sim} j $$ And: $$ d(i,j) \geq b \hspace{2mm} \forall \hspace{1mm} i,j \in S \hspace{10mm} \text{s.t. } i \underset{\Gamma}{\not \sim} j $$ </div> <div id="gamma-forcing"></div> <div class="definition"> <strong>Definition ($\Gamma$-Forcing).</strong> <br> For a partition $\Gamma$ of $S$ and clustering function $f$, a pair of real numbers $(a, b)$ is called <i>$\Gamma$-forcing</i> with respect to $f$ if $f(d) = \Gamma$ for all distance functions that are $(a,b)$-conforming to $\Gamma$. </div> <hr> <h2 id="the-impossibility-theorem">The Impossibility Theorem</h2> <p>Kleinberg’s impossibility theorem is based upon a set of three axioms that characterize a “good” clustering function, $f$.</p> <div id="axioms"></div> <div class="definition"> <strong>Definition (Kleinberg's Axioms).</strong> <ul> <li> <strong>Scale Invariance:</strong> For any distance function $d$ and positive real $\alpha &gt; 0$, $f(d) = f(\alpha \cdot d)$. A scale-invariant clustering function should not change its output when all distances between points are changed by some factor. </li> <li> <strong>Richness:</strong> $\text{Range}(f)$ should be all possible paritions of $S$. Richness implies that our clustering function is flexible enough that any partition of $S$ can be achieved if we find the right distance function.</li> <li> <strong>Consistency:</strong> For any two distance functions $d$ and $d'$ such that $f(d) = \Gamma$ and $d'$ is a $\Gamma$-transformation of $d$, $f(d') = \Gamma$. A consistent clustering function will output the same partition if we make points within the same cluster closer together and make points in different clusters farther apart.</li> </ul> </div> <p>Kleinberg goes on to show that these axioms imply a semi-surprising result.</p> <div class="theorem"> <strong>Theorem 2.1 (Impossibility Theorem).</strong> <ul id="impossibility-theorem" class="tab" data-tab="394a2543-2ad5-4143-8cf6-89e80373c6fe" data-name="impossibility-theorem"> <li class="active" id="impossibility-theorem-statement"> <a href="#">statement </a> </li> <li id="impossibility-theorem-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="394a2543-2ad5-4143-8cf6-89e80373c6fe" data-name="impossibility-theorem"> <li class="active"> <p>For $n \geq 2$, there does not exists a clustering function $f$ that is scale-invariant, rich, and consistent.</p> </li> <li> <p>The proof of Theorem 2.1 rests upon the claim that a scale-invariant and consistent clustering fucntion $f$ cannot possibly be rich as $\text{Range(f)}$ forms an antichain (Theorem 3.1 in paper).</p> <p>First, suppose we have a consistent clustering function $f$, and let $\Gamma$ be some partition in $\text{Range}(f)$. Because $\Gamma \in \text{Range}(f)$, $\exists d$ such that $f(d) = \Gamma$ (by definition). Define $a’$ as the minimum distance between any two points in the same cluster over all clusters in $\Gamma$, and define $b’$ as the maximum distance between any two points in different clusters over all clusters in $\Gamma$.</p> <p>Select positive real numbers $a, b$ such that $a &lt; b$, $a \leq a’$, and $b \geq b’$. Notice that, for any distance function $d’$ that $(a,b)$-conforms to $\Gamma$ is a $\Gamma$-transformation of $d$. This is due to the fact that $d’(i,j) \leq a \leq a’ \leq d(i,j)$ for $i,j \in S$ such that $i \underset{\Gamma}{\sim} j$ and $d’(i,j) \geq b \geq b’ \geq d(i,j)$ for $i,j \in S$ such that $i \underset{\Gamma}{\not \sim} j$, which is precisely the definition of a $\Gamma$-transformation.</p> <p>Since $d’$ is a $\Gamma$-transformation of $d$ and $f$ is consistent by assumption, $(a,b)$ is $\Gamma$-forcing.</p> <p>Now we further assume that $f$ is scale-invariant and that $\exists \Gamma_0, \Gamma_1 \in \text{Range}(f)$ such that $\Gamma_0$ is a refinement of $\Gamma_1$. Define $(a_0, b_0)$ and $(a_1, b_1)$ be pairs of reals that are $\Gamma_0$-forcing and $\Gamma_1$-forcing, respectively, such that $a_0 &lt; b_0$ and $a_1 &lt; b_1$. Such pairs can be found by taking any two partitions in $\text{Range}(f)$ and setting $\Gamma_0$ and $\Gamma_1$ according to the argument above.</p> <p>Let $a_2$ be some number such that $a_2 \leq a_1$ and set $\epsilon$ such that $0 &lt; \epsilon &lt; \frac{a_0 a_2}{b_0}$. We now construct the distance function $d$ that satisfies the following:</p> <ul> <li>$d(i,j) \leq \epsilon$ for $i,j$ such that $\Gamma_0^i = \Gamma_0^j$</li> <li>$a_2 \leq d(i,j) \leq a_1$ for $i,j$ such that $\Gamma_1^i = \Gamma_1^j$ and $\Gamma_0^i \neq \Gamma_0^j$</li> <li>$d(i,j) \geq b_1$ for $i,j$ such that $\Gamma_1^i \neq \Gamma_1^j$</li> </ul> <p>According to the above conditions, $d$ $(a_1, b_1)$-conforms to $\Gamma_1$. Since $f$ is still assumed to be consistent, this implies $f(d) = \Gamma_1$.</p> <p>Let $\alpha = \frac{b_0}{a_2}$ and $d’ = \alpha \cdot d$. Since $f$ is scale-invariant, $f(d’) = f(d) = \Gamma_1$. However, for $i,j$ such that $\Gamma_0^i = \Gamma_0^j$, $d’(i,j) = \alpha d(i,j) \leq \alpha \epsilon &lt; \frac{a_0 a_2 b_0}{a_2 b_0} = a_0$. In addition, for $i,j$ such that $\Gamma_0^i \neq \Gamma_0^j$, $d’(i,j) = \alpha d(i,j) \geq \alpha a_2 = \frac{b_0 a_2}{a_2} = b_0$. This implies that $d’$ $(a_0, b_0)$-conforms to $\Gamma_0$, which implies that $f(d’) = \Gamma_0$.</p> <p>Since $\Gamma_0$ is a refinement of $\Gamma_1$, they are distinct partitions, so $\Gamma_0 \neq \Gamma_1$, and we arrive at a contradition.</p> </li> </ul> </div> <p>Kleinberg proves an additional theorem that describes the partitions achievable by scale-invariant, consistent clustering functions $f$.</p> <div class="theorem"> <strong>Theorem 3.2 (Characterization Theorem).</strong> <ul id="characterization-theorem" class="tab" data-tab="029e1606-4f69-4156-b1cb-94a601382420" data-name="characterization-theorem"> <li class="active" id="characterization-theorem-statement"> <a href="#">statement </a> </li> <li id="characterization-theorem-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="029e1606-4f69-4156-b1cb-94a601382420" data-name="characterization-theorem"> <li class="active"> <p>Let $\mathcal{A}$ be any antichain of partitions. There exists a scale-invariant, consistent clustering function $f$ such that $\text{Range}(f) = \mathcal{A}$.</p> </li> <li> <p>Kleinberg’s proof uses the <i>sum-of-pairs</i> clustering method which outputs the partition $\Gamma \in \mathcal{A}$ that minimizes $\Phi_d(\Gamma) = \sum_{(i,j) \sim \Gamma} d(i,j)$ where the notation $(i,j) \sim \Gamma$ indicates $i \underset{\Gamma}{\sim} j$.</p> <p>Notice that for any $\alpha &gt; 0$, $\Phi_{\alpha \cdot d}(\Gamma) = \sum_{(i,j) \sim \Gamma} \alpha d(i,j) = \alpha \sum_{(i,j) \sim \Gamma} d(i,j) = \alpha \Phi_{d}(\Gamma)$. Since $\alpha$ is positive, the argmin of $\Phi_{\alpha \cdot d}(\Gamma)$ is equivalent to the argmin of $\Phi_d(\Gamma)$, which implies $f(d) = f(\alpha \cdot d)$ so $f$ is scale-invariant.</p> <p>Fix some $\Gamma \in \mathcal{A}$. Let $d$ be a distance function satisfying:</p> <ul> <li>$d(i,j) &lt; \frac{1}{n^3}$ for $i,j$ such that $i \underset{\Gamma}{\sim} j$</li> <li>$d(i,j) \geq 1$ for $(i,j)$ such that $i \underset{\Gamma}{\not \sim} j$</li> </ul> <p>Notice that $\Phi_d(\Gamma) = \sum_{(i,j) \sim \Gamma} d(i,j) &lt; 1$ since the summation is only over $i,j$ in the same clusters of which there can be, at most, $n^2$ pairs (if there is only one cluster).</p> <p>Also notice that $\Phi_d(\Gamma’) &lt; 1$ only if $\Gamma’$ is a refinement of $\Gamma$. To see why, consider $\Gamma’$ that is <i>not</i> a refinement of $\Gamma$. This implies that there is some cluster $C’ \in \Gamma’$ such that $C’ \not\subseteq C$ for all $C \in \Gamma$. This implies that there exist points $i,j$ such that $i \underset{\Gamma’}{\sim} j$, so they are included in the summation in $\Phi_d(\Gamma’)$, but $i \underset{\Gamma}{\not \sim} j$, so $d(i,j) &gt; 1$. If $\Gamma’$ is a refinement of $\Gamma$, then $i \underset{\Gamma}{\sim} j$, so $d(i,j) &lt; \frac{1}{n^3}$.</p> <p>Because $\mathcal{A}$ is an antichain, there is no refinement $\Gamma’ \in \mathcal{A}$ of $\Gamma$. Thus, $\Gamma = \underset{\Gamma^* \in \mathcal{A}}{\arg\min} \Phi_d(\Gamma^*)$, which implies $f(d) = \Gamma$. Thus, $f$ is rich, since there exists a $d$ such that $f(d) = \Gamma$ for any $\Gamma \in \mathcal{A}$.</p> <p>Let $d$ be a distance function such that $f(d) = \Gamma$, and let $d’$ be a $\Gamma$-transformation of $d$. Furthermore, let $\Gamma’$ be some other partition and define $\Delta(\Gamma’) := \Phi_d(\Gamma’) - \Phi_{d’}(\Gamma’)$. We have that $\Delta(\Gamma) = \sum_{(i,j) \sim \Gamma}d(i,j) - \sum_{(i,j) \sim \Gamma} d’(i,j)$ and $\Delta(\Gamma) = \sum_{(i,j) \sim \Gamma’} d(i,j) - \sum_{(i,j) \sim \Gamma’} d’(i,j)$. Then:</p> \[\begin{aligned} \Delta(\Gamma) &amp;= \sum_{(i,j) \sim \Gamma} d(i,j) - \sum_{(i,j) \sim \Gamma} d'(i,j) \\ &amp;= \sum_{(i,j) \sim \Gamma} d(i,j) - d'(i,j) \\ &amp;\overset{(i)}{\geq} \sum_{(i,j) \sim \Gamma, (i,j) \sim \Gamma'} d(i,j) - d'(i,j) \\ &amp;\overset{(ii)}{\geq} \sum_{(i,j) \sim \Gamma'} d(i,j) - d'(i,j) \\ &amp;= \Delta(\Gamma') \end{aligned}\] <p>We know $d’$ is a $\Gamma$-transformation of $d$. Thus, $d’(i,j) \leq d(i,j)$ for $(i,j) \sim \Gamma$, implying that $d(i,j) - d’(i,j) \geq 0$ for all $(i,j) \sim \Gamma$. $(i)$ follows from the fact that $\rvert (i,j) \sim \Gamma \cap (i,j) \sim \Gamma’ \rvert \leq \rvert (i,j) \sim \Gamma \rvert$.</p> <p>Similarly, $(ii)$ is equivalent to the summation in $(i)$ plus $(i,j) \sim \Gamma’$ such that $(i,j) \not\sim \Gamma$. For these pairs, $d’(i,j) \geq d(i,j)$, since $d’$ is a $\Gamma$-transformation of $d$. This means $d(i,j) - d’(i,j) \leq 0$ for these pairs, which implies $(ii)$.</p> <p>The above argument implies $\Delta(\Gamma) \geq \Delta(\Gamma’)$ for any $\Gamma’ \in \mathcal{A}$. This implies $\Phi_d(\Gamma) - \Phi_{d’}(\Gamma) \geq \Phi_d(\Gamma’) - \Phi_{d’}(\Gamma’)$, which implies $\Phi_d(\Gamma) - \Phi_d(\Gamma’) \geq \Phi_{d’}(\Gamma) - \Phi_{d’}(\Gamma’)$.</p> <p>Since $\Gamma’$ minimizes $\Phi_{d’}$ and $\Gamma$ minimizs $\Phi_{d}$, we have that $\Phi_d(\Gamma) - \Phi_{d’}(\Gamma) \leq 0$. Chaining this together with the previous yields $0 \geq \Phi_d’(\Gamma) - \Phi_{d’}(\Gamma’)$, which implies $\Phi_{d’}(\Gamma’) \geq \Phi_{d’}(\Gamma)$. If $\Gamma’$ minimizes, $\Phi_{d’}$, then it must be the case that $\Gamma’ = \Gamma$. Thus, $f(d’) = \Gamma$, and therefore $f$ is consistent.</p> </li> </ul> </div> <hr> <h2 id="relaxations">Relaxations</h2> <p>Due to the impossibility theorem, it might be worthwhile to look into easing up the conditions in the axioms. Kleinberg provides a few examples.</p> <h4 id="relaxing-richness">Relaxing Richness</h4> <p>Theorem 3.2 is an example of a relaxation of the richness property. If we were satisfied with a clustering function that is scale-invariant, consistent, but only achieves an antichain as its range, then the sum-of-pairs method will work.</p> <h4 id="relaxing-consistency">Relaxing Consistency</h4> <p>Kleinberg proposes <i>Refinement-Consistency</i>, in which a $\Gamma$-transformation should output a refinement of $\Gamma$. Unfortunately, this is not yet enough; a scale-invariant, rich, and refinement-consistent clustering function does not exist. However, if one also relaxes richness to say that all but one (trivial) partition can be achieved by $f$ — termed <i>Near-Richness</i> —, then Theorem 2.1 does not hold.</p> <p>An alternative is to relax consistency to something I’ll call <i>Weak Consistency</i>, which is where if $d’$ is a $f(d)$-transformation of $d$, then either $f(d’)$ is a refinement of $f(d)$ or $f(d)$ is a refinement of $f(d’)$. There do exist clustering functions that satisfy all three of scale-invariance, richness, and weak consistency.</p> <p>In some ways, this relaxation may be more reasonable. For example, consider some partition that results in four clusters arranged in a square. Now, construct a distance function that just puts more distance between the left clusters and the right clusters. Although this new distance function is a $\Gamma$-transformation, it might be better to combine the left clusters and right clusters to have a partition with two groups. Ackerman and Ben-David provide this example as an illustration in Figure 1 of their paper.</p> <h4 id="relaxing-scale-invariance">Relaxing Scale-Invariance</h4> <p>Kleinberg does not discuss a relaxation of scale-invariance at length. He mentions that <i>single-linkage clustering</i> where we stop combining clusters when their distances exceed some value $r$ satisfies consistency and richness. It satisfies a weaker scale-invariance where we let $f(\alpha \cdot d)$ be a refinement of $f(d)$ when $\alpha &gt; 1$. Besides this, I don’t see why relaxing scale-invariance any more than this would be an attractive property of a clustering function.</p> <aside><p>Single-linkage clustering is an agglomerative method in which we start by letting each observation be its own cluster. Then we iteratively combine clusters based upon distances until some stopping criterion is met.</p></aside> <hr> <h2 id="the-consistency-theorem">The Consistency Theorem</h2> <p>Kleinberg’s main result is a theorem (and its proof) stating that there does not exist a clustering function that satisfies three simple and desirable properties. This seems quite disappointing as clustering is a popular subtopic in unsupervised learning, and it seems to imply that clustering is at least impossibly difficult to define precisely if not simply impossible to actually do.</p> <p>Ackerman and Ben-David push back against this interpretation and state that the way in which Kleinberg axiomatized clustering was part of the reason for the impossibility result. They provide a slightly different perspective and provide a set of axioms for the function used to assess clustering quality rather than the clustering function itself.</p> <p>Ackerman and Ben-David adjust Steinberg’s work to apply to CQMs rather than clustering functions, which results in a consistent set of axioms. They also add one additional property in order to make their set of axioms satisfy two properties (<i>soundness</i> and <i>completeness</i>, which I won’t go into here) that make it more useful for defining what methods should be used for clustering.</p> <div id="axioms2"></div> <div class="definition"> <strong>Definition (Ackerman and Ben-David Axioms).</strong> <ul> <li> <strong>Scale Invariance:</strong> A CQM $m$ is called <i>scale-invariant</i> if for every partition $\Gamma$ of set $S$ with respect to distance $d$ and ever $\alpha &gt; 0$, we have $m(\Gamma, S, d) = m(\Gamma, S, \alpha \cdot d)$. </li> <li> <strong>Richness:</strong> A CQM $m$ is called _rich_ if for each non-trivial partitions $\Gamma$ of $S$, there exists distance function $d$ such that $\Gamma = \underset{\Gamma'}{\arg \max} \left[ m(\Gamma', S, d) \right]$. A CQM will satisfy the richness property if, for each non-trivial $\Gamma$, we have $m(\Gamma, S, d) \geq m(\Gamma', S, d)$ for all possible partitions $\Gamma'$ of $S$ with $d$ (which may be chosen for each $\Gamma$). The $\max$ becomes a $\min$, and the inequality is reversed for CQMs that assign lower values to better clusterings.</li> <li> <strong>Consistency:</strong> A CQM $m$ is called <i>consistent</i> if for every $\Gamma$ of $S$ with $d$, $m(\Gamma, X, d') \geq m(\Gamma, S, d)$ for any $\Gamma$-transformation, $d'$, of $d$. This condition is weaker than the consistency defined by Kleinberg as it does not penalize clustering functions that are only weakly consistent (recall that this means $f(d)$ and $f(d')$ are permitted to be refinements of each other).</li> <li> <strong>Isomorphism Invariance:</strong> A CQM $m$ is called <i>isomorphism-invariant</i> if, for all $\Gamma$, $\Gamma'$ of $S$ with $d$ such that $\Gamma$ and $\Gamma'$ are isomorphic, we have $m(\Gamma, S, d) = m(\Gamma', S, d)$. This condition basically states that if we have two clusterings that would be the same if we swapped the points around (in a special way...), they should have the same score according to the CQM.</li> </ul> </div> <p>Ackerman and Ben-David then prove that consistent CQMs exist.</p> <div class="theorem"> <strong> Theorem 2 and 3.<d-cite key="bendavid2008"></d-cite></strong> <ul id="theorem-2-3" class="tab" data-tab="5834fb37-e73e-48d8-89c7-7be44c1a6326" data-name="theorem-2-3"> <li class="active" id="theorem-2-3-statement"> <a href="#">statement </a> </li> <li id="theorem-2-3-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="5834fb37-e73e-48d8-89c7-7be44c1a6326" data-name="theorem-2-3"> <li class="active"> <p>There exists a clustering quality measure that satisfies all four of scale-invariance, richness, consistency. and isomorphism invariance. That is, the four properties comprise a consistent set of axioms.</p> </li> <li> <p>It suffices to construct a CQM and prove it satisfies the three axioms.</p> <p>First, we define the <i>Relative Point Margin</i>. For distance $d$ and clustering $\Gamma$ containing $k$ clusters, the <i>$G$-Relative Point Margin</i> is defined as $G$-\(RM_{S, d}(x) = \frac{d(x, g_x)}{d(x, g'_{x})}\) where $g_x \in G$ is the closest cluster center to $x$, \(g'_{x} \in G\) is the second closest cluster center to $x$, and $G \subseteq S$.</p> <p>Next, define the <i>Relative Margin</i>. Let $\mathcal{G}$ denote the set of all possible representative sets of $\Gamma$. Then the relative margin of $\Gamma$ over $(S, d)$ is $RM_{S, d}(\Gamma) = \underset{G \in \mathcal{G}}{\min} \left{ \underset{x \in S \setminus G}{\textit{avg}} G\text{-}RM_{S, d}(x) \right}$. This is the representative set that achieves the minimum average relative point margin where the average is taken over all points not in the representative set. The relative margin assigns lower values to better clusterings (so the inequalities in the richness and consistency definitions will be reversed).</p> <p>Let $\Gamma$ be an arbitrary clustering of the set $S$ with distance function $d$ in the following.</p> <p><strong>Scale-Invariance.</strong> Let $d’$ be a distance function satisfying $d’(i,j) = \alpha d(i, j)$ for all $i,j \in S$ and all $\alpha &gt; 0$. For any $i,j,k \in S$, we have:</p> \[\frac{d'(i,j)}{d'(i,k)} = \frac{\alpha d(i,j)}{\alpha d(i,k)} = \frac{d(i,j)}{d(i,k)} \implies \frac{d'(i, g_i)}{d'(i, g'_i)} = \frac{d(i, g_i)}{d(i, g'_i)}\] <p>since scaling all distances will result in the same centers. This implies that $RM_{S, d’}(\Gamma) = RM_{S, d}(\Gamma)$.</p> <p><strong>Consistency.</strong> Let $d’$ be a $\Gamma$-transformation of $d$. We have:</p> \[\begin{aligned} \begin{cases} d'(i,j) \leq d(i,j) &amp; \text{for } i \underset{\Gamma}{\sim} j \\ d'(i,j) \geq d(i,j) &amp; \text{for } i \underset{\Gamma}{\not \sim} j \end{cases} &amp;\implies \frac{d'(i,j)}{d'(i,k)} \leq \frac{d(i,j)}{d(i,k)} \text{ for } i \underset{\Gamma}{\sim} j; i \underset{\Gamma}{\not \sim} k \\ &amp;\implies G\text{-}RM_{S,d'}(i) \leq G\text{-}RM_{S,d}(i) \text{ for any } G \in \mathcal{G} \end{aligned}\] <p>The first implication follows from the fact that the closest cluster center to $i$ will be a point in the same cluster, and the second closest cluster center to $i$ will be a point in a different cluster (by the definition of a representative set).</p> <p><strong>Richness.</strong> Let $\Gamma$ be an arbitrary non-trivial clustering of $S$ with $d$. Define the distance function $d$ as:</p> \[\begin{cases} d(i,j) = 1 &amp; \text{for } i \underset{\Gamma}{\sim} j \\ d(i,j) = 10 &amp; \text{for } i \underset{\Gamma}{\not \sim} j \end{cases} \nonumber\] <p>It follows that:</p> \[\begin{aligned} \underset{\Gamma'}{\arg \min} \left\{ RM_{S, d}(\Gamma') \right\} &amp;= \underset{\Gamma'}{\arg \min} \left\{ \underset{G \in \mathcal{G}}{\min} \left[ \underset{x \in S \setminus G}{\textit{avg}} G\text{-}RM_{S,d}(x) \right] \right\} \\ &amp;= \underset{\Gamma'}{\arg \min} \left\{ \underset{G \in \mathcal{G}}{\min} \left[ \frac{1}{\rvert S \setminus G \rvert} \sum_{x \in S \setminus G} \frac{d(x, g_x)}{d(x, g'_x)} \right] \right\} \\ &amp;= \underset{\Gamma'}{\arg \min} \left\{ \underset{G \in \mathcal{G}}{\min} \left[ \frac{1}{\rvert S \setminus G \rvert} \sum_{x \in S \setminus G} \frac{1}{10} \right] \right\} \\ &amp;= \underset{\Gamma'}{\arg \min} \left\{ \underset{G \in \mathcal{G}}{\min} \left[ \frac{1}{10} \right] \right\} \end{aligned}\] <p>We arrive at the fact that, for this choice of $d$, the minimum $RM_{S,d}(\Gamma’)$ is achieved by every non-trivial partition of $S$. Thus, $\Gamma = \underset{\Gamma’}{\arg \min} \left{ RM_{S, d}(\Gamma’) \right}$.</p> <p><strong>Isomorphism Invariance.</strong> Let $\Gamma’$ be a partition such that $\Gamma \underset{d}{\approx} \Gamma’$. Since they are isomorphic, there exists a distance-preserving isomorphism $\phi$. Let $G’ := { \phi(x): x \in G }$, and let $\mathcal{G}’$ be the set of all $G’$. Thus:</p> \[\begin{aligned} \frac{d(i, g_i)}{d(i, g'_i)} = \frac{d(\phi, g_{\phi(i)})}{d(\phi, g'_{\phi(i)})} &amp;\implies G\text{-}RM_{S,d}(i) = G'\text{-}RM_{S,d}(i) \\ &amp;\implies \underset{i \in S \setminus G}{avg} \left\{ G\text{-}RM_{S,d}(i)\right\} = \underset{i \in S \setminus G'}{avg} \left\{ G'\text{-}RM_{S,d}(i) \right\} \\ &amp;\implies \underset{G \in \mathcal{G}}{\min} \left[ \underset{i \in S \setminus G}{avg} \left\{ G\text{-}RM_{S,d}(i)\right\}\right] = \underset{G \in \mathcal{G}}{\min} \left[ \underset{i \in S \setminus G'}{avg} \left\{ G'\text{-}RM_{S,d}(i)\right\}\right] \\ &amp;\implies RM_{S,d}(\Gamma) = RM_{S,d}(\Gamma') \end{aligned}\] </li> </ul> </div> <h2 id="examples-and-extensions">Examples and Extensions</h2> <p>Ackerman and Ben-David provide a few different examples of CQMs that satisfy their axioms.</p> <h4 id="weakest-link">Weakest Link</h4> <p>Suppose we are in a linkage-based regime. Define the <i>Weakest Link Between Points</i> as the following. Let $\gamma_k$ be the $k$-th cluster in $\Gamma$. For $\Gamma$ over $S$ with $d$:</p> \[\Gamma\text{-}WL_{S, d}(i, j) = \underset{x \in \gamma_k \forall k}{\min} \left\{ \max \left[ d(i, x), d(i, j) \right] \right\}\] <p>The <i>Weakest Link</i> of $\Gamma$ over $S$ with $d$ is:</p> \[WL(\Gamma) = \frac{\underset{i \underset{\Gamma}{\sim} j}{\max} \Gamma\text{-}WL_{S,d}(i,j)}{\underset{i \underset{\Gamma}{\not \sim}}{\min} d(i,j)}\] <p>This is calculable in $O(n^3)$ time.</p> <h4 id="additive-margin">Additive Margin</h4> <p>Suppose we are in a center-based clustering (like $k$-means). Define the <i>Additive Point Margin</i> as:</p> \[G\text{-}AM_{S, d}(i) = d(i, \gamma'_i) - d(i, \gamma_i)\] <p>The <i>Additive Margin</i> of $\Gamma$ over $S$ with $d$ is:</p> \[AM_{S,d}(\Gamma) = \underset{G \in \mathcal{G}}{\min} \left\{ \frac{ \frac{1}{\rvert S \rvert} \sum_{i \in S} G\text{-}AM_{S, d}(i) }{ \frac{1}{\rvert \left\{ \{ i, j\} \subseteq S \rvert i \underset{\Gamma}{\sim} j \right\}} \sum_{i \underset{\Gamma}{\sim} j} d(i, j) } \right\}\] <p>This is calculable in $O(n^{k+1})$ time.</p> <h4 id="functions-of-cqms">Functions of CQMs</h4> <p>One can also use functions of clustering-quality measures to create a new one. If one had a CQM defined for partitions of $k$ clusters, one could consider taking the minimum, maximum, or average over all subsets of size $k$ for a clustering of arbitrary size greater than $k$.</p> <h4 id="cluster-number-dependence">Cluster Number Dependence</h4> <p>The above CQMs do not depend on the number of clusters in a partition, which makes it easy to compare clusterings that are of different sizes. Ackerman and Ben-David extend their framework to CQMs that <strong>do</strong> depend on cluster number, such as those that are based upon the objective functions of some clustering methods (such as $k$-means).</p> <p>In order to make these CQMs compliant with the scale-invariance property, the quality scores must be normalized in some way. An example is <i>$\mathcal{L}$-normalization</i>, which scales the loss of a clustering by the loss of a trivial clustering that has a single cluster of all observations.</p> <p>Loss-based clustering functions also tend to either reward or punish more clusters. Ackerman and Ben-David term CQMs based on a loss function that “prefers” more clusters as <i>refinement-preferring</i> and those based on losses that prefer fewer clusters as <i>coarsening-preferring</i>. More explicitly, a refinement-preferring CQM will assign a better quality score to refinements of $\Gamma$ than to $\Gamma$, and a coarsening-preferring CQM will assign better quality scores to $\Gamma$ than to its refinements. These CQMs do not satisfy the richness property.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2025-01-13-impossibility-clustering.bib"></d-bibliography> <d-article> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Anna Rosengart. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: February 02, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?d4c3ed73337d78e34b10d24890d1fc56"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/tabs.min.js?b8748955e1076bbe0dabcf28f2549fdc"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>