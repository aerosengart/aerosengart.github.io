<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Betting-Based Confidence Sequences | Anna Rosengart </title> <meta name="author" content="Anna Rosengart"> <meta name="description" content=""> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://aerosengart.github.io/paper-notes/betting-bounds/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Betting-Based Confidence Sequences",
            "description": "",
            "published": "May 21, 2025",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Anna Rosengart </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">curriculum vitae </a> </li> <li class="nav-item "> <a class="nav-link" href="/music/">music </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/stats-ml/">stats &amp; ml</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/measure-theory/">measure theory</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/paper-notes/">paper notes</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Betting-Based Confidence Sequences</h1> <p></p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#background">Background</a> </div> <ul> <li> <a href="#notation">Notation</a> </li> <li> <a href="#confidence-sets-and-sequences">Confidence Sets and Sequences</a> </li> </ul> <div> <a href="#results">Results</a> </div> <div> <a href="#predictable-plug-ins">Predictable Plug-Ins</a> </div> <ul> <li> <a href="#hoeffding">Hoeffding</a> </li> <li> <a href="#empirical-bernstein">Empirical Bernstein</a> </li> </ul> <div> <a href="#a-betting-perspective">A Betting Perspective</a> </div> <ul> <li> <a href="#hedging-our-bets">Hedging Our Bets</a> </li> </ul> </nav> </d-contents> <p>A colleague introduced me to some recent work from Waudby-Smith and Ramdas here at Carnegie Mellon. Since I’ve been working on applications of concentration bounds, it certainly seems important to review their paper <i>Estimating means of bounded random variables by betting</i>, which can often achieve better bounds than the older work from Maurer and Pontil<d-cite key="maurer2009"></d-cite>. Here, I’m going to use the preprint, but their work has been published in JRSSB<d-cite key="waudbysmith2022"></d-cite>.</p> <p>This paper (among many others<d-cite key="ramdas2022"></d-cite><d-cite key="ramdas2023"></d-cite><d-cite key="shafer2019"></d-cite><d-cite key="howard2021"></d-cite>) set inference in the context of playing a betting game with two players. For each “round”, we obtain an observation (i.e. a realization of some random variable). The Forecaster assigns a probability to the observations, and the Skeptic places a bet of the form of a real-valued function of the observations. In each “round”, the Skeptic pays a cost equal to the expected value of the observation and earns a payout of the observed value.</p> <p>A lot of the discussion relies on measure and probability theory. I’ve covered most of the basic concepts in <a href="/posts/2025/04/30/measure-theory">my post on measure theory</a>, so I’ll refer the reader to that if you are looking for definitions and intuition for the relevant concepts here.</p> <p>Note: Not all of the proofs are finished/included. I am hoping to find the time to return to this post and complete them.</p> <hr> <h2 id="background">Background</h2> <p>The paper focuses on constructing valid (and tight) confidence sets for the mean of a sequence of bounded random variables.</p> <h3 id="notation">Notation</h3> <p>Here, we’ll recap the important notation introduced in Waudby-Smith and Ramdas<d-cite key="waudbysmith2022"></d-cite>. We denote the set of all distributions on $[0, 1]$ with mean $m$ with $\mathcal{Q}^m$.</p> <p>Let \((X_t)_{t = 1}^\infty \sim P\) be a sequence of random variables taking values in $[0, 1]$ with (unknown) conditional mean $\mu \in [0, 1]$ where $P \in \mathcal{P}^\mu$ (we use the notation we defined earlier; i.e. $\mathcal{P}^\mu$ is the set of all distributions on $[0, 1]^\infty$ such that \(\mathbb{E}_P[X_t \rvert X_1, \dots, X_{t-1}] = \mu\)).</p> <p>We’ll use $(a \pm b)$ as an abbreviation of the interval $(a-b, a+b)$. We’ll also use $X_1^t$ to denote $(X_1, \dots, X_t)$. Let \(\mathcal{F}_t = \sigma(X_1^t)\) denote the $\sigma$-field generated by $X_1^t$. We’ll suppose that $\mathcal{F}_0$ is the trivial $\sigma$-field (that is, the $\sigma$-field containing the empty set and the entire set itself). We’ll use \(\mathcal{F} = \{ \mathcal{F}_t \}_{t = 0}^\infty\) to denote the canonical filtration of $X_1^t$ where \(\mathcal{F}_0 \subset \mathcal{F}_1 \subset \mathcal{F}_2 \subset \dots\).</p> <p>A stochastic process $(M_t)_{t = 0}^\infty$ adapted to canonical filtration $\mathcal{F}$ with $M_0 = 1$ is termed a <i>test supermartingale for $P$</i> if:</p> \[\mathbb{E}_P[M_t \rvert \mathcal{F}_{t - 1}] \leq M_{t-1} \hspace{5mm} \forall t \geq 1\] <p>If instead, \(\mathbb{E}_P[M_t \rvert \mathcal{F}_{t - 1}] = M_{t-1}\), then the process is called a <i>test martingale</i>. If either of these hold for all $P \in \mathcal{P}$, then we say that \((M_t)_{t = 0}^\infty\) is a <i>test (super)martingale for $\mathcal{P}$</i>. Finally, a sequence \((\lambda_t)_{t = 1}^\infty\) is called <i>predictable</i> if $\lambda_t$ is $\mathcal{F}_{t-1}$-measurable for all $t \geq 1$.</p> <h3 id="confidence-sets-and-sequences">Confidence Sets and Sequences</h3> <p>Recall that for some random sample $(X_t)_1^n \sim P$ and error tolerance $\alpha \in (0, 1)$, a $(1 - \alpha)$ confidence interval (CI) for $\mu$ is given by the random set (which is dependent upon our sample) such that the true conditional $\mu$ is contained in our interval with probability at least $1 - \alpha$ for any distribution in family $\mathcal{P}^\mu$.</p> \[C_n = C(X_1, \dots, X_n) \subseteq [0, 1] \hspace{5mm} \text{such that } \forall n \geq 1, \underset{P \in \mathcal{P}^\mu}{\inf} \left\{ P(\mu \in C_n) \right\} \geq 1 - \alpha\] <p>Hoeffding’s inequality leads to the CI:</p> \[C_n = \left( \bar{X}_n \pm \sqrt{\frac{\log(2/\alpha)}{2n}} \right) \cap [0, 1]\] <p>where the $n$ comes from the fact that $X_i \in [0, 1]$, and the intersection restricts our interval to the known possible values that $\mu$ can take on.</p> <p>An interval implies a contiguous region of $\mathbb{R}$, but we could have the union of disjoint subsets. This is why we use the term “confidence set”.</p> <p>A confidence set is constructed for some sample of fixed size $n$. An alternative scenario is that we are observing data in batches, and we want to construct a new confidence set for each batch such that the sequence of sets $(C_t)_{t=1}^\infty$ satisfies:</p> \[\underset{P \in \mathcal{P}^\mu}{\sup} \left\{ P(\exists t \geq 1: \mu \neq C_t) \right\} \leq \alpha\] <p>In words, we want the sequence of sets to be such that the probability of there being some set that does not contain $\mu$ to be at most $\alpha$ for any distribution in the family $\mathcal{P}^\mu$. We call such a $(C_t)_{t=1}^\infty$ a <i>confidence sequence</i>, and, ideally, we want $\underset{t \rightarrow \infty}{\lim} C_t = { \mu }$ (that is, we want the sequence to converge to the confidence set that is only the true value as we observe more and more data).</p> <p>Notably, the above definitions hold for arbitrary stopping times, $\tau$. Since these can, and are often desired to be, data-dependent, we need a way to decide when our data is sufficient enough to stop constructing our sequences (we have achieved the result we wanted, we have enough data, etc.).</p> <div id="sequential-test"></div> <div class="definition"> <strong>Definition (Level-$\alpha$ Sequential Test).</strong> <br> Let $H_0$ be some hypothesis, and let $\alpha \in [0, 1]$ be some tolerance. A <i>level-$\alpha$ sequential test</i> is a sequence $(\psi_t)_{t \in T}$ (with index set $T$) taking values in $\{ 0, 1 \}$ such that $\mathbb{P}_{H_0}(\psi_\tau = 1) \leq \alpha$ for any arbitrary stopping time, $\tau$. </div> <p>A level-$\alpha$ sequential test will control the Type I error rate at $\alpha$ for <i>any stopping time</i>. This is by definition; the probability that $\psi_\tau$ outputs a $1$ is at most $\alpha$ under the null hypothesis.</p> <hr> <h2 id="results">Results</h2> <p>The first theorem is a procedure to construct a confidence set for the mean of a sequence of bounded random variables using supermartingales.</p> <div id="theorem-1"></div> <div class="theorem"> <strong>Theorem 1.<d-cite key="waudbysmith2022"></d-cite></strong> <ul id="ws-theorem1" class="tab" data-tab="1e69fe85-b7f3-45c4-83d6-cdcf2876e61c" data-name="ws-theorem1"> <li class="active" id="ws-theorem1-statement"> <a href="#">statement </a> </li> <li id="ws-theorem1-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="1e69fe85-b7f3-45c4-83d6-cdcf2876e61c" data-name="ws-theorem1"> <li class="active"> <p>Let \((X_t)_{t = 1}^\infty \sim P\) be a sequence of (infinitely or finitely many) random variables taking values in $[0, 1]$ with unknown conditional mean $\mu$ and distribution \(P \sim \mathcal{P}^\mu\) where \(\mathcal{P}^\mu\) is the collection of all distributions on \([0, 1]^\infty\) such that \(\mathbb{E}_P\left[ X_t \rvert X_1 \dots X_{t-1}\right]\).</p> <ol> <li>For each $m \in [0, 1]$, we have a null hypothesis $H_0^m: P \in \mathcal{P}^m$.</li> <li>For each $m \in [0, 1]$, construct the non-negative process $M_t^m = M^m(X_1, \dots, X_t)$ (a function of $X_1, \dots, X_t$) such that $(M_t^\mu)_{t = 0}^\infty$ satisfies for each $P \in \mathcal{P}^\mu$, $(M_t^\mu)_{t = 0}^\infty$ is upperbounded by a test supermartingale for $P$. Note that this test supermartingale can change with $P$.</li> <li>For each $m \in [0, 1]$, let $(\phi_t^m)_{t = 1}^\infty$ denote the sequential test where $\phi_t^m = \mathbb{1}\left\{ M_t^m \geq \frac{1}{\alpha} \right\}$. Here, $\phi_t^m = 1$ indicates rejection of $H_0^m$ after $t$ observations.</li> <li>Define $C_t = \left\{ m \in [0, 1] \rvert \phi_t^{m} = 0 \right\}$ as the set of $m \in [0, 1]$ such that $\phi_t^m$ failes to reject $H_0^m$.</li> </ol> <p>$(C_t)_{t = 1}^\infty$ forms a $(1 - \alpha)$-confidence sequence fo $\mu$. That is, the following holds:</p> \[\underset{P \in \mathcal{P}}{\sup} \left\{ \mathbb{P}(\exists t \geq 1 \rvert \mu \not\in C_t) \right\} \leq \alpha\] </li> <li> <p>The proof is fairly simple and relies mainly on Ville’s inequality.</p> <p>For any $m \in [0, 1]$, by Step 2, if $M_t^m \geq \frac{1}{\alpha}$, then we know $\phi_t^m = 1$. Thus, under any $P \in \mathcal{P}^\mu$, the probability $\mathbb{P}_P(\exists t \geq 1 \rvert \phi_t^m = 1)$ is the same as the probability $\mathbb{P}_P\left(\exists t \geq 1 \rvert M_t^m \geq \frac{1}{\alpha}\right)$.</p> <p>By construction, $(M_t^\mu)_{t = 0}^\infty$ is upperbounded by some test supermartingale for each $P \in \mathcal{P}^\mu$, so we can apply Ville’s inequality to get:</p> \[\mathbb{P}_P(\exists t \geq 1 \rvert \phi_t^m = 1) = \mathbb{P}_P\left(\exists t \geq 1 \rvert M_t^m \geq \frac{1}{\alpha}\right) \leq \alpha \nonumber\] <p>Thus, $(\phi_t^m)_{t = 1}^\infty$ is a level $\alpha$ sequential hypothesis test.</p> <p>The construction of $(C_t)_{t=0}^\infty$ entails only choosing those $m \in [0, 1]$ such that $H_0^m$ is rejected (for a particular choice of $t$). Thus, there is some $t \geq 1$ such that $\mu \not\in C_t$ if, and only if, there is some $t \geq 1$ such that $\phi_t^\mu = 1$.</p> <p>The above holds for any $P \in \mathcal{P}^\mu$, which yields the desired result:</p> \[\underset{P \in \mathcal{P}^\mu}{\sup} \left\{ \mathbb{P}_P(\exists t \geq 1 \rvert \mu \not\in C_t) \right\} = \underset{P \in \mathcal{P}^\mu}{\sup} \left\{ \mathbb{P}_P(\exists t \geq 1 \rvert \phi_t^\mu = 1) \right\} \leq \alpha \nonumber\] </li> </ul> </div> <p>A few notes are worth mentioning. First, the process $(M_t^m)_{t = 0}^\infty$ that is constructed in Step 2 of the above theorem is an example of an <i>$e$-process</i> for $\mathcal{P}$ (see <a href="/posts/2025/martingales">my post on martingales</a>). Secondly, the power and utility of confidence sequences constructed in this way depends upon the choice of upperbounding test supermartingales. Poor choices can lead to bad performance (statistically speaking) and computational infeasibility. Lastly, because the confidence sequence is time-uniform (holds over all time points), we can pretend we have a fixed stopping point to derive a valid $(1-\alpha)$-confidence set/interval for $\mu$ for a fixed sample size (no more batches are gathered).</p> <hr> <h2 id="predictable-plug-ins">Predictable Plug-Ins</h2> <p>The main results of this paper are based on the <i>predictable plug-in</i>, which is a sequence of predictable real-valued random variables that are chosen specially to be plugged into other sequences in order to form test (super)martingales. The choice of predictable plug-in depends upon the following Lemma.</p> <div id="lemma-1"></div> <div class="theorem"> <strong>Lemma 1: Predictable Plug-In Chernoff Supermartingales.<d-cite key="waudbysmith2022"></d-cite></strong> <ul id="ws-lemma1" class="tab" data-tab="2f450637-2751-41c5-8a16-abae53e612e0" data-name="ws-lemma1"> <li class="active" id="ws-lemma1-statement"> <a href="#">statement </a> </li> <li id="ws-lemma1-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="2f450637-2751-41c5-8a16-abae53e612e0" data-name="ws-lemma1"> <li class="active"> <p>Let \((X_t)_{t = 1}^\infty \sim P\) be a sequence of random variables. Let $\mathcal{F}$ denote the canonical filtration with respect to \((X_t)_{t =1}^\infty\).</p> <p>Suppose that, for some choice of $\mu$, $v_t$, and $\psi(\lambda)$, the following is satisfied, for each $t \geq 1$, for any $\lambda \in \Lambda \subseteq \mathbb{R}$:</p> \[\mathbb{E}_P\left[ \exp\left(\lambda(X_t - \mu) - v_t \psi(\lambda) \right) \rvert \mathcal{F}_{t-1} \right] \leq 1\] <p>For any $(\lambda_t)_{t=1}^\infty$ with $\lambda_t \in \Lambda$ for all $t$ that is predictable with respect to $\mathcal{F}$, we have that:</p> \[M_t^\psi(\mu) = \prod_{i = 1}^t \exp\left( \lambda_i(X_i - \mu) - v_i \psi(\lambda_i) \right)\] <p>is a test supermartingale with respect to $\mathcal{F}$.</p> </li> <li> <p>I am not sure how to prove it, but I think that the process defined above as \(M_t^\psi(\mu)\) is an example of Bernstein’s supermartingale<d-cite key="shafer2019"></d-cite> and is therefore a supermartingale. It suffices now to show that \(M_t^\psi(\mu)\) has expectation at most $1$ at $t = 1$ and that \(\mathbb{E}_P [ M_t^\psi(\mu) \rvert \mathcal{F}_{t-1} ] \leq M_{t-1}^\psi(\mu)\).</p> <p>Notice that, for $t = 1$, the conditional expectation of $M_t^\psi(\mu)$ is:</p> \[\mathbb{E}_P \left[M_1^\psi(\mu) \bigg\rvert \mathcal{F}_0 \right] = \mathbb{E}_P\left[ \exp\left(\lambda_1(X_1 - \mu) - v_1\psi(\lambda_1) \right) \right] \leq 1\] <p>by assumption. Because $\mathcal{F}_0$ is the trivial $\sigma$-field, which gives us no information, we drop the conditioning.</p> <p>Now consider $t \geq 2$. For any $i \leq t - 1$, \(\exp(\lambda_i(X_i - \mu) - v_i \psi(\lambda_i))\) is \(\mathcal{F}_{t-1}\)-measurable. This is because \((\lambda_t)_{t=1}^\infty\) is predictable with respect to $\mathcal{F}$, implying $\lambda_i$ is \(\mathcal{F}_{t-1}\)-measurable. To see why, consider probability space \((\Omega, \mathcal{A}, P)\) and sub-$\sigma$-field \(\mathcal{A}_1 \subset \mathcal{A}\). If a random variable $X$ is \(\mathcal{A}_1\)-measurable, then for any Borel set $B$, \(X^{-1}(B) \in \mathcal{A}_1 \subset \mathcal{A}\). Thus, $X$ is also \(\mathcal{A}\)-measurable. In addition, \(\mathcal{F}\) is the canonical filtration, so $X_i$ must be \(\mathcal{F}_{t - 1}\)-measurable since \(\mathcal{F}_{t-1} = \sigma(X_1, \dots, X_{t-1})\).</p> <p>Since \(\exp(\lambda_i(X_i - \mu) - v_i \psi(\lambda_i))\) is $\mathcal{F}_{t-1}$-measurable for all $i \leq t-1$, we know that \(\exp(\lambda_i(X_i - \mu) - v_i \psi(\lambda_i)) \in \mathcal{F}_{t-1}\). Since we are dealing with bounded random variables, we can use Theorem 4.1.14 from Durrett<d-cite key="durrett2019"></d-cite> to write the conditional expectation of $M_t^\psi(\mu)$ as:</p> \[\begin{aligned} \mathbb{E}_P \left[ M_t^\psi(\mu) \bigg\rvert \mathcal{F}_{t-1} \right] &amp;= \mathbb{E}_P \left[ \prod_{i = 1}^t \exp\left(\lambda_i(X_i - \mu) - v_i \psi(\lambda_i) \right) \bigg\rvert \mathcal{F}_{t-1}\right] \\ &amp;= \prod_{i = 1}^{t-1} \exp(\lambda_i(X_i - \mu) - v_i\psi(\lambda_i)) \underbrace{\mathbb{E}_P\left[ \exp\left( \lambda_t (X_t - \mu) - v_t \psi(\lambda_t)\right) \rvert \mathcal{F}_{t-1} \right]}_{\leq 1} \\ &amp;\leq \prod_{i = 1}^{t-1} \exp(\lambda_i(X_i - \mu) - v_i\psi(\lambda_i)) \\ &amp;= M_{t-1}^\psi(\mu) \end{aligned}\] <p>Since \(\mathbb{E}_P[M_t^\psi(\mu) \rvert \mathcal{F}_{t-1}] \leq M_{t-1}^\psi(\mu)\) for all $t \geq 1$, and \(\mathbb{E}_P[M_1^\psi(\mu) \rvert \mathcal{F}_0] \leq 1\), \((M_t^\psi(\mu))_{t = 1}^\infty\) is a test supermartingale.</p> </li> </ul> </div> <h3 id="hoeffding">Hoeffding</h3> <p>Let’s first define the <i>Hoeffding process</i>.</p> <div id="hoeffding-process"></div> <div class="definition"> <strong>Definition (Hoeffding Process).</strong> <br> Let $(X_t)_{t = 1}^\infty \sim P$ be a sequence of random variables taking values in $[0, 1]$ with distribution $P \in \mathcal{P}^\mu$ where $\mathcal{P}$ is the set of distributions on $[0, 1]^\infty$ such that $\mathbb{E}_P[X_t \vert \mathcal{F}_{t-1}] = \mu$ for each $t$. <br> The <i>Hoeffding process</i> for (<i>candidate mean</i>) $m \in [0, 1]$, denoted by $(M_t^H(m))_{t=0}^\infty$ is defined as: $$ M_t^{\text{PrPl-H}}(m) = \prod_{i = 1}^t \exp\left( \lambda_i(X_i - m) - \phi_H(\lambda_i)\right) $$ where $\psi_H(\lambda) = \frac{\lambda^2}{8}$ is an upper bound on the cumulant generating function for random variables in $[0, 1]$, and $(\lambda_t)_{t = 1}^\infty$ is a sequence of <i>predictable</i> $\lambda_t \in \mathbb{R}$ that is constructed. We also assume $M_0^{\text{PrPl-H}}(m) = 1$. </div> <p>This sequence is a test supermartingale for $\mathcal{P}^m$, and we call $(\lambda_t)_{t=1}^\infty$ a <i>predictable plug-in</i>.</p> <p>This result implies that if one can be clever about their choice of $(\lambda_t)_{t=1}^\infty$, one can construct a test supermartingale that can be used in Step 2 of <a href="#theorem-1">Theorem 1</a> to get a confidence sequence for $\mu$. This is formalized in the following:</p> <div id="proposition-1"></div> <div class="theorem"> <strong>Proposition 1: Predictable Plug-In Hoeffding CS.<d-cite key="waudbysmith2022"></d-cite></strong> <ul id="ws-prop1" class="tab" data-tab="0d150d42-42e5-4407-b26d-333df4ede4f0" data-name="ws-prop1"> <li class="active" id="ws-prop1-statement"> <a href="#">statement </a> </li> <li id="ws-prop1-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="0d150d42-42e5-4407-b26d-333df4ede4f0" data-name="ws-prop1"> <li class="active"> <p>Let \((X_t)_{t=1}^\infty \sim P\) be a stochastic process with distribution \(P \in \mathcal{P}^\mu\) where \(\mathcal{P}^\mu\) is the set of distributions on \([0, 1]^\infty\) such that \(\mathbb{E}_P[X \rvert \mathcal{F}_{t-1}] = \mu\) for each $t$. For any predictable \((\lambda_t)_{t=1}^\infty\) where \(\lambda_t \in \mathbb{R}\) for all $t$, we can construct a $(1-\alpha)$ confidence sequence for $\mu$ as:</p> \[C_t^{\text{PrPl-H}} = \left(\frac{\sum_{i =1 }^t \lambda_i X_i }{\sum_{i = 1}^t \lambda_i} \pm \frac{\log(2/\alpha) + \sum_{i = 1}^t \psi_H(\lambda_i)}{\sum_{i = 1}^t \lambda_i} \right)\] <p>The running intersection, \(\cap_{i \leq t} C_t^\text{PrPl-H}\), is also a valid $(1-\alpha)$ confidence sequence for $\mu$.</p> </li> <li> <p>Proof to be completed.</p> </li> </ul> </div> <p>The authors recommend choosing $(\lambda_t)_{t=1}^\infty$ such that:</p> \[\lambda_t^{PrPl-H} = \min \left\{ \sqrt{\frac{8 \log(2/\alpha)}{t \log(t + 1)}}, 1 \right\}\] <h3 id="empirical-bernstein">Empirical Bernstein</h3> <p>Similar to the Hoeffding process, we can define the <i>empirical Bernstein process</i>.</p> <div id="empirical-bernstein-process"></div> <div class="definition"> <strong>Definition (Empirical Bernstein Process).</strong> <br> Let $(X_t)_{t = 1}^\infty \sim P$ be a sequence of random variables taking values in $[0, 1]$ with distribution $P \in \mathcal{P}^\mu$ where $\mathcal{P}$ is the set of distributions on $[0, 1]^\infty$ such that $\mathbb{E}_P[X_t \vert \mathcal{F}_{t-1}] = \mu$ for each $t$. The <i>empirical Bernstein process</i> for (<i>candidate mean</i>) $m \in [0, 1]$, denoted by $(M_t^{EB}(m))_{t=0}^\infty$ is defined as: $$ M_t^{EB}(m) = \prod_{i = 1}^t \exp\left( \lambda_i(X_i - m) - v_i \psi_{EB}(\lambda_i) \right) $$ where $v_i = 4(X_i - \hat{\mu}_{i - 1})^2$, $\psi_{EB}(\lambda) = (-\log(1-\lambda) - \lambda)/4$ for $\lambda \in [0, 1)$, and $(\lambda_t)_{t = 1}^\infty$ is a sequence of <i>predictable</i> $\lambda_t \in \mathbb{R}$ that is constructed. </div> <p>One can perform a similar procedure as before, using $(M_t^\text{PrPl-EB}(m))$ in Step 2 of <a href="#theorem-1">Theorem 1</a> to get a confidence sequence.</p> <div id="theorem-2"></div> <div class="theorem"> <strong>Theorem 2: Predictable Plug-In Empirical Bernstein CS.<d-cite key="waudbysmith2022"></d-cite></strong> <ul id="ws-theorem2" class="tab" data-tab="436c3dee-3eb4-4b5a-909b-367302f8022f" data-name="ws-theorem2"> <li class="active" id="ws-theorem2-statement"> <a href="#">statement </a> </li> <li id="ws-theorem2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="436c3dee-3eb4-4b5a-909b-367302f8022f" data-name="ws-theorem2"> <li class="active"> <p>Let \((X_t)_{t=1}^\infty \sim P\) be a stochastic process with distribution \(P \in \mathcal{P}^\mu\) where \(\mathcal{P}^\mu\) is the set of distributions on \([0, 1]^\infty\) such that \(\mathbb{E}_P[X \rvert \mathcal{F}_{t-1}] = \mu\) for each $t$. For any predictable \((\lambda_t)_{t=1}^\infty\) where $\lambda_t \in (0, 1)$ for all $t$, we can construct a $(1-\alpha)$ confidence sequence for $\mu$ as:</p> \[C_t^{\text{PrPl-EB}} = \left(\frac{\sum_{i =1 }^t \lambda_i X_i }{\sum_{i = 1}^t \lambda_i} \pm \frac{\log(2/\alpha) + \sum_{i = 1}^t v_i \psi_{EB}(\lambda_i)}{\sum_{i = 1}^t \lambda_i} \right)\] <p>The running intersection, \(\cap_{i \leq t} C_t^\text{PrPl-EB}\), is also a valid $(1-\alpha)$ confidence sequence for $\mu$.</p> </li> <li> <p>Proof to be completed.</p> </li> </ul> </div> <p>The authors recommend choosing $(\lambda_t)_{t=1}^\infty$ such that:</p> \[\begin{equation} \label{eq:empirical-bernstein-choices} \lambda_t^{PrPl-EB} = \min \left\{ \sqrt{\frac{2\log(2/\alpha)}{\hat{\sigma}_{t-1}^2 t \log(1 + t)}}, c \right\} \end{equation}\] <p>where:</p> \[\begin{aligned} \hat{\sigma}_t^2 &amp;= \frac{\frac{1}{4} + \sum_{i = 1}^t (X_i - \hat{\mu}_i)^2}{t+1} \\ \hat{\mu}_t &amp;= \frac{\frac{1}{2} + \sum_{i = 1}^t X_i}{t + 1} \\ c &amp;= \frac{1}{2} \text{ or } \frac{3}{4} \end{aligned}\] <p>This procedure yields two sequences, \((\hat{\mu}_t)_{t=1}^\infty\) and \((\hat{\sigma}_t^2)_{t=1}^\infty\), which can be thought of as predictable, regularized sample means and variances.</p> <p>For fixed time, we can simply use the running intersection to form a $(1-\alpha)$ confidence interval for $\mu$. Suppose we have $n$ samples. Then we can form a confidence interval as $C_n^{\text{PrPl-EB_CI}} = \cap_{i \leq n} C_i^{\text{PrPl-EB}}$ with any predictable sequence $(\lambda_t)_{t = 1}^\infty$. The authors recommend choosing:</p> \[\lambda_t^{\text{PrPl-EB}(n)} = \min \left\{ \sqrt{\frac{2 \log(2/\alpha)}{n \hat{\sigma}_{t-1}^2}} \right\}\] <p>with $\hat{\sigma}_i^2$ and $c$ as in Eq. \eqref{eq:empirical-bernstein-choices}.</p> <p>Something very interesting is that the width of the above confidence interval goes to $\sigma \sqrt{2 \log(2/\alpha)}$ for i.i.d. data (where $\sigma$ is the true standard deviation). In contrast, the original empriical Bernstein confidence intervals introduced by Maurer and Pontil (2009) only go to $\sigma \sqrt{2 \log(4 / \alpha)}$.</p> <hr> <h2 id="a-betting-perspective">A Betting Perspective</h2> <p>In order to improve the confidence sequences derived above, the authors dive deeper into the gambling analogy. Suppose we are playing a game in which we can accumulate wealth by making bets against some hypothesis, $H_0^m$, being true. We start off with one dollar, and, in each “round”, we will make a bet that is determined by our predictable sequence $(\lambda_t^m)_{t=1}^\infty$. More specifically, we make bet $b_t = s_t\rvert \lambda_t^m \rvert$ where $s_t = -1$ if we think $\mu &lt; m$ and $s_t = 1$ if we think $\mu &gt; m$. The second term, $\rvert \lambda_t^m \rvert$, is the amount we are willing to lose/win in our bet (e.g. $\rvert \lambda_t^m \rvert = 0$ implies we are risking nothing).</p> <p>We can conceptualize the total wealth we have at any given time with the <i>capital process</i>.</p> <div id="capital-process"></div> <div class="definition"> <strong>Definition (Capital Process).</strong> <br> Let $(X_t)_{t = 1}^\infty \sim P$ be a sequence of random variables taking values in $[0, 1]$ with distribution $P \in \mathcal{P}^\mu$ where $\mathcal{P}$ is the set of distributions on $[0, 1]^\infty$ such that $\mathbb{E}_P[X_t \vert \mathcal{F}_{t-1}] = \mu$ for each $t$. <br> The <i>capital process</i> for any $m \in [0, 1]$ is defined as: $$ \mathcal{K}_t(m) = \prod_{i = 1}^t ( 1 + \lambda_i(m) \cdot (X_i - m)) $$ where $\mathcal{K}_0(m) = 1$ and $(\lambda_t(m))_{t = 1}^\infty$ is a predictable sequence taking values in $\left(-\frac{1}{1-m}, \frac{1}{m}\right)$ (where $\frac{1}{m} = \infty$ if $m = 0$ and $\frac{1}{1-m} = \infty$ if $m = 1$). </div> <p>There is a deep connection between test (super)martingales, bounded random variables, and the capital process that the authors emphasize. This is formalized below.</p> <div id="proposition-2"></div> <div class="theorem"> <strong>Proposition 2.<d-cite key="waudbysmith2022"></d-cite></strong> <ul id="ws-prop2" class="tab" data-tab="c742e173-4708-4e99-b531-5a045eafe60d" data-name="ws-prop2"> <li class="active" id="ws-prop2-statement"> <a href="#">statement </a> </li> <li id="ws-prop2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="c742e173-4708-4e99-b531-5a045eafe60d" data-name="ws-prop2"> <li class="active"> <p>Let $X_1, X_2, \dots \sim P$ be a sequence of random variables taking values in $[0, 1]$, and let $\mu \in [0, 1]$. The following are equivalent:</p> <ol> <li>$\mathbb{E}_P[X_t \rvert \mathcal{F}_{t-1}] = \mu$ for all $t \in \mathbb{N}$ where $\mathcal{F}_{t-1} = \sigma(X_1, \dots, X_{t-1})$</li> <li>There exists $\lambda \in \mathbb{R} \setminus \{ 0 \}$ such that the capital process $(\mathcal{K}_t(\mu))_{t = 0}^\infty$ is a strictly positive test martingale for $P$</li> <li>For any $\lambda \in \left(-\frac{1}{1-\mu}, \frac{1}{\mu} \right)$, the capital process $(\mathcal{K}_t(\mu))_{t = 0}^\infty$ is a test martingale for $P$</li> <li>For all $\left(-\frac{1}{1-\mu}, \frac{1}{\mu} \right)$-valued predictable sequence $(\lambda_t)_{t = 1}^\infty$, the capital process $(\mathcal{K}_t(\mu))_{t = 0}^\infty$ is a test martingale for $P$</li> </ol> </li> <li> <p>Proof to be completed.</p> </li> </ul> </div> <p>The main takeaway is that \((\mathcal{K}_t(\mu))_{t=0}^\infty\) is a test martingale (<i>not</i> supermartingale) for $P$ if the random variables have conditional expectation equal to $\mu$. If the hypothesis $H_0^m$ is true (i.e. $m = \mu$), then, by <a href="#proposition-2">Proposition 2</a>, we expect to not make any money since the conditional expectation is constant at every round (i.e. \(\mathbb{E}_P[X_t \rvert \mathcal{F}_{t-1}] = \mu\) for all $t \in \mathbb{N}$). However, if nature is wrong and $m \neq \mu$, then, by Ville’s inequality, the probability of ever earning at least $\frac{1}{\alpha}$ capital is at most $\alpha$.</p> <p>This naturally extends to hypothesis testing: under the null hypothesis, the probability of earning a lot of money is very low, so we should reject $H_0^m$ if our capital becomes too large because it goes against the expectation of constant wealth. The authors then explain that one could imagine playing a game for every value of $m \in [0, 1]$. For a time $t$, one could the construct a confidence sequence as the collection of games with small capital at $t$.</p> <p>The reason the authors introduce this concept is because the capital process is a test <i>martingale</i> when $m = \mu$. Unlike the Hoeffding and empirical Bernstein processes, which are test <i>supermartingales</i>. Since test supermartingales expect the capital to decrease, test constructed from them can be more conservative (this is because we expect to be losing wealth over time, so $\mathbb{E}[X_0]$ will be the maximal value, and Ville’s inequality will yield a relatively larger bound).</p> <h3 id="hedging-our-bets">Hedging Our Bets</h3> <p>Since the capital process describes the wealth we accumulate during a better game, we can ask the question: Is there a way to mitigate our risk? In general betting games, one can <i>hedge</i> one’s bets. This means that, after placing an initial bet, the player bets again on a different outcome (or multiple different outcomes) as they see the game play out. This can reduce their net loss because they could possibly win back some money with their later bets that are based on more information about the game.</p> <p>Now, consider making two simultaneous bets: one with some part $(\theta)$ of one’s current wealth and the other with the rest. One bet will be made for the case that $\mu \geq m$, and the other will be made for the case that $\mu &lt; m$. We can describe separate capital processes for these two bets and use these to describe the wealth accumulated across both betting strategies. This is called the <i>hedged capital process</i>.</p> <div id="hedged-capital-process"></div> <div class="definition"> <strong>Definition (Hedged Capital Process).</strong> <br> Let $(X_t)_{t = 1}^\infty \sim P$ be a sequence of random variables taking values in $[0, 1]$ with distribution $P \in \mathcal{P}^\mu$ where $\mathcal{P}$ is the set of distributions on $[0, 1]^\infty$ such that $\mathbb{E}_P[X_t \vert \mathcal{F}_{t-1}] = \mu$ for each $t$. <br> Let $(\mathcal{K}_t^+(m))_{t = 1}^\infty$ and $(\mathcal{K}_t^-(m))_{t=1}^\infty$ be sequences where: $$ \mathcal{K}_t^+ = \prod_{i = 1}^t (1 + \lambda_i^+(m) \cdot (X_i - m)) \hspace{5mm} \text{ and } \hspace{5mm} \mathcal{K}_t^- = \prod_{i = 1}^t (1 + \lambda_i^-(m) \cdot (X_i - m)) \nonumber $$ for predictable sequences $(\lambda_t^+(m))_{t=1}^\infty$ and $(\lambda_t^-(m))_{t=1}^\infty$ with $\lambda_t^+(m) \in \left[0, \frac{1}{m}\right)$ and $\lambda_t^-(m) \in \left[0, \frac{1}{1-m}\right)$. <br> The <i>hedged capital process</i> for any $m \in [0, 1]$ and any $\theta \in [0, 1]$ is defined as: $$ \mathcal{K}_t^\pm(m) = \max \left\{ \theta \mathcal{K}_t^+(m), (1-\theta) \mathcal{K}_t^-(m) \right\} $$ </div> <p>In the event that $m = \mu$, we would expect our hedged bets to both yield no earnings. However, if $m \neq \mu$, then one bet should lose and the other should win money. With this capital process, we can construct a confidence sequence.</p> <div id="theorem-3"></div> <div class="theorem"> <strong>Theorem 3: Hedged Capital CS.<d-cite key="waudbysmith2022"></d-cite></strong> <ul id="ws-theorem3" class="tab" data-tab="76c7dc7c-7c43-4b5a-b7d3-9034f0e6c965" data-name="ws-theorem3"> <li class="active" id="ws-theorem3-statement"> <a href="#">statement </a> </li> <li id="ws-theorem3-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="76c7dc7c-7c43-4b5a-b7d3-9034f0e6c965" data-name="ws-theorem3"> <li class="active"> <p>Let \((X_t)_{t = 1}^\infty \sim P\) be a sequence of random variables taking values in \([0, 1]\) with distribution \(P \in \mathcal{P}^\mu\) where \(\mathcal{P}\) is the set of distributions on \([0, 1]^\infty\) such that \(\mathbb{E}_P[X_t \vert \mathcal{F}_{t-1}] = \mu\) for each $t$.</p> <p>Let \((\tilde{\lambda}_t^+)_{t=1}^\infty\) and \((\tilde{\lambda}_t^-)_{t=1}^\infty\) be predictable, real-valued sequences, independent of $m$, and define, for each $t \geq 1$:</p> \[\lambda_t^+(m) = \min\left\{ \rvert \tilde{\lambda}_t^+\rvert, \frac{c}{m} \right\} \hspace{5mm} \text{ and } \hspace{5mm} \lambda_t^-(m) = \min\left\{ \rvert \tilde{\lambda}_t^-\rvert, \frac{c}{1-m} \right\}\] <p>and for some chosen \($c \in [0, 1)\). The sequence \((\mathcal{B}_t^{\pm})_{t = 1}^\infty\) forms a $(1-\alpha)$ confidence sequence for $\mu$ where:</p> \[\mathcal{B}_t^{\pm} = \left\{ m \in [0, 1] \bigg\rvert \mathcal{K}_t^\pm(m) &lt; \frac{1}{\alpha} \right\}\] <p>The running intersection, \(\cap_{i \leq t} \mathcal{B}_i^\pm\) is also a valid $(1-\alpha)$ confidence sequence for $\mu$. Furthermore, \(\mathcal{B}_t^{\pm}\) is a valid $(1-\alpha)$ confidence interval for each $t \geq 1$.</p> </li> <li> <p>Proof to be completed.</p> </li> </ul> </div> <p>The authors recommend using $\tilde{\lambda}_t^+ = \tilde{\lambda}_t^- = \lambda_t^{\text{PrPl}\pm}$ for the predictable plug-ins, where:</p> \[\begin{equation} \label{eq:hedged-capital-choices} \lambda_t^{\text{PrPl}\pm} = \sqrt{\frac{2 \log(2/\alpha)}{\hat{\sigma}_{t-1}^2 t \log(t+1)}} \end{equation}\] <p>where:</p> \[\begin{aligned} \hat{\sigma}_t^2 &amp;= \frac{\frac{1}{4} + \sum_{i = 1}^t (X_i - \hat{\mu}_i)^2}{t+1} \\ \hat{\mu}_t &amp;= \frac{\frac{1}{2} + \sum_{i = 1}^t X_i}{t + 1} \\ c &amp;= \frac{1}{2} \text{ or } \frac{3}{4} \end{aligned}\] <p>Similar to the empirical Bernstein case above, we can use the running intersection, $\cap_{i \leq n} \mathcal{B}_i^\pm$, to form a $(1- \alpha)$ confidence interval for a fixed sample size $n$, where we choose:</p> \[\tilde{\lambda}_t^+ = \tilde{\lambda}_t^- = \tilde{\lambda}_t^\pm = \sqrt{\frac{2 \log(2/\alpha)}{n \hat{\sigma}_{t - 1}^2}}\] <p>for the same $\hat{\sigma}^2$ as defined in Eq. \eqref{eq:hedged-capital-choices}.</p> <p>A key finding of Waudby-Smith and Ramdas is that, for large sample sizes, the hedged capital confidence intervals are <i>almost surely better</i> than the confidence intervals one can create from Hoeffding’s inequality. Even more, the hedged capital confidence intervals converge at a rate of $O(1/\sqrt{n})$, which is optimal.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2025-05-21-betting-sequences.bib"></d-bibliography> <d-article> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Anna Rosengart. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: February 10, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?d4c3ed73337d78e34b10d24890d1fc56"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/tabs.min.js?b8748955e1076bbe0dabcf28f2549fdc"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>