<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> A Score Test for Variance Components in Generalized Linear Models | Anna Rosengart </title> <meta name="author" content="Anna Rosengart"> <meta name="description" content=""> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://aerosengart.github.io/paper-notes/score-test-lin/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "A Score Test for Variance Components in Generalized Linear Models",
            "description": "",
            "published": "January 05, 2026",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Anna Rosengart </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">curriculum vitae </a> </li> <li class="nav-item "> <a class="nav-link" href="/music/">music </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/stats-ml/">stats &amp; ml</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/measure-theory/">measure theory</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/paper-notes/">paper notes</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>A Score Test for Variance Components in Generalized Linear Models</h1> <p></p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#set-up">Set-Up</a> </div> <div> <a href="#quasi-likelihood">Quasi-Likelihood</a> </div> <div> <a href="#score">Score</a> </div> <ul> <li> <a href="#intermediate-quantities">Intermediate Quantities</a> </li> <li> <a href="#derivative-terms">Derivative Terms</a> </li> <li> <a href="#computing-the-score">Computing The Score</a> </li> </ul> <div> <a href="#individual-variance-component-tests">Individual Variance Component Tests</a> </div> <ul> <li> <a href="#intermediate-quantities">Intermediate Quantities</a> </li> </ul> </nav> </d-contents> <p>In this page, I will be trying to derive the results in Lin (1997)<d-cite key="lin1997"></d-cite> myself as a way to make sure I understand what’s going on.</p> <h2 id="set-up">Set-Up</h2> <p>We will assume to have $N$ total observations. For each, we have some response, $y_i$, a $p \times 1$ fixed effects covariate vector, $\mathbf{x}_i$, and a $q \times 1$ random effects covariate vector, $\mathbf{z}_i$.</p> <p>We’ll assume to have a clustered design, so each observation will belong to some cluster $k \in [K]$. The observations in cluster $k$ will be denoted with a superscript $k$ (i.e. $y_j^k$), and we’ll let $n_k$ denote the number of observations in cluster $k$ (such that $\sum_{k = 1}^K n_k = N$).</p> <p>We assume that, conditional on the $q \times 1$ vector of random effects, $\beta^k$, the responses are independent and have means and variances:</p> \[\begin{aligned} \mathbb{E}[y^k_j \rvert \beta_k] &amp;= \mu^k_j \\ \text{Var}(y^k_j \rvert \beta_k) &amp;= V(\mu^k_j) = \phi v(\mu^k_j) \end{aligned}\] <p>for some scale parameter, $\phi$. We further assume a generalized linear model is appropriate:</p> \[\begin{aligned} g(\mu^k_j) &amp;= \eta^k_j = \mathbf{x}_i^\top \alpha + \mathbf{z}_i^\top \beta_k \\ \beta_k &amp;\overset{iid}{\sim} \mathcal{N}(\mathbf{0}_q, D(\tau^2)) \end{aligned}\] <p>with monotonic and differentiable $g(\cdot)$, and $p \times 1$ vector of fixed effects. We assume that $D(\tau^2)$ is a $q \times q$ diagonal matrix parametrized by the $m$-dimensional vector-valued variance component, $\tau^2 = (\tau^2_1, \dots, \tau^2_m)^\top$.</p> <p>We’ll construct vectors of our data for each cluster:</p> \[\begin{aligned} \mathbf{y}^k &amp;= (y^k_1, \dots, y^k_{n_k})^\top \\ \mu^k &amp;= (\mu^k_1, \dots, \mu^k_{n_k})^\top \\ \eta^k &amp;= (\eta^k_1, \dots, \eta^k_{n_k})^\top \end{aligned}\] <p>And we can concatenate these vectors to construct large vectors for the complete data:</p> \[\begin{aligned} \mathbf{y} &amp;= (\mathbf{y}^1, \dots, \mathbf{y}^K)^\top \\ \mu &amp;= (\mu^1, \dots \mu^K)^\top \\ \eta &amp;= (\eta^1, \dots, \eta^K)^\top \end{aligned}\] <p>Furthermore, we will let $\mathbf{V}(\mu)$ denote the diagonal matrix with the $V(\mu_i) = \phi v(\mu_i)$ terms along its main diagonal, and $\mathbf{X}$ will be the $N \times p$ matrix with rows equal to the $\mathbf{x}_i^\top$ vectors.</p> <div class="example"> <strong>Example.</strong> <br> In this example, we'll assume we have negative binomial data with a scalar variance component (i.e. $m = 1$), a single random effect, and a fixed, cluster-specific intercept: $$ \begin{aligned} g(\mu_j^k) &amp;= \alpha_k + \beta_k z_i \\ \beta &amp;\sim \mathcal{N}(\mathbf{0}_K, \tau^2 \mathbb{I}_{K \times K}) \end{aligned} $$ The link and variance functions are: $$ \begin{aligned} g(\mu^k_i) &amp;= \log(\mu^k_i) \\ v(\mu_i^k) &amp;= \mu_i^k + \frac{1}{\gamma} (\mu_i^k)^2 \end{aligned} $$ where we assume $\gamma$ is a known dispersion parameter. </div> <h3 id="some-non-standard-notation">Some Non-Standard Notation</h3> <p>Let \(\tilde{\mathbf{z}}_i\) be the $(q \times k)$-dimensional vector of zeros where the coordinates corresponding to observation $i$ are replaced with $\mathbf{z}_i$. We’ll use \(\tilde{\mathbf{Z}}\) to denote the \(N \times (q \times K)\) matrix whose $i$-th row is \(\tilde{\mathbf{z}}_i^\top\). Similarly, we’ll use \(\tilde{D(\tau^2)} = D(\tau^2) \otimes \mathbb{I}_{k \times k}\) to denote the $(q \times K) \times (q \times K)$ block covariance matrix for <i>all</i> of the random effects.</p> <p>We’ll also use a bar (e.g. \(\bar{\mu}_i^k\)) to denote a term evaluated under the null hypothesis that \(\beta = \mathbf{0}_{q \times k}\), and a hat (e.g. \(\hat{\mu}_i^k\)) to denote a term evaluated at the parameter maximum likelihood estimates under the null hypothesis as well.</p> <hr> <h2 id="quasi-likelihood">Quasi-Likelihood</h2> <p>The model has parameters $\alpha$, $\phi$, $\tau^2$, and random effects $\beta = (\beta_1^\top, \dots, \beta_K^\top)^\top$. Let $\theta = (\alpha, \phi, \tau^2)$. The conditional log quasi-likelihood is:</p> \[\ell_q(y^k_i; \theta \rvert \beta_k) = \int_{y^k_i}^{\mu_i^k} \frac{y^k_i - u}{\phi v(u)} du\] <p>Let $f(\beta; \tau^2)$ and $\mathcal{F}(\beta; \tau^2)$ denote the log density and distribution functions of $\beta$. Under the assumption that $D(\tau^2)$ is diagonal (i.e. the random effects are independent), the integrated (marginal) quasi-likelihood is:</p> \[\begin{aligned} \mathcal{L}_q(\mathbf{y}; \theta) &amp;= \int \exp\left( \sum_{k = 1}^K \sum_{i = 1}^{n_k} \ell_q(y^k_i; \theta \rvert \beta_k) \right) d \mathcal{F}(\beta; \tau^2) \\ &amp;= \int \exp\left( \sum_{k = 1}^K \sum_{i = 1}^{n_k} \ell_q(y^k_i; \theta \rvert \beta_k) \right) \exp(f(\beta; \tau^2)) d \beta \end{aligned}\] <p>And the marginal log quasi-likelihood is the logarithm of this function:</p> \[\mathcal{L}_q(\mathbf{y}; \theta) = \log \left[ \int \exp\left( \sum_{k = 1}^K \sum_{i = 1}^{n_k} \ell_q(y^k_i; \theta \rvert \beta_k) \right) \exp(f(\beta; \tau^2)) d \beta \right]\] <p>This integral is unwieldy, so we use a second-order Taylor approximation of the conditional quasi-likelihood about the random effects mean to simplify our calculations. If observation $i$ is a member of cluster $k$, we will let $\tilde{\mathbf{z}}_i$ denote the $(q \times K)$-dimensional whose only non-zero entries are the $qk$ to $q(k + 1)$ (those corresponding to the cluster of observation $i$). The integrated (conditional) quasi-likelihood can be written via a Taylor expansion:</p> \[\begin{aligned} \mathcal{L}_q\left( \mathbf{y}; \theta \rvert \beta \right) &amp;= \exp\left( \sum_{i = 1}^{N} \ell_q(y_i; \theta \rvert \beta) \right) \\ &amp;= \left. \exp\left( \sum_{i = 1}^{N} \ell_q(y_i; \theta \rvert \beta) \right) \right\rvert_{\beta = \mathbf{0}_{q \times k}} + \left. \frac{\partial}{\partial \beta} \left[ \exp\left( \sum_{i = 1}^{N} \ell_q(y_i; \theta \rvert \beta) \right) \right]\right\rvert_{\beta = \mathbf{0}_{q \times k}} \beta + \frac{1}{2}\beta^\top \left[ \left. \frac{\partial}{\partial \beta \partial \beta^\top} \left[ \exp\left( \sum_{i = 1}^{N} \ell_q(y_i; \theta \rvert \beta) \right) \right] \right\rvert_{\beta = \mathbf{0}_{q \times k}} \right] \beta + \dots \\ &amp;= \left. \exp\left( \sum_{i = 1}^{N} \ell_q(y_i; \theta \rvert \beta) \right) \right\rvert_{\beta = \mathbf{0}_{q \times k}} \left( 1 + \left. \frac{\partial}{\partial \beta} \left[ \sum_{i = 1}^{N} \ell_q(y_i; \theta \rvert \beta) \right]\right\rvert_{\beta = \mathbf{0}_{q \times k}} \beta + \frac{1}{2}\beta^\top \left[ \left.\frac{\partial}{\partial \beta \partial \beta^\top} \left[ \sum_{i = 1}^{N} \ell_q(y_i; \theta \rvert \beta) \right] \right\rvert_{\beta = \mathbf{0}_{q \times k}} \right] \beta + \epsilon \right) \end{aligned}\] <p>where $\epsilon$ is the residual that holds the higher order terms. Taking the expectation and applying the chain rule yields an approximate marginal quasi log-likelihood:</p> \[\mathcal{L}_q(\mathbf{y}; \theta) \approx \left. \exp\left(\sum_{i = 1}^N \ell_q(y_i; \theta \rvert \beta) \right)\right\rvert_{\beta = \mathbf{0}_{q \times k}} \left( 1 + \frac{1}{2} \text{tr} \left[ \left( \left(\sum_{i = 1}^N \left. \frac{\partial \ell_q(y_i; \theta \rvert \beta)}{\partial \eta_i} \right\rvert_{\beta = \mathbf{0}_{q \times k}} \tilde{\mathbf{z}}_i \right) \left(\sum_{i = 1}^N \left. \frac{\partial \ell_q(y_i; \theta \rvert \beta )}{\partial \eta_i} \right\rvert_{\beta = \mathbf{0}_{q \times k}} \tilde{\mathbf{z}}_i^\top \right) + \sum_{i = 1}^N \left. \frac{\partial^2 \ell_q(y_i; \theta \rvert \beta)}{\partial \eta_i^2}\right\rvert_{\beta = \mathbf{0}_{q \times k}} \tilde{\mathbf{z}}_i \tilde{\mathbf{z}}_i^\top \right) D(\tau^2) \right] + o(\rvert \rvert \tau^2 \rvert \rvert) \right)\] <p>Taking the logarithm and using the fact that $\log(x + 1) \approx x$ for small $x$, we can write the marginal log quasi-likelihood approximation as:</p> \[\begin{aligned} \ell_q(\mathbf{y}; \theta) &amp;\approx \sum_{i = 1}^N \left. \ell_q(y_i; \theta \rvert \beta)\right\rvert_{\beta = \mathbf{0}_{q \times k}} + \frac{1}{2} \text{tr} \left[ \left( \left(\sum_{i = 1}^N \left. \frac{\partial \ell_q(y_i; \theta \rvert \beta)}{\partial \eta_i} \right\rvert_{\beta = \mathbf{0}_{q \times k}} \tilde{\mathbf{z}}_i \right) \left(\sum_{i = 1}^N \left. \frac{\partial \ell_q(y_i; \theta \rvert \beta )}{\partial \eta_i} \right\rvert_{\beta = \mathbf{0}_{q \times k}} \tilde{\mathbf{z}}_i^\top \right) + \sum_{i = 1}^N \left. \frac{\partial^2 \ell_q(y_i; \theta \rvert \beta)}{\partial \eta_i^2} \right\rvert_{\beta = \mathbf{0}_{q \times k}} \tilde{\mathbf{z}}_i \tilde{\mathbf{z}}_i^\top \right) \tilde{D}(\tau^2) \right] + o(\rvert \rvert \tau^2 \rvert \rvert) \\ &amp;= \sum_{i = 1}^N \left. \ell_q(y_i; \theta \rvert \beta)\right\rvert_{\beta = \mathbf{0}_{q \times k}}+ \frac{1}{2} \text{tr} \left[ \tilde{\mathbf{Z}}^\top \left( \left. \left[ \frac{\partial \ell_q(\mathbf{y}; \theta \rvert \beta)}{\partial \eta} \frac{\partial \ell_q(\mathbf{y}; \theta \rvert \beta)}{\partial \eta^\top} + \frac{\partial^2 \ell_q(\mathbf{y}; \theta \rvert \beta )}{\partial \eta \partial \eta^\top} \right] \right\rvert_{\beta = \mathbf{0}_{q \times k}} \right) \tilde{\mathbf{Z}} \tilde{D}(\tau^2) \right] + o(\rvert \rvert \tau^2 \rvert \rvert) \end{aligned}\] <p>where \(\frac{\partial \ell_q(\mathbf{y}; \theta \rvert \beta)}{\partial \eta}\) is the $N$-dimensional vector whose $i$-th element is equal to \(\frac{\partial \ell_q(y_i; \theta \rvert \beta)}{\partial \eta_i}\), and \(\frac{\partial^2 \ell_q(\mathbf{y}; \theta \rvert \beta )}{\partial \eta \partial \eta^\top}\) is the $N \times N$ dimensional diagonal matrix whose $i$-th diagonal element is equal to \(\frac{\partial^2 \ell_q(y_i; \theta \rvert \beta)}{\partial \eta_i^2}\).</p> <p>We will drop the $o(\rvert \rvert \tau^2 \rvert \rvert)$ in what follows as we have assumed moment conditions hold that make this term negligible.</p> <hr> <h2 id="score">Score</h2> <h3 id="intermediate-quantities">Intermediate Quantities</h3> <p>We will define some helpful quantities that will make the notation easier as we continue. Recall that a single subscript (with no superscript) directly indexes the <i>entire</i> vector (over all clusters). We should also note that we further assume that $g(\cdot)$ is <i>strictly monotone</i> and <i>continuous</i> so that:</p> \[\begin{equation} \label{eq:deriv-assumption} \frac{\partial g^{-1}(\eta_i)}{\partial \eta_i} = \frac{\partial \mu_i}{\partial \eta_i} = \left(\frac{\partial \eta_i}{\partial \mu_i}\right)^{-1} \end{equation}\] <p>This implies that $g(\cdot)$ is invertible, and its derivative is never $0$. This holds for many standard choices of link function (e.g. $\log$, $\text{logit}$, the identity).</p> <p>We now define:</p> \[\begin{equation} \label{eq:intermediate-terms} \begin{aligned} \delta_i &amp;= \left[ \frac{\partial g(\mu_i)}{\partial\mu_i}\right]^{-1} \\ \omega_i &amp;= \left[ V(\mu_i) \left(\frac{\partial g(\mu_i)}{\partial\mu_i}\right)^2 \right]^{-1} = \left[ \phi v(\mu_i) \left(\frac{\partial g(\mu_i)}{\partial\mu_i}\right)^2 \right]^{-1} = \frac{\delta_i^2}{\phi v(\mu_i)} \\ e_i &amp;= \frac{\frac{\partial V(\mu_i)}{\partial \mu_i} \frac{\partial g(\mu_i)}{\partial \mu_i} + V(\mu_i) \frac{\partial^2 g(\mu_i)}{\partial \mu_i^2}}{(V(\mu_i)^2)\left(\frac{\partial g(\mu_i)}{\partial \mu_i}\right)^3} = \frac{\phi\left(\frac{\partial v(\mu_i)}{\partial\mu_i}\right)\left(\frac{\partial g(\mu_i)}{\partial\mu_i}\right) + \phi v(\mu_i)\left(\frac{\partial^2 g(\mu_i)}{\partial \mu_i^2}\right)}{\phi^2 v^2(\mu_i) \left(\frac{\partial g(\mu_i)}{\partial\mu_i}\right)^3} \\ \xi_i &amp;= \omega_i + e_i(y_i - \mu_i) \end{aligned} \end{equation}\] <aside><p>In the original publication, $\omega_i$ is denoted by $w_i$, and $\xi_i$ is denoted by $w_{oi}$.</p></aside> <p>We also define the $N \times N$ diagonal matrices $\Delta$, $\Omega$, and $\Xi$, which have the $\delta_i$, $\omega_i$, and $\xi_i$ terms along their main diagonals (respectively).</p> <div class="example"> <strong>Example.</strong> <br> As another reminder, the link function is $\log(\cdot)$, and the variance function is $v(\mu_i) = \mu_i + \frac{1}{\gamma} (\mu_i)^2$. Thus: $$ \begin{aligned} \frac{\partial g(\mu_i)}{\partial \mu_i} &amp;= \frac{\partial}{\partial \mu_i} \left[ \log(\mu_i) \right] = \frac{1}{\mu_i} \\ \frac{\partial^2 g(\mu_i)}{\partial \mu_i^2} &amp;= \frac{\partial}{\partial \mu_i} \left[ \frac{1}{\mu_i} \right] = -\frac{1}{\mu_i^2} \\ \frac{\partial v(\mu_i)}{\partial \mu_i} &amp;= \frac{\partial}{\partial \mu_i}\left[ \mu_i + \frac{1}{\gamma}\mu_i^2 \right] = 1 + \frac{2}{\gamma} \mu_i \end{aligned} $$ The intermediate terms are then: $$ \begin{aligned} \delta_i &amp;= \left[ \frac{1}{\mu_i} \right]^{-1} = \mu_i \\ \omega_i &amp;= \frac{\mu_i^2}{\phi \left(\mu_i + \frac{1}{\gamma} (\mu_i)^2\right)} = \frac{\mu_i}{\phi \left(1 + \frac{1}{\gamma} \mu_i\right)} \\ e_i &amp;= \frac{\phi \left(1 + \frac{2}{\gamma} \mu_i\right) \left(\frac{1}{\mu_i}\right) + \phi\left(\mu_i + \frac{1}{\gamma} \mu_i^2 \right)\left(- \frac{1}{\mu_i^2}\right)}{\phi^2 \left(\mu_i + \frac{1}{\gamma}\mu_i^2 \right)^2\left(\frac{1}{\mu_i^3}\right)} = \frac{\frac{1}{\mu_i} + \frac{2}{\gamma} - \frac{1}{\mu_i} - \frac{1}{\gamma}}{\phi \left(1+ \frac{1}{\gamma}\mu_i\right)^2 \frac{1}{\mu_i}} = \frac{\mu_i}{\phi \gamma\left(1+ \frac{1}{\gamma} \mu_i \right)^2}\\ \xi_i &amp;= \frac{\mu_i}{\phi\left(1 + \frac{1}{\gamma} \mu_i \right)} + \frac{\mu_i(y_i - \mu_i)}{\phi \gamma\left(1+ \frac{1}{\gamma} \mu_i \right)^2} = \frac{\gamma \mu_i \left(1 + \frac{1}{\gamma} \mu_i \right) + \mu_i}{\phi \gamma \left(1 + \frac{1}{\gamma} \mu_i \right)^2} = \frac{\mu \left( 1 + \gamma + \mu_i \right)}{\phi \gamma \left(1 + \frac{1}{\gamma} \mu_i \right)^2} \end{aligned} $$ </div> <h3 id="derivative-terms">Derivative Terms</h3> <p>Notice that the expression above all involve the partial derivatives of \(\ell_q(\mathbf{y}; \theta \rvert \beta)\) with respect to the components of the linear predictor, $\eta$. With an application of the chain rule, we can derive the first order partial derivatives. Recall that a bar above a variable indicates its evaluation at the true parameter values under $H_0$ (i.e. \(\beta = \mathbf{0}_{q \times k}\)). To streamline our analysis later, let’s do some initial derivations. We have:</p> <ul id="deriv-1-mu" class="tab" data-tab="fe4d1eee-c7f9-49a8-8131-61695d274a09" data-name="deriv-1-mu"> <li class="active" id="deriv-1-mu-equation"> <a href="#">equation </a> </li> <li id="deriv-1-mu-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="fe4d1eee-c7f9-49a8-8131-61695d274a09" data-name="deriv-1-mu"> <li class="active"> \[\begin{equation} \label{eq:deriv-1-mu} \begin{aligned} \left. \frac{\partial \ell_q(y_i; \theta \rvert \beta)}{\partial \mu_i} \right\rvert_{\beta = \mathbf{0}_{q \times k}} &amp;= \frac{y_i - \bar{\mu}_i}{\phi v(\bar{\mu}_i)} \\ \implies \left. \frac{\partial \ell_q(\mathbf{y}; \theta \rvert \beta)}{\partial \mu} \right\rvert_{\beta = \mathbf{0}_{q \times k}} &amp;= \mathbf{V}^{-1}(\bar{\mu})(\mathbf{y}_i - \mu_i) \end{aligned} \end{equation}\] </li> <li> \[\begin{aligned} \left. \frac{\partial \ell_q(y_i; \theta \rvert \beta)}{\partial \mu_i} \right\rvert_{\beta = \mathbf{0}_{q \times k}} &amp;= \left. \frac{\partial}{\partial \mu_i} \left[ \int_{y_i}^{\mu_i} \frac{y_i - u}{\phi v(u)} du \right] \right\rvert_{\beta = \mathbf{0}_{q \times k}} \\ &amp;= \left. \left[ \frac{y_i - \mu_i}{\phi v(\mu_i)} \right] \right\rvert_{\beta = \mathbf{0}_{q \times k}} &amp; \left(\text{FTC}\right) \\ &amp;= \frac{y_i - \bar{\mu}_i}{\phi v(\bar{\mu}_i)} \end{aligned}\] </li> </ul> <ul id="deriv-1-eta" class="tab" data-tab="558f6060-7cb7-4b37-b823-4243eda60bdd" data-name="deriv-1-eta"> <li class="active" id="deriv-1-eta-equation"> <a href="#">equation </a> </li> <li id="deriv-1-eta-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="558f6060-7cb7-4b37-b823-4243eda60bdd" data-name="deriv-1-eta"> <li class="active"> \[\begin{equation} \label{eq:deriv-1-eta} \begin{aligned} \left. \frac{\partial \ell_q(y_i; \theta \rvert \beta)}{\partial \eta_i} \right\rvert_{\beta = \mathbf{0}_{q \times k}} &amp;= \bar{\omega}_i \bar{\delta}_i^{-1} \left(y_i - \bar{\mu}_i\right) \\ \implies \left. \frac{\partial \ell_q(\mathbf{y}; \theta \rvert \beta)}{\partial \eta} \right\rvert_{\beta = \mathbf{0}_{q \times k}} &amp;= \bar{\Omega}\bar{\Delta}^{-1} (\mathbf{y} - \bar{\mu}) \end{aligned} \end{equation}\] </li> <li> \[\begin{aligned} \left. \frac{\partial \ell_q(y_i; \theta \rvert \beta)}{\partial \eta_i} \right\rvert_{\beta = \mathbf{0}_{q \times k}} &amp;= \left. \left[ \frac{\partial \mu_i}{\partial \eta_i} \frac{\partial \ell_q(y_i; \theta \rvert \beta)}{\partial \mu_i} \right] \right\rvert_{\beta = \mathbf{0}_{q \times k}} &amp; \left(\text{chain rule} \right)\\ &amp;= \left. \left[ \frac{\partial \mu_i}{\partial \eta_i} \frac{\partial}{\partial \mu_i} \left[ \int_{y_i}^{\mu_i} \frac{y_i - u}{\phi v(u)} du \right] \right] \right\rvert_{\beta = \mathbf{0}_{q \times k}} \\ &amp;= \left. \left[ \left(\frac{\partial \mu_i}{\partial \eta_i}\right) \left(\frac{y_i - \mu_i}{\phi v(\mu_i)}\right) \right] \right\rvert_{\beta = \mathbf{0}_{q \times k}} &amp; \left( \text{FTC} \right) \\ &amp;= \left. \frac{\partial \mu_i}{\partial \eta_i} \right\rvert_{\beta = \mathbf{0}_{q \times k}} \left(\frac{y_i - \bar{\mu}_i}{\phi v(\bar{\mu}_i)}\right) \end{aligned}\] <p>Note that, by Eq. \eqref{eq:deriv-assumption}:</p> \[\begin{aligned} \delta_i &amp;= \left[\frac{\partial g(\mu_i)}{\partial \mu_i}\right]^{-1} = \left[\frac{\partial \eta_i}{\partial \mu_i}\right]^{-1} = \frac{\partial \mu_i}{\partial \eta_i} \end{aligned}\] <p>So we have:</p> \[\begin{aligned} \left. \frac{\partial \ell_q(y_i; \theta \rvert \beta)}{\partial \eta_i} \right\rvert_{\beta = \mathbf{0}_{q \times k}} &amp;= \left. \frac{\partial \mu_i}{\partial \eta_i} \right\rvert_{\beta = \mathbf{0}_{q \times k}} \left(\frac{y_i - \bar{\mu}_i}{\phi v(\bar{\mu}_i)}\right) \\ &amp;= \left. \frac{\left(\frac{\partial \mu_i}{\partial \eta_i}\right)^2}{\phi v(\mu_i) \frac{\partial \mu_i}{\partial \eta_i}}\right\rvert_{\beta = \mathbf{0}_{q \times k}} \left(y_i - \bar{\mu}_i\right) \\ &amp;= \bar{\omega}_i \bar{\delta}_i^{-1} \left(y_i - \bar{\mu}_i\right) \end{aligned}\] </li> </ul> <p>Similarly, we can find the second-order partial derivatives with respect to the linear predictor:</p> <ul id="deriv-2-eta" class="tab" data-tab="c66d8a96-d880-484e-ac35-98014e2a5770" data-name="deriv-2-eta"> <li class="active" id="deriv-2-eta-equation"> <a href="#">equation </a> </li> <li id="deriv-2-eta-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="c66d8a96-d880-484e-ac35-98014e2a5770" data-name="deriv-2-eta"> <li class="active"> \[\begin{equation} \label{eq:deriv-2-eta} \begin{aligned} \left. \frac{\partial^2 \ell_q(y_i; \theta \rvert \beta)}{\partial \eta_i^2} \right\rvert_{\beta = \mathbf{0}_{q \times k}} &amp;= -\bar{\xi}_i \\ \implies \left. \frac{\partial^2 \ell_q(\mathbf{y}; \theta \rvert \beta)}{\partial \eta \partial \eta^\top} \right\rvert_{\beta = \mathbf{0}_{q \times k}} &amp;= -\bar{\Xi} \end{aligned} \end{equation}\] </li> <li> <p>First, we note that:</p> \[\begin{equation} \label{eq:temp} \begin{aligned} \frac{\partial^2 \mu_i}{\partial \eta_i^2} &amp;= \left(\frac{\partial \mu_i}{\partial \eta_i} \right) \frac{\partial}{\partial \mu_i}\left[ \frac{\partial \mu_i}{\partial \eta_i}\right] &amp;\left(\text{chain rule}\right) \\ &amp;= \left(\frac{\partial \mu_i}{\partial \eta_i} \right) \frac{\partial}{\partial \mu_i}\left[ \left( \frac{\partial \eta_i}{\partial \mu_i} \right)^{-1} \right] &amp;\left(\text{Eq. } \eqref{eq:deriv-assumption} \right) \\ &amp;= - \left(\frac{\partial \mu_i}{\partial \eta_i} \right) \left(\frac{\partial \eta_i}{\partial \mu_i} \right)^{-2} \frac{\partial}{\partial \mu_i}\left[ \frac{\partial \eta_i}{\partial \mu_i} \right] \\ &amp;= - \left(\frac{\partial \eta_i}{\partial \mu_i} \right)^{-3} \left(\frac{\partial^2 \eta_i}{\partial \mu_i^2} \right) &amp; \left(\text{Eq. } \eqref{eq:deriv-assumption}\right)\\ \implies \frac{\partial^2 \eta_i}{\partial \mu_i^2} &amp;= -\left(\frac{\partial \eta_i}{\partial \mu_i}\right)^{3} \left(\frac{\partial^2 \mu_i}{\partial \eta_i^2}\right) \end{aligned} \end{equation}\] <p>It follows that:</p> \[\begin{aligned} \frac{\partial^2 \ell_q(\mathbf{y}; \theta \rvert \beta)}{\partial \eta_i^2} &amp;= \frac{\partial}{\partial \eta_i}\left[ \omega_i \delta_i^{-1} \left(\frac{y_i - \mu_i}{\phi v(\mu_i)} \right)\right] \\ &amp;= \frac{\partial}{\partial \eta_i}\left[\left(\frac{\partial \mu_i}{\partial \eta_i}\right) \left(\frac{y_i - \mu_i}{\phi v(\mu_i)}\right)\right] &amp; \left(\text{see proof of Eq. \eqref{eq:deriv-1-eta}}\right) \\ &amp;= \frac{\partial}{\partial \eta_i} \left[ \frac{\partial \mu_i}{\partial \eta_i}\right]\left(\frac{y_i - \mu_i}{\phi v(\mu_i)}\right) + \left(\frac{\partial \mu_i}{\partial \eta_i}\right) \frac{\partial}{\partial \eta_i}\left[\frac{y_i - \mu_i}{\phi v(\mu_i)}\right] &amp; \left(\text{product rule}\right)\\ &amp;= \left(\frac{\partial^2 \mu_i}{\partial \eta_i^2}\right)\left(\frac{y_i - \mu_i}{\phi v(\mu_i)}\right) + \left(\frac{\partial \mu_i}{\partial \eta_i}\right)^2 \frac{\partial}{\partial \mu_i}\left[\frac{y_i - \mu_i}{\phi v(\mu_i)}\right] &amp; \left(\text{chain rule}\right)\\ &amp;= \left(\frac{\partial^2 \mu_i}{\partial \eta_i^2}\right)\left(\frac{y_i - \mu_i}{\phi v(\mu_i)}\right) + \left(\frac{\partial \mu_i}{\partial \eta_i}\right)^2 \left(\frac{- \phi v(\mu_i) - \phi (y_i - \mu_i)\frac{\partial v(\mu_i)}{\partial \mu_i}}{(\phi v(\mu_i))^2}\right) &amp; \left(\text{quotient rule}\right)\\ &amp;= - \left(\frac{\partial \eta_i}{\partial \mu_i}\right)^3 \left(\frac{\partial^2 \eta_i}{\partial \mu_i^2}\right)\left(\frac{y_i - \mu_i}{\phi v(\mu_i)}\right) + \left(\frac{\partial \mu_i}{\partial \eta_i}\right)^2 \left(\frac{- \phi v(\mu_i) - \phi (y_i - \mu_i)\frac{\partial v(\mu_i)}{\partial \mu_i}}{(\phi v(\mu_i))^2}\right) &amp; \left(\text{Eq. \eqref{eq:temp}}\right) \\ &amp;= - \frac{\frac{\partial^2 \eta_i}{\partial \mu_i^2} \left( \frac{y_i - \mu_i}{\phi v(\mu_i)}\right)}{\left(\frac{\partial \eta_i}{\partial \mu_i}\right)^{3}} + \frac{-\phi v(\mu_i) - \phi \frac{\partial v(\mu_i)}{\partial \mu_i}(y_i - \mu_i)}{(\phi v(\mu_i))^2 \left( \frac{\partial \eta_i}{\partial \mu_i}\right)^2 } &amp; \left( \frac{\partial \mu_i}{\partial \eta_i} = \left(\frac{\partial \eta_i}{\partial \mu_i} \right)^{-1} \text{ by Eq. \eqref{eq:deriv-assumption}}\right) \\ &amp;= -(y_i - \mu_i) \left(\frac{\frac{\partial^2 \eta_i}{\partial \mu_i^2}}{\phi v(\mu_i) \left(\frac{\partial \eta_i}{\partial \mu_i}\right)^{3}}\right) - \frac{1}{\phi v(\mu_i) \left(\frac{\partial \eta_i}{\partial \mu_i} \right)^2} - (y_i - \mu_i) \left(\frac{\phi \frac{\partial v(\mu_i)}{\partial \mu_i}}{(\phi v(\mu_i))^2 \left( \frac{\partial \eta_i}{\partial \mu_i}\right)^2}\right) \\ &amp;= -(y_i - \mu_i) \left[\frac{\frac{\partial^2 \eta_i}{\partial \mu_i^2}}{\phi v(\mu_i) \left(\frac{\partial \eta_i}{\partial \mu_i}\right)^{3}}+ \frac{\phi \frac{\partial v(\mu_i)}{\partial \mu_i}}{(\phi v(\mu_i))^2 \left( \frac{\partial \eta_i}{\partial \mu_i}\right)^2} \right] - \frac{1}{\phi v(\mu_i) \left(\frac{\partial \eta_i}{\partial \mu_i} \right)^2}\\ &amp;= -(y_i - \mu_i) \left[\frac{\phi v(\mu_i) \frac{\partial^2 \eta_i}{\partial \mu_i^2}}{(\phi v(\mu_i))^2 \left(\frac{\partial \eta_i}{\partial \mu_i}\right)^{3}}+ \frac{\phi \left(\frac{\partial v(\mu_i)}{\partial \mu_i}\right)\left(\frac{\partial \eta_i}{\partial \mu_i}\right)}{(\phi v(\mu_i))^2 \left( \frac{\partial \eta_i}{\partial \mu_i}\right)^3} \right] - \omega_i &amp; \left(\text{Eq. \eqref{eq:deriv-assumption}}\right)\\ &amp;= -(y_i - \mu_i) \left[\frac{\phi v(\mu_i) \frac{\partial^2 \eta_i}{\partial \mu_i^2} + \phi \left(\frac{\partial v(\mu_i)}{\partial \mu_i}\right)\left(\frac{\partial \eta_i}{\partial \mu_i}\right)}{(\phi v(\mu_i))^2 \left( \frac{\partial \eta_i}{\partial \mu_i}\right)^3} \right] - \omega_i &amp; \left(\text{Eq. \eqref{eq:deriv-assumption}}\right)\\ &amp;= -\omega_i - e_i(y_i - \mu_i) \\ &amp;= -\xi_i \end{aligned}\] </li> </ul> <p>We can then rewrite the approximate marginal log quasi-likelihood as:</p> \[\begin{equation} \label{eq:rewrite-mlql} \begin{aligned} \ell_q(\mathbf{y}; \theta) &amp;\approx \left. \ell_q(\mathbf{y}; \theta \rvert \beta)\right\rvert_{\beta = \mathbf{0}_{q \times k}}+ \frac{1}{2} \text{tr} \left[ \tilde{\mathbf{Z}}^\top \left( \bar{\Omega}\bar{\Delta}^{-1}(\mathbf{y} - \bar{\mu})(\mathbf{y} - \bar{\mu})^\top \bar{\Delta}^{-1} \bar{\Omega} - \bar{\Xi} \right) \tilde{\mathbf{Z}} \tilde{D}(\tau^2) \right] \end{aligned} \end{equation}\] <div class="example"> <strong>Example.</strong> <br> Recall that in our negative binomial setting, the link function is $\log(\cdot)$, and the variance function is $v(\mu_i^k) = \mu_i^k + \frac{1}{\gamma} (\mu_i^k)^2$. Thus: $$ \begin{aligned} \frac{\partial \mu_i^k}{\partial \eta_i^k} &amp;= \frac{\partial}{\partial \eta_i^k} \left[ \exp(\eta_i^k) \right] = \exp(\eta_i^k) = \mu_i^k \\ \frac{\partial v(\mu_i^k)}{\partial \mu_i^k} &amp;= \frac{\partial}{\partial \mu_i^k}\left[ \mu_i^k + \frac{1}{\gamma}(\mu_i^k)^2 \right] = 1 + \frac{2}{\gamma} \mu_i^k \end{aligned} $$ Plugging these values into Eqs. \eqref{eq:deriv-1-eta} and \eqref{eq:deriv-2-eta} gives us: $$ \begin{aligned} \frac{\partial \ell_q(y_i^k; \theta \rvert \beta_i = 0)}{\partial \eta_i^k} &amp;= \left(\frac{y_i^k - \bar{\mu}_i^k}{\phi\left(\bar{\mu}_i^k + \frac{1}{\gamma}(\bar{\mu}_i^k)^2\right)}\right)\left(\bar{\mu}_i^k \right) \\ &amp;= \frac{\bar{\mu}_i^k (y_i^k - \bar{\mu}_i^k)}{\phi\left(\bar{\mu}_i^k + \frac{1}{\gamma}(\bar{\mu}_i^k)^2\right)} \\ \frac{\partial^2 \ell_q(y_i^k; \theta \rvert \beta_i)}{\partial (\eta^i_k)^2} &amp;= \left(- \frac{\bar{\mu}_i + \frac{1}{\gamma}(\bar{\mu}_i^k)^2 + \left(1 + \frac{2}{\gamma} \bar{\mu}_i^k \right)(y_i^k - \bar{\mu}_i^k)}{\phi \left(\bar{\mu}_i^k + \frac{1}{\gamma}(\bar{\mu}_i^k)^2\right)^2}\right)\left(\bar{\mu}_i^k \right)^2 + \left(\frac{y_i^k - \bar{\mu}_i^k}{\phi \left(\bar{\mu}_i^k + \frac{1}{\gamma}(\bar{\mu}_i^k)^2 \right)}\right) \left(\bar{\mu}_i^k \right) \\ &amp;= - \frac{\bar{\mu}^k_i + \frac{1}{\gamma}(\bar{\mu}_i^k)^2 + \left(1 + \frac{2}{\gamma} \bar{\mu}_i^k \right)(y_i^k - \bar{\mu}_i^k)}{\phi \left(1 + \frac{1}{\gamma}\bar{\mu}_i^k\right)^2} + \frac{y_i^k - \bar{\mu}_i^k}{\phi \left(1 + \frac{1}{\gamma}\bar{\mu}_i^k \right)} \end{aligned} $$ </div> <h3 id="computing-the-score">Computing The Score</h3> <p>This brings us to the actual score, which is the gradient of the log-likelihood (or of the approximate marginal log quasi-likelihood in this case). Unfortunately, the derivative with respect to the $\alpha_j$ terms is very difficult to compute, and we don’t actually need it explicitly for the test statistic. We’ll just focus on the derivative with respect to the variance component, $\tau^2$, here.</p> <p>Notice that $\tau^2$ only appears in the covariance matrix for the random effects within the approximate marginal log quasi-likelihood. Thus:</p> \[\begin{equation} \label{eq:score-tau} \begin{aligned} U_{\tau^2_j}(\hat{\theta}) &amp;= \left. \frac{\partial \ell_q(\mathbf{y}; \theta \rvert \beta)}{\partial \tau^2_j} \right\rvert_{\theta = \hat{\theta}} \\ &amp;= \frac{1}{2} \text{tr} \left[\tilde{\mathbf{Z}}^\top \left(\bar{\Omega} \bar{\Delta}^{-1}(\mathbf{y} - \bar{\mu})(\mathbf{y} - \bar{\mu})^\top \bar{\Delta}^{-1} \bar{\Omega} - \bar{\Xi}\right) \tilde{\mathbf{Z}} \left. \left[ \frac{\partial \tilde{D}(\tau^2)}{\partial \tau^2_j} \right] \right\rvert_{\theta = \hat{\theta}} \right] \end{aligned} \end{equation}\] <hr> <h2 id="information">Information</h2> <h3 id="intermediate-quantities-1">Intermediate Quantities</h3> <p>Let $\kappa_{r, i}$ denote the $r$-th cumulant of $y_i$. If we assume that the following relationship holds for $r = 2$ and $r = 3$:</p> \[\begin{aligned} \kappa_{2, i} &amp;= \phi v(\mu_i) \\ \kappa_{(r+1), i} &amp;= \kappa_{2, i} \frac{\partial \kappa_{r, i}}{\partial \mu_i} \end{aligned}\] <p>then we have:</p> \[\begin{aligned} \kappa_{3, i} &amp;= \phi^2 v(\mu_i) \frac{\partial v(\mu_i)}{\partial\mu_i} \\ \kappa_{4, i} &amp;= \phi^3 v(\mu_i) \left( v(\mu_i) \frac{\partial^2 v(\mu_i)}{\partial\mu_i^2} + \left(\frac{\partial v(\mu_i)}{\partial \mu_i} \right)^2 \right) \end{aligned}\] <div class="example"> <strong>Example.</strong> <br> $$ \begin{aligned} \kappa_{2, i} &amp;= \phi \left(\mu_i + \frac{1}{\gamma} \mu_i^2 \right) \\ \kappa_{3, i} &amp;= \phi^2 \left(\mu_i + \frac{1}{\gamma} \mu_i^2\right)\left(1 + \frac{2}{\gamma} \mu_i \right) \\ &amp;= \phi^2 \mu_i \left(1 + \frac{1}{\gamma} \mu_i \right)\left(1 + \frac{2}{\gamma} \mu_i \right) \\ \kappa_{4, i} &amp;= \phi^3 \left(\mu_i + \frac{1}{\gamma} \mu_i^2\right) \left( \left(\mu_i + \frac{1}{\gamma} \mu_i^2\right) \frac{\partial}{\partial \mu_i} \left[1 + \frac{2}{\gamma} \mu_i \right] + \left(1 + \frac{2}{\gamma} \mu_i \right)^2 \right)\\ &amp;= \phi^3\left[ \frac{2}{\gamma} \left(\mu_i + \frac{1}{\gamma} \mu_i^2\right)^2 + \left(\mu_i + \frac{1}{\gamma} \mu_i^2\right) \left(1 + \frac{2}{\gamma} \mu_i \right)^2 \right] \end{aligned} $$ </div> <p>We define:</p> \[\begin{aligned} r_{i,i} &amp;= \omega_i^4 \delta_i^{-4} \kappa_{4, i} + 2 \omega_i^2 + e^2_i \kappa_{2, i} - 2 \omega_i^2 \delta_i^{-2} e_i \kappa_{3, i} \\ r_{i, i'} &amp;= 2 \omega_i \omega_{i'} \\ c_i &amp;= \omega_i^3 \delta_i^{-3} \kappa_{3,i} - \omega_i \delta_i^{-1} e_i \kappa_{2,i} \\ \mathbf{A}_j &amp;= \tilde{\mathbf{Z}} \left. \left[ \frac{\partial D(\tau^2)}{\partial \tau^2_j} \right] \right\rvert_{\tau^2 = \mathbf{0}_m} \tilde{\mathbf{Z}}^\top \end{aligned}\] <div class="example"> <strong>Example.</strong> <br> First note that: $$ \omega_i \delta_i^{-1} = \frac{\mu_i}{\mu_i \phi \left(1 + \frac{1}{\gamma} \mu_i\right)} = \frac{1}{\phi\left(1 + \frac{1}{\gamma} \mu_i \right)} $$ Then: $$ \begin{aligned} r_{i,i} &amp;= \frac{\mu_i \left[ \frac{2}{\gamma} \mu_i \left(1 + \frac{1}{\gamma} \mu_i\right) + \left(1 + \frac{2}{\gamma} \mu_i \right)^2 \right]}{\phi\left(1 + \frac{1}{\gamma} \mu_i \right)^3} + \frac{2 \mu_i^2}{\phi^2 \left( 1 + \frac{1}{\gamma} \mu_i\right)^2} + \frac{\mu_i^3}{\gamma^2 \phi \left(1 + \frac{1}{\gamma} \mu_i\right)^3} - \frac{2 \mu_i^2 \left(1 + \frac{2}{\gamma} \mu_i \right)}{\gamma^3 \phi \left(1 + \frac{1}{\gamma} \mu_i\right)^5} \\ r_{i,i'} &amp;= \frac{2 \mu_i \mu_{i'}}{\phi^2\left(1 + \frac{1}{\gamma} \mu_i \right)\left(1 + \frac{1}{\gamma} \mu_{i'} \right)} \\ c_i &amp;= \frac{ \mu_i \left(1 + \frac{2}{\gamma} \mu_i \right)}{\phi \left(1 + \frac{1}{\gamma} \mu_i\right)^2} \\ \mathbf{A}_j &amp;= \tilde{\mathbf{Z}} \tilde{\mathbf{Z}}^\top \end{aligned} $$ <details> <summary>Proof.</summary> $$ \begin{aligned} r_{i,i} &amp;= \frac{\kappa_{4, i}}{\phi^4\left(1 + \frac{1}{\gamma} \mu_i \right)^4} + \frac{2 \mu_i^2}{\phi^2 \left( 1 + \frac{1}{\gamma} \mu_i\right)^2} + \frac{\kappa_{2,i} \mu_i^2}{\phi^2 \gamma^2 \left(1 + \frac{1}{\gamma} \mu_i\right)^4} - \frac{2 \kappa_{3,i} \mu_i}{\phi^3 \gamma^3\left(1 + \frac{1}{\gamma} \mu_i\right)^6} \\ &amp;= \frac{\phi^3\left[ \frac{2}{\gamma} \left(\mu_i + \frac{1}{\gamma} \mu_i^2\right)^2 + \left(\mu_i + \frac{1}{\gamma} \mu_i^2\right) \left(1 + \frac{2}{\gamma} \mu_i \right)^2 \right]}{\phi^4\left(1 + \frac{1}{\gamma} \mu_i \right)^4} + \frac{2 \mu_i^2}{\phi^2 \left( 1 + \frac{1}{\gamma} \mu_i\right)^2} + \frac{\phi \left(\mu_i + \frac{1}{\gamma} \mu_i^2 \right) \mu_i^2}{\phi^2 \gamma^2 \left(1 + \frac{1}{\gamma} \mu_i\right)^4} - \frac{2 \phi^2 \mu_i \left(1 + \frac{1}{\gamma} \mu_i \right)\left(1 + \frac{2}{\gamma} \mu_i \right) \mu_i}{\phi^3 \gamma^3\left(1 + \frac{1}{\gamma} \mu_i\right)^6} \\ &amp;= \frac{\left[ \frac{2}{\gamma} \mu_i^2 \left(1 + \frac{1}{\gamma} \mu_i\right)^2 + \mu_i \left(1 + \frac{1}{\gamma} \mu_i\right) \left(1 + \frac{2}{\gamma} \mu_i \right)^2 \right]}{\phi\left(1 + \frac{1}{\gamma} \mu_i \right)^4} + \frac{2 \mu_i^2}{\phi^2 \left( 1 + \frac{1}{\gamma} \mu_i\right)^2} + \frac{\left(1 + \frac{1}{\gamma} \mu_i\right) \mu_i^3}{\gamma^2 \phi \left(1 + \frac{1}{\gamma} \mu_i\right)^4} - \frac{2 \mu_i^2 \left(1 + \frac{2}{\gamma} \mu_i \right)}{\gamma^3 \phi \left(1 + \frac{1}{\gamma} \mu_i\right)^5} \\ &amp;= \frac{\mu_i \left(1 + \frac{1}{\gamma} \mu_i\right)\left[ \frac{2}{\gamma} \mu_i \left(1 + \frac{1}{\gamma} \mu_i\right) + \left(1 + \frac{2}{\gamma} \mu_i \right)^2 \right]}{\phi\left(1 + \frac{1}{\gamma} \mu_i \right)^4} + \frac{2 \mu_i^2}{\phi^2 \left( 1 + \frac{1}{\gamma} \mu_i\right)^2} + \frac{\mu_i^3}{\gamma^2 \phi \left(1 + \frac{1}{\gamma} \mu_i\right)^3} - \frac{2 \mu_i^2 \left(1 + \frac{2}{\gamma} \mu_i \right)}{\gamma^3 \phi \left(1 + \frac{1}{\gamma} \mu_i\right)^5} \\ &amp;= \frac{\mu_i \left[ \frac{2}{\gamma} \mu_i \left(1 + \frac{1}{\gamma} \mu_i\right) + \left(1 + \frac{2}{\gamma} \mu_i \right)^2 \right]}{\phi\left(1 + \frac{1}{\gamma} \mu_i \right)^3} + \frac{2 \mu_i^2}{\phi^2 \left( 1 + \frac{1}{\gamma} \mu_i\right)^2} + \frac{\mu_i^3}{\gamma^2 \phi \left(1 + \frac{1}{\gamma} \mu_i\right)^3} - \frac{2 \mu_i^2 \left(1 + \frac{2}{\gamma} \mu_i \right)}{\gamma^3 \phi \left(1 + \frac{1}{\gamma} \mu_i\right)^5} \\ c_i &amp;= \frac{\kappa_{3,i}}{\phi^3 \left(1 + \frac{1}{\gamma} \mu_i\right)^3} \\ &amp;= \frac{\phi^2 \mu_i \left(1 + \frac{1}{\gamma} \mu_i \right) \left(1 + \frac{2}{\gamma} \mu_i \right)}{\phi^3 \left(1 + \frac{1}{\gamma} \mu_i\right)^3} \\ &amp;= \frac{ \mu_i \left(1 + \frac{2}{\gamma} \mu_i \right)}{\phi \left(1 + \frac{1}{\gamma} \mu_i\right)^2} \end{aligned} $$ </details> </div> <p>We also define \(\mathbf{a}_j = \text{diag}(\mathbf{A}_j)\) (which is $n$-dimensional) and has elements denoted by \(a^j_{i,i}\).</p> <h3 id="information-1">Information</h3> <p>The Fisher information is the variance of the score.</p> \[\begin{aligned} \mathcal{I}_{\tau^2_j, \tau^2_k} &amp;= \frac{1}{4} \left( \sum_{i = 1}^n \left[ a_{i,i}^j a_{i,i}^k r_{i,i} + \sum_{i \neq i'}^n a_{i,i'}^j a_{i,i'}^k r_{i,i'} \right] \right) \\ \mathcal{I}_{\alpha, \tau^2_j} &amp;= \frac{1}{2} \sum_{i = 1}^n a_{i,i}^j c_i \mathbf{x}_i \\ \mathcal{I}_{\alpha, \alpha} &amp;= \sum_{i = 1}^n \omega_i \mathbf{x}_i \mathbf{x}_i^\top \end{aligned}\] <p>Recall that the trace of the product of two $n \times n$ matrices can be written as a double summation:</p> \[\text{tr}[AB] = \sum_{i = 1}^n \sum_{i' = 1}^n A_{i,i'} B_{i,i'}\] \[\begin{aligned} U_{\tau^2_j}(\hat{\theta}) &amp;= \frac{1}{2} \text{tr}\left[ \tilde{\mathbf{Z}}^\top \left( \bar{\Omega}\bar{\Delta}^{-1}(\mathbf{y} - \bar{\mu})(\mathbf{y} - \bar{\mu})^\top \bar{\Delta}^{-1} \bar{\Omega} - \bar{\Xi} \right) \tilde{\mathbf{Z}} \left. \left[ \frac{\partial \tilde{D}(\tau^2)}{\partial \tau^2_j} \right] \right\rvert_{\theta = \hat{\theta}} \right] \\ &amp;= \frac{1}{2} \text{tr}\left[\left( \bar{\Omega}\bar{\Delta}^{-1}(\mathbf{y} - \bar{\mu})(\mathbf{y} - \bar{\mu})^\top \bar{\Delta}^{-1} \bar{\Omega} - \bar{\Xi} \right) \tilde{\mathbf{Z}} \left. \left[ \frac{\partial \tilde{D}(\tau^2)}{\partial \tau^2_j} \right] \right\rvert_{\theta = \hat{\theta}} \tilde{\mathbf{Z}}^\top \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^n \sum_{i' = 1}^n \left( \bar{\Omega}\bar{\Delta}^{-1}(\mathbf{y} - \bar{\mu})(\mathbf{y} - \bar{\mu})^\top \bar{\Delta}^{-1} \bar{\Omega} - \bar{\Xi} \right)_{i,i'} \left( \tilde{\mathbf{Z}} \left. \left[ \frac{\partial \tilde{D}(\tau^2)}{\partial \tau^2_j} \right] \right\rvert_{\theta = \hat{\theta}} \tilde{\mathbf{Z}}^\top \right)_{i,i'} \end{aligned}\] <p>Let \(\zeta_{i,i'} = \left( \tilde{\mathbf{Z}} \left[ \frac{\partial \tilde{D}(\tau^2)}{\partial \tau^2_j} \right]\tilde{\mathbf{Z}}^\top \right)_{i,i'}\), and let \(\bar{\psi}_i = \bar{\omega}_i \bar{\delta}_i^{-1}\). We can then write this as:</p> \[\begin{aligned} U_{\tau^2_j}(\hat{\theta}) &amp;= \frac{1}{2} \sum_{i = 1}^n \sum_{i' = 1}^n \left( \bar{\Omega}\bar{\Delta}^{-1}(\mathbf{y} - \bar{\mu})(\mathbf{y} - \bar{\mu})^\top \bar{\Delta}^{-1} \bar{\Omega} - \bar{\Xi} \right)_{i,i'} \left( \tilde{\mathbf{Z}} \left[ \frac{\partial \tilde{D}(\tau^2)}{\partial \tau^2_j} \right] \tilde{\mathbf{Z}}^\top \right)_{i,i'} \\ &amp;= \frac{1}{2}\sum_{i = 1}^n \sum_{i' = 1}^n \left( \bar{\omega}_i \bar{\omega}_{i'} \bar{\delta}_i^{-1} \bar{\delta}_{i'}^{-1} (y_i - \bar{\mu}_i)(y_{i'} - \bar{\mu}_{i'}) - \bar{\xi}_{i,i'} \right) \zeta_{i,i'} \\ &amp;= \frac{1}{2} \left[ \sum_{i = 1}^n \sum_{i' = 1}^n \bar{\psi}_i \bar{\psi}_{i'} (y_i - \bar{\mu}_i)(y_{i'} - \bar{\mu}_{i'}) \zeta_{i,i'} - \sum_{i = 1}^n \bar{\xi}_i \zeta_{i,i} \right] &amp; \left(\Xi \text{ is diagonal}\right) \\ &amp;= \frac{1}{2} \left[ \sum_{i = 1}^n \sum_{i' = 1}^n \bar{\psi}_i \bar{\psi}_{i'} (y_i - \bar{\mu}_i)(y_{i'} - \bar{\mu}_{i'}) \zeta_{i,i'} - \sum_{i = 1}^n (\bar{\omega}_i + \bar{e}_i (y_i - \bar{\mu}_i)) \zeta_{i,i} \right] \end{aligned}\] <p>This leads us to:</p> \[\begin{aligned} \mathcal{I}_{\tau^2_r, \tau^2_s} &amp;= \mathbb{E}_{H_0} \left[ U_{\tau^2_r} U_{\tau^2_s} \right] \\ &amp;= \frac{1}{4} \mathbb{E}_{H_0} \left[ \left( \sum_{i = 1}^n \sum_{i' = 1}^n \bar{\psi}_i \bar{\psi}_{i'} (y_i - \bar{\mu}_i)(y_{i'} - \bar{\mu}_{i'}) \zeta_{i,i'} - \sum_{i = 1}^n (\bar{\omega}_i + \bar{e}_i (y_i - \bar{\mu}_i)) \zeta_{i,i} \right) \left( \sum_{j = 1}^n \sum_{j' = 1}^n \bar{\psi}_j \bar{\psi}_{j'} (y_j - \bar{\mu}_j)(y_{j'} - \bar{\mu}_{j'}) \zeta_{j,j'} - \sum_{j = 1}^n (\bar{\omega}_j + \bar{e}_j (y_j - \bar{\mu}_j)) \zeta_{j,j} \right) \right] \\ &amp;= \frac{1}{4} \left[ \underbrace{\sum_{i = 1}^n \sum_{i' = 1}^n \sum_{j = 1}^n \sum_{j' = 1}^n \bar{\psi}_i \bar{\psi}_{i'} \bar{\psi}_j \bar{\psi}_{j'} \zeta_{i,i'} \zeta_{j, j'} \mathbb{E}_{H_0}\left[ (y_i - \bar{\mu}_i)(y_{i'} - \bar{\mu}_{i'})(y_j - \bar{\mu}_j)(y_{j'} - \bar{\mu}_{j'}) \right]}_{(i)} \right. \\ &amp;\hspace{5mm} -\left. \underbrace{\sum_{i = 1}^n \sum_{i' = 1}^n \sum_{j = 1}^n \bar{\psi}_i \bar{\psi}_{i'} \zeta_{i,i'} \zeta_{j,j} \left ( \bar{\omega}_j \mathbb{E}_{H_0} \left[ (y_i - \bar{\mu}_i)(y_{i'} - \bar{\mu}_{i'}) \right] + \bar{e}_i \mathbb{E}_{H_0} \left[ (y_i - \bar{\mu}_i)(y_{i'} - \bar{\mu}_{i'})(y_j - \bar{\mu}_j) \right] \right)}_{(ii)} \right. \\ &amp;\hspace{5mm} - \left. \underbrace{\sum_{j = 1}^n \sum_{j' = 1}^n \sum_{i = 1}^n \bar{\psi}_j \bar{\psi}_{j'} \zeta_{j,j'} \zeta_{i,i} \left( \bar{\omega}_i \mathbb{E}_{H_0} \left[ (y_j - \bar{\mu}_j)(y_{j'} - \bar{\mu}_{j'}) \right] + \bar{e}_i \mathbb{E}_{H_0} \left[ (y_j - \bar{\mu}_j)(y_{j'} - \bar{\mu}_{j'}) (y_i - \bar{\mu}_i) \right] \right)}_{(iii)} \right. \\ &amp;\hspace{5mm} + \left. \underbrace{\sum_{i = 1}^n \sum_{j = 1}^n \zeta_{i,i} \zeta_{j,j}\mathbb{E}_{H_0}\left[ (\bar{\omega}_i + \bar{e}_i(y_i - \bar{\mu}_i))(\bar{\omega}_j + \bar{e}_j(y_j - \bar{\mu}_j)) \right]}_{(iv)} \right] \\ &amp;= \frac{1}{4} \left[ \sum_{i = 1}^n \bar{\psi}_i^4 \zeta_{i,i}^2 \nu_i^4 + \sum_{i = 1}^n \sum_{j = 1}^n \bar{\psi}_i^2 \bar{\psi}_j^2 \zeta_{i,i} \zeta_{j,j} \nu_i^2 \nu_j^2 + 2 \sum_{i = 1}^n \sum_{j = 1}^n \bar{\psi}_i^2 \bar{\psi}_j^2 \zeta_{i,j}^2 \nu_i^2 \nu_j^2 \right. \\ &amp;\hspace{5mm} - \left. \sum_{i = 1}^n \bar{\psi}^2 \zeta_{i,i}^2 \left(\bar{\omega}_i \nu_i^2 + \bar{e}_i \nu_i^3 \right) + \sum_{i = 1}^n \sum_{j = 1}^n \bar{\psi}_i^2 \zeta_{i,i} \zeta_{j,j} \bar{\omega}_j \nu_i^2 \right. \\ &amp;\hspace{5mm} - \left. \sum_{i = 1}^n \bar{\psi}_i^2 \zeta_{i,i}^2 \left(\bar{\omega}_i \nu_i^2 + \bar{e}_i \nu_i^3 \right) + \sum_{i = 1}^n \sum_{j = 1}^n \bar{\psi}_i^2 \zeta_{i,i} \zeta_{j,j} \bar{\omega}_j \nu^2_i \right. \\ &amp;\hspace{5mm} + \left. \sum_{i = 1}^n \zeta_{i,i}^2 \left(\omega_i^2 + \bar{e}_i^2 \nu_i^2 \right) \right] \\ &amp;= \frac{1}{4} \left[ \sum_{i = 1}^n \bar{\psi}_i^4 \zeta_{i,i}^2 \nu_i^4 + \sum_{i = 1}^n \sum_{j = 1}^n \bar{\psi}_i^2 \bar{\psi}_j^2 \zeta_{i,i} \zeta_{j,j} \nu_i^2 \nu_j^2 + 2 \sum_{i = 1}^n \sum_{j = 1}^n \bar{\psi}_i^2 \bar{\psi}_j^2 \zeta_{i,j}^2 \nu_i^2 \nu_j^2 \right. \\ &amp;\hspace{5mm} - \left. 2\sum_{i = 1}^n \bar{\psi}^2 \zeta_{i,i}^2 \left(\bar{\omega}_i \nu_i^2 + \bar{e}_i \nu_i^3 \right) - 2\sum_{i = 1}^n \sum_{j = 1}^n \bar{\psi}_i^2 \zeta_{i,i} \zeta_{j,j} \bar{\omega}_j \nu_i^2 + \sum_{i = 1}^n \zeta_{i,i}^2 \left(\omega_i^2 + \bar{e}_i^2 \nu_i^2 \right) \right] \\ \end{aligned}\] <p>Now, notice that $\nu_i^2 = V(\mu_i) = \kappa_{2,i}$, $\nu_i^3 = \kappa_{3,i}$, and $\nu_i^4 = \kappa_{4,i} + 3 \kappa_{2,i}^2$. Furthermore, we have that:</p> \[\bar{\psi}_i^2 \nu_i^2 = \bar{\omega}_i^2 \bar{\delta}^{-2}_i V(\mu_i) = \bar{\omega}_i^2 \frac{V(\mu_i)}{\bar{\delta}_i^2} = \frac{\bar{\omega}_i^2}{\bar{\omega}_i} = \bar{\omega}_i\] <p>Thus:</p> \[\begin{aligned} \mathcal{I}_{\tau^2_r, \tau^2_s} &amp;= \frac{1}{4} \left[ \sum_{i = 1}^n \bar{\psi}_i^4 \zeta_{i,i}^2 \nu_i^4 + \sum_{i = 1}^n \sum_{j = 1}^n \bar{\psi}_i^2 \bar{\psi}_j^2 \zeta_{i,i} \zeta_{j,j} \nu_i^2 \nu_j^2 + 2 \sum_{i = 1}^n \sum_{j = 1}^n \bar{\psi}_i^2 \bar{\psi}_j^2 \zeta_{i,j}^2 \nu_i^2 \nu_j^2 \right. \\ &amp;\hspace{5mm} - \left. 2\sum_{i = 1}^n \bar{\psi}^2 \zeta_{i,i}^2 \left(\bar{\omega}_i \nu_i^2 + \bar{e}_i \nu_i^3 \right) - 2\sum_{i = 1}^n \sum_{j = 1}^n \bar{\psi}_i^2 \zeta_{i,i} \zeta_{j,j} \bar{\omega}_j \nu_i^2 + \sum_{i = 1}^n \zeta_{i,i}^2 \left(\omega_i^2 + \bar{e}_i^2 \nu_i^2 \right) \right] \\ &amp;= \frac{1}{4} \left[ \sum_{i = 1}^n \bar{\psi}_i^4 \zeta_{i,i}^2 (\kappa_{4,i} + 3 (V(\mu_i))^2) + \sum_{i = 1}^n \sum_{j = 1}^n \bar{\omega}_i \bar{\omega}_j \zeta_{i,i} \zeta_{j,j} + 2 \sum_{i = 1}^n \sum_{j = 1}^n \bar{\omega}_i \bar{\omega}_j \zeta_{i,j}^2 \right. \\ &amp;\hspace{5mm} - \left. 2\sum_{i = 1}^n \bar{\psi}^2 \zeta_{i,i}^2 \left(\bar{\omega}_i V(\mu_i) + \bar{e}_i \kappa_{3,i} \right) - 2\sum_{i = 1}^n \sum_{j = 1}^n \bar{\omega}_i \bar{\omega}_j \zeta_{i,i} \zeta_{j,j} + \sum_{i = 1}^n \zeta_{i,i}^2 \left(\omega_i^2 + \bar{e}_i^2 V(\mu_i) \right) \right] \\ \end{aligned}\] <hr> <p>Notice that we have four different indices: $i$, $i’$, $j$, and $j’$. There are four possible cases: all indices are equal, three are equal and one is different, two are equal and the others are different, two pairs are equal, and all are different. If we have a case where we are taking the expectation of the product of residuals and a single index is different from all of the others (the second, third, and last case), then the entire expectation will equal zero by the independence of observations. <br> Though we only need to consider the remaining cases when evaluating $(i)$, $(ii)$, $(iii)$, and $(iv)$, we must be sure to remember that there are three possible ways to choose two pairs. <br> Let $\nu_i^j$ denote the $j$-th central moment for observation $i$. We have:</p> \[\begin{aligned} (i) &amp;= \sum_{i = 1}^n \sum_{i' = 1}^n \sum_{j = 1}^n \sum_{j' = 1}^n \bar{\psi}_i \bar{\psi}_{i'} \bar{\psi}_j \bar{\psi}_{j'} \zeta_{i,i'} \zeta_{j, j'} \mathbb{E}_{H_0}\left[ (y_i - \bar{\mu}_i) (y_{i'} - \bar{\mu}_{i'}) (y_j - \bar{\mu}_j)(y_{j'} - \bar{\mu}_{j'}) \right] \\ &amp;= \underbrace{\sum_{i = 1}^n \bar{\psi}_i^4 \zeta_{i,i}^2 \mathbb{E}_{H_0}\left[ (y_i - \bar{\mu}_i)^4 \right]}_{\text{i = i' = j = j'}} \\ &amp;\hspace{5mm} + \underbrace{\sum_{i = 1}^n \sum_{j = 1}^n \bar{\psi}_i^2 \bar{\psi}_j^2 \zeta_{i,i} \zeta_{j,j} \mathbb{E}_{H_0} \left[(y_i - \bar{\mu}_i)^2 \right] \mathbb{E}_{H_0} \left[ (y_j - \bar{\mu}_j)^2 \right]}_{i = i'; j = j'} \\ &amp;\hspace{5mm} + \underbrace{\sum_{i = 1}^n \sum_{i' = 1}^n \bar{\psi}_i^2 \bar{\psi}_{i'}^2 \zeta_{i,i'}^2 \mathbb{E}_{H_0} \left[(y_i - \bar{\mu}_i)^2 \right] \mathbb{E}_{H_0} \left[ (y_{i'} - \bar{\mu}_{i'})^2 \right]}_{i = j; i' = j'} \\ &amp;\hspace{5mm} + \underbrace{\sum_{i = 1}^n \sum_{i' = 1}^n \bar{\psi}_i^2 \bar{\psi}_{i'}^2 \zeta_{i,i'} \zeta_{i',i} \mathbb{E}_{H_0} \left[(y_i - \bar{\mu}_i)^2 \right] \mathbb{E}_{H_0} \left[ (y_{i'} - \bar{\mu}_{i'})^2 \right]}_{i = j'; i' = j} \\ &amp;= \sum_{i = 1}^n \bar{\psi}_i^4 \zeta_{i,i}^2 \mathbb{E}_{H_0}\left[ (y_i - \bar{\mu}_i)^4 \right] \\ &amp;\hspace{5mm} + \sum_{i = 1}^n \sum_{j = 1}^n \bar{\psi}_i^2 \bar{\psi}_j^2 \zeta_{i,i} \zeta_{j,j} \mathbb{E}_{H_0} \left[(y_i - \bar{\mu}_i)^2 \right] \mathbb{E}_{H_0} \left[ (y_j - \bar{\mu}_j)^2 \right]\\ &amp;\hspace{5mm} + 2\sum_{i = 1}^n \sum_{i' = 1}^n \bar{\psi}_i^2 \bar{\psi}_{i'}^2 \zeta_{i,i'}^2 \mathbb{E}_{H_0} \left[(y_i - \bar{\mu}_i)^2 \right] \mathbb{E}_{H_0} \left[ (y_{i'} - \bar{\mu}_{i'})^2 \right] &amp; \left(\zeta \text{ is symmetric}\right)\\ &amp;= \sum_{i = 1}^n \bar{\psi}_i^4 \zeta_{i,i}^2 \nu_i^4 + \sum_{i = 1}^n \sum_{j = 1}^n \bar{\psi}_i^2 \bar{\psi}_j^2 \zeta_{i,i} \zeta_{j,j} \nu_i^2 \nu_j^2 + 2 \sum_{i = 1}^n \sum_{j = 1}^n \bar{\psi}_i^2 \bar{\psi}_j^2 \zeta_{i,j}^2 \nu_i^2 \nu_j^2 &amp; \left(\text{switch index}\right) \end{aligned}\] \[\begin{aligned} (ii) &amp;= \sum_{i = 1}^n \sum_{i' = 1}^n \sum_{j = 1}^n \bar{\psi}_i \bar{\psi}_{i'} \zeta_{i,i'} \zeta_{j,j} \left ( \bar{\omega}_j \mathbb{E}_{H_0} \left[ (y_i - \bar{\mu}_i)(y_{i'} - \bar{\mu}_{i'}) \right] + \bar{e}_i \mathbb{E}_{H_0} \left[ (y_i - \bar{\mu}_i)(y_{i'} - \bar{\mu}_{i'})(y_j - \bar{\mu}_j) \right] \right) \\ &amp;= \underbrace{\sum_{i = 1}^n \bar{\psi}_i^2 \zeta_{i,i}^2 \left(\bar{\omega}_i \mathbb{E}_{H_0}\left[ (y_i - \bar{\mu}_i)^2 \right] + \bar{e}_i \mathbb{E}_{H_0}\left[ (y_i - \bar{\mu}_i)^3 \right]\right)}_{i=i'=j} \\ &amp;\hspace{5mm} + \underbrace{\sum_{i = 1}^n \sum_{j = 1}^n \bar{\psi}_i^2 \zeta_{i,i} \zeta_{j,j} \bar{\omega}_j \mathbb{E}_{H_0}\left[(y_i - \bar{\mu}_i)^2 \right] }_{i = i' \neq j} \\ &amp;= \sum_{i = 1}^n \bar{\psi}^2 \zeta_{i,i}^2 \left(\bar{\omega}_j \nu_i^2 + \bar{e}_i \nu_i^3 \right) + \sum_{i = 1}^n \sum_{j = 1}^n \bar{\psi}_i^2 \zeta_{i,i} \zeta_{j,j} \bar{\omega}_j \nu_i^2 \end{aligned}\] \[\begin{aligned} (iii) &amp;= \sum_{j = 1}^n \sum_{j' = 1}^n \sum_{i = 1}^n \bar{\psi}_j \bar{\psi}_{j'} \zeta_{j,j'} \zeta_{i,i} \left( \bar{\omega}_i \mathbb{E}_{H_0} \left[ (y_j - \bar{\mu}_j)(y_{j'} - \bar{\mu}_{j'}) \right] + \bar{e}_i \mathbb{E}_{H_0} \left[ (y_j - \bar{\mu}_j)(y_{j'} - \bar{\mu}_{j'}) (y_i - \bar{\mu}_i) \right] \right) \\ &amp;= \underbrace{\sum_{j = 1}^n \bar{\psi}_j^2 \zeta_{j,j}^2 \left(\bar{\omega}_j \mathbb{E}_{H_0}\left[ (\mathbf{y}_j - \bar{\mu}_j)^2 \right] + \bar{e}_j \mathbb{E}_{H_0}\left[ (y_j - \bar{\mu}_j)^3 \right] \right)}_{j = j' = i} \\ &amp;\hspace{5mm} + \underbrace{\sum_{j = 1}^n \sum_{i = 1}^n \bar{\psi}_j^2 \zeta_{j,j}\zeta_{i,i} \bar{\omega}_i \mathbb{E}_{H_0}\left[ (y_j - \bar{\mu}_j)^2 \right]}_{j = j' \neq i} \\ &amp;= \sum_{i = 1}^n \bar{\psi}_i^2 \zeta_{i,i}^2 \left(\bar{\omega}_i \nu_i^2 + \bar{e}_i \nu_i^3 \right) + \sum_{i = 1}^n \sum_{j = 1}^n \bar{\psi}_i^2 \zeta_{i,i} \zeta_{j,j} \bar{\omega}_j \nu^2_i \end{aligned}\] \[\begin{aligned} (iv) &amp;= \sum_{i = 1}^n \sum_{j = 1}^n \zeta_{i,i} \zeta_{j,j}\mathbb{E}_{H_0}\left[ (\bar{\omega}_i + \bar{e}_i(y_i - \bar{\mu}_i))(\bar{\omega}_j + \bar{e}_j(y_j - \bar{\mu}_j)) \right] \\ &amp;= \underbrace{\sum_{i = 1}^n \zeta_{i,i}^2 \left( \bar{\omega}_i^2 + \bar{e}_i^2 \mathbb{E}_{H_0}\left[ (y_i - \bar{\mu}_i)^2 \right]\right)}_{i = j} \\ &amp;= \sum_{i = 1}^n \zeta_{i,i}^2 \left(\omega_i^2 + \bar{e}_i^2 \nu_i^2 \right) \end{aligned}\] <hr> <h2 id="individual-variance-component-tests">Individual Variance Component Tests</h2> <p>We may also be interested in testing whether a single coordinate of the variance component is zero while not impposing that the other are. This is testing the hypotheses:</p> \[H_0: \tau^2_j = 0 \hspace{15mm} \text{vs.} \hspace{15mm} H_1: \tau^2_j &gt; 0\] <p>where we notice that the alternative is restricted to the positive reals.</p> <p>We’ll use a subscript $-j$ to denote a vector with the $j$-th component removed; i.e. \(\mathbf{v}_{-j} = (\mathbf{v}_1, \dots, \mathbf{v}_{j - 1}, \mathbf{v}_{j + 1}, \dots, \mathbf{v}_m)^\top\). We’ll let $f(\beta_j)$and $f(\beta_{-j})$ denote the log density functions of $\beta_j$ and $\beta_{-j}$, respectively. Define:</p> \[\ell_q(\mathbf{y}; \beta_j) = \log \int \exp \left( \sum_{i = 1}^N \ell_q(y_i; \theta \rvert \beta) + f(\beta_{-j}) \right) d\beta_{-j}\] <p>and rewrite the marginal log quasi-likelihood as:</p> \[\begin{aligned} \ell_q(\mathbf{y}; \theta) &amp;= \log \int \exp\left( \ell_q(\mathbf{y}; \theta \rvert \beta_j) + f(\beta_j) \right) d \beta_j \\ &amp;= \log \int \exp\left( \log \int \exp \left( \sum_{i = 1}^N \ell_q(y_i; \theta \rvert \beta) + f(\beta_{-j}) \right) d\beta_{-j} + f(\beta_j) \right) d \beta_j \\ &amp;= \log \int \end{aligned}\] </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2026-01-05-score-test-lin.bib"></d-bibliography> <d-article> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Anna Rosengart. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: February 17, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?d4c3ed73337d78e34b10d24890d1fc56"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/tabs.min.js?b8748955e1076bbe0dabcf28f2549fdc"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>