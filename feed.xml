<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://aerosengart.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://aerosengart.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-12-16T16:47:40+00:00</updated><id>https://aerosengart.github.io/feed.xml</id><title type="html">Anna Rosengart</title><entry><title type="html">Score Calcuations</title><link href="https://aerosengart.github.io/blog/2025/score-test-neg-binom/" rel="alternate" type="text/html" title="Score Calcuations"/><published>2025-12-11T00:00:00+00:00</published><updated>2025-12-11T00:00:00+00:00</updated><id>https://aerosengart.github.io/blog/2025/score-test-neg-binom</id><content type="html" xml:base="https://aerosengart.github.io/blog/2025/score-test-neg-binom/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>This post is just a catch-all for my derivations for my score test project with negative binomial outcomes. Our set-up is as follows. We have $n$ observations coming from $k$ different clusters, each of size $n_t$ for $t \in [k]$. The full data will be denoted by $\mathbf{y}$. Though $\mathbf{y}$ is a vector, we’ll denote the $j$-th observation from cluster $i$ with $\mathbf{y}_{i,j}$. For example, \(\mathbf{y}_{i,j}\) denotes element \(\sum_{l = 1}^{i - 1} n_l + j\) of $\mathbf{y}$. We’ll also denote the $n_i$-dimensional vector of responses for cluster $i$ with $\mathbf{y}_i$.</p> <p>For each observation, we will have $p$ fixed effect covariates arranged in a $p$-dimensional vector, \(\mathbf{x}_{i, j}\), and $q$ random effects covariates in a $q$-dimensional vector, \(\mathbf{z}_{i,j}\). We’ll assume that the observations within the same cluster are independent.</p> <p>Our model comes in the form of a specification of the conditional mean, $\mu_{i,j} = \mathbb{E}[\mathbf{y}_{i,j} \rvert \beta_i]$ (where we suppress the addition conditioning on the covariates themselves). For a monotonic and differentiable link function (e.g. $\log(\cdot)$ or $\text{logit}(\cdot)$), the conditional mean of the $j$-th observation in group $i$ is assumed to be given by:</p> \[\mu_{i,j} = g^{-1}\left(\alpha^\top \mathbf{x}_{i,j} + \beta_i^\top \mathbf{z}_{i,j} \right) \label{eq:glmm}\] <p>We then assume that the observations themselves follow some exponential family distribution with measurement errors, $\epsilon_{i,j}$, which is the deviation of the response from its (unit-specific) conditional mean. These errors are assumed to have mean zero and be independent of each other and of the random effects. We further assume the responses, $\mathbf{y}_{i,j}$, conditional on the random effects (and the covariates), are independent with variances equal to some function of the conditional mean.</p> <p>In general, we will assume that:</p> \[\beta_i \overset{iid}{\sim} \mathcal{N}\left(\mathbf{0}_q, D(\tau^2) \right)\] <p>for some variance component, $\tau^2$. We’ll use $[\cdot] \rvert_{H_0}$ to denote evaluation of the function in brackets when setting $\beta$ equal to $\beta_0$. We’ll also use a superscript $0$ (e.g. $\mu^0$, $\eta^0$, etc.) to denote the quantity under the null hypothesis (i.e. $\tau^2 = \mathbf{0} \implies \beta = \mathbf{0}$).</p> <hr/> <h2 id="negative-binomial-case">Negative Binomial Case</h2> <h3 id="set-up">Set-Up</h3> <p>In this example, we’ll let the responses be negative binomial. To keep things simple, we’ll say we only have a single fixed intercept and a single random slope. We let $\phi &gt; 0$ denote the <i>unknown</i> dispersion parameter and assume the conditional mean to be given by:</p> \[\mu_{i,j} = \exp\left( \alpha_i + \beta_i \mathbf{z}_{i,j} \right) \label{eq:neg-bin-mean}\] <p>The likelihood based on a single observation, $\mathbf{y}_{i,j}$, is given by:</p> \[\mathcal{L}(\mathbf{y}_{i,j}; \alpha_i, \tau^2 \rvert \beta_i) = \frac{\Gamma\left(\mathbf{y}_{i,j} + \frac{1}{\phi}\right)}{\Gamma(\mathbf{y}_{i,j} + 1) \Gamma\left(\frac{1}{\phi} \right)}\left(\frac{1}{1 + \phi \mathbf{y}_{i,j}}\right)^{\frac{1}{\phi}} \left( \frac{\phi \mu_{i,j}}{1 + \phi \mu_{i,j}} \right)^{\mathbf{y}_{i,j}} \label{eq:neg-bin-single-lik}\] <p>where $\Gamma(\cdot)$ is the gamma function:</p> \[\Gamma(x) = \int_0^\infty t^{x - 1} \exp(-t) dt\] <p>The above parametrization of the likelihood implies that the conditional variance of the responses is given by:</p> \[V(\mu_{i,j}) = \mu_{i,j} + \frac{1}{\phi} \mu_{i,j}^2\] <p>The conditional log-likelihood based on cluster $i$ is:</p> \[\ell(\mathbf{y}_i; \alpha_i, \tau^2 \rvert \beta_i) = \sum_{j = 1}^{n_i} \left[ \log \Gamma \left( \mathbf{y}_{i,j} + \frac{1}{\phi} \right) - \log \Gamma\left(\mathbf{y}_{i,j} + 1\right) - \log\Gamma\left(\frac{1}{\phi} \right) - \frac{1}{\phi} \log\left(1 + \phi \mathbf{y}_{i,j} \right) + \mathbf{y}_{i,j} \left( \log(\phi \mu_{i,j}) - \log(1 + \phi \mu_{i,j}) \right) \right] \label{eq:neg-bin-full-cond-ll}\] <p>We assume to have the following generalized linear mixed model:</p> \[\begin{equation} \label{eq:glmm-y} \begin{aligned} \mathbf{y}_{i,j} \rvert \beta_i &amp;\sim \text{NegBin}(\mu_{i,j}, \phi) \\ \mu_{i,j} &amp;= \exp\left(\eta_{i,j}\right) = \exp\left(\alpha_i + \beta_i \mathbf{z}_{i,j}\right) \end{aligned} \end{equation}\] <h3 id="approximation">Approximation</h3> <p>We follow a pseudo-likelihood approach (see <a href="/blog/2025/glmm.html">here</a>) which permits a Gaussian approximation of the outcome distribution via a linearization. Suppose we find the MLEs of the $\alpha_i$ and $\phi$ terms under $H_0$, which we can do with <a href="/blog/2025/glm.html#weighted-least-squares">iteratively reweighted least squares</a> or some comparable algorithm (basically just fitting the null model). Denote these estimates with $\tilde{\alpha_i}$ and $\tilde{\phi}$, respectively. We can compute our <i>working</i> responses and errors as:</p> \[\begin{equation} \label{eq:working-responses} \mathbf{y}^\star_{i,j} = \alpha_i + \beta_i \mathbf{z}_{i,j} + \epsilon^\star_{i,j}; \hspace{10mm} \epsilon^\star_{i,j} \sim \mathcal{N}\left(0, \frac{V(\tilde{\mu}_{i,j})}{\delta^2(\tilde{\eta}_{i,j})}\right) \end{equation}\] <p>where</p> \[\tilde{\eta}_{i,j} = \tilde{\alpha}_i; \hspace{10mm} \delta(\tilde{\eta}_{i,j}) = \frac{\partial g^{-1}(\eta_{i,j})}{\partial \eta_{i,j}}\bigg\rvert_{\eta_{i,j} = \tilde{\eta}_{i,j}}\] <p>Since $g(\cdot) = \log(\cdot)$, we have that \(\delta(\tilde{\eta}_{i,j}) = \exp(\tilde{\eta}_{i,j})\), implying that the working error variances are:</p> \[\frac{V(\tilde{\mu}_{i,j})}{\delta^2(\tilde{\eta}_{i,j})} = \frac{\exp(\tilde{\eta}_{i,j}) + \frac{1}{\phi}\exp(\tilde{\eta}_{i,j})}{\exp^2(\tilde{\eta}_{i,j})} = \frac{1}{\exp(\tilde{\eta}_{i,j})}\left(1 + \frac{1}{\tilde{\phi}}\right)\] <p>where we recall that $\tilde{\eta}_{i,j} = \tilde{\alpha}_i$ under $H_0$. Eq. \eqref{eq:working-responses} describes a linear mixed model (i.e. Gaussian outcomes), defined in terms of the working responses and errors. The only big difference is that we force the error variances to all be different and fix them at:</p> \[\sigma_{i,j}^2 = \frac{V(\tilde{\mu}_{i,j})}{\delta^2(\tilde{\eta}_{i,j})} = \frac{1}{\exp(\tilde{\alpha}_i)} \left(1 + \frac{1}{\tilde{\phi}} \right)\] <p>We have simplified some of the calculations since we can now use the Gaussian likelihood instead:</p> \[\begin{aligned} \mathcal{L}(\theta; \mathbf{y}^\star) &amp;= \prod_{i = 1}^k (2 \pi)^{-\frac{n}{2}} \rvert \Sigma_{y_i} \rvert^{-\frac{1}{2}} \exp\left(- \frac{1}{2} (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \Sigma_{y_i}^{-1} (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right) \\ \ell(\theta; \mathbf{y}^\star) &amp;= \sum_{i = 1}^k \left[ -\frac{n}{2} \log(2 \pi) - \frac{1}{2}\log(\rvert \Sigma_{y^\star_i} \rvert) - \frac{1}{2} (\mathbf{y}^\star_i - \alpha_i \mathbf{1}_n)^\top \Sigma_{y^\star_i}^{-1} (\mathbf{y}^\star_i - \alpha_i \mathbf{1}_n) \right] \end{aligned}\] <p>In the rest of the post, I’ll drop the $\star$ superscript for readability. Just be sure to remember that we are not dealing with the original observations but with their transformations.</p> <h3 id="score">Score</h3> <p>The marginal covariance matrix is very similar to the Gaussian outcome model above. The only thing that has changed is that each error has its own variance:</p> \[\Sigma_{y_i} = \text{diag}\left(\begin{bmatrix} \sigma^2_{i,1} &amp; \dots &amp; \sigma^2_{i, n}\end{bmatrix}\right) + \tau^2 \mathbf{z}_i \mathbf{z}_i^\top\] <p>Its inverse, $\Sigma^{-1}_{y_i}$, is:</p> <ul id="sigma-inv-2" class="tab" data-tab="5916a7a4-9d41-4858-a38f-f6dee331311d" data-name="sigma-inv-2"> <li class="active" id="sigma-inv-2-equation"> <a href="#">equation </a> </li> <li id="sigma-inv-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="5916a7a4-9d41-4858-a38f-f6dee331311d" data-name="sigma-inv-2"> <li class="active"> \[\Sigma^{-1}_{y_i} = \text{diag}\left( \begin{bmatrix} \frac{1}{\sigma^2_{i, 1}} &amp; \dots &amp; \frac{1}{\sigma^2_{i,n}} \end{bmatrix} \right) - \text{diag}\left( \begin{bmatrix} \frac{\tau^2}{(\sigma_{i,1}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)} &amp; \dots &amp; \frac{\tau^2}{(\sigma_{i,n}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)} \end{bmatrix} \right) \mathbf{z}_i \mathbf{z}_i^\top\] </li> <li> <p>First, let’s let \(\mathbf{w}_i = \tau^2 \mathbf{z}_i\), which is the $n$-vector $\mathbf{z}_i$ where each coordinate has been multiplied by $\tau^2$. We’ll also let \(\sigma^2 = (\sigma^2_{i,1}, \dots, \sigma^2_{i, n})^\top\), the vector of the error variances for cluster $i$, and $\frac{1}{\sigma^2}$ will be the vector of the reciprocals of the coordinates of $\sigma^2$. Using the <a href="https://en.wikipedia.org/wiki/Sherman–Morrison_formula">Sherman-Morrison formula</a>, we have:</p> \[\begin{aligned} \Sigma^{-1}_{y_i} &amp;= \left(\text{diag}(\sigma^2) - \mathbf{w}_i \mathbf{v}_i^\top\right)^{-1} \\ &amp;= \text{diag}^{-1}(\sigma^2) - \frac{\text{diag}^{-1}(\sigma^2) \mathbf{w}_i \mathbf{z}_i^\top \text{diag}^{-1}(\sigma^2)}{1 + \mathbf{z}_i^\top \text{diag}^{-1}(\sigma^2) \mathbf{w}_i} \\ &amp;= \text{diag}\left(\frac{1}{\sigma^2}\right) - \frac{\text{diag}\left(\frac{\tau^2}{(\sigma^2)^2}\right) \mathbf{z}_i \mathbf{z}_i^\top}{1 + \mathbf{z}_i^\top \text{diag}\left(\frac{\tau^2}{\sigma^2}\right) \mathbf{z}_i} \\ &amp;= \text{diag}\left(\begin{bmatrix} \frac{1}{\sigma^2_{i, 1}} &amp; \dots &amp; \frac{1}{\sigma^2_{i,n}} \end{bmatrix} \right) - \frac{\text{diag}\left( \begin{bmatrix} \frac{\tau^2}{(\sigma_{i,1}^2)^2} &amp; \dots &amp; \frac{\tau^2}{(\sigma_{i,n}^2)^2} \end{bmatrix} \right) \mathbf{z}_i \mathbf{z}_i^\top}{1 + \mathbf{z}_i^\top \text{diag}\left( \left[ \frac{\tau^2}{\sigma_{i,1}^2}, \dots, \frac{\tau^2}{\sigma_{i,n}^2} \right] \right) \mathbf{z}_i} \\ &amp;= \text{diag}\left( \begin{bmatrix} \frac{1}{\sigma^2_{i, 1}} &amp; \dots &amp; \frac{1}{\sigma^2_{i,n}} \end{bmatrix} \right) - \frac{\text{diag}\left( \begin{bmatrix} \frac{\tau^2}{(\sigma_{i,1}^2)^2} &amp; \dots &amp; \frac{\tau^2}{(\sigma_{i,n}^2)^2} \end{bmatrix} \right) \mathbf{z}_i \mathbf{z}_i^\top}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}} \\ &amp;= \text{diag}\left( \begin{bmatrix} \frac{1}{\sigma^2_{i, 1}} &amp; \dots &amp; \frac{1}{\sigma^2_{i,n}} \end{bmatrix} \right) - \text{diag}\left( \begin{bmatrix} \frac{\tau^2}{(\sigma_{i,1}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)} &amp; \dots &amp; \frac{\tau^2}{(\sigma_{i,n}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)} \end{bmatrix} \right) \mathbf{z}_i \mathbf{z}_i^\top \end{aligned}\] <p>We also have:</p> \[[\Sigma^{-1}_{y_i}]_{j,j'} = - \frac{\tau^2 \mathbf{z}_{i,j} \mathbf{z}_{i,j'}}{(\sigma_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)} \hspace{10mm} \text{and} \hspace{10mm} [\Sigma^{-1}_{y_i}]_{j,j} = \frac{1}{\sigma^2_{i,j}} - \frac{\tau^2 \mathbf{z}_{i,j}^2}{(\sigma_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)}\] </li> </ul> <h4 id="derivatives">Derivatives</h4> <p>We first find the derivative with respect to $\tau^2$:</p> <ul id="deriv-tau-2" class="tab" data-tab="b0a8be94-b64a-4686-a813-1d012d7e914d" data-name="deriv-tau-2"> <li class="active" id="deriv-tau-2-equation"> <a href="#">equation </a> </li> <li id="deriv-tau-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="b0a8be94-b64a-4686-a813-1d012d7e914d" data-name="deriv-tau-2"> <li class="active"> \[\frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} = - \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{j = 1}^n \left[ \frac{\mathbf{z}_{i,j}^2}{\sigma^2_{i,j}} - \sum_{h = 1}^n \frac{\tau^2}{(\sigma^2_{i,j})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)} \mathbf{z}_{i,j}^2 \mathbf{z}^2_{i,h}\right] - \sum_{j = 1}^n \sum_{h = 1}^n (\mathbf{y}_{i,j} - \alpha_i)(\mathbf{y}_{i,h} - \alpha_i) \frac{\mathbf{z}_{i,j} \mathbf{z}_{i,h}}{(\sigma^2_{i,j})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^2} \right]\] </li> <li> <p>First, the log determinant term:</p> \[\begin{aligned} \frac{\partial}{\partial \tau^2} \left[ \log (\rvert \Sigma_{y_i} \rvert) \right] &amp;= \text{tr}\left[ \Sigma^{-1}_{y_i} \frac{\partial}{\partial \tau^2} \left[ \Sigma_{y_i} \right] \right] \\ &amp;= \text{tr} \left[ \left( \text{diag}\left( \begin{bmatrix} \frac{1}{\sigma^2_{i, 1}} &amp; \dots &amp; \frac{1}{\sigma^2_{i,n}} \end{bmatrix} \right) - \text{diag}\left( \begin{bmatrix} \frac{\tau^2}{(\sigma_{i,1}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)} &amp; \dots &amp; \frac{\tau^2}{(\sigma_{i,n}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)} \end{bmatrix} \right) \mathbf{z}_i \mathbf{z}_i^\top \right) \mathbf{z}_i \mathbf{z}_i^\top \right] \\ &amp;= \text{tr} \left[ \text{diag}\left( \begin{bmatrix} \frac{1}{\sigma^2_{i, 1}} &amp; \dots &amp; \frac{1}{\sigma^2_{i,n}} \end{bmatrix} \right) \mathbf{z}_{i} \mathbf{z}_i^\top \right] - \text{tr} \left[ \text{diag}\left( \begin{bmatrix} \frac{\tau^2}{(\sigma_{i,1}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)} &amp; \dots &amp; \frac{\tau^2}{(\sigma_{i,n}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)} \end{bmatrix} \right) \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \\ &amp;= \sum_{j = 1}^n \sum_{k = 1}^n \left( \text{diag}\left( \begin{bmatrix} \frac{1}{\sigma^2_{i, 1}} &amp; \dots &amp; \frac{1}{\sigma^2_{i,n}} \end{bmatrix} \right) \right)_{j,k} \left(\mathbf{z}_i \mathbf{z}_{i}^\top \right)_{k,j} - \sum_{j = 1}^n \sum_{k = 1}^n \left( \text{diag}\left( \begin{bmatrix} \frac{\tau^2}{(\sigma_{i,1}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)} &amp; \dots &amp; \frac{\tau^2}{(\sigma_{i,n}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)} \end{bmatrix} \right) \mathbf{z}_i \mathbf{z}_i^\top \right)_{j,k} \left( \mathbf{z}_i \mathbf{z}_i^\top \right)_{k,j} \\ &amp;= \sum_{j = 1}^n \frac{\mathbf{z}_{i,j}^2}{\sigma^2_{i,j}} - \sum_{j = 1}^n \sum_{k = 1}^n \frac{\tau^2}{(\sigma^2_{i,j})^2 \left( 1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)} \mathbf{z}_{i,j} \mathbf{z}_{i,k} \mathbf{z}_{i,k} \mathbf{z}_{i,j} \\ &amp;= \sum_{j = 1}^n \left[ \frac{\mathbf{z}_{i,j}^2}{\sigma^2_{i,j}} - \sum_{k = 1}^n \frac{\tau^2}{(\sigma^2_{i,j})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)} \mathbf{z}_{i,j}^2 \mathbf{z}^2_{i,k}\right] \end{aligned}\] <p>Next, the quadratic term. We will compute the derivative of $\Sigma^{-1}_{y_i}$ with respect to $\tau^2$ element-wise:</p> \[\begin{aligned} \frac{\partial}{\partial \tau^2}\left[ [\Sigma^{-1}_{y_i}]_{j,j'}\right] &amp;= \frac{\partial}{\partial \tau^2} \left[ - \frac{\tau^2 \mathbf{z}_{i,j} \mathbf{z}_{i,j'}}{(\sigma_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)} \right] \\ &amp;= -\frac{(\sigma_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)(\mathbf{z}_{i,j}\mathbf{z}_{i,j'}) - (\tau^2 \mathbf{z}_{i,j}\mathbf{z}_{i,j'})\left( (\sigma_{i,j}^2)^2\sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)}{(\sigma_{i,j}^2)^4 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^2} \\ &amp;= -\frac{\mathbf{z}_{i,j}\mathbf{z}_{i,j'} \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}- \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)}{(\sigma_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^2} \\ &amp;= -\frac{\mathbf{z}_{i,j}\mathbf{z}_{i,j'}}{(\sigma_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^2} \\ \frac{\partial}{\partial \tau^2}\left[ [\Sigma^{-1}_{y_i}]_{j,j}\right] &amp;= \frac{\partial}{\partial \tau^2} \left[ \frac{1}{\sigma^2_{i,j}} - \frac{\tau^2 \mathbf{z}_{i,j}^2}{(\sigma_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)} \right] \\ &amp;= - \frac{(\sigma_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)\mathbf{z}_{i,j}^2 - \tau^2 \mathbf{z}_{i,j}^2 (\sigma_{i,j}^2)^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}}{(\sigma_{i,j}^2)^4 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^2} \\ &amp;= - \frac{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)\mathbf{z}_{i,j}^2 - \tau^2 \mathbf{z}_{i,j}^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}}{(\sigma_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^2} \\ &amp;= - \frac{\mathbf{z}_{i,j}^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}- \tau^2\sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)}{(\sigma_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^2} \\ &amp;= - \frac{\mathbf{z}_{i,j}^2}{(\sigma_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^2} \\ \end{aligned}\] <p>In matrix notation, we have:</p> \[\frac{\partial}{\partial \tau^2}\left[ \Sigma^{-1}_{y_i} \right] = \text{diag}\left(\left[ -\frac{1}{(\sigma^2_{i,1})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^2}, \dots, -\frac{1}{(\sigma^2_{i,n})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^2} \right] \right) \mathbf{z}_i \mathbf{z}_i^\top\] <p>Then:</p> \[\begin{aligned} (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \frac{\partial}{\partial \tau^2} \left[ \Sigma^{-1}_{y_i} \right](\mathbf{y}_i - \alpha_i \mathbf{1}_n) &amp;= (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ \text{diag}\left(\left[ -\frac{1}{(\sigma^2_{i,1})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^2}, \dots, -\frac{1}{(\sigma^2_{i,n})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^2} \right] \right) \mathbf{z}_i \mathbf{z}_i^\top \right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ &amp;= \sum_{j = 1}^n \sum_{h = 1}^n (\mathbf{y}_{i,j} - \alpha_i)(\mathbf{y}_{i,h} - \alpha_i) \left[ \text{diag}\left(\left[ -\frac{1}{(\sigma^2_{i,1})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^2}, \dots, -\frac{1}{(\sigma^2_{i,n})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^2} \right] \right) \mathbf{z}_i \mathbf{z}_i^\top \right]_{j,k} \\ &amp;= - \sum_{j = 1}^n \sum_{h = 1}^n (\mathbf{y}_{i,j} - \alpha_i)(\mathbf{y}_{i,h} - \alpha_i) \frac{\mathbf{z}_{i,j} \mathbf{z}_{i,h}}{(\sigma^2_{i,j})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^2} \end{aligned}\] <p>And thus:</p> \[\begin{aligned} \frac{\partial}{\partial \tau^2} \left[ \ell(\theta; \mathbf{y}) \right] &amp;= \frac{\partial}{\partial \tau^2} \left[ \sum_{i = 1}^k \left[ -\frac{n}{2} \log(2 \pi) - \frac{1}{2}\log(\rvert \Sigma_{y_i} \rvert) - \frac{1}{2} (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \Sigma_{y_i}^{-1} (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \right] \\ &amp;= - \frac{1}{2} \sum_{i = 1}^k \left[ \frac{\partial}{\partial \tau^2} \left[ \log(\rvert \Sigma_{y_i} \rvert) \right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \frac{\partial}{\partial \tau^2} \left[ \Sigma_{y_i}^{-1} \right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right]\\ &amp;= - \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{j = 1}^n \left[ \frac{\mathbf{z}_{i,j}^2}{\sigma^2_{i,j}} - \sum_{k = 1}^n \frac{\tau^2}{(\sigma^2_{i,j})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)} \mathbf{z}_{i,j}^2 \mathbf{z}^2_{i,k}\right] - \sum_{j = 1}^n \sum_{k = 1}^n (\mathbf{y}_{i,j} - \alpha_i)(\mathbf{y}_{i,k} - \alpha_i) \frac{\mathbf{z}_{i,j} \mathbf{z}_{i,k}}{(\sigma^2_{i,j})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^2} \right] \end{aligned}\] </li> </ul> <p>Next, we find the derivative with respect to $\phi$:</p> <ul id="deriv-phi-1" class="tab" data-tab="69b797d1-b4f2-420c-9c0f-594bf887833f" data-name="deriv-phi-1"> <li class="active" id="deriv-phi-1-equation"> <a href="#">equation </a> </li> <li id="deriv-phi-1-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="69b797d1-b4f2-420c-9c0f-594bf887833f" data-name="deriv-phi-1"> <li class="active"> <p>Coming soon…</p> </li> <li> <p>First, let’s find the derivative of the diagonal components of $\Sigma_{y_i}$:</p> \[\begin{aligned} \frac{\partial}{\partial \phi} \left[ (\Sigma_{y_i})_{j,j} \right] &amp;= \frac{\partial}{\partial \phi} \left[ \sigma^2_{i,j} + \tau^2 \mathbf{z}_{i,j}^2 \right] \\ &amp;= \frac{\partial}{\partial \phi} \left[ \frac{1}{\tilde{\mu}_{i,j}} \left(1 + \frac{1}{\phi} \right) + \tau^2 \mathbf{z}_{i,j}^2 \right] \\ &amp;= \frac{1}{\tilde{\mu}_{i,j}} \frac{\partial}{\partial \phi} \left[ \frac{1}{\phi} \right] \\ &amp;= -\frac{1}{\tilde{\mu}_{i,j} \phi^2 } \end{aligned}\] <p>This implies that \(\frac{\partial}{\partial \phi}\left[ \Sigma_{y_i} \right] = \text{diag}\left( \begin{bmatrix} -\frac{1}{\tilde{\mu}_{i,1} \phi^2 } &amp; \dots &amp; -\frac{1}{\tilde{\mu}_{i,n} \phi^2 } \end{bmatrix} \right)\).</p> \[\begin{aligned} \frac{\partial}{\partial \phi} \left[ \log(\rvert \Sigma_{y_i} \rvert) \right] &amp;= \text{tr} \left[ \Sigma_{y_i}^{-1} \frac{\partial}{\partial \tau^2} \left[ \Sigma_{y_i} \right] \right] \\ &amp;= \text{tr} \left[ \left( \text{diag}\left( \begin{bmatrix} \frac{1}{\sigma^2_{i, 1}} &amp; \dots &amp; \frac{1}{\sigma^2_{i,n}} \end{bmatrix} \right) - \text{diag}\left( \begin{bmatrix} \frac{\tau^2}{(\sigma_{i,1}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)} &amp; \dots &amp; \frac{\tau^2}{(\sigma_{i,n}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)} \end{bmatrix} \right) \mathbf{z}_i \mathbf{z}_i^\top \right) \text{diag}\left( \begin{bmatrix} -\frac{1}{\tilde{\mu}_{i,1} \phi^2 } &amp; \dots &amp; -\frac{1}{\tilde{\mu}_{i,n} \phi^2 } \end{bmatrix} \right) \right] \\ &amp;= \text{tr} \left[ \text{diag}\left(\begin{bmatrix} - \frac{1}{\tilde{\mu}_{i,1} \sigma^2_{i,1} \phi^2} &amp; \dots &amp; - \frac{1}{\tilde{\mu}_{i,n} \sigma^2_{i,n} \phi^2} \end{bmatrix}\right) \right] - \text{tr} \left[ \text{diag}\left( \begin{bmatrix} \frac{\tau^2}{(\sigma_{i,1}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)} &amp; \dots &amp; \frac{\tau^2}{(\sigma_{i,n}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)} \end{bmatrix} \right) (\mathbf{z}_i \mathbf{z}_i^\top) \text{diag}\left( \begin{bmatrix} -\frac{1}{\tilde{\mu}_{i,1} \phi^2 } &amp; \dots &amp; -\frac{1}{\tilde{\mu}_{i,n} \phi^2 } \end{bmatrix} \right) \right] \\ &amp;= \sum_{j = 1}^n \sum_{h = 1}^n \left[ \text{diag}\left(\begin{bmatrix} - \frac{1}{\tilde{\mu}_{i,1} \sigma^2_{i,1} \phi^2} &amp; \dots &amp; - \frac{1}{\tilde{\mu}_{i,n} \sigma^2_{i,n} \phi^2} \end{bmatrix}\right)\right]_{j,h} - \text{tr}\left[ \mathbf{z}_i^\top \text{diag}\left( \begin{bmatrix} -\frac{1}{\tilde{\mu}_{i,1} \phi^2 } &amp; \dots &amp; -\frac{1}{\tilde{\mu}_{i,n} \phi^2 } \end{bmatrix} \right) \text{diag}\left( \begin{bmatrix} \frac{\tau^2}{(\sigma_{i,1}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)} &amp; \dots &amp; \frac{\tau^2}{(\sigma_{i,n}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)} \end{bmatrix} \right) \mathbf{z}_i \right] \\ &amp;= \sum_{j = 1}^n -\frac{1}{\tilde{\mu}_{i,j} \sigma^2_{i,j} \phi^2} - \mathbf{z}_i^\top \text{diag}\left( \begin{bmatrix} -\frac{1}{\tilde{\mu}_{i,1} \phi^2 } &amp; \dots &amp; -\frac{1}{\tilde{\mu}_{i,n} \phi^2 } \end{bmatrix} \right) \text{diag}\left( \begin{bmatrix} \frac{\tau^2}{(\sigma_{i,1}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)} &amp; \dots &amp; \frac{\tau^2}{(\sigma_{i,n}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)} \end{bmatrix} \right) \mathbf{z}_i \\ &amp;= \sum_{j = 1}^n -\frac{1}{\tilde{\mu}_{i,j} \sigma^2_{i,j} \phi^2} - \mathbf{z}_{i}^\top \text{diag} \left( \begin{bmatrix} - \frac{\tau^2}{\tilde{\mu}_{i,1} \phi^2 (\sigma^2_{i,1})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)} &amp; \dots &amp; - \frac{\tau^2}{\tilde{\mu}_{i,n} \phi^2 (\sigma^2_{i,n})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)} \end{bmatrix} \right) \mathbf{z}_i \\ &amp;= - \sum_{j = 1}^n \frac{1}{\tilde{\mu}_{i,j} \left(\frac{1}{\tilde{\mu}_{i,j}} \left(1 + \frac{1}{\phi}\right) \right) \phi^2} - \sum_{j = 1}^n \sum_{h = 1}^n \mathbf{z}_{i,j} \mathbf{z}_{i,h} \left[ \text{diag} \left( \begin{bmatrix} - \frac{\tau^2}{\tilde{\mu}_{i,1} \phi^2 (\sigma^2_{i,1})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)} &amp; \dots &amp; - \frac{\tau^2}{\tilde{\mu}_{i,n} \phi^2 (\sigma^2_{i,n})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)} \end{bmatrix} \right) \right]_{j,h} \\ &amp;= - \sum_{j = 1}^n \frac{1}{\phi^2 + \phi} - \sum_{j = 1}^n \mathbf{z}_{i,j}^2 \left(- \frac{\tau^2}{\tilde{\mu}_{i,j} \phi^2 (\sigma^2_{i,j})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)} \right) \\ &amp;= - \sum_{j = 1}^n \left[ \frac{1}{\phi^2 + \phi} - \frac{\tau^2 \mathbf{z}_{i,j}^2 }{\tilde{\mu}_{i,n} \phi^2 \left( \frac{1}{\tilde{\mu}_{i,j}} \left(1 + \frac{1}{\phi}\right) \right)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right) }\right] \\ &amp;= - \sum_{j = 1}^n \left[ \frac{1}{\phi^2 + \phi} - \frac{\tau^2 \mathbf{z}_{i,j}^2 }{ \sigma^2_{i,j} (\phi^2 + \phi) \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right) }\right] \\ &amp;= -\sum_{j = 1}^n \frac{\sigma^2_{i,j} \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right) - \tau^2 \mathbf{z}_{i,j}^2 }{\sigma^2_{i,j} (\phi^2 + \phi) \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right) } &amp;= \sum_{j = 1}^n \frac{\tau^2 \mathbf{z}_{i,j}^2 - \left( \sigma^2_{i,j} + \sigma^2_{i,j} \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)}{\sigma^2_{i,j} (\phi^2 + \phi) \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right) } \end{aligned}\] <p>Next, the quadratic term. This requires the following:</p> \[\begin{aligned} \frac{\partial}{\partial \phi} \left[ [ \Sigma^{-1}_{y_i}]_{j,j'} \right] &amp;= \frac{\partial}{\partial \phi} \left[ - \frac{\tau^2 \mathbf{z}_{i,j} \mathbf{z}_{i,j'}}{(\sigma^2_{i,j})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)}\right] \\ &amp;= - \tau^2 \mathbf{z}_{i,j} \mathbf{z}_{i,j'} \frac{\partial}{\partial \phi} \left[ (\sigma^2_{i,j})^{-2} \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^{-1} \right] \\ &amp;= - \tau^2 \mathbf{z}_{i,j} \mathbf{z}_{i,j'} \left[ -2(\sigma^2_{i,j})^{-3} \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^{-1} \frac{\partial}{\partial \phi} \left[ \sigma^2_{i,j} \right] - \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^{-1} (\sigma^2_{i,j})^{-2} \frac{\partial}{\partial \phi} \left[ 1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right] \right] \\ &amp;= - \tau^2 \mathbf{z}_{i,j} \mathbf{z}_{i,j'} \left[ -2(\sigma^2_{i,j})^{-3} \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^{-1} \frac{\partial}{\partial \phi} \left[ \frac{1}{\tilde{\mu}_{i,j}}\left(1 + \frac{1}{\phi} \right) \right] - \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^{-1} (\sigma^2_{i,j})^{-2} \tau^2 \sum_{l = 1}^n \mathbf{z}_{i,l}^2 \frac{\partial}{\partial \phi} \left[ \left( \sigma^2_{i,l} \right)^{-1}\right] \right] \\ &amp;= - \tau^2 \mathbf{z}_{i,j} \mathbf{z}_{i,j'} \left[ 2(\sigma^2_{i,j})^{-3} \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^{-1} \tilde{\mu}^{-1}_{i,j} \phi^{-2} - \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^{-1} (\sigma^2_{i,j})^{-2} \tau^2 \sum_{l = 1}^n - \mathbf{z}^2_{i,l} (\sigma^2_{i,l})^{-2} \frac{\partial}{\partial \phi} \left[ \sigma^2_{i,l} \right] \right] \\ &amp;= - \tau^2 \mathbf{z}_{i,j} \mathbf{z}_{i,j'} \left[ 2(\sigma^2_{i,j})^{-3} \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^{-1} \tilde{\mu}^{-1}_{i,j} \phi^{-2} + \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^{-1} (\sigma^2_{i,j})^{-2} \tau^2 \sum_{l = 1}^n \mathbf{z}^2_{i,l} (\sigma^2_{i,l})^{-2} \tilde{\mu}_{i,l}^{-1} \phi^{-2} \right] \\ &amp;= - \tau^2 \mathbf{z}_{i,j} \mathbf{z}_{i,j'} \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^{-1} (\sigma^2_{i,j})^{-2} \phi^{-2} \left[ 2(\sigma^2_{i,j})^{-1} \tilde{\mu}^{-1}_{i,j} + \tau^2 \sum_{l = 1}^n \mathbf{z}^2_{i,l} (\sigma^2_{i,l})^{-2} \tilde{\mu}_{i,l}^{-1} \right] \\ &amp;= - \frac{\tau^2 \mathbf{z}_{i,j} \mathbf{z}_{i,j'}}{(\sigma^2_{i,j})^{2} \phi^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right) } \left[ \frac{2}{1 + \frac{1}{\phi}} + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}^2_{i,l}}{ \sigma^2_{i,l} \left(1 + \frac{1}{\phi} \right)} \right] \\ &amp;= - \frac{\tau^2 \mathbf{z}_{i,j} \mathbf{z}_{i,j'}}{(\sigma^2_{i,j})^{2} \phi^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right) \left(1 + \frac{1}{\phi} \right)} \left[ 2 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}^2_{i,l}}{ \sigma^2_{i,l} } \right] \\ \end{aligned}\] <p>And:</p> \[\begin{aligned} \frac{\partial}{\partial \phi} \left[ [ \Sigma^{-1}_{y_i}]_{j,j} \right] &amp;= \frac{\partial}{\partial \phi} \left[ \frac{1}{\sigma^2_{i,j}} - \frac{\tau^2 \mathbf{z}_{i,j} \mathbf{z}_{i,j'}}{(\sigma^2_{i,j})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)}\right] \\ &amp;= -\frac{1}{(\sigma^2_{i,j})^2 } \frac{\partial}{\partial\phi} \left[ \sigma^2_{i,j} \right] - \frac{\partial}{\partial \phi} \left[ \frac{\tau^2 \mathbf{z}_{i,j} \mathbf{z}_{i,j'}}{(\sigma^2_{i,j})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)}\right] \\ &amp;= \frac{1}{(\sigma^2_{i,j})^2 \tilde{\mu}_{i,j} \phi^2} - \frac{\tau^2 \mathbf{z}_{i,j} \mathbf{z}_{i,j'}}{(\sigma^2_{i,j})^{2} \phi^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right) \left(1 + \frac{1}{\phi} \right)} \left[ 2 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}^2_{i,l}}{ \sigma^2_{i,l} } \right] \\ &amp;= \frac{1}{\sigma^2_{i,j} \phi^2 \left(1 + \frac{1}{\phi} \right)} - \frac{\tau^2 \mathbf{z}_{i,j} \mathbf{z}_{i,j'}}{(\sigma^2_{i,j})^{2} \phi^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right) \left(1 + \frac{1}{\phi} \right)} \left[ 2 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}^2_{i,l}}{ \sigma^2_{i,l} } \right] \\ &amp;= \frac{1}{\sigma^2_{i,j}\left( \phi^2 + \phi\right)} - \frac{\tau^2 \mathbf{z}_{i,j} \mathbf{z}_{i,j'}}{(\sigma^2_{i,j})^{2} \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right) \left( \phi^2 + \phi\right)} \left[ 2 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}^2_{i,l}}{ \sigma^2_{i,l} } \right] \end{aligned}\] <p>In matrix notation:</p> \[\begin{aligned} \frac{\partial}{\partial \phi} \left[ \Sigma^{-1}_{y_i} \right] &amp;= \text{diag}\left( \begin{bmatrix} \frac{1}{\sigma^2_{i,1} (\phi^2 + \phi)} &amp; \dots &amp; \frac{1}{\sigma^2_{i,n} (\phi^2 + \phi)} \end{bmatrix} \right) - \text{diag} \left( \begin{bmatrix} \frac{ \tau^2 \left(2 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)}{(\sigma^2_{i,1})^2 (\phi^2 + \phi) \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)} &amp; \dots &amp; \frac{ \tau^2 \left(2 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)}{(\sigma^2_{i,n})^2 (\phi^2 + \phi) \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)} \end{bmatrix} \right) \mathbf{z}_i \mathbf{z}_i^\top \end{aligned}\] </li> </ul> <p>And now we find the gradient with respect to $\alpha$:</p> <ul id="deriv-alpha-2" class="tab" data-tab="28b1bead-5aec-47b5-9d00-0765b7fb32a4" data-name="deriv-alpha-2"> <li class="active" id="deriv-alpha-2-equation"> <a href="#">equation </a> </li> <li id="deriv-alpha-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="28b1bead-5aec-47b5-9d00-0765b7fb32a4" data-name="deriv-alpha-2"> <li class="active"> \[\frac{\partial}{\partial \alpha}[ \ell(\theta; \mathbf{y})] = \begin{bmatrix} (\mathbf{y}_1 - \alpha_1 \mathbf{1}_n)^\top \Sigma_{y_1}^{-1} \mathbf{1}_n \\ \vdots \\ (\mathbf{y}_k - \alpha_k \mathbf{1}_n)^\top \Sigma_{y_k}^{-1} \mathbf{1}_n \end{bmatrix}\] </li> <li> <p>We do this component-wise:</p> \[\begin{aligned} \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} &amp;= \frac{\partial}{\partial \alpha_j} \left[ \sum_{i = 1}^k \left[ -\frac{n}{2} \log(2 \pi) - \frac{1}{2}\log(\rvert \Sigma_{y_i} \rvert) - \frac{1}{2} (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \Sigma_{y_i}^{-1} (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \right] \\ &amp;= - \frac{1}{2} \frac{\partial}{\partial \alpha_j} \left[(\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \Sigma_{y_j}^{-1} (\mathbf{y}_j - \alpha_j \mathbf{1}_n) \right] \\ &amp;= - \frac{1}{2}\left(2 (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \Sigma_{y_j}^{-1} (-\mathbf{1}_n) \right) \\ &amp;= (\mathbf{y}_i - \alpha_j \mathbf{1}_n)^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ \implies \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha} &amp;= \begin{bmatrix} (\mathbf{y}_1 - \alpha_1 \mathbf{1}_n)^\top \Sigma_{y_1}^{-1} \mathbf{1}_n \\ \vdots \\ (\mathbf{y}_k - \alpha_k \mathbf{1}_n)^\top \Sigma_{y_k}^{-1} \mathbf{1}_n \end{bmatrix} \end{aligned}\] </li> </ul> <h4 id="mles">MLEs</h4> <p>We can find $\hat{\theta}$ by setting the above equal to zero and substitute $\tau^2 = 0$. We get:</p> <ul id="mle-2" class="tab" data-tab="fd8bf38c-15f5-4d15-a600-f48038d23d6c" data-name="mle-2"> <li class="active" id="mle-2-equation"> <a href="#">equation </a> </li> <li id="mle-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="fd8bf38c-15f5-4d15-a600-f48038d23d6c" data-name="mle-2"> <li class="active"> \[\hat{\theta} = \begin{bmatrix} \left(\sum_{l' = 1}^n \frac{1}{\sigma^2_{1,l'}} \right)^{-1} \sum_{l' = 1}^n \frac{\mathbf{y}_{1,l'}}{\sigma^2_{1,l'}} \\ \vdots \\ \left(\sum_{l' = 1}^n \frac{1}{\sigma^2_{k,l'}} \right)^{-1} \sum_{l' = 1}^n \frac{\mathbf{y}_{k,l'}}{\sigma^2_{k,l'}} \\ 0 \end{bmatrix}\] </li> <li> <p>We only need to deal with $\alpha$, which we can again do component-wise. First notice that:</p> \[\begin{aligned} \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} &amp;= (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ &amp;= (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left( \text{diag}\left(\left[ \frac{1}{\sigma^2_{j, 1}}, \dots, \frac{1}{\sigma^2_{j,n}} \right]\right) - \frac{\text{diag}\left( \left[ \frac{\tau^2}{(\sigma_{j,1}^2)^2}, \dots, \frac{\tau^2}{(\sigma_{j,n}^2)^2} \right] \right) \mathbf{z}_j \mathbf{z}_j^\top}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\sigma^2_{j,l}}} \right) \mathbf{1}_n \\ &amp;= (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left(\begin{bmatrix} \frac{1}{\sigma^2_{j, 1}} \\ \vdots \\ \frac{1}{\sigma^2_{j,n}} \end{bmatrix} - \left( \frac{1}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\sigma_{j,l}^2}} \right)\text{diag}\left( \left[ \frac{\tau^2}{(\sigma_{j,1}^2)^2}, \dots, \frac{\tau^2}{(\sigma_{j,n}^2)^2} \right] \right) \begin{bmatrix} \sum_{l = 1}^n \mathbf{z}_{j,1} \mathbf{z}_{j,l} \\ \vdots \\ \sum_{l = 1}^n \mathbf{z}_{j,n} \mathbf{z}_{j,l} \end{bmatrix} \right) \\ &amp;= \left( \sum_{l = 1}^n \frac{\mathbf{y}_{j,l} - \alpha_j}{\sigma^2_{j,l}} \right)- \left( \frac{1}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\sigma_{j,l}^2}} \right) (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \begin{bmatrix} \frac{\tau^2}{(\sigma_{j,1}^2)^2} \sum_{l = 1}^n \mathbf{z}_{j,1} \mathbf{z}_{j,l} \\ \vdots \\ \frac{\tau^2}{(\sigma_{j,n}^2)^2} \sum_{l = 1}^n \mathbf{z}_{j,n} \mathbf{z}_{j,l} \end{bmatrix} \\ &amp;= \left( \sum_{l = 1}^n \frac{\mathbf{y}_{j,l} - \alpha_j}{\sigma^2_{j,l}} \right)- \left( \frac{1}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\sigma_{j,l}^2}} \right) \sum_{l' = 1}^n (\mathbf{y}_{j,l'} - \alpha_j) \frac{\tau^2}{(\sigma_{j,l'}^2)^2} \sum_{l = 1}^n \mathbf{z}_{j,l'} \mathbf{z}_{j,l} \\ &amp;= \left( \sum_{l = 1}^n \frac{\mathbf{y}_{j,l} - \alpha_j}{\sigma^2_{j,l}} \right)- \left( \frac{\sum_{l = 1}^n \mathbf{z}_{j,l} }{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\sigma_{j,l}^2}} \right) \left( \sum_{l' = 1}^n \frac{\tau^2 (\mathbf{y}_{j,l'} - \alpha_j) \mathbf{z}_{j,l'} }{(\sigma_{j,l'}^2)^2} \right) \end{aligned}\] <p>So then we solve for $\alpha_j$ in:</p> \[\begin{aligned} 0 &amp;= (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ \implies 0 &amp;= \left( \sum_{l = 1}^n \frac{\mathbf{y}_{j,l} - \alpha_j}{\sigma^2_{j,l}} \right)- \left( \frac{\sum_{l = 1}^n \mathbf{z}_{j,l} }{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\sigma_{j,l}^2}} \right) \left( \sum_{l' = 1}^n \frac{\tau^2 (\mathbf{y}_{j,l'} - \alpha_j) \mathbf{z}_{j,l'} }{(\sigma_{j,l'}^2)^2} \right) \\ \implies 0 &amp;= \sum_{l = 1}^n \frac{\mathbf{y}_{j,l}}{\sigma^2_{j,l}} - \alpha_j \sum_{l = 1}^n \frac{1}{\sigma^2_{j,l}} - \left( \frac{\sum_{l = 1}^n \mathbf{z}_{j,l} }{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\sigma_{j,l}^2}} \right) \left[ \sum_{l' = 1}^n \frac{\tau^2 \mathbf{y}_{j,l'} \mathbf{z}_{j,l'}}{(\sigma^2_{j,l'})^2} - \alpha_j \sum_{l' =1}^n \frac{\tau^2 \mathbf{z}_{j,l'}}{(\sigma^2_{j,l'})} \right] \\ \implies \alpha_j \left[ \sum_{l = 1}^n \frac{1}{\sigma^2_{j,l}} - \sum_{l = 1}^n \frac{\tau^2 \mathbf{z}_{j,l'}}{(\sigma^2_{j,l'})^2}\right] &amp;= \sum_{l = 1}^n \frac{\mathbf{y}_{j,l}}{\sigma^2_{j,l}} - \left( \frac{\sum_{l = 1}^n \mathbf{z}_{j,l} }{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\sigma_{j,l}^2}} \right) \sum_{l' = 1}^n\frac{\tau^2 \mathbf{y}_{j,l'} \mathbf{z}_{j,l'}}{(\sigma^2_{j,l'})^2} \\ \implies \alpha_j \left(\sum_{l' = 1}^n \frac{1}{\sigma^2_{j,l'}} \left[1 - \frac{\tau^2 \mathbf{z}_{j,l'}}{\sigma^2_{j,l'}} \right]\right) &amp;= \sum_{l' = 1}^n \frac{\mathbf{y}_{j,l'}}{\sigma^2_{j,l'}} \left[ 1 - \left( \frac{\sum_{l = 1}^n \mathbf{z}_{j,l} }{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\sigma_{j,l}^2}} \right) \frac{\tau^2 \mathbf{z}_{j,l'}}{\sigma^2_{j,l'}} \right] \\ \implies \alpha_j &amp;= \left(\sum_{l' = 1}^n \frac{1}{\sigma^2_{j,l'}} \left[1 - \frac{\tau^2 \mathbf{z}_{j,l'}}{\sigma^2_{j,l'}} \right]\right)^{-1} \sum_{l' = 1}^n \frac{\mathbf{y}_{j,l'}}{\sigma^2_{j,l'}} \left[ 1 - \left( \frac{\sum_{l = 1}^n \mathbf{z}_{j,l} }{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\sigma_{j,l}^2}} \right) \frac{\tau^2 \mathbf{z}_{j,l'}}{\sigma^2_{j,l'}} \right] \end{aligned}\] <p>Under $H_0$, $\tau^2 = 0$, so we get:</p> \[\hat{\alpha}_j = \left(\sum_{l' = 1}^n \frac{1}{\sigma^2_{j,l'}} \right)^{-1} \sum_{l' = 1}^n \frac{\mathbf{y}_{j,l'}}{\sigma^2_{j,l'}}\] </li> </ul> <p>Thus, the score evaluated at $\theta = \hat{\theta}$ is then:</p> \[U_{\theta}(\hat{\theta}) = \begin{bmatrix} \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha} \bigg\rvert_{\theta = \hat{\theta}} \\ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \bigg\rvert_{\theta = \hat{\theta}} \end{bmatrix} = \begin{bmatrix} \left(\sum_{l' = 1}^n \frac{1}{\sigma^2_{1,l'}} \right)^{-1} \sum_{l' = 1}^n \frac{\mathbf{y}_{1,l'}}{\sigma^2_{1,l'}} \\ \vdots \\ \left(\sum_{l' = 1}^n \frac{1}{\sigma^2_{k,l'}} \right)^{-1} \sum_{l' = 1}^n \frac{\mathbf{y}_{k,l'}}{\sigma^2_{k,l'}} \\ - \frac{1}{2}\sum_{i = 1}^k \left[ \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma_{i,l}^2} + \sum_{l = 1}^n \sum_{l' = 1}^n \frac{\mathbf{z}_{i,l}\mathbf{z}_{i,l'}(\mathbf{y}_{i,l} - \hat{\alpha}_i)(\mathbf{y}_{i,l'} - \hat{\alpha}_i)}{\sigma^2_{i,l} \sigma^2_{i,l'}}\right] \end{bmatrix}\] <h3 id="information">Information</h3> <p>As before, to find the information, we need to compute the second-order derivatives of the log-likelihood, take the expectation under $H_0$ of minus those quantities, and evaluate them by plugging in $\hat{\theta}$.</p> <h4 id="derivatives-1">Derivatives</h4> <p>We’ll take all of the derivatives component-wise. We’ll start with those with respect to $\tau^2$.</p> <ul id="deriv-theta-tau-2" class="tab" data-tab="f12599a2-f947-4e22-8b57-1fe813c80755" data-name="deriv-theta-tau-2"> <li class="active" id="deriv-theta-tau-2-equation"> <a href="#">equation </a> </li> <li id="deriv-theta-tau-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="f12599a2-f947-4e22-8b57-1fe813c80755" data-name="deriv-theta-tau-2"> <li class="active"> \[\begin{aligned} \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \tau^2 \partial \theta} &amp;= \begin{bmatrix} - \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{1,l}}{\sigma^2_{1,l}}\right)^2} \left(\sum_{l' = 1}^n \frac{\mathbf{z}_{1,l'}(\mathbf{y}_{1,l'} - \alpha_1)}{(\sigma^2_{1,l'})^2}\right)\left(\sum_{l'' = 1}^n \mathbf{z}_{1,l''}\right) \\ \vdots \\ - \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{k,l}}{\sigma^2_{k,l}}\right)^2} \left(\sum_{l' = 1}^n \frac{\mathbf{z}_{k,l'}(\mathbf{y}_{k,l'} - \alpha_k)}{(\sigma^2_{k,l'})^2}\right)\left(\sum_{l'' = 1}^n \mathbf{z}_{k,l''}\right) \\ \frac{1}{2} \sum_{i = 1}^k \left[ \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^2} \left( \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^2 - \frac{2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^3} \left( \sum_{a = 1}^n (\mathbf{y}_{i,a} - \alpha_i) \frac{\mathbf{z}_{i,a}}{\sigma^2_{i,a}} \right)^2 \right] \end{bmatrix} \end{aligned} \label{eq:info-tau-tau}\] </li> <li> \[\begin{aligned} \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial (\tau^2)^2} &amp;= \frac{\partial}{\partial \tau^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] \\ &amp;= \frac{\partial}{\partial \tau^2} \left[ - \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} - \sum_{l = 1}^n \sum_{j = 1}^n \frac{\tau^2 \mathbf{z}_{i,l}^2 \mathbf{z}_{i,j}^2}{(\sigma_{i,l}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)} - \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \frac{\mathbf{z}_{i,a} \mathbf{z}_{i,b}}{(\sigma^2_{i,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^2} \right] \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[\sum_{l = 1}^n \sum_{j = 1}^n \frac{\partial}{\partial \tau^2} \left[ \frac{\tau^2 \mathbf{z}_{i,l}^2 \mathbf{z}_{i,j}^2}{(\sigma_{i,l}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)} \right] + \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \frac{\partial}{\partial \tau^2} \left[ \frac{\mathbf{z}_{i,a} \mathbf{z}_{i,b}}{(\sigma^2_{i,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^2} \right] \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \left(\frac{\mathbf{z}_{i,l}^2 \mathbf{z}_{i,j}^2 (\sigma^2_{i,l})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right) - \tau^2 \mathbf{z}_{i,l}^2 \mathbf{z}_{i,j}^2 (\sigma^2_{i,l})^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} }{\left((\sigma^2_{i,l})^2 \left( 1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)\right)^2}\right) + \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \left(\frac{-2\mathbf{z}_{i,a}\mathbf{z}_{i,b}(\sigma^2_{i,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma_{i,l}}\right) \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma_{i,l}^2}}{\left((\sigma^2_{i,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}} \right)^2\right)^2}\right) \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\sigma^2_{i,l})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^2} - 2 \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \left(\frac{\mathbf{z}_{i,a}\mathbf{z}_{i,b}(\sigma^2_{i,a})^2 \left(\sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}} \right)}{(\sigma^2_{i,a})^4 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}} \right)^3}\right) \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[ \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^2} \left( \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{(\sigma^2_{i,l})^2} \right) \left( \sum_{j = 1}^n \mathbf{z}_{i,j}^2 \right) - \frac{2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^3} \left( \sum_{a = 1}^n (\mathbf{y}_{i,a} - \alpha_i) \frac{\mathbf{z}_{i,a}}{(\sigma^2_{i,a})^2} \right) \left( \sum_{b = 1}^n (\mathbf{y}_{i,b} - \alpha_i) \mathbf{z}_{i,b} \right) \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[ \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^2} \left( \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{(\sigma^2_{i,l})^2} \right) \left( \sum_{l = 1}^n \mathbf{z}_{i,l}^2 \right) - \frac{2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^3} \left( \sum_{a = 1}^n (\mathbf{y}_{i,a} - \alpha_i) \frac{\mathbf{z}_{i,a}}{\sigma^2_{i,a}} \right) \left( \sum_{a = 1}^n (\mathbf{y}_{i,a} - \alpha_i) \frac{\mathbf{z}_{i,a}}{\sigma^2_{i,a}} \right) \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[ \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^2} \left( \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^2 - \frac{2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^3} \left( \sum_{a = 1}^n (\mathbf{y}_{i,a} - \alpha_i) \frac{\mathbf{z}_{i,a}}{\sigma^2_{i,a}} \right)^2 \right] \end{aligned}\] <p>Next, we take the derivative (with respect to $\tau^2$) of the derivative with respect to $\alpha_j$:</p> \[\begin{aligned} \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \tau^2 \partial \alpha_j } &amp;= \frac{\partial}{\partial \tau^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] \\ &amp;= \frac{\partial}{\partial \tau^2} \left[ (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \right] \\ &amp;= \begin{bmatrix} (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left[ \text{diag}\left( \begin{bmatrix} -\frac{1}{(\sigma^2_{j,1})^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\sigma^2_{j,l}}\right)^2} &amp; \dots &amp; -\frac{1}{(\sigma^2_{j,n})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\sigma^2_{j,l}}\right)^2} \end{bmatrix} \right) \mathbf{z}_{j} \mathbf{z}_j^\top \right] \mathbf{1}_n \end{bmatrix} \\ &amp;= \begin{bmatrix} -\frac{\mathbf{y}_{j, 1} - \alpha_j}{(\sigma^2_{j,1})^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\sigma^2_{j,l}}\right)^2} &amp; \dots &amp; -\frac{\mathbf{y}_{j,n} - \alpha_j}{(\sigma^2_{j,n})^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\sigma^2_{j,l}}\right)^2} \end{bmatrix} \mathbf{z}_j \mathbf{z}_j^\top \mathbf{1}_n \\ &amp;= \left(- \sum_{l' = 1}^n \frac{\mathbf{z}_{j,l'}(\mathbf{y}_{j,l'} - \alpha_j)}{(\sigma^2_{j,l'})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\sigma^2_{j,l}}\right)^2 }\right)\left(\sum_{l'' = 1}^n \mathbf{z}_{j,l''}\right) \\ &amp;= - \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\sigma^2_{j,l}}\right)^2} \left(\sum_{l' = 1}^n \frac{\mathbf{z}_{j,l'}(\mathbf{y}_{j,l'} - \alpha_j)}{(\sigma^2_{j,l'})^2}\right)\left(\sum_{l'' = 1}^n \mathbf{z}_{j,l''}\right) \end{aligned}\] <p>Putting the two together into a matrix yield:</p> \[\begin{aligned} \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \tau^2 \partial \theta } &amp;= \frac{\partial}{\partial \tau^2} \left[ \begin{bmatrix} \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha} \\ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \end{bmatrix} \right] \\ &amp;= \begin{bmatrix} - \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{1,l}}{\sigma^2_{1,l}}\right)^2} \left(\sum_{l' = 1}^n \frac{\mathbf{z}_{1,l'}(\mathbf{y}_{1,l'} - \alpha_1)}{(\sigma^2_{1,l'})^2}\right)\left(\sum_{l'' = 1}^n \mathbf{z}_{1,l''}\right) \\ \vdots \\ - \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{k,l}}{\sigma^2_{k,l}}\right)^2} \left(\sum_{l' = 1}^n \frac{\mathbf{z}_{k,l'}(\mathbf{y}_{k,l'} - \alpha_k)}{(\sigma^2_{k,l'})^2}\right)\left(\sum_{l'' = 1}^n \mathbf{z}_{k,l''}\right) \\ \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\sigma^2_{i,l})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^2} - 2 \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \left(\frac{\mathbf{z}_{i,a}\mathbf{z}_{i,b}(\sigma^2_{i,a})^2 \left(\sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}} \right)}{(\sigma^2_{i,l})^4 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}} \right)^3}\right) \right] \end{bmatrix} \end{aligned}\] </li> </ul> <p>We then do the same but with respect to $\alpha$.</p> <ul id="deriv-theta-alpha-2" class="tab" data-tab="e2b56ace-65df-4b61-b0ce-545a6b61e29d" data-name="deriv-theta-alpha-2"> <li class="active" id="deriv-theta-alpha-2-equation"> <a href="#">equation </a> </li> <li id="deriv-theta-alpha-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="e2b56ace-65df-4b61-b0ce-545a6b61e29d" data-name="deriv-theta-alpha-2"> <li class="active"> \[\frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j \partial \theta} = \begin{bmatrix} 0 \\ \vdots \\ - \mathbf{1}_n^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ \vdots \\ 0 \\ -\frac{1}{\left( 1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\sigma^2_{j,l}}\right)^2 } \sum_{a = 1}^n \frac{\mathbf{z}_{j,a}(\mathbf{y}_{j, a} - \alpha_j)}{(\sigma_{j,a}^2)^2} \left(\sum_{b = 1}^n \mathbf{z}_{j,b} \right) \end{bmatrix}\] </li> <li> <p>First, we find the gradient (with respect to $\alpha$) of the derivative with respect to $\tau^2$. This should be equivalent to the corresponding components of Eq. \eqref{eq:info-tau-tau}.</p> \[\begin{aligned} \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j \partial \tau^2 } &amp;= \frac{\partial}{\partial \alpha_j} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] \\ &amp;= \frac{\partial}{\partial \alpha_j} \left[ - \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} - \sum_{l = 1}^n \sum_{j = 1}^n \frac{\tau^2 \mathbf{z}_{i,l}^2 \mathbf{z}_{i,j}^2}{(\sigma_{i,l}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)} - \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \frac{\mathbf{z}_{i,a} \mathbf{z}_{i,b}}{(\sigma^2_{i,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^2} \right] \right] \\ &amp;= \frac{\partial}{\partial \alpha_j} \left[ \frac{1}{2} \sum_{i = 1}^k \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \frac{\mathbf{z}_{i,a} \mathbf{z}_{i,b}}{(\sigma^2_{i,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^2} \right] \\ &amp;= \frac{1}{2} \sum_{a = 1}^n \sum_{b = 1}^n \frac{\partial}{\partial \alpha_j} \left[ (\mathbf{y}_{j,a} - \alpha_j)(\mathbf{y}_{j,b} - \alpha_j) \frac{\mathbf{z}_{j,a} \mathbf{z}_{j,b}}{(\sigma^2_{j,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\sigma^2_{j,l}}\right)^2} \right] \\ &amp;= \frac{1}{2} \sum_{a = 1}^n \sum_{b = 1}^n \frac{\mathbf{z}_{j,a} \mathbf{z}_{j,b}}{(\sigma^2_{j,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\sigma^2_{j,l}}\right)^2} \frac{\partial}{\partial \alpha_j} \left[ \mathbf{y}_{j,a} \mathbf{y}_{j,b} - \alpha_j \mathbf{y}_{j,a} - \alpha \mathbf{y}_{j,b} + \alpha_j^2 \right] \\ &amp;= -\frac{1}{2 \left( 1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\sigma^2_{j,l}}\right)^2 } \sum_{a = 1}^n \sum_{b = 1}^n \left[ \frac{\mathbf{z}_{j,a}\mathbf{z}_{j,b}(\mathbf{y}_{j, a} - \alpha_j)}{(\sigma_{j,a}^2)^2} + \frac{\mathbf{z}_{j,a}\mathbf{z}_{j,b}(\mathbf{y}_{j,b} - \alpha_j)}{(\sigma_{j,a}^2)^2} \right] \\ &amp;= -\frac{1}{2 \left( 1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\sigma^2_{j,l}}\right)^2 } \left[ \sum_{a = 1}^n \frac{\mathbf{z}_{j,a}(\mathbf{y}_{j, a} - \alpha_j)}{(\sigma_{j,a}^2)^2} \sum_{b = 1}^n \mathbf{z}_{j,b} + \sum_{b = 1}^n \frac{\mathbf{z}_{j,b}(\mathbf{y}_{j,b} - \alpha_j)}{(\sigma_{j,b}^2)^2} \sum_{a = 1}^n \mathbf{z}_{j,a}\right] \\ &amp;= -\frac{1}{\left( 1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\sigma^2_{j,l}}\right)^2 } \sum_{a = 1}^n \frac{\mathbf{z}_{j,a}(\mathbf{y}_{j, a} - \alpha_j)}{(\sigma_{j,a}^2)^2} \left(\sum_{b = 1}^n \mathbf{z}_{j,b} \right) \end{aligned}\] <p>Now, we find the vector of second derivatives of the log-likelihood with respect to the components of $\alpha$:</p> \[\begin{aligned} \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j^2} &amp;= \frac{\partial}{\partial \alpha_j} \left[ (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \Sigma_{y_j}^{-1} \mathbf{1}_n\right] \\ &amp;= - \mathbf{1}_n^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ \frac{\partial^2 \ell(\theta; \mathbf{y})}{ \partial \alpha_{j'} \partial \alpha_j} &amp;= \frac{\partial}{\partial \alpha_{j'}} \left[ (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \Sigma_{y_j}^{-1} \mathbf{1}_n\right] \\ &amp;= 0 \\ \implies \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j \partial \alpha} &amp;= \begin{bmatrix} 0 \\ \vdots \\ - \mathbf{1}_n^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ \vdots \\ 0 \end{bmatrix} \end{aligned}\] <p>Putting the two above results together gives us:</p> \[\frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j \partial \theta} = \begin{bmatrix} 0 \\ \vdots \\ - \mathbf{1}_n^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ \vdots \\ 0 \\ -\frac{1}{\left( 1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\sigma^2_{j,l}}\right)^2 } \sum_{a = 1}^n \frac{\mathbf{z}_{j,a}(\mathbf{y}_{j, a} - \alpha_j)}{(\sigma_{j,a}^2)^2} \left(\sum_{b = 1}^n \mathbf{z}_{j,b} \right) \end{bmatrix}\] </li> </ul> <h4 id="expectations">Expectations</h4> <p>We now take the expectation of the above vectors. We’ll evaluate the second order partial derivatives with respect to $\tau^2$ first.</p> <ul id="info-tau-tau-2" class="tab" data-tab="38ea2473-67ce-4961-9f39-db137592a234" data-name="info-tau-tau-2"> <li class="active" id="info-tau-tau-2-equation"> <a href="#">equation </a> </li> <li id="info-tau-tau-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="38ea2473-67ce-4961-9f39-db137592a234" data-name="info-tau-tau-2"> <li class="active"> \[\mathbb{E}\left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \tau^2 \partial \theta } \right] = \begin{bmatrix} 0 \\ \vdots \\ 0 \\ \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\sigma^2_{i,l})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^2} \right] \end{bmatrix}\] </li> <li> \[\begin{aligned} \mathbb{E}\left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial (\tau^2)^2 }\right] &amp;= \mathbb{E}\left[ \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\sigma^2_{i,l})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^2} - 2 \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \left(\frac{\mathbf{z}_{i,a}\mathbf{z}_{i,b}(\sigma^2_{i,a})^2 \left(\sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}} \right)}{(\sigma^2_{i,l})^4 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}} \right)^3}\right) \right] \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\sigma^2_{i,l})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^2} - 2 \sum_{a = 1}^n \sum_{b = 1}^n \mathbb{E}\left[ (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i)\right] \left(\frac{\mathbf{z}_{i,a}\mathbf{z}_{i,b}(\sigma^2_{i,a})^2 \left(\sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}} \right)}{(\sigma^2_{i,l})^4 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}} \right)^3}\right) \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\sigma^2_{i,l})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^2} - 2 \sum_{a = 1}^n \sum_{b = 1}^n \text{Cov}\left(\mathbf{y}_{i,a}, \mathbf{y}_{i,b} \right) \left(\frac{\mathbf{z}_{i,a}\mathbf{z}_{i,b}(\sigma^2_{i,a})^2 \left(\sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}} \right)}{(\sigma^2_{i,l})^4 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}} \right)^3}\right) \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\sigma^2_{i,l})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^2} \right] &amp; \left(\text{obs. ind. under } H_0 \right) \\ \mathbb{E}\left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \tau^2 \partial \alpha_j}\right] &amp;= \mathbb{E}\left[ - \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\sigma^2_{j,l}}\right)^2} \left(\sum_{l' = 1}^n \frac{\mathbf{z}_{j,l'}(\mathbf{y}_{j,l'} - \alpha_j)}{(\sigma^2_{j,l'})^2}\right)\left(\sum_{l'' = 1}^n \mathbf{z}_{j,l''}\right) \right] \\ &amp;= - \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\sigma^2_{j,l}}\right)^2} \left(\sum_{l' = 1}^n \frac{\mathbf{z}_{j,l'} \mathbb{E}\left[ (\mathbf{y}_{j,l'} - \alpha_j)\right] }{(\sigma^2_{j,l'})^2}\right)\left(\sum_{l'' = 1}^n \mathbf{z}_{j,l''}\right) \\ &amp;= 0 \end{aligned}\] </li> </ul> <p>And then we do the same for $\alpha_j$:</p> <ul id="info-alpha-alpha-2" class="tab" data-tab="4f765fb1-7352-4640-aadc-7cae94f1bcd7" data-name="info-alpha-alpha-2"> <li class="active" id="info-alpha-alpha-2-equation"> <a href="#">equation </a> </li> <li id="info-alpha-alpha-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="4f765fb1-7352-4640-aadc-7cae94f1bcd7" data-name="info-alpha-alpha-2"> <li class="active"> \[\mathbb{E}\left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j \partial \theta } \right] = \begin{bmatrix} 0 \\ \vdots \\ - \mathbf{1}_n^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ \vdots \\ 0 \\ 0 \end{bmatrix}\] </li> <li> \[\begin{aligned} \mathbb{E}\left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j \partial \tau^2} \right] &amp;= \mathbb{E}\left[ -\frac{1}{\left( 1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\sigma^2_{j,l}}\right)^2 } \sum_{a = 1}^n \frac{\mathbf{z}_{j,a}(\mathbf{y}_{j, a} - \alpha_j)}{(\sigma_{j,a}^2)^2} \left(\sum_{b = 1}^n \mathbf{z}_{j,b} \right) \right] \\ &amp;= -\frac{1}{\left( 1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\sigma^2_{j,l}}\right)^2 } \sum_{a = 1}^n \frac{\mathbf{z}_{j,a} \mathbb{E}\left[\mathbf{y}_{j, a} - \alpha_j\right] }{(\sigma_{j,a}^2)^2} \left(\sum_{b = 1}^n \mathbf{z}_{j,b} \right) \\ &amp;= 0 \\ \mathbb{E}\left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j^2} \right] &amp;= \mathbb{E}\left[ - \mathbf{1}_n^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \right] \\ &amp;= - \mathbf{1}_n^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ \mathbb{E}\left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j \partial \alpha_{j'}} \right] &amp;= 0 \end{aligned}\] </li> </ul> <p>We then evaluate the Fisher information at the MLE:</p> <ul id="info-2" class="tab" data-tab="f49ed979-606d-47cb-81e3-562aa0d4ffb5" data-name="info-2"> <li class="active" id="info-2-equation"> <a href="#">equation </a> </li> <li id="info-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="f49ed979-606d-47cb-81e3-562aa0d4ffb5" data-name="info-2"> <li class="active"> \[\mathcal{I}_{\theta, \theta}(\hat{\theta}) = \begin{bmatrix} - \mathbf{1}_n^\top\text{diag}\left(\left[ \frac{1}{\sigma^2_{1,1}}, \dots, \frac{1}{\sigma^2_{1,n}}\right]\right) \mathbf{1}_n &amp; 0 &amp; \dots &amp; 0 \\ 0 &amp; - \mathbf{1}_n^\top \text{diag}\left(\left[ \frac{1}{\sigma^2_{2,1}}, \dots, \frac{1}{\sigma^2_{2,n}}\right]\right) \mathbf{1}_n &amp; \dots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \dots &amp; \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\sigma^2_{i,l})^2} \right] \end{bmatrix}\] </li> <li> <p>Note that:</p> \[\Sigma_{y_i}^{-1} \bigg\rvert_{\theta = \hat{\theta}} = \text{diag}\left(\left[ \frac{1}{\sigma^2_{i,1}}, \dots, \frac{1}{\sigma^2_{i,n}}\right]\right)\] <p>Then we have:</p> \[\begin{aligned} - \mathbb{E}\left. \left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \tau^2 \partial \theta} \right] \right\rvert_{\theta = \hat{\theta}} &amp;= - \mathbb{E} \left.\left[ \begin{bmatrix} 0 \\ \vdots \\ 0 \\ \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\sigma^2_{i,l})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^2} \right] \end{bmatrix} \right] \right\rvert_{\theta = \hat{\theta}} \\ &amp;= \begin{bmatrix} 0 \\ \vdots \\ 0 \\ \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\sigma^2_{i,l})^2} \right] \end{bmatrix} \\ - \mathbb{E} \left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j \partial \theta} \right] \bigg\rvert_{\theta = \hat{\theta}} &amp;= - \mathbb{E} \left. \left[ \begin{bmatrix} 0 \\ \vdots \\ - \mathbf{1}_n^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ \vdots \\ 0 \\ 0 \end{bmatrix} \right] \right\rvert_{\theta = \hat{\theta}} \\ &amp;= \begin{bmatrix} 0 \\ \vdots \\ - \mathbf{1}_n^\top \text{diag}\left(\left[ \frac{1}{\sigma^2_{j,1}}, \dots, \frac{1}{\sigma^2_{j,n}}\right]\right) \mathbf{1}_n \\ \vdots \\ 0 \\ 0 \end{bmatrix} \end{aligned}\] <p>Putting these together into a big matrix:</p> \[\begin{aligned} \mathcal{I}_{\theta, \theta}(\hat{\theta}) &amp;= - \mathbb{E}\left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \theta \partial \theta^\top} \right] \bigg\rvert_{\theta = \hat{\theta}} \\ &amp;= \begin{bmatrix} - \mathbf{1}_n^\top\text{diag}\left(\left[ \frac{1}{\sigma^2_{1,1}}, \dots, \frac{1}{\sigma^2_{1,n}}\right]\right) \mathbf{1}_n &amp; 0 &amp; \dots &amp; 0 \\ 0 &amp; - \mathbf{1}_n^\top \text{diag}\left(\left[ \frac{1}{\sigma^2_{2,1}}, \dots, \frac{1}{\sigma^2_{2,n}}\right]\right) \mathbf{1}_n &amp; \dots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \dots &amp; \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\sigma^2_{i,l})^2} \right] \end{bmatrix} \end{aligned}\] </li> </ul> <hr/> ]]></content><author><name></name></author><category term="glmm"/><category term="glmm"/><category term="information"/><category term="score"/><summary type="html"><![CDATA[The Negative Binomial Case]]></summary></entry><entry><title type="html">Score and Information</title><link href="https://aerosengart.github.io/blog/2025/score-info/" rel="alternate" type="text/html" title="Score and Information"/><published>2025-10-29T00:00:00+00:00</published><updated>2025-10-29T00:00:00+00:00</updated><id>https://aerosengart.github.io/blog/2025/score-info</id><content type="html" xml:base="https://aerosengart.github.io/blog/2025/score-info/"><![CDATA[<p>This post is just a catch-all for my derivations for my score test project. Our set-up is as follows. We have $n$ observations coming from $k$ different clusters, each of size $n_t$ for $t \in [k]$. The full data will be denoted by $\mathbf{y}$. Though $\mathbf{y}$ is a vector, we’ll denote the $j$-th observation from cluster $i$ with $\mathbf{y}_{i,j}$. For example, \(\mathbf{y}_{i,j}\) denotes element \(\sum_{l = 1}^{i - 1} n_l + j\) of $\mathbf{y}$. We’ll also denote the $n_i$-dimensional vector of responses for cluster $i$ with $\mathbf{y}_i$.</p> <p>For each observation, we will have $p$ fixed effect covariates arranged in a $p$-dimensional vector, \(\mathbf{x}_{i, j}\), and $q$ random effects covariates in a $q$-dimensional vector, \(\mathbf{z}_{i,j}\). We’ll assume that the observations within the same cluster are independent.</p> <p>Our model comes in the form of a specification of the conditional mean, $\mu_{i,j} = \mathbb{E}[\mathbf{y}_{i,j} \rvert \beta_i]$ (where we suppress the addition conditioning on the covariates themselves). For a monotonic and differentiable link function (e.g. $\log(\cdot)$ or $\text{logit}(\cdot)$), the conditional mean of the $j$-th observation in group $i$ is assumed to be given by:</p> \[\mu_{i,j} = g^{-1}\left(\alpha^\top \mathbf{x}_{i,j} + \beta_i^\top \mathbf{z}_{i,j} \right) \label{eq:glmm}\] <p>We then assume that the observations themselves follow some exponential family distribution with measurement errors, $\epsilon_{i,j}$, which is the deviation of the response from its (unit-specific) conditional mean. These errors are assumed to have mean zero and be independent of each other and of the random effects. We further assume the responses, $\mathbf{y}_{i,j}$, conditional on the random effects (and the covariates), are independent with variances equal to some function of the conditional mean.</p> <p>In general, we will assume that:</p> \[\beta_i \overset{iid}{\sim} \mathcal{N}\left(\mathbf{0}_q, D(\tau^2) \right)\] <p>for some variance component, $\tau^2$. We’ll use $[\cdot] \rvert_{H_0}$ to denote evaluation of the function in brackets when setting $\beta$ equal to $\beta_0$. We’ll also use a superscript $0$ (e.g. $\mu^0$, $\eta^0$, etc.) to denote the quantity under the null hypothesis (i.e. $\tau^2 = \mathbf{0} \implies \beta = \mathbf{0}$).</p> <hr/> <h2 id="gaussian-case">Gaussian Case</h2> <p>In this example, we’ll have the simple setting of a Gaussian response, which means $g(\cdot)$ is the identity function. We will have a fixed (but cluster-specific) intercept and a single random slope. We will have $k$ clusters and $n$ observations per cluster. We assume:</p> \[\mathbf{y}_{i, j} = \alpha_i + \beta_i \mathbf{z}_{i,j} + \epsilon_{i,j}, \hspace{8mm} \epsilon_{i,j} \overset{iid}{\sim} \mathcal{N}(0, \sigma^2), \hspace{5mm} \beta_i \overset{iid}{\sim} \mathcal{N}(0, \tau^2)\] <p>where we also assume the random effects and errors are independent. \(\mathbf{z}_i \in \mathbb{R}^n\) is the vector of covariate values for the $n$ samples in cluster $i$. We’ll denote the vector of responses for cluster $i$ with $\mathbf{y}_i$ so that \(\mathbf{y}_{i,j}\) denotes the $j$-th component of said vector. Marginally, the response vector $\mathbf{y}_i$ has mean \(\alpha_i \mathbb{1}_n\) and variance-covariance matrix:</p> <ul id="covar" class="tab" data-tab="a1edd82a-0282-43ee-9314-3a4e477e3de6" data-name="covar"> <li class="active" id="covar-equation"> <a href="#">equation </a> </li> <li id="covar-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="a1edd82a-0282-43ee-9314-3a4e477e3de6" data-name="covar"> <li class="active"> \[\Sigma_{y_i} = \sigma^2 \mathbb{I}_{n \times n} + \tau^2 \mathbf{z}_i \mathbf{z}_i^\top\] </li> <li> <p>For a single cluster:</p> \[\begin{aligned} \mathbb{E}\left[ (\mathbf{y}_{i,j} - \alpha_i)^2\right] &amp;= \mathbb{E}\left[ (\beta_i \mathbf{z}_{i,j} + \epsilon_{i,j})^2 \right] \\ &amp;= \mathbb{E}\left[\beta_i^2 \mathbf{z}_{i,j}^2 \right] + 2 \mathbb{E}\left[ \beta_i \mathbf{z}_{i,j} \epsilon_{i,j} \right] + \mathbb{E}\left[ \epsilon_{i,j}^2 \right] \\ &amp;= \tau^2 \mathbf{z}_{i,j}^2 + \sigma^2 \\ \mathbb{E}\left[ (\mathbf{y}_{i,j} - \alpha_i)(\mathbf{y}_{i,j'} - \alpha_i) \right] &amp;= \mathbb{E}\left[ (\beta_i \mathbf{z}_{i,j} + \epsilon_{i,j})(\beta_i \mathbf{z}_{i,j'} + \epsilon_{i,j'})\right] \\ &amp;= \mathbb{E}\left[ \beta_i^2 \mathbf{z}_{i,j} \mathbf{z}_{i,j'} \right] + \mathbb{E}\left[ \beta_i \mathbf{z}_{i,j} \epsilon_{i,j'}\right] + \mathbb{E}\left[ \beta_i \mathbf{z}_{i,j'} \epsilon_{i,j}\right] + \mathbb{E}\left[ \epsilon_{i,j} \epsilon_{i,j'}\right] \\ &amp;= \tau^2 \mathbf{z}_{i,j} \mathbf{z}_{i,j'} \nonumber \end{aligned}\] <p>Thus, the variance-covariance matrix for $\mathbf{y}_i$:</p> \[\Sigma_{y_i} = \begin{bmatrix} \sigma^2 + \tau^2 \mathbf{z}_{i,1}^2 &amp; \dots &amp; \tau^2 \mathbf{z}_{i,1} \mathbf{z}_{i,n} \\ \vdots &amp; \ddots &amp; \vdots \\ \tau^2 \mathbf{z}_{i,n} \mathbf{z}_{i, 1} &amp; \dots &amp; \sigma^2 + \tau^2 \mathbf{z}_{i, n}^2 \end{bmatrix} = \sigma^2 \mathbb{I}_{n \times n} + \tau^2 \mathbf{z}_i \mathbf{z}_i^\top \nonumber\] </li> </ul> <p>Since the $\beta_i$ are independent, observations from different clusters have covariance zero. Let $\mathbf{y} = (\mathbf{y}_1, \dots, \mathbf{y}_k)$ denote the full data, $\alpha = \begin{bmatrix} \alpha_1 &amp; \dots &amp; \alpha_k\end{bmatrix}^\top$, $\beta = \begin{bmatrix} \beta_1 &amp; \dots &amp; \beta_k\end{bmatrix}^\top$, and $\theta = (\alpha, \beta)$. The complete, marginal likelihood and log-likelihood are:</p> \[\begin{aligned} \mathcal{L}(\theta; \mathbf{y}) &amp;= \prod_{i = 1}^k (2 \pi)^{-\frac{n}{2}} \rvert \Sigma_{y_i} \rvert^{-\frac{1}{2}} \exp\left(- \frac{1}{2} (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \Sigma_{y_i}^{-1} (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right) \\ \ell(\theta; \mathbf{y}) &amp;= \sum_{i = 1}^k \left[ -\frac{n}{2} \log(2 \pi) - \frac{1}{2}\log(\rvert \Sigma_{y_i} \rvert) - \frac{1}{2} (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \Sigma_{y_i}^{-1} (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \end{aligned}\] <h3 id="score">Score</h3> <p>We first find the gradient of the log-likelihood with respect to $\theta$ parameter-wise. Using the Sherman-Morrison formula, we can find $\Sigma_{y_i}^{-1}$ to be:</p> <ul id="sigma-inv" class="tab" data-tab="e082a641-e44b-48d1-a24c-5328df9f12b9" data-name="sigma-inv"> <li class="active" id="sigma-inv-equation"> <a href="#">equation </a> </li> <li id="sigma-inv-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="e082a641-e44b-48d1-a24c-5328df9f12b9" data-name="sigma-inv"> <li class="active"> \[\Sigma_{y_i}^{-1} = \frac{1}{\sigma^2} \mathbb{I}_{n \times n} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top\] </li> <li> \[\begin{aligned} \Sigma_{y_i}^{-1} &amp;= \left[ \sigma^2 \mathbb{I}_{n \times n} + \mathbf{z}_i [\tau^2 \mathbb{I}_{n \times n}] \mathbf{z}_i^\top \right]^{-1} \\ &amp;= \frac{1}{\sigma^2} \mathbb{I}_{n \times n} - \left(1 + \tau^2 \mathbf{z}_i^\top \left[ \frac{1}{\sigma^2} \mathbb{I}_{n \times n} \right]\mathbf{z}_i \right)^{-1} \left( \left(\frac{1}{\sigma^2} \mathbb{I}_{n \times n}\right) \left(\tau^2 \mathbf{z}_{i} \mathbf{z}_i^\top \right) \left(\frac{1}{\sigma^2} \mathbb{I}_{n \times n}\right) \right) \\ &amp;= \frac{1}{\sigma^2} \mathbb{I}_{n \times n} - \left(1 + \frac{\tau^2}{\sigma^2} \mathbf{z}_i^\top \mathbf{z}_i \right)^{-1}\left(\frac{\tau^2}{(\sigma^2)^2} \mathbf{z}_i \mathbf{z}_i^\top \right) \\ &amp;= \frac{1}{\sigma^2} \mathbb{I}_{n \times n} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \end{aligned} \nonumber\] </li> </ul> <h4 id="derivatives">Derivatives</h4> <p>Let’s find the derivative with respect to $\sigma^2$:</p> <ul id="ell-deriv-1" class="tab" data-tab="39cc7e74-8174-41bf-9c34-ba10e4916fea" data-name="ell-deriv-1"> <li class="active" id="ell-deriv-1-equation"> <a href="#">equation </a> </li> <li id="ell-deriv-1-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="39cc7e74-8174-41bf-9c34-ba10e4916fea" data-name="ell-deriv-1"> <li class="active"> \[\frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} = -\frac{1}{2}\sum_{i = 1}^k \left[ \frac{n}{\sigma^2} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \text{tr}\left[\mathbf{z}_i \mathbf{z}_i^\top\right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2)^2} \mathbb{I}_{n \times n} + \frac{2\tau^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top - \frac{(\tau^2)^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top\right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right]\] </li> <li> \[\begin{aligned} \frac{\partial}{\partial \sigma^2} \left[ \log(\rvert \Sigma_{y_i} \rvert) \right] &amp;= \text{tr}\left[ \Sigma_{y_i}^{-1} \frac{\partial}{\partial \sigma^2} \left[\Sigma_{y_i}\right] \right] \\ &amp;= \text{tr}\left[ \Sigma_{y_i}^{-1} \mathbb{I}_{n \times n} \right] \\ &amp;= \text{tr}\left[ \Sigma^{-1} \right] \\ &amp;= \text{tr}\left[\frac{1}{\sigma^2} \mathbb{I}_{n \times n} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \right] \\ &amp;=\text{tr}\left[ \frac{1}{\sigma^2}\mathbb{I}_{n \times n} \right] \text{tr}\left[- \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top\right] \\ &amp;=\frac{n}{\sigma^2} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \text{tr}\left[\mathbf{z}_i \mathbf{z}_i^\top\right] \end{aligned} \nonumber\] \[\begin{aligned} \frac{\partial}{\partial \sigma^2} \left[ \Sigma_{y_i}^{-1} \right] &amp;= - \Sigma_{y_i}^{-1} \frac{\partial}{\partial \sigma^2} \left[ \Sigma_{y_i}\right] \Sigma_{y_i}^{-1} \\ &amp;= -\left[ \frac{1}{\sigma^2} \mathbb{I}_{n \times n} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \right] \mathbb{I}_{n \times n} \left[\frac{1}{\sigma^2} \mathbb{I}_{n \times n} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \right] \\ &amp;= - \left[ \frac{1}{(\sigma^2)^2} \mathbb{I}_{n \times n} - \frac{2\tau^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top + \frac{(\tau^2)^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \\ &amp;= - \frac{1}{(\sigma^2)^2} \mathbb{I}_{n \times n} + \frac{2\tau^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top - \frac{(\tau^2)^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \end{aligned}\] <p>The above imply:</p> \[\begin{aligned} \frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} &amp;= \frac{\partial}{\partial \sigma^2} \left[ \sum_{i = 1}^k - \frac{1}{2} \log(\rvert \Sigma_{y_i} \rvert) - \frac{1}{2}(\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \Sigma_{y_i}^{-1} (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \\ &amp;= -\frac{1}{2}\sum_{i = 1}^k \left[ \frac{n}{\sigma^2} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \text{tr}\left[\mathbf{z}_i \mathbf{z}_i^\top\right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2)^2} \mathbb{I}_{n \times n} + \frac{2\tau^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top - \frac{(\tau^2)^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top\right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \end{aligned}\] </li> </ul> <p>We do the same with $\tau^2$:</p> <ul id="ell-tau-1" class="tab" data-tab="5feedfa3-a7d3-4dae-8a4b-daa78404ad70" data-name="ell-tau-1"> <li class="active" id="ell-tau-1-equation"> <a href="#">equation </a> </li> <li id="ell-tau-1-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="5feedfa3-a7d3-4dae-8a4b-daa78404ad70" data-name="ell-tau-1"> <li class="active"> \[\frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} = - \frac{1}{2} \sum_{i = 1}^k \left[ \frac{1}{\sigma^2} \text{tr}[\mathbf{z}_i \mathbf{z}_i^\top] - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top\right] + (\mathbf{y}_i - \alpha_i\mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2)^2} \mathbf{z}_i \mathbf{z}_i^\top + \frac{2\tau^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{(\tau^2)^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top\mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}^\top \mathbf{z}_i \mathbf{z}_i^\top \right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right]\] </li> <li> \[\begin{aligned} \frac{\partial}{\partial \tau^2} \left[ \log(\rvert \Sigma_{y_i} \rvert) \right] &amp;= \text{tr}\left[ \Sigma_{y_i}^{-1} \frac{\partial}{\partial \tau^2} \left[\Sigma_{y_i}\right] \right] \\ &amp;= \text{tr}\left[ \Sigma_{y_i}^{-1} \mathbf{z}_i \mathbf{z}_i^\top \right] \\ &amp;= \text{tr} \left[ \left( \frac{1}{\sigma^2} \mathbb{I}_{n \times n} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top\right) \mathbf{z}_i \mathbf{z}_i^\top\right] \\ &amp;= \frac{1}{\sigma^2} \text{tr}[\mathbf{z}_i \mathbf{z}_i^\top] - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top\right] \end{aligned} \nonumber\] \[\begin{aligned} \frac{\partial}{\partial \tau^2} \left[ \Sigma_{y_i}^{-1} \right] &amp;= - \Sigma_{y_i}^{-1} \frac{\partial}{\partial \tau^2} \left[ \Sigma_{y_i}\right] \Sigma_{y_i}^{-1} \\ &amp;= -\left[ \frac{1}{\sigma^2} \mathbb{I}_{n \times n} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \right] \left[ \mathbf{z}_i \mathbf{z}_i^\top \right] \left[\frac{1}{\sigma^2} \mathbb{I}_{n \times n} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \right] \\ &amp;= - \frac{1}{(\sigma^2)^2} \mathbf{z}_i \mathbf{z}_i^\top + \frac{2\tau^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{(\tau^2)^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top\mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}^\top \mathbf{z}_i \mathbf{z}_i^\top \end{aligned} \nonumber\] \[\begin{aligned} \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} &amp;= \frac{\partial}{\partial \tau^2} \left[ \sum_{i = 1}^k - \frac{1}{2} \log(\rvert \Sigma_{y_i} \rvert) - \frac{1}{2}(\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \Sigma_{y_i}^{-1} (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \\ &amp;= - \frac{1}{2} \sum_{i = 1}^k \left[ \frac{1}{\sigma^2} \text{tr}[\mathbf{z}_i \mathbf{z}_i^\top] - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top\right] + (\mathbf{y}_i - \alpha_i\mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2)^2} \mathbf{z}_i \mathbf{z}_i^\top + \frac{2\tau^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{(\tau^2)^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top\mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}^\top \mathbf{z}_i \mathbf{z}_i^\top \right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \end{aligned}\] </li> </ul> <p>And then take the gradient with respect to $\alpha$:</p> <ul id="ell-alpha-1" class="tab" data-tab="0211d54d-61d9-40a8-8970-90131aaa8ae2" data-name="ell-alpha-1"> <li class="active" id="ell-alpha-1-equation"> <a href="#">equation </a> </li> <li id="ell-alpha-1-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="0211d54d-61d9-40a8-8970-90131aaa8ae2" data-name="ell-alpha-1"> <li class="active"> \[\frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha} = \begin{bmatrix} (\mathbf{y}_1 - \alpha_1 \mathbf{1}_n)^\top \Sigma_{y_i}^{-1} \mathbf{1}_n \\ \vdots \\ (\mathbf{y}_k - \alpha_k \mathbf{1}_n)^\top \Sigma_{y_i}^{-1} \mathbf{1}_n \end{bmatrix}\] </li> <li> <p>We do the computations component-wise: \(\begin{aligned} \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} &amp;= \sum_{i = 1}^k - \frac{1}{2} \frac{\partial}{\partial \alpha_j} \left[ (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \Sigma^{-1}_{y_i} (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \\ &amp;= - \frac{1}{2} \left(2 (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \Sigma_{y_i}^{-1}(- \mathbf{1}_n) \right) \\ &amp;= (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \Sigma_{y_i}^{-1} \mathbf{1}_n \end{aligned}\) So then: \(\begin{aligned} \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha} &amp;= \begin{bmatrix} (\mathbf{y}_1 - \alpha_1 \mathbf{1}_n)^\top \Sigma_{y_i}^{-1} \mathbf{1}_n \\ \vdots \\ (\mathbf{y}_k - \alpha_k \mathbf{1}_n)^\top \Sigma_{y_i}^{-1} \mathbf{1}_n \end{bmatrix} \end{aligned}\)</p> </li> </ul> <h4 id="mles">MLEs</h4> <p>We can then find the MLE vector, $\hat{\theta}$, by setting the above equations equal to zero, substituting $\tau^2 = 0$, and solving. The MLE vector, $\hat{\theta}$ is:</p> <ul id="mle-1" class="tab" data-tab="c3d1e085-cb5d-4872-bf44-a3e586fa0713" data-name="mle-1"> <li class="active" id="mle-1-equation"> <a href="#">equation </a> </li> <li id="mle-1-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="c3d1e085-cb5d-4872-bf44-a3e586fa0713" data-name="mle-1"> <li class="active"> \[\hat{\theta} = \begin{bmatrix} \frac{1}{n} \sum_{j = 1}^n \mathbf{y}_{1,j} \\ \vdots \\ \frac{1}{n} \sum_{j = 1}^n \mathbf{y}_{k,j} \\ \frac{1}{nk} \sum_{i = 1}^k (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ 0 \end{bmatrix}\] </li> <li> <p>We set the derivative with respect to $\sigma^2$ equal to zero, substitute $\tau^2 = 0$ (under $H_0$), and solve for $\sigma^2$:</p> \[\begin{aligned} 0 &amp;= \frac{\partial}{\partial \sigma^2} \left[ \ell(\theta; \mathbf{y}) \right] \bigg\rvert_{\theta = \theta_0}\\ 0 &amp;= -\frac{1}{2}\sum_{i = 1}^k \left[ \frac{n}{\sigma^2} - \frac{0}{\sigma^2(\sigma^2 + 0 \cdot\mathbf{z}_i^\top \mathbf{z}_i)} \text{tr}\left[\mathbf{z}_i \mathbf{z}_i^\top\right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2)^2} \mathbb{I}_{n \times n} + \frac{2\cdot 0}{(\sigma^2)^2(\sigma^2 + 0 \cdot \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top + \frac{(0)^2}{(\sigma^2)^2(\sigma^2 - 0 \cdot \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top\right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \\ 0 &amp;= -\frac{1}{2}\sum_{i = 1}^k \left[ \frac{n}{\sigma^2} + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2)^2} \mathbb{I}_{n \times n} \right](\mathbf{y}_i - \alpha_i \mathbf{1}_n)\right] \\ 0 &amp;= - \frac{nk}{2 \sigma^2} - \frac{1}{2} \sum_{i = 1}^k - \frac{1}{(\sigma^2)^2} (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ 0 &amp;= - \frac{nk}{2\sigma^2} + \frac{1}{2 (\sigma^2)^2} \sum_{i = 1}^k (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ \frac{nk}{2 \sigma^2} &amp;= \frac{1}{2(\sigma^2)^2} \sum_{i = 1}^k (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ \sigma^2 n k &amp;= \sum_{i = 1}^k (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ \sigma^2 &amp;= \frac{1}{nk} \sum_{i = 1}^k (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \end{aligned}\] <p>We set the gradient w.r.t $\alpha$ equal to zero, substitute $\tau^2 = 0$ (under $H_0$), and solve for $\alpha$.</p> \[\begin{aligned} \mathbf{0} &amp;= \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha} \bigg\rvert_{\theta = \theta_0} \\ \mathbf{0} &amp;= \begin{bmatrix} (\mathbf{y}_1 - \alpha_1 \mathbf{1}_n)^\top \left[\frac{1}{\sigma^2}\mathbb{I}_{n \times n} \right] \mathbf{1}_n \\ \vdots \\ (\mathbf{y}_k - \alpha_k \mathbf{1}_n)^\top \left[\frac{1}{\sigma^2}\mathbb{I}_{n \times n} \right] \mathbf{1}_n \end{bmatrix} \end{aligned}\] <p>Since each entry of the gradient only has one component of $\alpha$, we can solve then all separately:</p> \[\begin{aligned} 0 &amp;= (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ \frac{1}{\sigma^2} \mathbb{I}_{n \times n} \right] \mathbf{1}_n \\ 0 &amp;= \frac{1}{\sigma^2} (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \mathbf{1}_n \\ 0 &amp;= \frac{1}{\sigma^2} \sum_{j = 1}^n (\mathbf{y}_{i,j} - \alpha_i) \\ 0 &amp;= \frac{1}{\sigma^2} \left( \sum_{j =1 }^n \mathbf{y}_{i,j} - n \alpha_i \right) \\ n \alpha_i &amp;= \sum_{j =1 }^n \mathbf{y}_{i,j} \\ \alpha_i &amp;= \frac{1}{n} \sum_{j = 1}^n \mathbf{y}_{i,j} \end{aligned}\] </li> </ul> <p>It is easiest to write the score after evaluating it at the MLE of the parameter vector under $H_0$, which we denote with $\hat{\theta}$ (uncollapse the proof to see all of the details). The MLE is given by:</p> \[\hat{\theta} = \begin{bmatrix} \frac{1}{n} \sum_{j = 1}^n \mathbf{y}_{1,j} \\ \vdots \\ \frac{1}{n} \sum_{j = 1}^n \mathbf{y}_{k,j} \\ \frac{1}{nk} \sum_{i = 1}^k (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ 0 \end{bmatrix}\] <p>Thus, the score evaluated at $\theta = \hat{\theta}$ is:</p> \[\begin{aligned} U_\theta (\hat{\theta}) &amp;= \begin{bmatrix} \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha} \bigg\rvert_{\theta = \hat{\theta}} \\ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} \bigg\rvert_{\theta = \hat{\theta}} \\ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \bigg\rvert_{\theta = \hat{\theta}} \end{bmatrix} = \begin{bmatrix} \frac{1}{\hat{\sigma}^2} (\mathbf{y}_1 - \hat{\alpha}_1 \mathbf{1}_n)^\top \mathbf{1}_n \\ \vdots \\ \frac{1}{\hat{\sigma}^2} (\mathbf{y}_k - \hat{\alpha}_k \mathbf{1}_n)^\top \mathbf{1}_n \\ - \frac{1}{2} \sum_{i = 1}^k \left[ \frac{n}{\hat{\sigma}^2} - \frac{1}{(\hat{\sigma}^2)^2} (\mathbf{y}_i - \hat{\alpha}_i \mathbf{1}_n)^\top (\mathbf{y}_i - \hat{\alpha}_i \mathbf{1}_n) \right] \\ -\frac{1}{2} \sum_{i = 1}^k \left[ \frac{\text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right]}{\hat{\sigma}^2} + \frac{1}{(\hat{\sigma}^2)^2}(\mathbf{y}_i - \hat{\alpha}_i \mathbf{1}_n)^\top \mathbf{z}_i \mathbf{z}_i^\top (\mathbf{y}_i - \hat{\alpha}_i \mathbf{1}_n) \right] \end{bmatrix} \end{aligned}\] <h3 id="information">Information</h3> <p>To find the information, we need to compute the second-order derivatives of the log-likelihood, take the expectation under $H_0$ of minus those quantities, and evaluate them by plugging in $\hat{\theta}$.</p> <h4 id="derivatives-1">Derivatives</h4> <p>We start by taking the derivative with respect to $\theta$ (component-wise) of the first derivative with respect to $\sigma^2$:</p> <ul id="deriv-theta-sigma" class="tab" data-tab="5ff0bed2-cd1d-449b-8433-7a0be49e3ab5" data-name="deriv-theta-sigma"> <li class="active" id="deriv-theta-sigma-equation"> <a href="#">equation </a> </li> <li id="deriv-theta-sigma-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="5ff0bed2-cd1d-449b-8433-7a0be49e3ab5" data-name="deriv-theta-sigma"> <li class="active"> \[\begin{aligned} \frac{\partial}{\partial \sigma^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} \right] &amp;= - \frac{1}{2}\sum_{i = 1}^k -\frac{n}{(\sigma^2)^2} + \frac{\tau^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top\right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ \frac{2}{(\sigma^2)^3} \mathbb{I}_{n \times n} - \frac{2\tau^2(3\sigma^2 + 2\tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2}\mathbf{z}_i \mathbf{z}_i^\top + \frac{2(\tau^2)^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3 (\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z})^3}\mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right](\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ \frac{\partial}{\partial \sigma^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] &amp;= -\frac{1}{2}\sum_{i = 1}^k -\frac{1}{(\sigma^2)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] - \frac{-\tau^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ \frac{2}{(\sigma^2)^3} \mathbf{z}_i \mathbf{z}_i^\top + \frac{-2\tau^2(2\sigma^2 + 3\tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{-2(\tau^2)^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3 (\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^3} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ \frac{\partial}{\partial \sigma^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] &amp;= (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left[ -\frac{1}{(\sigma^2)^2} \mathbb{I}_{n \times n} + \frac{\tau^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^2 (\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \right] \mathbf{1}_n \end{aligned}\] </li> <li> \[\begin{aligned} \frac{\partial}{\partial \sigma^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} \right] &amp;= -\frac{1}{2}\sum_{i = 1}^k \frac{\partial}{\partial \sigma^2}\left[ \frac{n}{\sigma^2} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \text{tr}\left[\mathbf{z}_i \mathbf{z}_i^\top\right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2)^2} \mathbb{I}_{n \times n} + \frac{2\tau^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top - \frac{(\tau^2)^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top\right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \\ &amp;= - \frac{1}{2}\sum_{i = 1}^k -\frac{n}{(\sigma^2)^2} - \frac{-\tau^2(2\sigma^2+\tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top\right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ \frac{2}{(\sigma^2)^3} \mathbb{I}_{n \times n} + \frac{-2\tau^2(3\sigma^2 + 2\tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2}\mathbf{z}_i \mathbf{z}_i^\top - \frac{-2(\tau^2)^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3 (\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z})^3}\mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right](\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ &amp;= - \frac{1}{2}\sum_{i = 1}^k -\frac{n}{(\sigma^2)^2} + \frac{\tau^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top\right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ \frac{2}{(\sigma^2)^3} \mathbb{I}_{n \times n} - \frac{2\tau^2(3\sigma^2 + 2\tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2}\mathbf{z}_i \mathbf{z}_i^\top + \frac{2(\tau^2)^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3 (\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z})^3}\mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right](\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ \frac{\partial}{\partial \sigma^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] &amp;= - \frac{1}{2} \sum_{i = 1}^k \frac{\partial}{\partial \sigma^2} \left[ \frac{1}{\sigma^2} \text{tr}[\mathbf{z}_i \mathbf{z}_i^\top] - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top\right] + (\mathbf{y}_i - \alpha_i\mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2)^2} \mathbf{z}_i \mathbf{z}_i^\top + \frac{2\tau^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{(\tau^2)^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top\mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}^\top \mathbf{z}_i \mathbf{z}_i^\top \right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \\ &amp;= -\frac{1}{2}\sum_{i = 1}^k -\frac{1}{(\sigma^2)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] - \frac{-\tau^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ \frac{2}{(\sigma^2)^3} \mathbf{z}_i \mathbf{z}_i^\top + \frac{-2\tau^2(2\sigma^2 + 3\tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{-2(\tau^2)^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3 (\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^3} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ \frac{\partial}{\partial \sigma^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] &amp;= \frac{\partial}{\partial \sigma^2} \left[ (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left[ \frac{1}{\sigma^2} \mathbb{I}_{n \times n} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \right] \mathbf{1}_n \right] \\ &amp;= (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left[ -\frac{1}{(\sigma^2)^2} \mathbb{I}_{n \times n} + \frac{\tau^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^2 (\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \right] \mathbf{1}_n \end{aligned}\] </li> </ul> <p>Next, we do the same with the derivative with respect to $\tau^2$:</p> <ul id="deriv-theta-tau" class="tab" data-tab="08587ebf-68c1-44d9-a453-1627a9cef402" data-name="deriv-theta-tau"> <li class="active" id="deriv-theta-tau-equation"> <a href="#">equation </a> </li> <li id="deriv-theta-tau-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="08587ebf-68c1-44d9-a453-1627a9cef402" data-name="deriv-theta-tau"> <li class="active"> \[\begin{aligned} \frac{\partial}{\partial \tau^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} \right] &amp;= -\frac{1}{2}\sum_{i = 1}^k - \frac{1}{(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[\frac{2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top - \frac{2\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^3} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ \frac{\partial}{\partial \tau^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] &amp;= -\frac{1}{2}\sum_{i = 1}^k - \frac{1}{(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ \frac{2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2}\mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{2\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^3} \mathbf{z}_i \mathbf{z}_i^\top\mathbf{z}_i \mathbf{z}_i^\top\mathbf{z}_i \mathbf{z}_i^\top\right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ \frac{\partial}{\partial \tau^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] &amp;= (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \right] \mathbf{1}_n \\ \end{aligned}\] </li> <li> \[\begin{aligned} \frac{\partial}{\partial \tau^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} \right] &amp;= -\frac{1}{2}\sum_{i = 1}^k \frac{\partial}{\partial \tau^2}\left[ \frac{n}{\sigma^2} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \text{tr}\left[\mathbf{z}_i \mathbf{z}_i^\top\right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2)^2} \mathbb{I}_{n \times n} + \frac{2\tau^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top - \frac{(\tau^2)^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top\right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \\ &amp;= -\frac{1}{2}\sum_{i = 1}^k 0 - \frac{1}{(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[0 + \frac{2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top - \frac{2\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^3} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ &amp;= -\frac{1}{2}\sum_{i = 1}^k - \frac{1}{(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[\frac{2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top - \frac{2\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^3} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ \frac{\partial}{\partial \tau^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] &amp;= - \frac{1}{2} \sum_{i = 1}^k \frac{\partial}{\partial \tau^2} \left[ \frac{1}{\sigma^2} \text{tr}[\mathbf{z}_i \mathbf{z}_i^\top] - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top\right] + (\mathbf{y}_i - \alpha_i\mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2)^2} \mathbf{z}_i \mathbf{z}_i^\top + \frac{2\tau^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{(\tau^2)^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top\mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}^\top \mathbf{z}_i \mathbf{z}_i^\top \right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \\ &amp;= -\frac{1}{2}\sum_{i = 1}^k 0 - \frac{1}{(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ 0 + \frac{2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2}\mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{2\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^3} \mathbf{z}_i \mathbf{z}_i^\top\mathbf{z}_i \mathbf{z}_i^\top\mathbf{z}_i \mathbf{z}_i^\top\right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ &amp;= -\frac{1}{2}\sum_{i = 1}^k - \frac{1}{(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ \frac{2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2}\mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{2\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^3} \mathbf{z}_i \mathbf{z}_i^\top\mathbf{z}_i \mathbf{z}_i^\top\mathbf{z}_i \mathbf{z}_i^\top\right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ \frac{\partial}{\partial \tau^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] &amp;= \frac{\partial}{\partial \tau^2} \left[ (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left[ \frac{1}{\sigma^2} \mathbb{I}_{n \times n} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \right] \mathbf{1}_n \right] \\ &amp;= (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left[ 0 - \frac{1}{(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \right] \mathbf{1}_n \\ &amp;= (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \right] \mathbf{1}_n \end{aligned}\] </li> </ul> <p>And finally with $\alpha_j$:</p> <ul id="deriv-theta-alpha" class="tab" data-tab="6ed7acde-5dfd-4cc7-88f6-a5467e3142ad" data-name="deriv-theta-alpha"> <li class="active" id="deriv-theta-alpha-equation"> <a href="#">equation </a> </li> </ul> <ul class="tab-content" id="6ed7acde-5dfd-4cc7-88f6-a5467e3142ad" data-name="deriv-theta-alpha"> <li class="active"> \[\begin{aligned} \frac{\partial}{\partial \alpha_j} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} \right] &amp;= (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2)^2} \mathbb{I}_{n \times n} + \frac{2\tau^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_j^\top \mathbf{z}_j)} \mathbf{z}_j \mathbf{z}_j^\top - \frac{(\tau^2)^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_j^\top \mathbf{z}_j)^2} \mathbf{z}_j \mathbf{z}_j^\top \mathbf{z}_j \mathbf{z}_j^\top \right]\mathbf{1}_n \\ \frac{\partial}{\partial \alpha} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] &amp;= (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2)^2} \mathbf{z}_i \mathbf{z}_i^\top + \frac{2\tau^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{(\tau^2)^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top\mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \mathbf{1}_n \\ \frac{\partial}{\partial \alpha_j} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] &amp;= - \mathbf{1}_n^\top \left[ \frac{1}{\sigma^2} \mathbb{I}_{n \times n} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \right] \mathbf{1}_n \\ \frac{\partial}{\partial \alpha_{j'}} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] &amp;= 0 \end{aligned}\] </li> </ul> <h4 id="expectations">Expectations</h4> <p>We take the expectation under the null of all of the terms we found in the previous section. First we do the $\sigma^2$ terms:</p> <ul id="expectation-sigma-1" class="tab" data-tab="8b55fa00-f713-4ce4-96b9-6c0dc74ec82d" data-name="expectation-sigma-1"> <li class="active" id="expectation-sigma-1-equation"> <a href="#">equation </a> </li> <li id="expectation-sigma-1-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="8b55fa00-f713-4ce4-96b9-6c0dc74ec82d" data-name="expectation-sigma-1"> <li class="active"> \[\begin{aligned} \mathbb{E}\left[ \frac{\partial}{\partial \sigma^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} \right] \right] &amp;= - \frac{1}{2}\sum_{i = 1}^k -\frac{n}{(\sigma^2)^2} + \frac{\tau^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top\right] + \text{tr}\left[ \left[ \frac{2}{(\sigma^2)^3} \mathbb{I}_{n \times n} - \frac{2\tau^2(3\sigma^2 + 2\tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2}\mathbf{z}_i \mathbf{z}_i^\top + \frac{2(\tau^2)^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3 (\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z})^3}\mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \Sigma_{y_i} \right] \\ \mathbb{E}\left[ \frac{\partial}{\partial \sigma^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] \right] &amp;= -\frac{1}{2}\sum_{i = 1}^k -\frac{1}{(\sigma^2)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] - \frac{-\tau^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] + \text{tr}\left[ \left[ \frac{2}{(\sigma^2)^3} \mathbf{z}_i \mathbf{z}_i^\top + \frac{-2\tau^2(2\sigma^2 + 3\tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{-2(\tau^2)^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3 (\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^3} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \Sigma_{y_i} \right] \\ \mathbb{E}\left[ \frac{\partial}{\partial \sigma^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] \right] &amp;= 0 \end{aligned}\] </li> <li> \[\begin{aligned} \mathbb{E}\left[ \frac{\partial}{\partial \sigma^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} \right] \right] &amp;= \mathbb{E}\left[ - \frac{1}{2}\sum_{i = 1}^k -\frac{n}{(\sigma^2)^2} + \frac{\tau^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top\right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ \frac{2}{(\sigma^2)^3} \mathbb{I}_{n \times n} - \frac{2\tau^2(3\sigma^2 + 2\tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2}\mathbf{z}_i \mathbf{z}_i^\top + \frac{2(\tau^2)^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3 (\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z})^3}\mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right](\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \\ &amp;= - \frac{1}{2}\sum_{i = 1}^k -\frac{n}{(\sigma^2)^2} + \frac{\tau^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top\right] + \mathbb{E}\left[ (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ \frac{2}{(\sigma^2)^3} \mathbb{I}_{n \times n} - \frac{2\tau^2(3\sigma^2 + 2\tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2}\mathbf{z}_i \mathbf{z}_i^\top + \frac{2(\tau^2)^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3 (\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z})^3}\mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right](\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \\ &amp;= - \frac{1}{2}\sum_{i = 1}^k -\frac{n}{(\sigma^2)^2} + \frac{\tau^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top\right] + \text{tr}\left[ \left[ \frac{2}{(\sigma^2)^3} \mathbb{I}_{n \times n} - \frac{2\tau^2(3\sigma^2 + 2\tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2}\mathbf{z}_i \mathbf{z}_i^\top + \frac{2(\tau^2)^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3 (\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z})^3}\mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \mathbb{E}\left[ (\mathbf{y}_i - \alpha_i \mathbf{1}_n) (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \right] \right] \\ &amp;= - \frac{1}{2}\sum_{i = 1}^k -\frac{n}{(\sigma^2)^2} + \frac{\tau^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top\right] + \text{tr}\left[ \left[ \frac{2}{(\sigma^2)^3} \mathbb{I}_{n \times n} - \frac{2\tau^2(3\sigma^2 + 2\tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2}\mathbf{z}_i \mathbf{z}_i^\top + \frac{2(\tau^2)^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3 (\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z})^3}\mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \Sigma_{y_i} \right] \\ \mathbb{E}\left[ \frac{\partial}{\partial \sigma^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] \right] &amp;= -\frac{1}{2}\sum_{i = 1}^k -\frac{1}{(\sigma^2)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] - \frac{-\tau^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] + \text{tr}\left[ \left[ \frac{2}{(\sigma^2)^3} \mathbf{z}_i \mathbf{z}_i^\top + \frac{-2\tau^2(2\sigma^2 + 3\tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{-2(\tau^2)^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3 (\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^3} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \Sigma_{y_i} \right] \\ \mathbb{E}\left[ \frac{\partial}{\partial \sigma^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] \right] &amp;= 0 \end{aligned}\] </li> </ul> <p>And the $\tau^2$ terms:</p> \[\begin{aligned} \mathbb{E}\left[ \frac{\partial}{\partial \tau^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} \right] \right] &amp;= -\frac{1}{2}\sum_{i = 1}^k - \frac{1}{(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] + \text{tr} \left[ \left[ \frac{2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top - \frac{2\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^3} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \Sigma_{y_i} \right] \\ \mathbb{E}\left[ \frac{\partial}{\partial \tau^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] \right] &amp;= -\frac{1}{2}\sum_{i = 1}^k - \frac{1}{(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] +\text{tr} \left[\left[ \frac{2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2}\mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{2\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^3} \mathbf{z}_i \mathbf{z}_i^\top\mathbf{z}_i \mathbf{z}_i^\top\mathbf{z}_i \mathbf{z}_i^\top\right] \Sigma_{y_i} \right] \\ \mathbb{E}\left[ \frac{\partial}{\partial \tau^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] \right] &amp;= 0 \end{aligned}\] <p>And finally the $\alpha_j$ ones:</p> <ul id="expectation-alpha-1" class="tab" data-tab="7bd95b58-00b6-48c0-b542-7a773c3ec7dc" data-name="expectation-alpha-1"> <li class="active" id="expectation-alpha-1-equation"> <a href="#">equation </a> </li> <li id="expectation-alpha-1-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="7bd95b58-00b6-48c0-b542-7a773c3ec7dc" data-name="expectation-alpha-1"> <li class="active"> \[\begin{aligned} \mathbb{E}\left[ \frac{\partial}{\partial \alpha_j} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} \right] \right] &amp;= 0 \\ \mathbb{E}\left[ \frac{\partial}{\partial \alpha} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] \right] &amp;= 0 \\ \mathbb{E}\left[ \frac{\partial}{\partial \alpha_j} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] \right] &amp;= - \mathbf{1}_n^\top \left[ \frac{1}{\sigma^2} \mathbb{I}_{n \times n} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \right] \mathbf{1}_n \\ \mathbb{E}\left[\frac{\partial}{\partial \alpha_{j'}} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] \right] &amp;= 0 \end{aligned}\] </li> <li> \[\begin{aligned} \mathbb{E}\left[ \frac{\partial}{\partial \alpha_j} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} \right] \right] &amp;= \mathbb{E}\left[(\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2)^2} \mathbb{I}_{n \times n} + \frac{2\tau^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_j^\top \mathbf{z}_j)} \mathbf{z}_j \mathbf{z}_j^\top - \frac{(\tau^2)^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_j^\top \mathbf{z}_j)^2} \mathbf{z}_j \mathbf{z}_j^\top \mathbf{z}_j \mathbf{z}_j^\top \right]\mathbf{1}_n \right] \\ &amp;= 0 \\ \mathbb{E}\left[ \frac{\partial}{\partial \alpha} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] \right] &amp;= \mathbb{E}\left[(\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2)^2} \mathbf{z}_i \mathbf{z}_i^\top + \frac{2\tau^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{(\tau^2)^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top\mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \mathbf{1}_n \right] \\ &amp;= 0 \\ \mathbb{E}\left[ \frac{\partial}{\partial \alpha_j} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] \right] &amp;= - \mathbf{1}_n^\top \left[ \frac{1}{\sigma^2} \mathbb{I}_{n \times n} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \right] \mathbf{1}_n \\ \mathbb{E}\left[\frac{\partial}{\partial \alpha_{j'}} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] \right] &amp;= 0 \end{aligned}\] </li> </ul> <p>We then evaluate the Fisher information at the MLEs:</p> <ul id="fisher-1" class="tab" data-tab="64172551-2523-44a8-9a11-2774261adb3e" data-name="fisher-1"> <li class="active" id="fisher-1-equation"> <a href="#">equation </a> </li> <li id="fisher-1-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="64172551-2523-44a8-9a11-2774261adb3e" data-name="fisher-1"> <li class="active"> \[\begin{aligned} \mathcal{I}_{\theta, \theta} (\hat{\theta}) &amp;= -\mathbb{E}\left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \theta \partial \theta^\top}\right]\bigg\rvert_{\theta = \hat{\theta}} \\ &amp;= \begin{bmatrix} \frac{n}{\hat{\sigma}^2} &amp; \dots &amp; 0 &amp; 0 &amp; 0\\ \vdots &amp; \ddots &amp; \vdots &amp; \vdots &amp; \vdots \\ 0 &amp; \dots &amp; \frac{n}{\hat{\sigma}^2} &amp; 0 &amp; 0 \\ 0 &amp; \dots &amp; 0 &amp; \frac{nk}{2\hat{\sigma}^2} &amp; \frac{1}{2(\hat{\sigma}^2)^2} \sum_{i = 1}^k \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right]\\ 0 &amp; \dots &amp; 0 &amp; \frac{1}{2(\hat{\sigma}^2)^2} \sum_{i = 1}^k \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] &amp; \frac{1}{2(\hat{\sigma}^2)^2} \sum_{i = 1}^k \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \end{bmatrix} \end{aligned}\] </li> <li> <p>Note that:</p> \[\begin{aligned} \Sigma_{y_i} \bigg\rvert_{\theta = \hat{\theta}} &amp;= \hat{\sigma}^2 \mathbb{I}_{n \times n} \end{aligned}\] <p>Thus:</p> \[\begin{aligned} -\mathbb{E}\left[ \frac{\partial}{\partial \sigma^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} \right] \right] \bigg\rvert_{\theta = \hat{\theta}} &amp;= \frac{1}{2}\sum_{i = 1}^k -\frac{n}{(\hat{\sigma}^2)^2} + \frac{0 \cdot (2\hat{\sigma}^2 + 0)}{(\hat{\sigma}^2)^2(\hat{\sigma}^2 + 0)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top\right] + \text{tr}\left[ \left[ \frac{2}{(\hat{\sigma}^2)^3} \mathbb{I}_{n \times n} - 0 \cdot \mathbf{z}_i \mathbf{z}_i^\top + 0 \cdot \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \Sigma_{y_i}\rvert_{\theta = \hat{\theta}} \right] \\ &amp;= \frac{1}{2}\sum_{i = 1}^k - \frac{n}{(\hat{\sigma}^2)^2} + \text{tr}\left[ \left[ \frac{2}{(\hat{\sigma}^2)^3} \mathbb{I}_{n \times n} \right] \left[ \hat{\sigma}^2 \mathbb{I}_{n \times n} \right] \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[ -\frac{n}{(\hat{\sigma}^2)^2} + \frac{2}{\hat{\sigma}^2} \text{tr}\left[ \mathbb{I}_{n \times n} \right]\right] \\ &amp;= \frac{1}{2}\sum_{i = 1}^k \left[ -\frac{n}{(\hat{\sigma}^2)^2} + \frac{2n}{\hat{\sigma}^2} \right] \\ &amp;= \sum_{i = 1}^k \frac{n}{2 \hat{\sigma}^2} \\ &amp;= \frac{nk}{2\hat{\sigma}^2} \\ -\mathbb{E}\left[ \frac{\partial}{\partial \sigma^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] \right] \bigg\rvert_{\theta = \hat{\theta}} &amp;=\frac{1}{2}\sum_{i = 1}^k -\frac{1}{(\hat{\sigma}^2)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] - \frac{-0 \cdot (2\hat{\sigma}^2 + 0 \cdot \mathbf{z}_i^\top \mathbf{z}_i)}{(\hat{\sigma}^2)^2(\hat{\sigma}^2 + 0 \cdot \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] + \text{tr}\left[ \left[ \frac{2}{(\hat{\sigma}^2)^3} \mathbf{z}_i \mathbf{z}_i^\top + \frac{-0 \cdot (2\hat{\sigma}^2 + 0 \cdot \mathbf{z}_i^\top \mathbf{z}_i)}{(\hat{\sigma}^2)^3(\hat{\sigma}^2 + 0 \cdot \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{-2(0)^2(2\hat{\sigma}^2 + 0 \cdot \mathbf{z}_i^\top \mathbf{z}_i)}{(\hat{\sigma}^2)^3 (\hat{\sigma}^2 + 0 \cdot \mathbf{z}_i^\top \mathbf{z}_i)^3} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \Sigma_{y_i} \rvert_{\theta = \hat{\theta}} \right] \\ &amp;= \frac{1}{2}\sum_{i = 1}^k -\frac{1}{(\hat{\sigma}^2)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] + \text{tr}\left[ \left[ \frac{2}{(\hat{\sigma}^2)^3} \mathbf{z}_i \mathbf{z}_i^\top \right] \left[ \hat{\sigma}^2 \mathbb{I}_{n \times n} \right] \right] \\ &amp;= \frac{1}{2}\sum_{i = 1}^k \left( - \frac{1}{(\hat{\sigma}^2)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top\right] + \frac{2}{(\hat{\sigma}^2)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] \right) \\ &amp;= \sum_{i = 1}^k \frac{1}{2(\hat{\sigma}^2)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] \\ &amp;= \frac{1}{2(\hat{\sigma}^2)^2} \sum_{i = 1}^k \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] \\ -\mathbb{E}\left[ \frac{\partial}{\partial \sigma^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha} \right] \right] \bigg\rvert_{\theta = \hat{\theta}} &amp;= 0 \\ - \mathbb{E}\left[ \frac{\partial}{\partial \tau^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} \right] \right]\bigg\rvert_{\theta = \hat{\theta}} &amp;= \frac{1}{2}\sum_{i = 1}^k - \frac{1}{(\hat{\sigma}^2 + 0 \cdot \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] + \text{tr} \left[ \left[\frac{2}{\hat{\sigma}^2(\hat{\sigma}^2 + 0 \cdot \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top - \frac{2 \cdot 0 }{\hat{\sigma}^2(\hat{\sigma}^2 + 0 \cdot \mathbf{z}_i^\top \mathbf{z}_i)^3} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \Sigma_{y_i}\rvert_{\theta = \hat{\theta}} \right] \\ &amp;= \frac{1}{2}\sum_{i = 1}^k -\frac{1}{(\hat{\sigma}^2)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top\right] + \text{tr}\left[ \left[\frac{2}{(\hat{\sigma}^2)^3} \mathbf{z}_i \mathbf{z}_i^\top \right] \left[ \hat{\sigma}^2 \mathbb{I}_{n \times n} \right]\right] \\ &amp;= \frac{1}{2}\sum_{i =1 }^k \left( -\frac{1}{(\hat{\sigma}^2)^2} + \frac{2}{(\hat{\sigma}^2)^2} \right) \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] \\ &amp;= \frac{1}{2(\hat{\sigma}^2)^2} \sum_{i = 1}^k \text{tr} \left[ \mathbf{z}_i \mathbf{z}_i^\top \right] \\ -\mathbb{E}\left[ \frac{\partial}{\partial \tau^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] \right] &amp;= \frac{1}{2}\sum_{i = 1}^k - \frac{1}{(\hat{\sigma}^2 + 0 \cdot\mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] +\text{tr} \left[\left[ \frac{2}{\hat{\sigma}^2(\hat{\sigma}^2 + 0 \cdot \mathbf{z}_i^\top \mathbf{z}_i)^2}\mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{2\cdot 0}{\hat{\sigma}^2(\hat{\sigma}^2 + 0 \cdot \mathbf{z}_i^\top \mathbf{z}_i)^3} \mathbf{z}_i \mathbf{z}_i^\top\mathbf{z}_i \mathbf{z}_i^\top\mathbf{z}_i \mathbf{z}_i^\top\right] \Sigma_{y_i} \rvert_{\theta = \hat{\theta}} \right] \\ &amp;= \frac{1}{2}\sum_{i = 1}^k - \frac{1}{(\hat{\sigma}^2)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] +\text{tr} \left[\left[ \frac{2}{(\hat{\sigma}^2)^3}\mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \left[\hat{\sigma}^2 \mathbb{I}_{n \times n} \right] \right] \\ &amp;= \frac{1}{2}\sum_{i = 1}^k \left( -\frac{1}{(\hat{\sigma}^2)^2} + \frac{2}{(\hat{\sigma}^2)^2} \right)\text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \\ &amp;= \frac{1}{2(\hat{\sigma}^2)^2} \sum_{i = 1}^k \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \\ -\mathbb{E}\left[ \frac{\partial}{\partial \tau^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha} \right] \right] \bigg\rvert_{\theta = \hat{\theta}} &amp;= 0 \\ -\mathbb{E}\left[ \frac{\partial}{\partial \alpha_j} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} \right] \right] \bigg\rvert_{\theta = \hat{\theta}} &amp;= 0 \\ -\mathbb{E}\left[ \frac{\partial}{\partial \alpha} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] \right] \bigg\rvert_{\theta = \hat{\theta}} &amp;= 0 \\ - \mathbb{E}\left[ \frac{\partial}{\partial \alpha_j} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] \right] &amp;= \mathbf{1}_n^\top \left[ \frac{1}{\hat{\sigma}^2} \mathbb{I}_{n \times n} - \frac{0}{\hat{\sigma}^2(\hat{\sigma}^2 + 0 \cdot \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \right] \mathbf{1}_n \\ &amp;= \frac{n}{\hat{\sigma}^2} \\ - \mathbb{E}\left[\frac{\partial}{\partial \alpha_{j'}} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] \right] \bigg\rvert_{\theta = \hat{\theta}} &amp;= 0 \end{aligned}\] </li> </ul> <hr/> <h2 id="negative-binomial-case">Negative Binomial Case</h2> <p>In this example, we’ll let the responses be negative binomial. To keep things simple, we’ll say we only have a single fixed intercept and a single random effect. We let $\phi &gt; 0$, denote the <i>known</i> dispersion parameter and assume the conditional mean to be given by:</p> \[\mu_{i,j} = \exp\left( \alpha_i + \beta_i \mathbf{z}_{i,j} \right) \label{eq:neg-bin-mean}\] <p>The likelihood based on a single observation, $\mathbf{y}_{i,j}$, is given by:</p> \[\mathcal{L}(\mathbf{y}_{i,j}; \alpha_i, \tau^2 \rvert \beta_i) = \frac{\Gamma\left(\mathbf{y}_{i,j} + \frac{1}{\phi}\right)}{\Gamma(\mathbf{y}_{i,j} + 1) \Gamma\left(\frac{1}{\phi} \right)}\left(\frac{1}{1 + \phi \mathbf{y}_{i,j}}\right)^{\frac{1}{\phi}} \left( \frac{\phi \mu_{i,j}}{1 + \phi \mu_{i,j}} \right)^{\mathbf{y}_{i,j}} \label{eq:neg-bin-single-lik}\] <p>where $\Gamma(\cdot)$ is the gamma function:</p> \[\Gamma(x) = \int_0^\infty t^{x - 1} \exp(-t) dt\] <p>The above parametrization of the likelihood implies that the conditional variance of the responses is given by:</p> \[V(\mu_{i,j}) = \mu_{i,j} + \frac{1}{\phi} \mu_{i,j}^2\] <p>The conditional log-likelihood based on cluster $i$ is:</p> \[\ell(\mathbf{y}_i; \alpha_i, \tau^2 \rvert \beta_i) = \sum_{j = 1}^{n_i} \left[ \log \Gamma \left( \mathbf{y}_{i,j} + \frac{1}{\phi} \right) - \log \Gamma\left(\mathbf{y}_{i,j} + 1\right) - \log\Gamma\left(\frac{1}{\phi} \right) - \frac{1}{\phi} \log\left(1 + \phi \mathbf{y}_{i,j} \right) + \mathbf{y}_{i,j} \left( \log(\phi \mu_{i,j}) - \log(1 + \phi \mu_{i,j}) \right) \right] \label{eq:neg-bin-full-cond-ll}\] <p>We follow a pseudo-likelihood approach (see <a href="/posts/2025/06/04/glmm.html">here</a>). We assume to have the following generalized linear mixed model:</p> \[\mathbf{y}_{i,j} \rvert \beta_i \sim \text{NegBin}(\mu_{i,j}, \phi); \hspace{10mm} \mu_{i,j} = \exp\left(\eta_{i,j}\right) = \exp\left(\alpha_i + \beta_i \mathbf{z}_{i,j}\right) \label{eq:glmm-y}\] <p>We’ll use a superscript $\star$ to denote a quantity evaluated at the parameter estimates made under $H_0$ (i.e. $\tau^2 = \mathbf{0}$). Our <i>working</i> responses and errors are:</p> \[\mathbf{y}^\star_{i,j} = \alpha_i + \beta_i \mathbf{z}_{i,j} + \epsilon^\star_{i,j}; \hspace{10mm} \epsilon^\star_{i,j} \sim \mathcal{N}\left(0, \frac{V(\hat{\mu}_{i,j})}{\delta^2(\hat{\eta}_{i,j})}\right)\] <p>where \(\delta(\hat{\eta}_{i,j}) = \frac{\partial g^{-1}(\eta_{i,j})}{\partial \eta_{i,j}}\bigg\rvert_{\eta_{i,j} = \hat{\eta}_{i,j}}\). We can then just apply all of the results we found in the previous section to this case but make \(\hat{\sigma}^2\) different for each observation, where \(\hat{\sigma}^2_{i,j} = \text{Var}(\epsilon_{i,j})\).</p> <p>To do so, we need an estimate of $\alpha_i$ under $H_0$, which we can do with <a href="/posts/2025/06/03/glm.html#weighted-least-squares">iteratively reweighted least squares</a> or some comparable algorithm. With these estimates, we can compute \(\hat{\sigma}_{i,j}^2\) and \(\mathbf{y}^\star_{i,j}\) and proceed as in the Gaussian case but without $\sigma^2$ in the parameter vector since we assume them to be fixed at \(\hat{\sigma}^2_{i,j}\).</p> <p>Since $g(\cdot) = \log(\cdot)$, we have that \(\delta(\hat{\eta}_{i,j}) = \exp(\hat{\eta}_{i,j})\), implying that the working error variances are:</p> \[\frac{V(\hat{\mu}_{i,j})}{\delta^2(\hat{\eta}_{i,j})} = \frac{\exp(\hat{\eta}_{i,j}) + \frac{1}{\phi}\exp(\hat{\eta}_{i,j})}{\exp^2(\hat{\eta}_{i,j})} = \frac{1}{\exp(\hat{\eta}_{i,j})}\left(1 + \frac{1}{\phi}\right)\] <p>where we recall that we assume $\phi$ is know and $\hat{\eta}_{i,j} = \hat{\alpha}_i$ under $H_0$. Dropping the star superscript, the likelihood and log-likelihood functions we will work with are giving by:</p> \[\begin{aligned} \mathcal{L}(\theta; \mathbf{y}) &amp;= \prod_{i = 1}^k (2 \pi)^{-\frac{n}{2}} \rvert \Sigma_{y_i} \rvert^{-\frac{1}{2}} \exp\left(- \frac{1}{2} (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \Sigma_{y_i}^{-1} (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right) \\ \ell(\theta; \mathbf{y}) &amp;= \sum_{i = 1}^k \left[ -\frac{n}{2} \log(2 \pi) - \frac{1}{2}\log(\rvert \Sigma_{y_i} \rvert) - \frac{1}{2} (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \Sigma_{y_i}^{-1} (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \end{aligned}\] <p>but using the working responses and their respective covariances and whatnot.</p> <h3 id="score-1">Score</h3> <p>The marginal covariance matrix is very similar to the Gaussian outcome model above. The only thing that has changed is that each error has its own variance:</p> \[\Sigma_{y_i} = \text{diag}([\hat{\sigma}^2_{i,1}, \dots, \hat{\sigma}^2_{i, n}]) + \tau^2 \mathbf{z}_i \mathbf{z}_i^\top\] <p>Its inverse, $\Sigma^{-1}_{y_i}$, is:</p> <ul id="sigma-inv-2" class="tab" data-tab="fd183ab4-8cd8-48e8-89f4-a4ff432b99e8" data-name="sigma-inv-2"> <li class="active" id="sigma-inv-2-equation"> <a href="#">equation </a> </li> <li id="sigma-inv-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="fd183ab4-8cd8-48e8-89f4-a4ff432b99e8" data-name="sigma-inv-2"> <li class="active"> \[\Sigma^{-1}_{y_i} = \text{diag}\left(\left[ \frac{1}{\hat{\sigma}^2_{i, 1}}, \dots, \frac{1}{\hat{\sigma}^2_{i,n}} \right]\right) - \frac{\text{diag}\left( \left[ \frac{\tau^2}{(\hat{\sigma}_{i,1}^2)^2}, \dots, \frac{\tau^2}{(\hat{\sigma}_{i,n}^2)^2} \right] \right) \mathbf{z}_i \mathbf{z}_i^\top}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}}\] </li> <li> <p>First, let’s let \(\mathbf{w}_i = \tau^2 \mathbf{z}_i\), which is the $n$-vector $\mathbf{z}_i$ where each coordinate has been multiplied by $\tau^2$. We’ll also let \(\hat{\sigma}^2 = (\hat{\sigma}^2_{i,1}, \dots, \hat{\sigma}^2_{i, n})^\top\), the vector of the error variances for cluster $i$, and $\frac{1}{\hat{\sigma}^2}$ will be the vector of the reciprocals of the coordinates of $\hat{\sigma}^2$. Using the <a href="https://en.wikipedia.org/wiki/Sherman–Morrison_formula">Sherman-Morrison formula</a>, we have:</p> \[\begin{aligned} \Sigma^{-1}_{y_i} &amp;= \left(\text{diag}(\hat{\sigma}^2) - \mathbf{w}_i \mathbf{v}_i^\top\right)^{-1} \\ &amp;= \text{diag}^{-1}(\hat{\sigma}^2) - \frac{\text{diag}^{-1}(\hat{\sigma}^2) \mathbf{w}_i \mathbf{z}_i^\top \text{diag}^{-1}(\hat{\sigma}^2)}{1 + \mathbf{z}_i^\top \text{diag}^{-1}(\hat{\sigma}^2) \mathbf{w}_i} \\ &amp;= \text{diag}\left(\frac{1}{\hat{\sigma}^2}\right) - \frac{\text{diag}\left(\frac{\tau^2}{(\hat{\sigma}^2)^2}\right) \mathbf{z}_i \mathbf{z}_i^\top}{1 + \mathbf{z}_i^\top \text{diag}\left(\frac{\tau^2}{\hat{\sigma}^2}\right) \mathbf{z}_i} \\ &amp;= \text{diag}\left(\left[ \frac{1}{\hat{\sigma}^2_{i, 1}}, \dots, \frac{1}{\hat{\sigma}^2_{i,n}} \right]\right) - \frac{\text{diag}\left( \left[ \frac{\tau^2}{(\sigma_{i,1}^2)^2}, \dots, \frac{\tau^2}{(\sigma_{i,n}^2)^2} \right] \right) \mathbf{z}_i \mathbf{z}_i^\top}{1 + \mathbf{z}_i^\top \text{diag}\left( \left[ \frac{\tau^2}{\sigma_{i,1}^2}, \dots, \frac{\tau^2}{\sigma_{i,n}^2} \right] \right) \mathbf{z}_i} \\ &amp;= \text{diag}\left(\left[ \frac{1}{\hat{\sigma}^2_{i, 1}}, \dots, \frac{1}{\hat{\sigma}^2_{i,n}} \right]\right) - \frac{\text{diag}\left( \left[ \frac{\tau^2}{(\sigma_{i,1}^2)^2}, \dots, \frac{\tau^2}{(\sigma_{i,n}^2)^2} \right] \right) \mathbf{z}_i \mathbf{z}_i^\top}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}} \end{aligned}\] <p>We also have:</p> \[[\Sigma^{-1}_{y_i}]_{j,j'} = - \frac{\tau^2 \mathbf{z}_{i,j} \mathbf{z}_{i,j'}}{(\hat{\sigma}_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)} \hspace{10mm} \text{and} \hspace{10mm} [\Sigma^{-1}_{y_i}]_{j,j} = \frac{1}{\hat{\sigma}^2_{i,j}} - \frac{\tau^2 \mathbf{z}_{i,j}^2}{(\hat{\sigma}_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)}\] </li> </ul> <h4 id="derivatives-2">Derivatives</h4> <p>We first find the derivative with respect to $\tau^2$:</p> <ul id="deriv-tau-2" class="tab" data-tab="ba5e4bfb-02ec-4687-a7f9-ec1be60ad090" data-name="deriv-tau-2"> <li class="active" id="deriv-tau-2-equation"> <a href="#">equation </a> </li> <li id="deriv-tau-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="ba5e4bfb-02ec-4687-a7f9-ec1be60ad090" data-name="deriv-tau-2"> <li class="active"> \[\frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} = - \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} - \sum_{l = 1}^n \sum_{j = 1}^n \frac{\tau^2 \mathbf{z}_{i,l}^2 \mathbf{z}_{i,j}^2}{(\hat{\sigma}_{i,l}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}\right)} - \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \frac{\mathbf{z}_{i,a} \mathbf{z}_{i,b}}{(\hat{\sigma}^2_{i,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \right]\] </li> <li> <p>First, the log determinant term:</p> \[\begin{aligned} \frac{\partial}{\partial \tau^2} \left[ \log (\rvert \Sigma_{y_i} \rvert) \right] &amp;= \text{tr}\left[ \Sigma^{-1}_{y_i} \frac{\partial}{\partial \tau^2} \left[ \Sigma_{y_i} \right] \right] \\ &amp;= \text{tr} \left[ \left( \text{diag}\left(\left[ \frac{1}{\hat{\sigma}^2_{i, 1}}, \dots, \frac{1}{\hat{\sigma}^2_{i,n}} \right]\right) - \frac{\text{diag}\left( \left[ \frac{\tau^2}{(\hat{\sigma}_{i,1}^2)^2}, \dots, \frac{\tau^2}{(\hat{\sigma}_{i,n}^2)^2} \right] \right) \mathbf{z}_i \mathbf{z}_i^\top}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}} \right) \mathbf{z}_i \mathbf{z}_i^\top \right] \\ &amp;= \sum_{l = 1}^n \sum_{j = 1}^n \left( \text{diag}\left(\left[ \frac{1}{\hat{\sigma}^2_{i, 1}}, \dots, \frac{1}{\hat{\sigma}^2_{i,n}} \right]\right) - \frac{\text{diag}\left( \left[ \frac{\tau^2}{(\hat{\sigma}_{i,1}^2)^2}, \dots, \frac{\tau^2}{(\hat{\sigma}_{i,n}^2)^2} \right] \right) \mathbf{z}_i \mathbf{z}_i^\top}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}} \right)_{l,j} \left(\mathbf{z}_i \mathbf{z}_i^\top \right)_{l,j} \\ &amp;= \sum_{l = 1}^n \left( \text{diag}\left(\left[ \frac{1}{\hat{\sigma}^2_{i, 1}}, \dots, \frac{1}{\hat{\sigma}^2_{i,n}} \right]\right) - \frac{\text{diag}\left( \left[ \frac{\tau^2}{(\hat{\sigma}_{i,1}^2)^2}, \dots, \frac{\tau^2}{(\hat{\sigma}_{i,n}^2)^2} \right] \right) \mathbf{z}_i \mathbf{z}_i^\top}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}} \right)_{l,l} \left(\mathbf{z}_i \mathbf{z}_i^\top \right)_{l,l} + \sum_{l = 1}^n \sum_{j \neq l} \left( \text{diag}\left(\left[ \frac{1}{\hat{\sigma}^2_{i, 1}}, \dots, \frac{1}{\hat{\sigma}^2_{i,n}} \right]\right) - \frac{\text{diag}\left( \left[ \frac{\tau^2}{(\hat{\sigma}_{i,1}^2)^2}, \dots, \frac{\tau^2}{(\hat{\sigma}_{i,n}^2)^2} \right] \right) \mathbf{z}_i \mathbf{z}_i^\top}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}} \right)_{l,j} \left(\mathbf{z}_i \mathbf{z}_i^\top \right)_{l,j} \\ &amp;= \sum_{l = 1}^n \left( \frac{1}{\hat{\sigma}^2_{i,l}} - \frac{\frac{\tau^2}{(\hat{\sigma}_{i,l}^2)^2}\mathbf{z}_{i,l}^2}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}} \right) \mathbf{z}_{i,l}^2 - \sum_{l = 1}^n \sum_{j \neq l} \left( \frac{\frac{\tau^2}{(\hat{\sigma}^2_{i,l})^2} \mathbf{z}_{i,l} \mathbf{z}_{i,j}}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}} \right) \mathbf{z}_{i,l} \mathbf{z}_{i,j} \\ &amp;= \sum_{l = 1}^n \left( \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} - \frac{\frac{\tau^2}{(\hat{\sigma}_{i,l}^2)^2}\mathbf{z}_{i,l}^4}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}} \right) - \sum_{l = 1}^n \sum_{j \neq l} \left( \frac{\frac{\tau^2}{(\hat{\sigma}^2_{i,l})^2} \mathbf{z}_{i,l}^2 \mathbf{z}_{i,j}^2}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}} \right) \\ &amp;= \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} - \sum_{l = 1}^n \sum_{j = 1}^n \frac{\frac{\tau^2}{(\hat{\sigma}_{i,l}^2)^2}\mathbf{z}_{i,l}^2 \mathbf{z}_{i,j}^2}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}} \end{aligned}\] <p>Next, the quadratic term. We will compute the derivative of $\Sigma^{-1}_{y_i}$ with respect to $\tau^2$ element-wise:</p> \[\begin{aligned} \frac{\partial}{\partial \tau^2}\left[ [\Sigma^{-1}_{y_i}]_{j,j'}\right] &amp;= \frac{\partial}{\partial \tau^2} \left[ - \frac{\tau^2 \mathbf{z}_{i,j} \mathbf{z}_{i,j'}}{(\hat{\sigma}_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)} \right] \\ &amp;= -\frac{(\hat{\sigma}_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)(\mathbf{z}_{i,j}\mathbf{z}_{i,j'}) - (\tau^2 \mathbf{z}_{i,j}\mathbf{z}_{i,j'})\left( (\hat{\sigma}_{i,j}^2)^2\sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)}{(\hat{\sigma}_{i,j}^2)^4 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \\ &amp;= -\frac{\mathbf{z}_{i,j}\mathbf{z}_{i,j'} \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}- \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)}{(\hat{\sigma}_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \\ &amp;= -\frac{\mathbf{z}_{i,j}\mathbf{z}_{i,j'}}{(\hat{\sigma}_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \\ \frac{\partial}{\partial \tau^2}\left[ [\Sigma^{-1}_{y_i}]_{j,j}\right] &amp;= \frac{\partial}{\partial \tau^2} \left[ \frac{1}{\hat{\sigma}^2_{i,j}} - \frac{\tau^2 \mathbf{z}_{i,j}^2}{(\hat{\sigma}_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)} \right] \\ &amp;= - \frac{(\hat{\sigma}_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)\mathbf{z}_{i,j}^2 - \tau^2 \mathbf{z}_{i,j}^2 (\hat{\sigma}_{i,j}^2)^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}}{(\hat{\sigma}_{i,j}^2)^4 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \\ &amp;= - \frac{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)\mathbf{z}_{i,j}^2 - \tau^2 \mathbf{z}_{i,j}^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}}{(\hat{\sigma}_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \\ &amp;= - \frac{\mathbf{z}_{i,j}^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}- \tau^2\sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)}{(\hat{\sigma}_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \\ &amp;= - \frac{\mathbf{z}_{i,j}^2}{(\hat{\sigma}_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \\ \end{aligned}\] <p>In matrix notation, we have:</p> \[\frac{\partial}{\partial \tau^2}\left[ \Sigma^{-1}_{y_i} \right] = \text{diag}\left(\left[ -\frac{1}{(\hat{\sigma}^2_{i,1})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2}, \dots, -\frac{1}{(\hat{\sigma}^2_{i,n})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \right] \right) \mathbf{z}_i \mathbf{z}_i^\top\] <p>Then:</p> \[\begin{aligned} (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \frac{\partial}{\partial \tau^2} \left[ \Sigma^{-1}_{y_i} \right](\mathbf{y}_i - \alpha_i \mathbf{1}_n) &amp;= (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ \text{diag}\left(\left[ -\frac{1}{(\hat{\sigma}^2_{i,1})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2}, \dots, -\frac{1}{(\hat{\sigma}^2_{i,n})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \right] \right) \mathbf{z}_i \mathbf{z}_i^\top \right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ &amp;= \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \left[ \text{diag}\left(\left[ -\frac{1}{(\hat{\sigma}^2_{i,1})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2}, \dots, -\frac{1}{(\hat{\sigma}^2_{i,n})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \right] \right) \mathbf{z}_i \mathbf{z}_i^\top \right]_{a,b} \\ &amp;= - \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \frac{\mathbf{z}_{i,a} \mathbf{z}_{i,b}}{(\hat{\sigma}^2_{i,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \end{aligned}\] <p>And thus:</p> \[\begin{aligned} \frac{\partial}{\partial \tau^2} \left[ \ell(\theta; \mathbf{y}) \right] &amp;= \frac{\partial}{\partial \tau^2} \left[ \sum_{i = 1}^k \left[ -\frac{n}{2} \log(2 \pi) - \frac{1}{2}\log(\rvert \Sigma_{y_i} \rvert) - \frac{1}{2} (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \Sigma_{y_i}^{-1} (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \right] \\ &amp;= - \frac{1}{2} \sum_{i = 1}^k \left[ \frac{\partial}{\partial \tau^2} \left[ \log(\rvert \Sigma_{y_i} \rvert) \right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \frac{\partial}{\partial \tau^2} \left[ \Sigma_{y_i}^{-1} \right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right]\\ &amp;= - \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} - \sum_{l = 1}^n \sum_{j = 1}^n \frac{\frac{\tau^2}{(\hat{\sigma}_{i,l}^2)^2}\mathbf{z}_{i,l}^2 \mathbf{z}_{i,j}^2}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}} - \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \frac{\mathbf{z}_{i,a} \mathbf{z}_{i,b}}{(\hat{\sigma}^2_{i,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \right] \end{aligned}\] </li> </ul> <p>And now we find the gradient with respect to $\alpha$:</p> <ul id="deriv-alpha-2" class="tab" data-tab="92c9b633-4286-4488-a6e1-e7301776c345" data-name="deriv-alpha-2"> <li class="active" id="deriv-alpha-2-equation"> <a href="#">equation </a> </li> <li id="deriv-alpha-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="92c9b633-4286-4488-a6e1-e7301776c345" data-name="deriv-alpha-2"> <li class="active"> \[\frac{\partial}{\partial \alpha}[ \ell(\theta; \mathbf{y})] = \begin{bmatrix} (\mathbf{y}_1 - \alpha_1 \mathbf{1}_n)^\top \Sigma_{y_1}^{-1} \mathbf{1}_n \\ \vdots \\ (\mathbf{y}_k - \alpha_k \mathbf{1}_n)^\top \Sigma_{y_k}^{-1} \mathbf{1}_n \end{bmatrix}\] </li> <li> <p>We do this component-wise:</p> \[\begin{aligned} \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} &amp;= \frac{\partial}{\partial \alpha_j} \left[ \sum_{i = 1}^k \left[ -\frac{n}{2} \log(2 \pi) - \frac{1}{2}\log(\rvert \Sigma_{y_i} \rvert) - \frac{1}{2} (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \Sigma_{y_i}^{-1} (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \right] \\ &amp;= - \frac{1}{2} \frac{\partial}{\partial \alpha_j} \left[(\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \Sigma_{y_j}^{-1} (\mathbf{y}_j - \alpha_j \mathbf{1}_n) \right] \\ &amp;= - \frac{1}{2}\left(2 (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \Sigma_{y_j}^{-1} (-\mathbf{1}_n) \right) \\ &amp;= (\mathbf{y}_i - \alpha_j \mathbf{1}_n)^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ \implies \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha} &amp;= \begin{bmatrix} (\mathbf{y}_1 - \alpha_1 \mathbf{1}_n)^\top \Sigma_{y_1}^{-1} \mathbf{1}_n \\ \vdots \\ (\mathbf{y}_k - \alpha_k \mathbf{1}_n)^\top \Sigma_{y_k}^{-1} \mathbf{1}_n \end{bmatrix} \end{aligned}\] </li> </ul> <h4 id="mles-1">MLEs</h4> <p>We can find $\hat{\theta}$ by setting the above equal to zero and substitute $\tau^2 = 0$. We get:</p> <ul id="mle-2" class="tab" data-tab="4cb88224-12ff-4b26-85a6-01d5b8d863d0" data-name="mle-2"> <li class="active" id="mle-2-equation"> <a href="#">equation </a> </li> <li id="mle-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="4cb88224-12ff-4b26-85a6-01d5b8d863d0" data-name="mle-2"> <li class="active"> \[\hat{\theta} = \begin{bmatrix} \left(\sum_{l' = 1}^n \frac{1}{\hat{\sigma}^2_{1,l'}} \right)^{-1} \sum_{l' = 1}^n \frac{\mathbf{y}_{1,l'}}{\hat{\sigma}^2_{1,l'}} \\ \vdots \\ \left(\sum_{l' = 1}^n \frac{1}{\hat{\sigma}^2_{k,l'}} \right)^{-1} \sum_{l' = 1}^n \frac{\mathbf{y}_{k,l'}}{\hat{\sigma}^2_{k,l'}} \\ 0 \end{bmatrix}\] </li> <li> <p>We only need to deal with $\alpha$, which we can again do component-wise. First notice that:</p> \[\begin{aligned} \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} &amp;= (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ &amp;= (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left( \text{diag}\left(\left[ \frac{1}{\hat{\sigma}^2_{j, 1}}, \dots, \frac{1}{\hat{\sigma}^2_{j,n}} \right]\right) - \frac{\text{diag}\left( \left[ \frac{\tau^2}{(\hat{\sigma}_{j,1}^2)^2}, \dots, \frac{\tau^2}{(\hat{\sigma}_{j,n}^2)^2} \right] \right) \mathbf{z}_j \mathbf{z}_j^\top}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\hat{\sigma}^2_{j,l}}} \right) \mathbf{1}_n \\ &amp;= (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left(\begin{bmatrix} \frac{1}{\hat{\sigma}^2_{j, 1}} \\ \vdots \\ \frac{1}{\hat{\sigma}^2_{j,n}} \end{bmatrix} - \left( \frac{1}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\hat{\sigma}_{j,l}^2}} \right)\text{diag}\left( \left[ \frac{\tau^2}{(\hat{\sigma}_{j,1}^2)^2}, \dots, \frac{\tau^2}{(\hat{\sigma}_{j,n}^2)^2} \right] \right) \begin{bmatrix} \sum_{l = 1}^n \mathbf{z}_{j,1} \mathbf{z}_{j,l} \\ \vdots \\ \sum_{l = 1}^n \mathbf{z}_{j,n} \mathbf{z}_{j,l} \end{bmatrix} \right) \\ &amp;= \left( \sum_{l = 1}^n \frac{\mathbf{y}_{j,l} - \alpha_j}{\hat{\sigma}^2_{j,l}} \right)- \left( \frac{1}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\hat{\sigma}_{j,l}^2}} \right) (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \begin{bmatrix} \frac{\tau^2}{(\hat{\sigma}_{j,1}^2)^2} \sum_{l = 1}^n \mathbf{z}_{j,1} \mathbf{z}_{j,l} \\ \vdots \\ \frac{\tau^2}{(\hat{\sigma}_{j,n}^2)^2} \sum_{l = 1}^n \mathbf{z}_{j,n} \mathbf{z}_{j,l} \end{bmatrix} \\ &amp;= \left( \sum_{l = 1}^n \frac{\mathbf{y}_{j,l} - \alpha_j}{\hat{\sigma}^2_{j,l}} \right)- \left( \frac{1}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\hat{\sigma}_{j,l}^2}} \right) \sum_{l' = 1}^n (\mathbf{y}_{j,l'} - \alpha_j) \frac{\tau^2}{(\hat{\sigma}_{j,l'}^2)^2} \sum_{l = 1}^n \mathbf{z}_{j,l'} \mathbf{z}_{j,l} \\ &amp;= \left( \sum_{l = 1}^n \frac{\mathbf{y}_{j,l} - \alpha_j}{\hat{\sigma}^2_{j,l}} \right)- \left( \frac{\sum_{l = 1}^n \mathbf{z}_{j,l} }{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\hat{\sigma}_{j,l}^2}} \right) \left( \sum_{l' = 1}^n \frac{\tau^2 (\mathbf{y}_{j,l'} - \alpha_j) \mathbf{z}_{j,l'} }{(\hat{\sigma}_{j,l'}^2)^2} \right) \end{aligned}\] <p>So then we solve for $\alpha_j$ in:</p> \[\begin{aligned} 0 &amp;= (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ \implies 0 &amp;= \left( \sum_{l = 1}^n \frac{\mathbf{y}_{j,l} - \alpha_j}{\hat{\sigma}^2_{j,l}} \right)- \left( \frac{\sum_{l = 1}^n \mathbf{z}_{j,l} }{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\hat{\sigma}_{j,l}^2}} \right) \left( \sum_{l' = 1}^n \frac{\tau^2 (\mathbf{y}_{j,l'} - \alpha_j) \mathbf{z}_{j,l'} }{(\hat{\sigma}_{j,l'}^2)^2} \right) \\ \implies 0 &amp;= \sum_{l = 1}^n \frac{\mathbf{y}_{j,l}}{\hat{\sigma}^2_{j,l}} - \alpha_j \sum_{l = 1}^n \frac{1}{\hat{\sigma}^2_{j,l}} - \left( \frac{\sum_{l = 1}^n \mathbf{z}_{j,l} }{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\hat{\sigma}_{j,l}^2}} \right) \left[ \sum_{l' = 1}^n \frac{\tau^2 \mathbf{y}_{j,l'} \mathbf{z}_{j,l'}}{(\hat{\sigma}^2_{j,l'})^2} - \alpha_j \sum_{l' =1}^n \frac{\tau^2 \mathbf{z}_{j,l'}}{(\hat{\sigma}^2_{j,l'})} \right] \\ \implies \alpha_j \left[ \sum_{l = 1}^n \frac{1}{\hat{\sigma}^2_{j,l}} - \sum_{l = 1}^n \frac{\tau^2 \mathbf{z}_{j,l'}}{(\hat{\sigma}^2_{j,l'})^2}\right] &amp;= \sum_{l = 1}^n \frac{\mathbf{y}_{j,l}}{\hat{\sigma}^2_{j,l}} - \left( \frac{\sum_{l = 1}^n \mathbf{z}_{j,l} }{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\hat{\sigma}_{j,l}^2}} \right) \sum_{l' = 1}^n\frac{\tau^2 \mathbf{y}_{j,l'} \mathbf{z}_{j,l'}}{(\hat{\sigma}^2_{j,l'})^2} \\ \implies \alpha_j \left(\sum_{l' = 1}^n \frac{1}{\hat{\sigma}^2_{j,l'}} \left[1 - \frac{\tau^2 \mathbf{z}_{j,l'}}{\hat{\sigma}^2_{j,l'}} \right]\right) &amp;= \sum_{l' = 1}^n \frac{\mathbf{y}_{j,l'}}{\hat{\sigma}^2_{j,l'}} \left[ 1 - \left( \frac{\sum_{l = 1}^n \mathbf{z}_{j,l} }{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\hat{\sigma}_{j,l}^2}} \right) \frac{\tau^2 \mathbf{z}_{j,l'}}{\hat{\sigma}^2_{j,l'}} \right] \\ \implies \alpha_j &amp;= \left(\sum_{l' = 1}^n \frac{1}{\hat{\sigma}^2_{j,l'}} \left[1 - \frac{\tau^2 \mathbf{z}_{j,l'}}{\hat{\sigma}^2_{j,l'}} \right]\right)^{-1} \sum_{l' = 1}^n \frac{\mathbf{y}_{j,l'}}{\hat{\sigma}^2_{j,l'}} \left[ 1 - \left( \frac{\sum_{l = 1}^n \mathbf{z}_{j,l} }{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\hat{\sigma}_{j,l}^2}} \right) \frac{\tau^2 \mathbf{z}_{j,l'}}{\hat{\sigma}^2_{j,l'}} \right] \end{aligned}\] <p>Under $H_0$, $\tau^2 = 0$, so we get:</p> \[\hat{\alpha}_j = \left(\sum_{l' = 1}^n \frac{1}{\hat{\sigma}^2_{j,l'}} \right)^{-1} \sum_{l' = 1}^n \frac{\mathbf{y}_{j,l'}}{\hat{\sigma}^2_{j,l'}}\] </li> </ul> <p>Thus, the score evaluated at $\theta = \hat{\theta}$ is then:</p> \[U_{\theta}(\hat{\theta}) = \begin{bmatrix} \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha} \bigg\rvert_{\theta = \hat{\theta}} \\ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \bigg\rvert_{\theta = \hat{\theta}} \end{bmatrix} = \begin{bmatrix} \left(\sum_{l' = 1}^n \frac{1}{\hat{\sigma}^2_{1,l'}} \right)^{-1} \sum_{l' = 1}^n \frac{\mathbf{y}_{1,l'}}{\hat{\sigma}^2_{1,l'}} \\ \vdots \\ \left(\sum_{l' = 1}^n \frac{1}{\hat{\sigma}^2_{k,l'}} \right)^{-1} \sum_{l' = 1}^n \frac{\mathbf{y}_{k,l'}}{\hat{\sigma}^2_{k,l'}} \\ - \frac{1}{2}\sum_{i = 1}^k \left[ \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}_{i,l}^2} + \sum_{l = 1}^n \sum_{l' = 1}^n \frac{\mathbf{z}_{i,l}\mathbf{z}_{i,l'}(\mathbf{y}_{i,l} - \hat{\alpha}_i)(\mathbf{y}_{i,l'} - \hat{\alpha}_i)}{\hat{\sigma}^2_{i,l} \hat{\sigma}^2_{i,l'}}\right] \end{bmatrix}\] <h3 id="information-1">Information</h3> <p>As before, to find the information, we need to compute the second-order derivatives of the log-likelihood, take the expectation under $H_0$ of minus those quantities, and evaluate them by plugging in $\hat{\theta}$.</p> <h4 id="derivatives-3">Derivatives</h4> <p>We’ll take all of the derivatives component-wise. We’ll start with those with respect to $\tau^2$.</p> <ul id="deriv-theta-tau-2" class="tab" data-tab="667e9f52-99c9-463a-b41c-949ee2c0f4a4" data-name="deriv-theta-tau-2"> <li class="active" id="deriv-theta-tau-2-equation"> <a href="#">equation </a> </li> <li id="deriv-theta-tau-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="667e9f52-99c9-463a-b41c-949ee2c0f4a4" data-name="deriv-theta-tau-2"> <li class="active"> \[\begin{aligned} \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \tau^2 \partial \theta} &amp;= \begin{bmatrix} - \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{1,l}}{\hat{\sigma}^2_{1,l}}\right)^2} \left(\sum_{l' = 1}^n \frac{\mathbf{z}_{1,l'}(\mathbf{y}_{1,l'} - \alpha_1)}{(\hat{\sigma}^2_{1,l'})^2}\right)\left(\sum_{l'' = 1}^n \mathbf{z}_{1,l''}\right) \\ \vdots \\ - \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{k,l}}{\hat{\sigma}^2_{k,l}}\right)^2} \left(\sum_{l' = 1}^n \frac{\mathbf{z}_{k,l'}(\mathbf{y}_{k,l'} - \alpha_k)}{(\hat{\sigma}^2_{k,l'})^2}\right)\left(\sum_{l'' = 1}^n \mathbf{z}_{k,l''}\right) \\ \frac{1}{2} \sum_{i = 1}^k \left[ \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} \right)^2} \left( \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} \right)^2 - \frac{2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^3} \left( \sum_{a = 1}^n (\mathbf{y}_{i,a} - \alpha_i) \frac{\mathbf{z}_{i,a}}{\hat{\sigma}^2_{i,a}} \right)^2 \right] \end{bmatrix} \end{aligned} \label{eq:info-tau-tau}\] </li> <li> \[\begin{aligned} \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial (\tau^2)^2} &amp;= \frac{\partial}{\partial \tau^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] \\ &amp;= \frac{\partial}{\partial \tau^2} \left[ - \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} - \sum_{l = 1}^n \sum_{j = 1}^n \frac{\tau^2 \mathbf{z}_{i,l}^2 \mathbf{z}_{i,j}^2}{(\hat{\sigma}_{i,l}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}\right)} - \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \frac{\mathbf{z}_{i,a} \mathbf{z}_{i,b}}{(\hat{\sigma}^2_{i,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \right] \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[\sum_{l = 1}^n \sum_{j = 1}^n \frac{\partial}{\partial \tau^2} \left[ \frac{\tau^2 \mathbf{z}_{i,l}^2 \mathbf{z}_{i,j}^2}{(\hat{\sigma}_{i,l}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}\right)} \right] + \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \frac{\partial}{\partial \tau^2} \left[ \frac{\mathbf{z}_{i,a} \mathbf{z}_{i,b}}{(\hat{\sigma}^2_{i,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \right] \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \left(\frac{\mathbf{z}_{i,l}^2 \mathbf{z}_{i,j}^2 (\hat{\sigma}^2_{i,l})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}\right) - \tau^2 \mathbf{z}_{i,l}^2 \mathbf{z}_{i,j}^2 (\hat{\sigma}^2_{i,l})^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} }{\left((\hat{\sigma}^2_{i,l})^2 \left( 1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}\right)\right)^2}\right) + \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \left(\frac{-2\mathbf{z}_{i,a}\mathbf{z}_{i,b}(\hat{\sigma}^2_{i,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}_{i,l}}\right) \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}_{i,l}^2}}{\left((\hat{\sigma}^2_{i,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}} \right)^2\right)^2}\right) \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\hat{\sigma}^2_{i,l})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} \right)^2} - 2 \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \left(\frac{\mathbf{z}_{i,a}\mathbf{z}_{i,b}(\hat{\sigma}^2_{i,a})^2 \left(\sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}} \right)}{(\hat{\sigma}^2_{i,a})^4 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}} \right)^3}\right) \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[ \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} \right)^2} \left( \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{(\hat{\sigma}^2_{i,l})^2} \right) \left( \sum_{j = 1}^n \mathbf{z}_{i,j}^2 \right) - \frac{2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^3} \left( \sum_{a = 1}^n (\mathbf{y}_{i,a} - \alpha_i) \frac{\mathbf{z}_{i,a}}{(\hat{\sigma}^2_{i,a})^2} \right) \left( \sum_{b = 1}^n (\mathbf{y}_{i,b} - \alpha_i) \mathbf{z}_{i,b} \right) \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[ \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} \right)^2} \left( \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{(\hat{\sigma}^2_{i,l})^2} \right) \left( \sum_{l = 1}^n \mathbf{z}_{i,l}^2 \right) - \frac{2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^3} \left( \sum_{a = 1}^n (\mathbf{y}_{i,a} - \alpha_i) \frac{\mathbf{z}_{i,a}}{\hat{\sigma}^2_{i,a}} \right) \left( \sum_{a = 1}^n (\mathbf{y}_{i,a} - \alpha_i) \frac{\mathbf{z}_{i,a}}{\hat{\sigma}^2_{i,a}} \right) \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[ \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} \right)^2} \left( \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} \right)^2 - \frac{2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^3} \left( \sum_{a = 1}^n (\mathbf{y}_{i,a} - \alpha_i) \frac{\mathbf{z}_{i,a}}{\hat{\sigma}^2_{i,a}} \right)^2 \right] \end{aligned}\] <p>Next, we take the derivative (with respect to $\tau^2$) of the derivative with respect to $\alpha_j$:</p> \[\begin{aligned} \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \tau^2 \partial \alpha_j } &amp;= \frac{\partial}{\partial \tau^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] \\ &amp;= \frac{\partial}{\partial \tau^2} \left[ (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \right] \\ &amp;= \begin{bmatrix} (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left[ \text{diag}\left( \begin{bmatrix} -\frac{1}{(\hat{\sigma}^2_{j,1})^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2} &amp; \dots &amp; -\frac{1}{(\hat{\sigma}^2_{j,n})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2} \end{bmatrix} \right) \mathbf{z}_{j} \mathbf{z}_j^\top \right] \mathbf{1}_n \end{bmatrix} \\ &amp;= \begin{bmatrix} -\frac{\mathbf{y}_{j, 1} - \alpha_j}{(\hat{\sigma}^2_{j,1})^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2} &amp; \dots &amp; -\frac{\mathbf{y}_{j,n} - \alpha_j}{(\hat{\sigma}^2_{j,n})^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2} \end{bmatrix} \mathbf{z}_j \mathbf{z}_j^\top \mathbf{1}_n \\ &amp;= \left(- \sum_{l' = 1}^n \frac{\mathbf{z}_{j,l'}(\mathbf{y}_{j,l'} - \alpha_j)}{(\hat{\sigma}^2_{j,l'})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2 }\right)\left(\sum_{l'' = 1}^n \mathbf{z}_{j,l''}\right) \\ &amp;= - \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2} \left(\sum_{l' = 1}^n \frac{\mathbf{z}_{j,l'}(\mathbf{y}_{j,l'} - \alpha_j)}{(\hat{\sigma}^2_{j,l'})^2}\right)\left(\sum_{l'' = 1}^n \mathbf{z}_{j,l''}\right) \end{aligned}\] <p>Putting the two together into a matrix yield:</p> \[\begin{aligned} \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \tau^2 \partial \theta } &amp;= \frac{\partial}{\partial \tau^2} \left[ \begin{bmatrix} \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha} \\ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \end{bmatrix} \right] \\ &amp;= \begin{bmatrix} - \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{1,l}}{\hat{\sigma}^2_{1,l}}\right)^2} \left(\sum_{l' = 1}^n \frac{\mathbf{z}_{1,l'}(\mathbf{y}_{1,l'} - \alpha_1)}{(\hat{\sigma}^2_{1,l'})^2}\right)\left(\sum_{l'' = 1}^n \mathbf{z}_{1,l''}\right) \\ \vdots \\ - \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{k,l}}{\hat{\sigma}^2_{k,l}}\right)^2} \left(\sum_{l' = 1}^n \frac{\mathbf{z}_{k,l'}(\mathbf{y}_{k,l'} - \alpha_k)}{(\hat{\sigma}^2_{k,l'})^2}\right)\left(\sum_{l'' = 1}^n \mathbf{z}_{k,l''}\right) \\ \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\hat{\sigma}^2_{i,l})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} \right)^2} - 2 \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \left(\frac{\mathbf{z}_{i,a}\mathbf{z}_{i,b}(\hat{\sigma}^2_{i,a})^2 \left(\sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}} \right)}{(\hat{\sigma}^2_{i,l})^4 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}} \right)^3}\right) \right] \end{bmatrix} \end{aligned}\] </li> </ul> <p>We then do the same but with respect to $\alpha$.</p> <ul id="deriv-theta-alpha-2" class="tab" data-tab="326ab3e2-65d2-4259-bfc4-73fa9311f265" data-name="deriv-theta-alpha-2"> <li class="active" id="deriv-theta-alpha-2-equation"> <a href="#">equation </a> </li> <li id="deriv-theta-alpha-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="326ab3e2-65d2-4259-bfc4-73fa9311f265" data-name="deriv-theta-alpha-2"> <li class="active"> \[\frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j \partial \theta} = \begin{bmatrix} 0 \\ \vdots \\ - \mathbf{1}_n^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ \vdots \\ 0 \\ -\frac{1}{\left( 1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2 } \sum_{a = 1}^n \frac{\mathbf{z}_{j,a}(\mathbf{y}_{j, a} - \alpha_j)}{(\hat{\sigma}_{j,a}^2)^2} \left(\sum_{b = 1}^n \mathbf{z}_{j,b} \right) \end{bmatrix}\] </li> <li> <p>First, we find the gradient (with respect to $\alpha$) of the derivative with respect to $\tau^2$. This should be equivalent to the corresponding components of Eq. \eqref{eq:info-tau-tau}.</p> \[\begin{aligned} \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j \partial \tau^2 } &amp;= \frac{\partial}{\partial \alpha_j} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] \\ &amp;= \frac{\partial}{\partial \alpha_j} \left[ - \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} - \sum_{l = 1}^n \sum_{j = 1}^n \frac{\tau^2 \mathbf{z}_{i,l}^2 \mathbf{z}_{i,j}^2}{(\hat{\sigma}_{i,l}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}\right)} - \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \frac{\mathbf{z}_{i,a} \mathbf{z}_{i,b}}{(\hat{\sigma}^2_{i,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \right] \right] \\ &amp;= \frac{\partial}{\partial \alpha_j} \left[ \frac{1}{2} \sum_{i = 1}^k \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \frac{\mathbf{z}_{i,a} \mathbf{z}_{i,b}}{(\hat{\sigma}^2_{i,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \right] \\ &amp;= \frac{1}{2} \sum_{a = 1}^n \sum_{b = 1}^n \frac{\partial}{\partial \alpha_j} \left[ (\mathbf{y}_{j,a} - \alpha_j)(\mathbf{y}_{j,b} - \alpha_j) \frac{\mathbf{z}_{j,a} \mathbf{z}_{j,b}}{(\hat{\sigma}^2_{j,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2} \right] \\ &amp;= \frac{1}{2} \sum_{a = 1}^n \sum_{b = 1}^n \frac{\mathbf{z}_{j,a} \mathbf{z}_{j,b}}{(\hat{\sigma}^2_{j,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2} \frac{\partial}{\partial \alpha_j} \left[ \mathbf{y}_{j,a} \mathbf{y}_{j,b} - \alpha_j \mathbf{y}_{j,a} - \alpha \mathbf{y}_{j,b} + \alpha_j^2 \right] \\ &amp;= -\frac{1}{2 \left( 1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2 } \sum_{a = 1}^n \sum_{b = 1}^n \left[ \frac{\mathbf{z}_{j,a}\mathbf{z}_{j,b}(\mathbf{y}_{j, a} - \alpha_j)}{(\hat{\sigma}_{j,a}^2)^2} + \frac{\mathbf{z}_{j,a}\mathbf{z}_{j,b}(\mathbf{y}_{j,b} - \alpha_j)}{(\hat{\sigma}_{j,a}^2)^2} \right] \\ &amp;= -\frac{1}{2 \left( 1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2 } \left[ \sum_{a = 1}^n \frac{\mathbf{z}_{j,a}(\mathbf{y}_{j, a} - \alpha_j)}{(\hat{\sigma}_{j,a}^2)^2} \sum_{b = 1}^n \mathbf{z}_{j,b} + \sum_{b = 1}^n \frac{\mathbf{z}_{j,b}(\mathbf{y}_{j,b} - \alpha_j)}{(\hat{\sigma}_{j,b}^2)^2} \sum_{a = 1}^n \mathbf{z}_{j,a}\right] \\ &amp;= -\frac{1}{\left( 1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2 } \sum_{a = 1}^n \frac{\mathbf{z}_{j,a}(\mathbf{y}_{j, a} - \alpha_j)}{(\hat{\sigma}_{j,a}^2)^2} \left(\sum_{b = 1}^n \mathbf{z}_{j,b} \right) \end{aligned}\] <p>Now, we find the vector of second derivatives of the log-likelihood with respect to the components of $\alpha$:</p> \[\begin{aligned} \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j^2} &amp;= \frac{\partial}{\partial \alpha_j} \left[ (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \Sigma_{y_j}^{-1} \mathbf{1}_n\right] \\ &amp;= - \mathbf{1}_n^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ \frac{\partial^2 \ell(\theta; \mathbf{y})}{ \partial \alpha_{j'} \partial \alpha_j} &amp;= \frac{\partial}{\partial \alpha_{j'}} \left[ (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \Sigma_{y_j}^{-1} \mathbf{1}_n\right] \\ &amp;= 0 \\ \implies \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j \partial \alpha} &amp;= \begin{bmatrix} 0 \\ \vdots \\ - \mathbf{1}_n^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ \vdots \\ 0 \end{bmatrix} \end{aligned}\] <p>Putting the two above results together gives us:</p> \[\frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j \partial \theta} = \begin{bmatrix} 0 \\ \vdots \\ - \mathbf{1}_n^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ \vdots \\ 0 \\ -\frac{1}{\left( 1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2 } \sum_{a = 1}^n \frac{\mathbf{z}_{j,a}(\mathbf{y}_{j, a} - \alpha_j)}{(\hat{\sigma}_{j,a}^2)^2} \left(\sum_{b = 1}^n \mathbf{z}_{j,b} \right) \end{bmatrix}\] </li> </ul> <h4 id="expectations-1">Expectations</h4> <p>We now take the expectation of the above vectors. We’ll evaluate the second order partial derivatives with respect to $\tau^2$ first.</p> <ul id="info-tau-tau-2" class="tab" data-tab="6cb5e295-2b20-4464-961a-8a82f651d4b4" data-name="info-tau-tau-2"> <li class="active" id="info-tau-tau-2-equation"> <a href="#">equation </a> </li> <li id="info-tau-tau-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="6cb5e295-2b20-4464-961a-8a82f651d4b4" data-name="info-tau-tau-2"> <li class="active"> \[\mathbb{E}\left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \tau^2 \partial \theta } \right] = \begin{bmatrix} 0 \\ \vdots \\ 0 \\ \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\hat{\sigma}^2_{i,l})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} \right)^2} \right] \end{bmatrix}\] </li> <li> \[\begin{aligned} \mathbb{E}\left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial (\tau^2)^2 }\right] &amp;= \mathbb{E}\left[ \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\hat{\sigma}^2_{i,l})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} \right)^2} - 2 \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \left(\frac{\mathbf{z}_{i,a}\mathbf{z}_{i,b}(\hat{\sigma}^2_{i,a})^2 \left(\sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}} \right)}{(\hat{\sigma}^2_{i,l})^4 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}} \right)^3}\right) \right] \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\hat{\sigma}^2_{i,l})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} \right)^2} - 2 \sum_{a = 1}^n \sum_{b = 1}^n \mathbb{E}\left[ (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i)\right] \left(\frac{\mathbf{z}_{i,a}\mathbf{z}_{i,b}(\hat{\sigma}^2_{i,a})^2 \left(\sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}} \right)}{(\hat{\sigma}^2_{i,l})^4 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}} \right)^3}\right) \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\hat{\sigma}^2_{i,l})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} \right)^2} - 2 \sum_{a = 1}^n \sum_{b = 1}^n \text{Cov}\left(\mathbf{y}_{i,a}, \mathbf{y}_{i,b} \right) \left(\frac{\mathbf{z}_{i,a}\mathbf{z}_{i,b}(\hat{\sigma}^2_{i,a})^2 \left(\sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}} \right)}{(\hat{\sigma}^2_{i,l})^4 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}} \right)^3}\right) \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\hat{\sigma}^2_{i,l})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} \right)^2} \right] &amp; \left(\text{obs. ind. under } H_0 \right) \\ \mathbb{E}\left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \tau^2 \partial \alpha_j}\right] &amp;= \mathbb{E}\left[ - \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2} \left(\sum_{l' = 1}^n \frac{\mathbf{z}_{j,l'}(\mathbf{y}_{j,l'} - \alpha_j)}{(\hat{\sigma}^2_{j,l'})^2}\right)\left(\sum_{l'' = 1}^n \mathbf{z}_{j,l''}\right) \right] \\ &amp;= - \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2} \left(\sum_{l' = 1}^n \frac{\mathbf{z}_{j,l'} \mathbb{E}\left[ (\mathbf{y}_{j,l'} - \alpha_j)\right] }{(\hat{\sigma}^2_{j,l'})^2}\right)\left(\sum_{l'' = 1}^n \mathbf{z}_{j,l''}\right) \\ &amp;= 0 \end{aligned}\] </li> </ul> <p>And then we do the same for $\alpha_j$:</p> <ul id="info-alpha-alpha-2" class="tab" data-tab="3d52acf3-f005-450c-8989-a012e307f075" data-name="info-alpha-alpha-2"> <li class="active" id="info-alpha-alpha-2-equation"> <a href="#">equation </a> </li> <li id="info-alpha-alpha-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="3d52acf3-f005-450c-8989-a012e307f075" data-name="info-alpha-alpha-2"> <li class="active"> \[\mathbb{E}\left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j \partial \theta } \right] = \begin{bmatrix} 0 \\ \vdots \\ - \mathbf{1}_n^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ \vdots \\ 0 \\ 0 \end{bmatrix}\] </li> <li> \[\begin{aligned} \mathbb{E}\left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j \partial \tau^2} \right] &amp;= \mathbb{E}\left[ -\frac{1}{\left( 1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2 } \sum_{a = 1}^n \frac{\mathbf{z}_{j,a}(\mathbf{y}_{j, a} - \alpha_j)}{(\hat{\sigma}_{j,a}^2)^2} \left(\sum_{b = 1}^n \mathbf{z}_{j,b} \right) \right] \\ &amp;= -\frac{1}{\left( 1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2 } \sum_{a = 1}^n \frac{\mathbf{z}_{j,a} \mathbb{E}\left[\mathbf{y}_{j, a} - \alpha_j\right] }{(\hat{\sigma}_{j,a}^2)^2} \left(\sum_{b = 1}^n \mathbf{z}_{j,b} \right) \\ &amp;= 0 \\ \mathbb{E}\left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j^2} \right] &amp;= \mathbb{E}\left[ - \mathbf{1}_n^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \right] \\ &amp;= - \mathbf{1}_n^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ \mathbb{E}\left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j \partial \alpha_{j'}} \right] &amp;= 0 \end{aligned}\] </li> </ul> <p>We then evaluate the Fisher information at the MLE:</p> <ul id="info-2" class="tab" data-tab="f06e6265-0980-478d-90d3-c35510269a1f" data-name="info-2"> <li class="active" id="info-2-equation"> <a href="#">equation </a> </li> <li id="info-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="f06e6265-0980-478d-90d3-c35510269a1f" data-name="info-2"> <li class="active"> \[\mathcal{I}_{\theta, \theta}(\hat{\theta}) = \begin{bmatrix} - \mathbf{1}_n^\top\text{diag}\left(\left[ \frac{1}{\hat{\sigma}^2_{1,1}}, \dots, \frac{1}{\hat{\sigma}^2_{1,n}}\right]\right) \mathbf{1}_n &amp; 0 &amp; \dots &amp; 0 \\ 0 &amp; - \mathbf{1}_n^\top \text{diag}\left(\left[ \frac{1}{\hat{\sigma}^2_{2,1}}, \dots, \frac{1}{\hat{\sigma}^2_{2,n}}\right]\right) \mathbf{1}_n &amp; \dots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \dots &amp; \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\hat{\sigma}^2_{i,l})^2} \right] \end{bmatrix}\] </li> <li> <p>Note that:</p> \[\Sigma_{y_i}^{-1} \bigg\rvert_{\theta = \hat{\theta}} = \text{diag}\left(\left[ \frac{1}{\hat{\sigma}^2_{i,1}}, \dots, \frac{1}{\hat{\sigma}^2_{i,n}}\right]\right)\] <p>Then we have:</p> \[\begin{aligned} - \mathbb{E}\left. \left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \tau^2 \partial \theta} \right] \right\rvert_{\theta = \hat{\theta}} &amp;= - \mathbb{E} \left.\left[ \begin{bmatrix} 0 \\ \vdots \\ 0 \\ \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\hat{\sigma}^2_{i,l})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} \right)^2} \right] \end{bmatrix} \right] \right\rvert_{\theta = \hat{\theta}} \\ &amp;= \begin{bmatrix} 0 \\ \vdots \\ 0 \\ \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\hat{\sigma}^2_{i,l})^2} \right] \end{bmatrix} \\ - \mathbb{E} \left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j \partial \theta} \right] \bigg\rvert_{\theta = \hat{\theta}} &amp;= - \mathbb{E} \left. \left[ \begin{bmatrix} 0 \\ \vdots \\ - \mathbf{1}_n^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ \vdots \\ 0 \\ 0 \end{bmatrix} \right] \right\rvert_{\theta = \hat{\theta}} \\ &amp;= \begin{bmatrix} 0 \\ \vdots \\ - \mathbf{1}_n^\top \text{diag}\left(\left[ \frac{1}{\hat{\sigma}^2_{j,1}}, \dots, \frac{1}{\hat{\sigma}^2_{j,n}}\right]\right) \mathbf{1}_n \\ \vdots \\ 0 \\ 0 \end{bmatrix} \end{aligned}\] <p>Putting these together into a big matrix:</p> \[\begin{aligned} \mathcal{I}_{\theta, \theta}(\hat{\theta}) &amp;= - \mathbb{E}\left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \theta \partial \theta^\top} \right] \bigg\rvert_{\theta = \hat{\theta}} \\ &amp;= \begin{bmatrix} - \mathbf{1}_n^\top\text{diag}\left(\left[ \frac{1}{\hat{\sigma}^2_{1,1}}, \dots, \frac{1}{\hat{\sigma}^2_{1,n}}\right]\right) \mathbf{1}_n &amp; 0 &amp; \dots &amp; 0 \\ 0 &amp; - \mathbf{1}_n^\top \text{diag}\left(\left[ \frac{1}{\hat{\sigma}^2_{2,1}}, \dots, \frac{1}{\hat{\sigma}^2_{2,n}}\right]\right) \mathbf{1}_n &amp; \dots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \dots &amp; \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\hat{\sigma}^2_{i,l})^2} \right] \end{bmatrix} \end{aligned}\] </li> </ul> <hr/> ]]></content><author><name></name></author><category term="glmm"/><category term="glmm"/><category term="information"/><category term="score"/><summary type="html"><![CDATA[Calculations for GLMMs]]></summary></entry><entry><title type="html">Generalized Linear Mixed Models</title><link href="https://aerosengart.github.io/blog/2025/glmm/" rel="alternate" type="text/html" title="Generalized Linear Mixed Models"/><published>2025-06-04T00:00:00+00:00</published><updated>2025-06-04T00:00:00+00:00</updated><id>https://aerosengart.github.io/blog/2025/glmm</id><content type="html" xml:base="https://aerosengart.github.io/blog/2025/glmm/"><![CDATA[<p>This post is a primer on generalized linear mixed models (GLMM) and their associated estimation procedures. Throughout this post, we perform derivatives of matrices, vectors, and scalars. We denote all of them with $\partial$ even though they may not <i>technically</i> be partial derivatives. The meaning should be clear from the context if you keep track of the dimensions of the variables.</p> <p>We’ll use the notation $\text{vec}(\mathbf{M})$ denote the vectorization of matrix $\mathbf{M}$, where we flatten the matrix row-by-row. Also, any function $f: \mathbb{R} \rightarrow \mathbb{R}$ will be applied element-wise to vectors and matrices.</p> <hr/> <h2 id="background">Background</h2> <p>Let’s first explain <i>why</i> generalized linear mixed models are of interest and helpful in data analysis. Suppose we have a dataset in which the observations can be grouped in some way (i.e. we have repeated observations from the same unit). One example is longitudinal studies where we obtain measurements from the same individuals over time. Another example is in education where we may observe students from the same classes or schools. In these settings, it seems reasonable to assume that observations from the same unit would be more similar that those from different ones. This can be realized in our model by including a way for the effects of covariates to differ across units.</p> <p>As in generalized linear models, we assume the mean response is some function of a linear combination of covariates. However, GLMMs are called <i>mixed</i> because the effects of the covariates are of two types: <i>fixed</i> and <i>random</i>. The fixed effects represent population-level relationships, while the random effects represent unit-specific ones. In this way, we can use GLMMs to analyze between-subject variation (via the fixed effects) and within-subject variation (via the random effects): the fixed effects determine the relationship between the covariates and the mean response for the population (i.e. overall), and the random effects describe how each group’s mean response deviates from that. Later on, we introduce measurement errors, which account for deviation of each <i>observation</i> from its group’s mean.</p> <hr/> <h2 id="set-up">Set-Up</h2> <p>We have $n$ observations coming from $k$ different clusters, each of size $n_t$ for $t \in [k]$. The full data will be denoted by $\mathbf{y}$. Though $\mathbf{y}$ is a vector, we’ll denote the $j$-th observation from cluster $i$ with $\mathbf{y}_{i,j}$. For example, \(\mathbf{y}_{i,j}\) denotes element \(\sum_{l = 1}^{i - 1} n_l + j\) of $\mathbf{y}$. We’ll denote the $n_i$-dimensional vector of responses for cluster $i$ with $\mathbf{y}_i$.</p> <p>For each observation, we will have $p$ fixed effect covariates arranged in a $p$-dimensional vector, \(\mathbf{x}_{i, j}\), and $q$ random effects covariates in a $q$-dimensional vector, \(\mathbf{z}_{i,j}\). We’ll assume that the observations within the same cluster are independent.</p> <p>We’ll let $\alpha$ denote a $p$-dimensional fixed effect coefficient vector, $\alpha$, and $\beta_t$ will denote a $q$-dimensional random effect coefficient vector corresponding to group $t$. We assume $\beta_t \overset{iid}{\sim} \mathcal{F}$ for all $t \in [k]$ for an exponential family distribution, $\mathcal{F}$, and $q \times q$ covariance matrix, $D(\tau^2)$, depending on an $m$-dimensional variance component vector, $\tau^2$. These random effects are assumed to be independent of the covariates. When they are not, we can run into issues with bias when estimating the fixed effects.</p> <details> <summary><strong>Additional Assumptions.</strong></summary> A few auxiliary assumptions must also be made for the analysis later, which we list here: <ul> <li>The third moment and higher moments of $\beta$ are of order $o(\rvert \rvert \tau^2 \rvert \rvert)$.</li> <li>The entries of $D(\tau^2)$ are linear in $\tau^2$.</li> <li>$D(\tau^2) = \text{vec}(0)$ if $\tau^2 = \text{vec}(0)$</li> </ul> </details> <p>Our model comes in the form of a specification of the conditional mean, $\mu_{i,j} = \mathbb{E}[\mathbf{y}_{i,j} \rvert \beta_i]$ (where we suppress the addition conditioning on the covariates themselves). For a monotonic and differentiable link function (e.g. $\log(\cdot)$ or $\text{logit}(\cdot)$), the conditional mean of the $j$-th observation in group $i$ is assumed to be given by:</p> \[\begin{equation} \label{eq:glmm} \mu_{i,j} = g^{-1}\left(\mathbf{x}_{i,j}^\top \alpha + \mathbf{z}_{i,j}^\top \beta_i \right) \end{equation}\] <p>We then assume that the observations themselves follow some exponential family distribution with measurement errors, $\epsilon_{i,j}$, which is the deviation of the response from its (unit-specific) conditional mean. These errors are assumed to have mean zero and be independent of each other and of the random effects. We further assume the responses, $\mathbf{y}_{i,j}$, conditional on the random effects (and the covariates), are independent with variances equal to some function of the conditional mean.</p> <p>To write Eq. \eqref{eq:glmm} in matrix notation, we assume that the observations are ordered by group, so the first $n_1$ observations are all from group $1$. We can then define the following vectors/matrices:</p> \[\mu = \begin{bmatrix} \mu_{1, 1} \\ \vdots \\ \mu_{k, n_k} \end{bmatrix}, \hspace{2mm} \mathbf{X} = \begin{bmatrix} — &amp; \mathbf{x}_{1,1} &amp; — \\ &amp; \dots &amp; \\ — &amp; \mathbf{x}_{k, n_k} &amp; — \\ \end{bmatrix}, \hspace{2mm} \tilde{\mathbf{Z}}_i = \begin{bmatrix} — &amp; \mathbf{z}_{i, 1} &amp; — \\ &amp; \dots &amp; \\ — &amp; \mathbf{z}_{i, n_i} &amp; — \\ \end{bmatrix}, \hspace{2mm} \mathbf{Z} = \begin{bmatrix} \tilde{\mathbf{Z}}_1 &amp; \dots &amp; 0 \\ \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; \dots &amp; \tilde{\mathbf{Z}}_k \end{bmatrix}, \hspace{2mm} \beta = \begin{bmatrix} \beta_1 \\ \vdots \\ \beta_k \end{bmatrix}\] <p>where $\tilde{\mathbf{Z}}_i$ is constructed by stacking the \(\mathbf{z}_{i,j}\) vectors for group $i$ into a matrix, and where $\beta$ is the vector created by stacking together all of the $\beta_i$ vectors vertically. As a reminder, \(\mu \in \mathbb{R}^{n \times 1}\), \(\mathbf{X} \in \mathbb{R}^{n \times p}\), \(\tilde{\mathbf{Z}}_i \in \mathbb{R}^{n_i \times q}\), \(\mathbf{Z} \in \mathbb{R}^{n \times (q \times k)}\), $\alpha \in \mathbb{R}^p$, and \(\beta \in \mathbb{R}^{(q \times k) \times 1}\).</p> <p>The model is then:</p> \[\begin{equation} \label{eq:glmm-matrix} \mu = g^{-1}(\mathbf{X} \alpha + \mathbf{Z} \beta) \end{equation}\] <p>We’ll use $[\cdot] \rvert_{\beta = \beta_0}$ to denote evaluation of the function in brackets when setting $\beta$ equal to $\beta_0$. Similarly, we’ll use $[\cdot] \rvert_{H_0}$ to denote evaluation under the null hypothesis. We’ll also use a superscript $0$ (e.g. $\mu^0$, $\eta^0$, etc.) to denote the quantity under the null hypothesis (i.e. $\tau^2 = \mathbf{0} \implies \beta = \mathbf{0}$).</p> <div class="example"> <strong>Example.</strong> <body> To keep things simple, we'll assume to a fixed intercept (that is cluster-specific) and a random slope. Thus, $\alpha, \beta \in \mathbb{R}^{k}$, and $\mathbf{z}_{i, j} \in \mathbb{R}$ as well. <br/> In this simple case, $\tilde{\mathbf{Z}}_i \in \mathbb{R}^{n_t}$, so $\mathbf{Z}$ is $n \times k$. Our model is then: $$ \mathbf{y}_{i,j} \rvert \beta_i \sim \text{Poi}\left(\mu_{i,j}\right), \hspace{5mm} \mu_{i,j} = \exp\left(\alpha_i + \beta_i \mathbf{z}_{i,j} \right) $$ We'll have a scalar-valued variance component that we call $\tau^2$. The random effects will have distribution: $$ \beta_i \overset{iid}{\sim} \mathcal{N}(0, \tau^2), \hspace{5mm} \forall i \in [k] $$ If we let $\mathbf{A}$ denote the $n$-dimensional vector of intercepts where the $i$-th entry of $\alpha$ is repeated $n_i$ times, then we can write our model in vector form as: $$ \mu = \exp(\mathbf{A} + \mathbf{Z} \beta) $$ </body> </div> <hr/> <h2 id="likelihood">Likelihood</h2> <p>We can write the conditional log-likelihood using the exponential family form (see my <a href="/posts/2025/06/03/glm.html">generalized linear models post</a>).</p> \[\begin{equation} \label{eq:condition-log-lik} \begin{aligned} \ell(\mathbf{y}; \alpha, \tau^2 \rvert \beta) &amp;= \sum_{i = 1}^{k} \sum_{j = 1}^{n_i} \left[ \frac{\zeta_{i,j} \mathbf{y}_{i,j} - A(\zeta_{i,j})}{d(\phi, \omega_{i,j})} + \log(h(\mathbf{y}_{i,j}, \phi, \omega_{i,j}))\right] \\ \mathcal{L}(\mathbf{y}; \alpha, \tau^2 \rvert \beta) &amp;= \exp \left( \sum_{i = 1}^n \left[ \frac{\zeta_i \mathbf{y}_i - A(\zeta_i)}{d(\phi, \omega_i)} + \log(h(\mathbf{y}_i, \phi, \omega_i))\right] \right) \end{aligned} \end{equation}\] <p>where $\phi &gt; 0$ is a dispersion/scale parameter; $\omega_{i,j}$ is a (prior) dispersion weights; $\zeta_{i,j}$ is a distribution parameter; and $h(\cdot)$ and $A(\cdot)$ are known functions. We assume that $d(\phi, \omega_{i,j}) = \phi \omega_i^{-1}$.</p> <p>The conditional variance of the responses is given by:</p> \[\text{Cov}(\mathbf{y} \rvert \beta) = \text{diag}(d(\phi, \omega)) \underbrace{\frac{\partial^2 A(\tau^2)}{\partial \tau^2 \partial (\tau^2)^\top}}_{=V(\mu)} = \text{diag}^{-1}\left( \frac{\omega}{\phi}\right) V(\mu)\] <hr/> <h2 id="quasi-likelihood">Quasi-Likelihood</h2> <p>We don’t actually need to specify the true likelihood, even though (in practice) we will usually be able to do so. Using quasi-likelihood methods can often be less computationally expensive than maximum likelihood for count data.</p> <p>In the quasi-likelihood scheme, we keep the conditional mean assumption that we discussed above and also make the assumption that the conditional variance can be expressed as a function of the mean in the form $\text{Cov}(\mathbf{y}) = \text{diag}\left(d(\phi, \omega)\right)V(\mu)$ for some function $V(\cdot)$. Later, we’ll denote diagonal elements of $\text{Cov}(\mathbf{y})$ with $v(\mu_{i,j})$.</p> <p>The conditional log quasi-likelihood and quasi-likelihood are given by:</p> \[\begin{equation} \label{eq:quasi-lik} \begin{aligned} \ell_q(\mathbf{y}_{i,j}; \alpha, \tau^2 \rvert \beta_i) &amp;= \int_{\mathbf{y}_{i,j}}^{\mu_{i,j}} \frac{\omega_{i,j}(\mathbf{y}_{i,j} - u)}{\phi V(u)} du \\ \mathcal{L}_q(\mathbf{y}_{i,j}; \alpha, \tau^2 \rvert \beta_i) &amp;= \exp\left(\int_{\mathbf{y}_{i,j}}^{\mu_{i,j}} \frac{\omega_{i,j}(\mathbf{y}_{i,j} - u)}{\phi V(u)} du \right) \end{aligned} \end{equation}\] <p>When $\mathcal{F}$ <i>is</i> an exponential family distribution, the log-likelihood and log quasi-likelihood are equal. See my <a href="/posts/2025/05/30/quasi-likelihood.html">post on quasi-likelihood</a> for a more in-depth discussion.</p> <p>Let $f$ denote the density associated with $\mathcal{F}$, the distribution of the random effects. The unconditional quasi-likelihood is then given by integrating out the random effects from the joint quasi-likelihood:</p> \[\begin{equation} \label{eq:uncondition-log-lik} \begin{aligned} \mathcal{L}_q(\mathbf{y}; \alpha, \tau^2) &amp;= \prod_{i = 1}^k \mathcal{L}_q(\mathbf{y}_i; \alpha, \tau^2) = \prod_{i = 1}^k \int\prod_{j = 1}^{n_i} \mathcal{L}_q(\mathbf{y}_{i,j}; \alpha, \tau^2 \rvert \beta_i) f(\beta_i) d\beta_i \end{aligned} \end{equation}\] <p>Remember that the above integral is multi-dimensional if $\beta_i$ has dimension greater than $1$!</p> <p>We usually do maximum likelihood estimation for the parameter vector $\beta$ by taking the derivative of the log of the above, setting it equal to zero, and solving for $\beta$. However, depending upon the form of $f(\cdot)$ and the number of random effects we have, Eq. \eqref{eq:uncondition-log-lik} may be a multi-dimensional integral that is difficult to evaluate, let alone differentiate. One way is to use quadrature to approximate the integral as a summation. However, there are two other, computationally more feasible, methods that provide a work-around.</p> <p>There are two common methods of approximate inference for GLMMs: penalized quasi-likelihood (PQL) and marginal quasi-likelihood (MQL). They are very similar in that they both perform a Taylor approximation of the conditional log quasi-likelihood to evaluate the integral in Eq. \eqref{eq:uncondition-log-lik}. However, PQL uses the maximum a priori estimates of the random effects as the operating point for the expansion, while MQL uses the random effects mean. See below for a discussion of the approaches.</p> <h3 id="penalized-quasi-likelihood">Penalized Quasi-Likelihood</h3> <p>Penalized quasi-likelihood (PQL) essentially uses Laplace’s method to approximate the integral above. The general idea behind <a href="https://en.wikipedia.org/wiki/Laplace%27s_method">Laplace’s method</a> is to approximate integrals of a certain form as:</p> \[\int \exp(M f(x)) dx \approx \exp(M f(x_0)) \int \exp\left( - \frac{1}{2} M \rvert f''(x_0) \rvert (x - x_0)^2 \right) dx\] <p>where $M$ is a big scalar, $f$ is a twice-differentiable function, and $x_0$ is a global maximum of $f$. If \(f''(x_0) &lt; 0\) and $M \rightarrow \infty$, then the integrand above is basically a Gaussian kernel and the approximation becomes:</p> \[\begin{equation} \label{eq:ql-gauss} \int \exp(M f(x)) dx \approx \left(\frac{2 \pi}{M \rvert f''(x_0) \rvert} \right)^{\frac{1}{2}} \exp(M f(x_0)) \end{equation}\] <ul id="claim-3" class="tab" data-tab="c3118f99-b276-47fe-a619-384ad37654ed" data-name="claim-3"> <li class="active" id="claim-3-claim"> <a href="#">claim </a> </li> <li id="claim-3-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="c3118f99-b276-47fe-a619-384ad37654ed" data-name="claim-3"> <li class="active"> <p>Letting $c = (2 \pi)^{-\frac{m}{2}}$ and $d_{i,j}(y_{i,j}, \mu_{i,j}) = -2 \int_{y_{i,j}}^{\mu_{i,j}} \frac{y_{i,j} - z}{a_{i,j} V(z)} dz$, we can rewrite the integral as:</p> \[\begin{aligned} \mathcal{L}_q(\mathbf{y}; \alpha, \tau^2) &amp;= c \rvert D(\tau^2) \rvert^{-\frac{1}{2}} \int \exp\left( -\kappa(\beta) \right) d \beta; \\ \kappa(\beta) &amp;= \frac{1}{2\phi}\sum_{i = 1}^n \sum_{j = 1}^{n_i} d_{i,j}(y_{i,j}, \mu_{i,j}) + \frac{1}{2}\beta^\top D^{-1}(\tau^2) \beta \end{aligned}\] </li> <li> \[\begin{aligned} \mathcal{L}_q(\mathbf{y}; \alpha, \tau^2) &amp;= \int \mathcal{L}_q(\mathbf{y}; \alpha, \tau^2 \rvert \beta) \mathcal{L}(\beta) d \beta \\ &amp;= \int \left[ \prod_{i = 1}^n \prod_{j = 1}^{n_i} \exp\left( \ell_q(y_{i,j}; \mu_{i,j} \rvert \beta) \right) \right] (2 \pi)^{-\frac{m}{2}} \rvert D(\tau^2) \rvert^{-\frac{1}{2}} \exp\left(- \frac{1}{2}\beta^\top D^{-1}(\tau^2) \beta \right) d\beta \\ &amp;= (2 \pi)^{-\frac{m}{2}} \rvert D(\tau^2) \rvert^{-\frac{1}{2}} \int \exp\left(\sum_{i = 1}^n \sum_{j = 1}^{n_i} \left( \int_{y_{i,j}}^{\mu_{i,j}} \frac{y_{i,j} - z}{\phi a_{i,j} V(z)} dz \right) - \frac{1}{2}\beta^\top D^{-1}(\tau^2) \beta \right) d\beta \\ &amp;= (2 \pi)^{-\frac{m}{2}} \rvert D(\tau^2) \rvert^{-\frac{1}{2}} \int \exp\left(- \frac{1}{2 \phi} \sum_{i = 1}^n \sum_{j = 1}^{n_i} \left( -2 \int_{y_{i,j}}^{\mu_{i,j}} \frac{y_{i,j} - z}{a_{i,j} V(z)} dz \right) - \frac{1}{2}\beta^\top D^{-1}(\tau^2) \beta \right) d\beta \\ &amp;= (2 \pi)^{-\frac{m}{2}} \rvert D(\tau^2) \rvert^{-\frac{1}{2}} \int \exp\left(- \frac{1}{2 \phi} \sum_{i = 1}^n \sum_{j = 1}^{n_i} d_{i,j}(y_{i,j}, \mu_{i,j}) - \frac{1}{2}\beta^\top D^{-1}(\tau^2) \beta \right) d\beta &amp; \left( d_{i,j}(y_{i,j}, \mu_{i,j}) = -2 \int_{y_{i,j}}^{\mu_{i,j}} \frac{y_{i,j} - z}{a_{i,j} V(z)} dz \right) \end{aligned}\] </li> </ul> <p>Note that the first term in $\kappa(\beta)$ <i>does</i> involve $\beta$ since \(\mu_{i,j} = g^{-1}(\mathbf{x}_{i,j}^\top \alpha + \mathbf{z}_{i,j}^\top \beta_j)\). Let $\kappa’$ and $\kappa’’$ denote the vector of first-order partial derivatives and the matrix of second-order partial derivatives, respectively, of $\kappa$ with respect to $\beta$.</p> <ul id="claim-4" class="tab" data-tab="037937c1-2107-48e1-bcb3-753c47b6b647" data-name="claim-4"> <li class="active" id="claim-4-claim"> <a href="#">claim </a> </li> <li id="claim-4-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="037937c1-2107-48e1-bcb3-753c47b6b647" data-name="claim-4"> <li class="active"> <p>These have the form:</p> \[\begin{aligned} \kappa'_k(\beta) &amp;= -\sum_{j = 1}^{n_k} \left(\frac{y_{k,j} - \mu_{k,j}}{\phi a_{k,j} V(\mu_{k,j})\frac{\partial g(\mu_{i,j})}{\partial \mu_{i,j}}}\right)\mathbf{z}_{i,j} + D^{-1}(\tau^2) \beta_k \\ \kappa''(\beta) &amp;\approx \tilde{\mathbf{Z}}^\top \mathbf{W} \tilde{\mathbf{Z}} + D^{-1}(\tau^2) \end{aligned}\] <p>where $\mathbf{W}$ is the $n \times n$ diagonal matrix with elements $\frac{1}{\phi a_{i,j} V(\mu_{i,j}) \frac{\partial g(\mu_{i,j})}{\partial \mu_{i,j}}}$, and $\tilde{\mathbf{Z}}$ is the $n \times q$ matrix formed by concatenating the $\tilde{\mathbf{Z}}^t$ matrices.</p> </li> <li> \[\begin{aligned} \kappa'_k(\beta) &amp;= \frac{\partial}{\partial \beta_k} \left[\frac{1}{2 \phi} \sum_{i = 1}^k \sum_{j = 1}^{n_i} d_{i,j}(y_{i,j}, \mu_{i,j}) + \frac{1}{2} \beta^\top D^{-1}(\tau^2) \beta \right] \\ &amp;= \frac{1}{2 \phi} \sum_{i = 1}^k \sum_{j = 1}^{n_i} \frac{\partial}{\partial \beta_k} \left[d_{i,j}(y_{i,j}, \mu_{i,j}) \right] + D^{-1}(\tau^2) \beta \\ &amp;= \frac{1}{2 \phi} \sum_{j = 1}^{n_k} \frac{\partial \mu_{k,j}}{\partial \beta_k} \frac{\partial}{\partial \mu_{k,j}}\left[ -2 \int_{y_{k,j}}^{\mu_{k,j}} \frac{y_{k,j} - z}{a_{k,j} V(z)}dz \right] + D^{-1}(\tau^2) \beta_k \\ &amp;= \frac{1}{2 \phi} \sum_{j = 1}^{n_k} \frac{\partial \mu_{k,j}}{\partial \beta_k} \left(-2 \frac{y_{k,j} - \mu_{k,j}}{a_{k,j} V(\mu_{k,j})}\right) + D^{-1}(\tau^2) \beta_k \\ &amp;= -\sum_{j = 1}^{n_k} \frac{\partial \mu_{k,j}}{\partial \beta_k} \left(\frac{y_{k,j} - \mu_{k,j}}{\phi a_{k,j} V(\mu_{k,j})}\right) + D^{-1}(\tau^2) \beta_k \\ \end{aligned}\] <p>Assuming $g(\cdot)$ is <i>strictly monotone and continuous</i>, then we have:</p> \[\frac{\partial g^{-1}(\eta_{i,j})}{\partial \eta_{i,j}} = \frac{\partial \mu_{i,j}}{\partial \eta_{i,j}} = \left(\frac{\partial \eta_{i,j}}{\partial \mu_{i,j}}\right)^{-1} = \left(\frac{\partial g(\mu_{i,j})}{\partial \mu_{i,j}}\right)^{-1}\] <p>This implies:</p> \[\frac{\partial \mu_{k,j}}{\partial \beta_k} = \frac{\partial \eta_{k,j}}{\partial \beta_k} \frac{\partial \mu_{k, j}}{\partial \eta_{i,j}} = \mathbf{z}_{i,j} \left(\frac{\partial \eta_{i,j}}{\partial \mu_{i,j}}\right)^{-1} = \frac{\mathbf{z}_{i,j}}{\frac{\partial g(\mu_{i,j})}{\partial \mu_{i,j}}}\] <p>Thus, $\kappa’_k(\beta)$ is:</p> \[\kappa'_k(\beta) = -\sum_{j = 1}^{n_k} \left(\frac{y_{k,j} - \mu_{k,j}}{\phi a_{k,j} V(\mu_{k,j})\frac{\partial g(\mu_{k,j})}{\partial \mu_{k,j}}}\right)\mathbf{z}_{k,j} + D^{-1}(\tau^2) \beta_k\] <p>Looking at the second-order partial derivatives:</p> \[\begin{aligned} \kappa''_{k,k}(\beta) &amp;= \frac{\partial}{\partial \beta_k^\top} \left[ -\sum_{j = 1}^{n_k} \left(\frac{y_{k,j} - \mu_{k,j}}{\phi a_{k,j} V(\mu_{k,j})\frac{\partial g(\mu_{k,j})}{\partial \mu_{k,j}}}\right)\mathbf{z}_{k,j} + D^{-1}(\tau^2) \beta_k \right] \\ &amp;= - \sum_{j = 1}^{n_k} \left( \frac{\mathbf{z}_{k,j}}{\phi a_{k,j} V(\mu_{k,j}) \frac{\partial g(\mu_{k,j})}{\partial \mu_{k,j}}} \frac{\partial}{\partial \beta_k^\top} \left[ y_{k,j} - \mu_{k,j} \right] + \mathbf{z}_{k,j}(y_{k,j} - \mu_{k,j})\frac{\partial}{\partial \beta_k^\top} \left[ \frac{1}{\phi a_{k,j} V(\mu_{k,j}) \frac{\partial g(\mu_{k,j})}{\partial \mu_{k,j}}} \right] \right) + D^{-1}(\tau^2) \\ &amp;= - \sum_{j = 1}^{n_k} \left( \frac{-\mathbf{z}_{k,j}\mathbf{z}_{k, j}^\top}{\phi a_{k,j} V(\mu_{k,j}) \left(\frac{\partial g(\mu_{k,j})}{\partial \mu_{k,j}}\right)^2} + \mathbf{z}_{k,j}(y_{k,j} - \mu_{k,j})\frac{\partial}{\partial \beta_k^\top} \left[ \frac{1}{\phi a_{k,j} V(\mu_{k,j}) \frac{\partial g(\mu_{k,j})}{\partial \mu_{k,j}}} \right] \right) + D^{-1}(\tau^2) \\ &amp;= \sum_{j = 1}^{n_k} \frac{\mathbf{z}_{k,j}\mathbf{z}_{k, j}^\top}{\phi a_{k,j} V(\mu_{k,j}) \left(\frac{\partial g(\mu_{k,j})}{\partial \mu_{k,j}}\right)^2} + D^{-1}(\tau^2) + R_k \\ &amp;=(\tilde{\mathbf{Z}}^k)^\top \mathbf{W}_k \tilde{\mathbf{Z}}^k + D^{-1}(\tau^2) + R_k \end{aligned}\] <p>where in $(i)$, we set:</p> \[R_k = -\sum_{j = 1}^{n_k} (y_{k,j} - \mu_{k,j}) \mathbf{z}_{k,j} \frac{\partial}{\partial \beta_k^\top} \left[ \frac{1}{\phi a_{k,j} V(\mu_{k,j}) \frac{\partial g(\mu_{k,j})}{\partial \mu_{k,j}}} \right]\] <p>and in $(ii)$, $\mathbf{W}_k$ is the diagonal matrix with elements:</p> \[\frac{1}{\phi a_{k,j} V(\mu_{k,j}) \frac{\partial g(\mu_{k,j})}{\partial \mu_{k,j}}}\] <p>Since \(\kappa'_k(\beta)\) only involves $\beta_k$, \(\kappa''_{k, k'}(\beta) = \mathbf{0}\). Stacking all of the $\mathbf{W}_k$ and $R_k$ together, we get:</p> \[\kappa''(\beta) = \tilde{\mathbf{Z}}^\top \mathbf{W} \tilde{\mathbf{Z}} + D^{-1}(\tau^2) + R \approx \tilde{\mathbf{Z}}^\top \mathbf{W} \tilde{\mathbf{Z}} + D^{-1}(\tau^2)\] <p>where we drop $R$ for the approximation.</p> </li> </ul> <p>Letting $\hat{\beta}$ be a solution to the equation $\kappa’(\beta) = \mathbf{0}$, we can ignore constants and remainders (by assuming they are negligible in the long run…) to obtain the approximation in Eq. \eqref{eq:ql-gauss}:</p> \[\begin{aligned} \ell_q(\mathbf{y}; \alpha, \tau^2) &amp;\approx \rvert D(\tau^2) \rvert^{-\frac{1}{2}} \log\left( \left( \frac{2 \pi}{\rvert \kappa''(\beta_0) \rvert}\right)^{\frac{1}{2}} \exp(-\kappa(\beta_0)) \right) \\ &amp;= \frac{1}{2}\log(2 \pi) - \frac{1}{2} \log(\kappa''(\beta_0)) + \kappa(\beta_0) \\ &amp;\approx -\frac{1}{2} \log\left( \rvert D(\tau^2) \rvert \right) -\frac{1}{2} \log\left( \rvert \tilde{\mathbf{Z}}^\top \mathbf{W} \mathbf{\tilde{Z}} + D^{-1}(\tau^2) \rvert \right) + \frac{1}{2\phi} \sum_{i = 1}^k \sum_{j = 1}^{n_i} d_{i,j}(y_{i,j}, \mu_{i,j}) + \frac{1}{2}\beta^\top D^{-1}(\tau^2) \beta \end{aligned}\] <h3 id="marginal-quasi-likelihood">Marginal Quasi-Likelihood</h3> <p>An alternative way to think about a generalized linear mixed model is from a marginal perspective, where we instead focus on the <i>unconditional</i> expectation of the response:</p> \[\mathbb{E}[y_{i,j}] = \mu_{i,j} = g(\mathbf{x}_{i,j}^\top \alpha)\] <p>In contrast to PQL, marginal quasi-likelihood (MQL) performs a Taylor approximation of the log quasi-likelihood about the random effects (prior) mean $\beta = \mathbf{0}$ to get a closed form solution to the multi-dimensional integral. For the $i$-th cluster, this approximation is:</p> \[\begin{aligned} \ell_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i) \approx \ell_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i)\rvert_{\beta_i = \mathbf{0}} + \beta_i^\top \left[ \frac{\partial \ell_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i)}{\partial \beta_i} \right] \bigg\rvert_{\beta_i = \mathbf{0}} + \frac{1}{2} \beta_i^\top \left[ \frac{\partial^2 \ell_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i)}{\partial \beta_i \partial \beta_i^\top} \right] \bigg\rvert_{\beta_i = \mathbf{0}} \beta_i \end{aligned}\] <p>Thus:</p> \[\begin{aligned} \mathcal{L}_q(\mathbf{y}; \alpha, \tau^2) &amp;= \prod_{i = 1}^k (2 \pi)^{-\frac{q}{2}} \rvert D(\tau^2) \rvert^{-\frac{1}{2}} \int \exp\left( \ell_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i) - \frac{1}{2}\beta_i^\top D^{-1}(\tau^2) \beta_i \right) d \beta_i \\ &amp;\approx \prod_{i = 1}^k (2 \pi)^{-\frac{q}{2}} \rvert D(\tau^2) \rvert^{-\frac{1}{2}} \int \exp\left( [\ell_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i)]_{\beta_i = \mathbf{0}} + \beta_i^\top \left[ \frac{\partial \ell_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i)}{\partial \beta_i} \right] \bigg\rvert_{\beta_i = \mathbf{0}} + \frac{1}{2} \beta_i^\top \left[ \frac{\partial^2 \ell_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i)}{\partial \beta_i \partial \beta_i^\top} \right] \bigg\rvert_{\beta_i = \mathbf{0}} \beta_i - \frac{1}{2}\beta_i^\top D^{-1}(\tau^2) \beta_i \right) d \beta_i \\ &amp;= \prod_{i = 1}^k (2 \pi)^{-\frac{q}{2}} \rvert D(\tau^2) \rvert^{-\frac{1}{2}} \left[\mathcal{L}_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i) \right] \bigg\rvert_{\beta_i = \mathbf{0}} \int \exp\left( \beta_i^\top \left[ \frac{\partial \ell_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i)}{\partial \beta_i} \right] \bigg\rvert_{\beta_i = \mathbf{0}} - \frac{1}{2} \beta_i^\top \left[ D^{-1}(\tau^2) - \left[ \frac{\partial^2 \ell_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i)}{\partial \beta_i \partial \beta_i^\top} \right] \bigg\rvert_{\beta_i = \mathbf{0}} \right] \beta_i\right) d \beta_i \end{aligned}\] <p>We can rewrite the above integral in <i>canonical form</i> by defining:</p> \[\begin{aligned} \mathbf{K}_i &amp;= D^{-1}(\tau^2) - \left[ \frac{\partial^2 \ell_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i)}{\partial \beta_i \partial \beta_i^\top} \right] \bigg\rvert_{\beta_i = \mathbf{0}} \\ \mathbf{h}_i &amp;= \left[ \frac{\partial \ell_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i)}{\partial \beta_i} \right] \bigg\rvert_{\beta_i = \mathbf{0}} \end{aligned}\] <p>And:</p> \[\bar{\Sigma}_i^{-1} = \mathbf{K}_i; \hspace{5mm} \bar{\mu}_i = \bar{\Sigma}_i \mathbf{h}_i; \hspace{5mm} g_i = - \frac{1}{2} \bar{\mu}_i^\top \bar{\Sigma}_i^{-1} \bar{\mu}_i - \log\left((2 \pi)^{\frac{q}{2}} \rvert \bar{\Sigma}_i \rvert^{\frac{1}{2}} \right)\] <ul id="claim-5" class="tab" data-tab="bae1e958-660f-4297-9b17-76f8ce33992f" data-name="claim-5"> <li class="active" id="claim-5-claim"> <a href="#">claim </a> </li> <li id="claim-5-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="bae1e958-660f-4297-9b17-76f8ce33992f" data-name="claim-5"> <li class="active"> <p>Then:</p> \[\begin{aligned} \mathcal{L}_q(\mathbf{y}; \alpha, \tau^2) &amp;\approx \prod_{i = 1}^k \left\rvert \mathbb{I}_{q \times q} - D(\tau^2) \left[ \frac{\partial^2 \ell_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i)}{\partial \beta_i \partial \beta_i^\top} \right] \bigg\rvert_{\beta_i = \mathbf{0}}\right\rvert^{-\frac{1}{2}} \left[\mathcal{L}_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i) \right]\bigg\rvert_{\beta_i = \mathbf{0}} \exp\left(\frac{1}{2} \left[\mathbf{h}_i \mathbf{K}_i^{-1} \mathbf{h}_i \right] \right) \\ \implies \ell_q(\mathbf{y}; \alpha, \tau^2) &amp;\approx \sum_{i = 1}^k \left[ -\frac{1}{2} \log\left( \left\rvert \mathbb{I}_{q \times q} - D(\tau^2) \left[ \frac{\partial^2 \ell_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i)}{\partial \beta_i \partial \beta_i^\top} \right] \bigg\rvert_{\beta_i = \mathbf{0}}\right\rvert \right) + \left[ \ell_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i) \right] \bigg\rvert_{\beta_i = \mathbf{0}} + \frac{1}{2} \mathbf{h}_i^\top \mathbf{K}_i^{-1} \mathbf{h}_i \right] \end{aligned}\] </li> <li> \[\begin{aligned} \mathcal{L}_q(\mathbf{y}; \alpha, \tau^2) &amp;\approx \prod_{i = 1}^k (2 \pi)^{-\frac{q}{2}} \rvert D(\tau^2) \rvert^{-\frac{1}{2}} \left[\mathcal{L}_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i) \right] \bigg\rvert_{\beta_i = \mathbf{0}} \underbrace{\int \exp\left( \beta_i^\top \mathbf{h}_i - \frac{1}{2} \beta_i^\top \mathbf{K}_i \beta_i\right) d \beta_i}_{= \exp(-g_i)} \\ &amp;= \prod_{i = 1}^k (2 \pi)^{-\frac{q}{2}} \rvert D(\tau^2) \rvert^{-\frac{1}{2}} \left[\mathcal{L}_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i) \right] \bigg\rvert_{\beta_i = \mathbf{0}} \exp\left(\frac{1}{2}\bar{\mu}_i^\top \bar{\Sigma}_i^{-1}\bar{\mu}_i + \log\left((2 \pi)^{\frac{q}{2}} \rvert \bar{\Sigma}_i \rvert^{\frac{1}{2}}\right) \right) \\ &amp;= \prod_{i = 1}^k (2 \pi)^{-\frac{q}{2}} \rvert D(\tau^2) \rvert^{-\frac{1}{2}} \left[\mathcal{L}_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i) \right]\bigg\rvert_{\beta_i = \mathbf{0}} \exp\left(\frac{1}{2}\bar{\mu}_i^\top \bar{\Sigma}_i^{-1}\bar{\mu}_i\right) (2 \pi)^{\frac{q}{2}} \rvert \bar{\Sigma}_i \rvert^{\frac{1}{2}} \\ &amp;= \prod_{i = 1}^k \rvert D(\tau^2)\rvert^{-\frac{1}{2}} \rvert \bar{\Sigma}_i^{-1} \rvert^{-\frac{1}{2}}\left[\mathcal{L}_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i) \right]\bigg\rvert_{\beta_i = \mathbf{0}} \exp\left(\frac{1}{2}\bar{\mu}_i^\top \bar{\Sigma}_i^{-1}\bar{\mu}_i\right) \\ &amp;= \prod_{i = 1}^k \rvert D(\tau^2) \mathbf{K}_i \rvert^{-\frac{1}{2}} \left[\mathcal{L}_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i) \right]\bigg\rvert_{\beta_i = \mathbf{0}} \exp\left(\frac{1}{2} \left[\mathbf{h}_i (\mathbf{K}_i^{-1})^\top \mathbf{K}_i \mathbf{K}_i^{-1} \mathbf{h}_i \right] \right) \\ &amp;= \prod_{i = 1}^k \left\rvert D(\tau^2) \left[ D^{-1}(\tau^2) - \left[ \frac{\partial^2 \ell_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i)}{\partial \beta_i \partial \beta_i^\top} \right] \bigg\rvert_{\beta_i = \mathbf{0}}\right] \right\rvert^{-\frac{1}{2}} \left[\mathcal{L}_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i) \right]\bigg\rvert_{\beta_i = \mathbf{0}} \exp\left(\frac{1}{2} \left[\mathbf{h}_i \mathbf{K}_i^{-1} \mathbf{h}_i \right] \right) \\ &amp;= \prod_{i = 1}^k \left\rvert \mathbb{I}_{q \times q} - D(\tau^2) \left[ \frac{\partial^2 \ell_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i)}{\partial \beta_i \partial \beta_i^\top} \right] \bigg\rvert_{\beta_i = \mathbf{0}} \right\rvert^{-\frac{1}{2}} \left[\mathcal{L}_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i) \right]\bigg\rvert_{\beta_i = \mathbf{0}} \exp\left(\frac{1}{2} \left[ \frac{\partial \ell_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i)}{\partial \beta_i^\top } \right] \bigg\rvert_{\beta_i = \mathbf{0}} \left[ D^{-1}(\tau^2) - \left[ \frac{\partial^2 \ell_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i)}{\partial \beta_i \partial \beta_i^\top} \right] \bigg\rvert_{\beta_i = \mathbf{0}} \right]^{-1} \left[ \frac{\partial \ell_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i)}{\partial \beta_i} \right] \bigg\rvert_{\beta_i = \mathbf{0}}\right) \end{aligned}\] </li> </ul> <h3 id="an-aside">An Aside</h3> <p>Penalized quasi-likelihood and marginal quasi-likelihood are quite similar, but they do have notable differences. For one, MQL will not result in predictions for the random effects. Thus, it is not suitable for inference at the group level. In addition, the estimates of the fixed effects are essentially for the marginal model. Fitzmaurice et al.<d-cite key="fitzmaurice2011"></d-cite> note that MQL estimates are highly biased unless the random effects variance is near zero, regardless of the number of repeated measurements.</p> <p>In contrast, though PQL can be used to estimate the fixed effects and predict the random effects, there are some complications. For small counts or low numbers of repeated measurements, PQL underestimates the fixed effects as well as the variance components. PQL and MQL can also be reframed as approximations of a GLMM with an adjusted LMM (see below).</p> <p>We can reframe penalized and marginal quasi-likelihood as arising from a Gaussian approximation of the GLMM at hand. We’ll follow Chapter 15 in Fitzmaurice et al.<d-cite key="fitzmaurice2011"></d-cite> We assume to have the following generalized linear mixed model:</p> \[\begin{equation} \label{eq:glmm-y} \begin{aligned} \mathbf{y}_{i,j} \rvert \beta_i &amp;\sim \mathcal{F}(\mu_{i,j}); \\ \mu_{i,j} &amp;= g^{-1}\left(\eta_{i,j}\right) = g^{-1}\left(\alpha^\top \mathbf{x}_{i,j} + \beta_i^\top \mathbf{z}_{i,j}\right) \end{aligned} \end{equation}\] <p>We first approximate the response by assuming it can be represented as its conditional mean plus some Gaussian error:</p> \[\mathbf{y}_{i,j} \approx \mu_{i,j} + \epsilon_{i,j}\] <p>where, for some variance function $V(\cdot)$ of the conditional mean (e.g. $V(\mu_{i,j}) = \mu_{i,j}$ in the Poisson case), the errors satisfy:</p> \[\mathbb{E}\left[ \epsilon_{i,j} \right] = 0; \hspace{5mm} \text{Var}(\epsilon_{i,j} ) = V(\mu_{i,j}); \hspace{5mm} \text{Cov}(\epsilon_{i,j}, \epsilon_{i', j'} ) = 0\] <ul id="claim-6" class="tab" data-tab="570d8bbf-edf6-44c2-bbe3-b3c949c91074" data-name="claim-6"> <li class="active" id="claim-6-claim"> <a href="#">claim </a> </li> <li id="claim-6-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="570d8bbf-edf6-44c2-bbe3-b3c949c91074" data-name="claim-6"> <li class="active"> <p>Since the link function is non-linear, we will linearize the mean with a first-order Taylor approximation about $\hat{\eta}_{i,j}$, the linear predictor estimated under $H_0$:</p> \[\mathbf{y}_{i,j} \approx g^{-1}(\hat{\eta}_{i,j}) + \delta(\hat{\eta}_{i,j}) (\eta_{i,j} - \hat{\eta}_{i,j}) + \epsilon_{i,j}\] <p>where \(\delta(\hat{\eta}_{i,j}) = \frac{\partial g^{-1}(\eta_{i,j})}{\partial \eta_{i,j}}\bigg\rvert_{\eta_{i,j} = \hat{\eta}_{i,j}}\), the gradient of the inverse link function w.r.t $\eta_{i,j}$ evaluated at the estimate under $H_0$.</p> </li> <li> \[\begin{aligned} \mathbf{y}_{i,j} &amp;\approx \left[g^{-1}(\eta_{i,j})\right]\bigg\rvert_{\eta_{i,j} = \hat{\eta}_{i,j}} + \frac{\partial g^{-1}(\eta_{i,j})}{\partial \eta_{i,j}}\bigg\rvert_{\eta_{i,j} = \hat{\eta}^{(c)}_{i,j}} (\eta_{i,j} - \hat{\eta}_{i,j}) + \epsilon_{i,j} \\ &amp;= g^{-1}(\hat{\eta}_{i,j}) + \delta(\hat{\eta}_{i,j}) (\eta_{i,j} - \hat{\eta}_{i,j}) + \epsilon_{i,j} &amp; \left(\delta(\hat{\eta}_{i,j}) = \frac{\partial g^{-1}(\eta_{i,j})}{\partial \eta_{i,j}}\bigg\rvert_{\eta_{i,j} = \hat{\eta}^{(c)}_{i,j}}\right) \end{aligned}\] </li> </ul> <ul id="claim-7" class="tab" data-tab="f2c3af7e-af7b-4572-847b-554c42be9a0f" data-name="claim-7"> <li class="active" id="claim-7-claim"> <a href="#">claim </a> </li> <li id="claim-7-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="f2c3af7e-af7b-4572-847b-554c42be9a0f" data-name="claim-7"> <li class="active"> <p>We can rearrange the above as:</p> \[\mathbf{y}_{i,j}^\star \approx \alpha^\top \mathbf{x}_{i,j} + \beta_i^\top \mathbf{z}_{i,j} + \epsilon^\star_{i,j}\] <p>where \(\mathbf{y}_{i,j}^\star = \frac{1}{\delta(\hat{\eta}_{i,j})}\left(\mathbf{y}_{i,j} - \hat{\mu}_{i,j}\right) + \hat{\eta}_{i,j}\) and \(\epsilon^\star_{i,j} = \frac{1}{\delta(\hat{\eta}_{i,j})}\epsilon_{i,j}\).</p> </li> <li> \[\begin{aligned} &amp;\mathbf{y}_{i,j} \approx g^{-1}(\hat{\eta}_{i,j}) + \delta(\hat{\eta}_{i,j}) (\eta_{i,j} - \hat{\eta}_{i,j}) + \epsilon_{i,j} \\ \implies &amp;\mathbf{y}_{i,j} \approx \hat{\mu}_{i,j} + \delta(\hat{\eta}_{i,j})(\alpha^\top \mathbf{x}_{i,j} + \beta_i^\top \mathbf{z}_{i,j}) - \delta(\hat{\eta}_{i,j})(\hat{\alpha}^\top \mathbf{x}_{i,j} + \mathbf{0}^\top \mathbf{z}_{i,j}) + \epsilon_{i,j} \\ \implies &amp;\mathbf{y}_{i,j} + \delta(\hat{\eta}_{i,j})(\hat{\alpha}^\top \mathbf{x}_{i,j}) \approx \hat{\mu}_{i,j} + \delta(\hat{\eta}_{i,j})(\alpha^\top \mathbf{x}_{i,j} + \beta_i^\top \mathbf{z}_{i,j}) + \epsilon_{i,j} \\ \implies &amp;\mathbf{y}_{i,j} - \hat{\mu}_{i,j} + \delta(\hat{\eta}_{i,j})(\hat{\alpha}^\top \mathbf{x}_{i,j}) \approx \delta(\hat{\eta}_{i,j})(\alpha^\top \mathbf{x}_{i,j} + \beta_i^\top \mathbf{z}_{i,j}) + \epsilon_{i,j} \\ \implies &amp;\frac{1}{\delta(\hat{\eta}_{i,j})}\left(\mathbf{y}_{i,j} - \hat{\mu}_{i,j}\right) + \hat{\alpha}^\top \mathbf{x}_{i,j} \approx \alpha^\top \mathbf{x}_{i,j} + \beta_i^\top \mathbf{z}_{i,j} + \frac{1}{\delta(\hat{\eta}_{i,j})}\epsilon_{i,j} \\ \implies &amp;\frac{1}{\delta(\hat{\eta}_{i,j})}\left(\mathbf{y}_{i,j} - \hat{\mu}_{i,j}\right) + \hat{\eta}_{i,j} \approx \alpha^\top \mathbf{x}_{i,j} + \beta_i^\top \mathbf{z}_{i,j} + \frac{1}{\delta(\hat{\eta}_{i,j})}\epsilon_{i,j} \end{aligned}\] </li> </ul> <p>Notice that since \(\epsilon^*_{i,j}\) is just a scaled version of $\epsilon_{i,j}$, it is also Gaussian with:</p> \[\begin{aligned} \mathbb{E}\left[ \epsilon^\star_{i,j} \right] &amp;= \mathbb{E}\left[ \frac{1}{\delta(\hat{\eta}_{i,j})} \epsilon_{i,j} \right] = 0; \\ \text{Var}\left(\epsilon^\star_{i,j} \right) &amp;= \text{Var}\left( \frac{1}{\delta(\hat{\eta}_{i,j})} \epsilon_{i,j} \right) = \frac{1}{\delta^2(\hat{\eta}_{i,j})} \text{Var}(\epsilon_{i,j}) = \frac{V(\hat{\mu}_{i,j})}{\delta^2(\hat{\eta}_{i,j})} \end{aligned}\] <p>The above essentially specifies a linear mixed model:</p> \[\mathbf{y}^\star_{i,j} \sim \mathcal{N}\left(\alpha^\top \mathbf{x}_{i,j} + \beta_i^\top \mathbf{z}_{i,j}, \frac{V(\hat{\mu}_{i,j})}{\delta^2(\hat{\eta}_{i,j})}\right)\] <p>To use this in practice, we can first estimate the parameters with iteratively reweighted least squares (or something like that), then use the estimates to compute the working response and errors. Then we can proceed how we would with a linear mixed model.</p>]]></content><author><name></name></author><category term="regression"/><category term="likelihood"/><category term="models"/><summary type="html"><![CDATA[A Primer]]></summary></entry><entry><title type="html">Generalized Linear Models</title><link href="https://aerosengart.github.io/blog/2025/glm/" rel="alternate" type="text/html" title="Generalized Linear Models"/><published>2025-06-03T00:00:00+00:00</published><updated>2025-06-03T00:00:00+00:00</updated><id>https://aerosengart.github.io/blog/2025/glm</id><content type="html" xml:base="https://aerosengart.github.io/blog/2025/glm/"><![CDATA[<p>This post is a primer on generalized linear models and their associated estimation procedures.</p> <h2 id="set-up">Set-Up</h2> <p>Let’s assume we have covariate matrix, $\mathbf{X}$, response vector $\mathbf{y}$, and parameter (coefficient) vector, $\beta$, and error vector, $\epsilon$, given by:</p> \[\mathbf{X} = \begin{bmatrix} 1 &amp; x_{1,1} &amp; \dots &amp; x_{1, m} \\ 1 &amp; x_{2, 1} &amp; \dots &amp; x_{2, m} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 1 &amp; x_{n, 1} &amp; \dots &amp; x_{n, m} \end{bmatrix}, \hspace{8mm} \mathbf{y} = \begin{bmatrix} y_1 \\ \vdots \\ y_n \end{bmatrix}, \hspace{8mm} \beta = \begin{bmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_m \end{bmatrix}, \hspace{8mm} \epsilon = \begin{bmatrix} \epsilon_1 \\ \vdots \\ \epsilon_n \end{bmatrix}\] <p>Assume that the following holds:</p> \[\begin{equation} \label{eq:assumption-1} g(\mathbf{y}) = \mathbf{X} \beta + \epsilon; \hspace{5mm} \epsilon_i \overset{iid}{\sim} F \end{equation}\] <p>where $g(\cdot)$ is some function (called a <i>link</i> function) and $F$ is some noise distribution. Equivalently, we can assume:</p> \[\begin{equation} \label{eq:assumption-2} \mathbb{E}\left[ g(\mathbf{y}) \rvert \mathbf{X} \right] = \mathbf{X}\beta + \mathbb{E}[\epsilon] \end{equation}\] <p>For the rest of the post, we’ll use $\mathbf{x}_i$ to denote:</p> \[(1, x_{i, 1}, \dots, x_{i, m})^\top\] <hr/> <h2 id="linear-models">Linear Models</h2> <p>Let’s first cover the basics of linear regression, which will be a good base upon which to build the theory of generalized linear models. For basic linear regression, we assume that $g(\cdot)$ is thhe identity function (i.e. $g(\mathbf{y}) = \mathbf{y}$) and that, conditional on $\mathbf{X}$, $F = \mathcal{N}(0, \sigma^2)$.</p> <h3 id="least-squares">Least Squares</h3> <p>We usually estimate $\beta$ via <i>ordinary least squares (OLS)</i>, which provides a closed form solution. The least squares objective is to minimize the <i>residual sum of squares</i>:</p> \[RSS(\beta) = \sum_{i = 1}^n (y_i - \mathbf{x}_i^\top \beta )^2 = (\mathbf{y} - \mathbf{X} \beta)^\top (\mathbf{y} - \mathbf{X} \beta)\] <p>The above function is quadratic in $\beta$, so we can differentiate with respect to $\beta$, set that equal to $0$, and solve for $\beta$ to find a minimizer:</p> \[\begin{aligned} &amp;\mathbf{X}^\top(\mathbf{y} - \mathbf{X} \beta) = 0 \\ \implies &amp;\mathbf{X}^\top \mathbf{y} - \mathbf{X}^\top \mathbf{X} \beta = 0 \\ \implies &amp;\hat{\beta}_{OLS} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y} \end{aligned}\] <p>This solution is unique if $\mathbf{X}^\top \mathbf{X}$ is non-singular.</p> <h4 id="generalized-least-squares">Generalized Least Squares</h4> <p><a href="https://en.wikipedia.org/w/index.php?title=Generalized_least_squares&amp;oldid=1292245629">Generalized least squares (GLS)</a> is a method to estimate the parameters in a linear regression model when there is correlation between the errors. Suppose we are in the linear regression setting as before except we assume that we have a <i>known</i> and non-singular conditional covariance matrix, $\Omega$, for the errors. That is:</p> \[\text{Cov}(\epsilon \rvert \mathbf{X}) = \Omega\] <p>GLS estimates $\beta$ through the following optimization problem:</p> \[\begin{aligned} \hat{\beta}_{GLS} &amp;= \underset{\beta}{\arg \min} \left\{ (\mathbf{y} - \mathbf{X} \beta)^\top \Omega^{-1} (\mathbf{y} - \mathbf{X}\beta) \right\} \\ &amp;= \underset{\beta}{\arg \min} \left\{ -2 \beta^\top \mathbf{X}^\top \Omega^{-1} \mathbf{y} + \beta^\top \mathbf{X}^\top \Omega^{-1} \mathbf{X} \beta \right\} \end{aligned}\] <p>The above expression is quadratic in $\beta$, so we take the gradient, set it equal to zero, and solve for $\beta$:</p> \[\begin{aligned} 0 &amp;= -2\mathbf{X}^\top \Omega^{-1} \mathbf{y} + 2\mathbf{X}^\top \Omega^{-1} \mathbf{X} \beta \\ \implies \hat{\beta}_{GLS} &amp;= (\mathbf{X}^\top \Omega{-1} \mathbf{X})^{-1} \mathbf{X}^\top \Omega{-1} \mathbf{y} \end{aligned}\] <p>Just like in OLS, the GLS estimator of $\beta$ is unbiased, and its conditional covariance matrix also has a similiar form:</p> \[\mathbb{E}[\hat{\beta}_{GLS} \rvert \mathbf{X}] = \beta, \hspace{8mm} \text{Cov}(\hat{\beta}_{GLS} \rvert \mathbf{X}) = (\mathbf{X}^\top \Omega^{-1} \mathbf{X})^{-1}\] <h4 id="weighted-least-squares">Weighted Least Squares</h4> <p><a href="https://en.wikipedia.org/w/index.php?title=Weighted_least_squares&amp;oldid=1279139018">Weighted least squares (WLS)</a> is equivalent to generalized least squares when $\Omega$ is a diagonal matrix. However, we’ll discuss it in more detail here for completeness.</p> <p>The idea behind WLS is that we may have more or less confidence in the reliability of our observations, so we want to weight their contributions to the objective function during estimation. Let $\mathbf{W}$ be an $n \times n$ diagonal matrix of weights. The WLS goal is to find:</p> \[\hat{\beta}_{WLS} = \underset{\beta}{\arg \min} \left\{ (\mathbf{y} - \mathbf{X} \beta)^\top \mathbf{W} (\mathbf{y} - \mathbf{X} \beta) \right\}\] <p>Since this is quadratic in $\beta$, we take the derivative with respect to $\beta$. Solving the following for $\beta$ will give us our estimate:</p> \[\begin{equation} \label{eq:wls-obj} \frac{\partial}{\partial \beta} \left[ (\mathbf{y} - \mathbf{X} \beta)^\top \mathbf{W} (\mathbf{y} - \mathbf{X} \beta) \right] = -2 \mathbf{X}^\top \mathbf{W} (\mathbf{y} - \mathbf{X}\beta) = 0 \end{equation}\] <p>As we showed in the <a href="#generalized-least-squares">generalized least squares section</a>, the solution to this problem is to set:</p> \[\hat{\beta}_{WLS} = (\mathbf{X}^\top \mathbf{W} \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{W} \mathbf{y}\] <p>However, this solution isn’t finished because we don’t know what $\mathbf{W}$ is! By the same argument in the OLS case, $\hat{\beta}_{WLS}$ is unbiased for $\beta$ for any $\mathbf{W}$, so how can we determine what the best weight matrix is?</p> <p>One way is to minimize the (weighted) mean squared error. For any vector of weights $\lambda = (\lambda_0, \lambda_1, \dots, \lambda_m)^\top$, we want to find the $\mathbf{W}$ such that $\hat{\beta}_{WLS}$ minimizes the expression:</p> \[\mathbb{E}\left[ (\lambda^\top(\hat{\beta}_{WLS} - \beta))^2 \right] = \text{Var}(\lambda^\top \hat{\beta}_{WLS})\] <p>where the equality follows from the unbiasedness of the WLS estimator.</p> <h3 id="likelihood">Likelihood</h3> <p>Since we have assumed a particular distribution for the errors, we can also estimate the coefficient vector with maximum likelihood (see <a href="/posts/2025/02/03/likelihood-theory.html">my likelihood post</a> for details). Using Eq. \eqref{eq:assumption-2}, our assumption is:</p> \[\mathbb{E}\left[ \mathbf{y} \rvert \mathbf{X} \right] = \mathbf{X}\beta\] <p>Since our observations are i.i.d., the (full) log-likelihood (technically conditional on $\mathbf{X}$…) is:</p> \[\begin{aligned} \ell(\mathbf{y}; \mathbf{X}, \beta) &amp;= \log\left( \prod_{i = 1}^n \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(- \frac{(y_i - \mathbf{x}_i \beta)^2}{2 \sigma^2} \right)\right) \\ &amp;= \frac{n}{\sqrt{2\pi \sigma^2}} - \frac{1}{2 \sigma^2}\sum_{i = 1}^n (y_i - \mathbf{x}_i \beta)^2 \end{aligned}\] <p>Taking the derivative with respect to $\beta$, setting this equal to zero, and solving for $\beta$ gives us our maximum likelihood estimate:</p> \[\begin{aligned} \frac{\partial \ell(\mathbf{y}; \mathbf{X}, \beta)}{\partial \beta} &amp;= - \frac{2}{2 \sigma^2}\sum_{i = 1}^n \mathbf{x}_i^\top (y_i - \mathbf{x}_i \beta) \\ &amp;= 0 \\ \implies \mathbf{X}^\top (\mathbf{y} - \mathbf{X} \beta) &amp;= 0 \\ \implies \hat{\beta}_{MLE} &amp;= (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y} \end{aligned}\] <p>We’ve just shown that the OLS and ML estimates of the parameters are equivalent!</p> <h3 id="uncertainty-quantification">Uncertainty Quantification</h3> <p>Often a simple estimate of $\beta$ is not enough; we want to be able to say how confident we are in our estimation. This is usually done through confidence intervals, which require estimates of the mean and variance of our estimates.</p> <p>Let $\hat{\beta}$ denote the OLS (and ML since they are equivalent) estimator for $\beta$. The mean vector of $\hat{\beta}$ is given by:</p> \[\begin{aligned} \mathbb{E}\left[ \hat{\beta} \rvert \mathbf{X} \right] &amp;= (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y} \\ &amp;= (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbb{E}\left[ \mathbf{y} \rvert \mathbf{X} \right] \\ &amp;= (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{X} \beta \\ &amp;= \beta \end{aligned}\] <p>We can similarly derive the covariance matrix of $\hat{\beta}$ as:</p> <ul id="covar-claim" class="tab" data-tab="b4e86faa-6a09-4fa8-967e-c2e9685a1ccd" data-name="covar-claim"> <li class="active" id="covar-claim-claim"> <a href="#">claim </a> </li> <li id="covar-claim-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="b4e86faa-6a09-4fa8-967e-c2e9685a1ccd" data-name="covar-claim"> <li class="active"> \[\text{Cov}(\hat{\beta} \rvert \mathbf{X}) = \sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1}\] </li> <li> <p>Let $\mathbf{X}$ be fixed, so we can drop the conditioning. Using the fact that $\mathbb{E}[\hat{\beta}] = \beta$, we see that:</p> \[\begin{aligned} \text{Cov}(\hat{\beta}) &amp;= \mathbb{E}\left[ (\hat{\beta} - \beta) (\hat{\beta} - \beta)^\top \right] \\ &amp;= \mathbb{E}\left[(\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y} \mathbf{y}^\top \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1} \right] - 2 \mathbb{E}\left[ \hat{\beta} \right]\beta^\top + \beta \beta^\top\\ &amp;= (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbb{E}\left[ \mathbf{y} \mathbf{y}^\top \right] \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1} - \beta \beta^\top \\ &amp;= (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbb{E}\left[ (\mathbf{X} \beta + \epsilon)(\mathbf{X} \beta + \epsilon)^\top \right] \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1} - \beta \beta^\top\\ &amp;= (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{X} \beta \beta^\top \mathbf{X}^\top \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1} - 2 (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{X} \beta \mathbb{E}\left[ \epsilon \right] \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1} + (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbb{E}\left[ \epsilon \epsilon^\top \right]\mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1} - \beta \beta^\top \\ &amp;= \beta \beta^\top + (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top (\sigma^2 \mathbb{I}_{n \times n}) \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1} - \beta \beta^\top \\ &amp;= \sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1} \end{aligned}\] </li> </ul> <p>Putting the above results together, we see that the estimator has a Gaussian distribution:</p> \[\hat{\beta} \sim \mathcal{N}(\beta, \sigma^2 (\mathbf{X}^\top \mathbf{X}){-1})\] <p>We can use this information for additional inference about the parameters (e.g. hypothesis testing).</p> <hr/> <h2 id="generalized-linear-models">Generalized Linear Models</h2> <p>Generalized linear models are not that different from classical linear regression models. In this case, we assume that $\mathbf{y}$ follow a distribution from an overdispersed exponential family (e.g. Poisson, Binomial, exponential, Gaussian, etc.).<d-cite key="mccullagh1989"></d-cite></p> <div id="exponential-family"></div> <div class="definition"> <body> <strong>Definition (Overdispersed Exponential Family).</strong> <br/> Let $\theta \in \mathbb{R}^{m \times 1}$ be a parameter vector. We say that the distribution of random vector $\mathbf{y} \in \mathbb{R}^{n \times 1}$ comes from an <i>overdispersed exponential family</i> if the probability density/mass function can be written as: $$ \begin{aligned} \label{eq:exp-fam} f_Y(\mathbf{y} \rvert \theta, \tau) = \prod_{i = 1}^n h(\mathbf{y}_i, \tau_i) \exp\left(\frac{\mathbf{b}(\theta_i) \mathbf{T}(\mathbf{y}_i) - A(\theta_i)}{d(\tau_i)} \right) \end{aligned} $$ <ul> <li>$\tau$ is the <i>dispersion parameter</i>, usually related to the variance somehow.</li> <li>$d(\tau_i)$ is some function of $\tau_i$ used to introduce the overdispersion, often taken to be $\frac{\tau}{\omega_i}$.</li> <li>$h(\mathbf{y}, \tau)$ is a non-negative function. </li> <li>The vector $\mathbf{b}(\theta)$ is a function of the parameter vector. If $\mathbf{b}(\theta) = \theta$, then we say the distribution is in <i>canonical form</i>. Any distribution can be written in canonical form by letting $\theta' = \theta$ and then using $\mathbf{b}(\theta') = \theta$.</li> <li>The vector $\mathbf{T}(\mathbf{y})$ is a sufficient statistic, meaning $f_Y(\mathbf{f} \rvert \mathbf{T}(\mathbf{y}))$ is independent of $\theta$.</li> <li>The scalar $A(\theta)$ is the <i>log-partition function</i> and is the natural logarithm of the normalization constant needed so that the above function integrates to one.</li> </ul> All $h(\mathbf{y}, \tau)$, $\mathbf{b}(\theta)$, $\mathbf{T}(\mathbf{y})$, $A(\theta)$, and $d(\tau)$ are assumed to be known. However, these functions are non-unique! For example, you can easily scale $\mathbf{T}(\theta)$ one by a constant and scale $\mathbf{b}(\theta)$ by the reciprocal. </body> </div> <h3 id="model">Model</h3> <p>We’ll assume the response, $\mathbf{y}$, comes from an exponential family distribution so that the likelihood has the form in Eq. \eqref{eq:exp-fam}. We’ll also take $\mathbf{T}(\mathbf{y}) = \mathbf{y}$, $\mathbf{b}(\theta) = \theta$, and we’ll rewrite $h(\mathbf{y}, \tau)$ as $\exp(\log(h(\mathbf{y}, \tau)))$ so we can bring it inside the exponential function. Taking the natural logarithm gives us the log-likelihood:</p> \[\begin{equation} \label{eq:log-lik-glm} \begin{aligned} \ell(\mathbf{y}; \theta, \tau) &amp;= \log(f_Y(\mathbf{y} \rvert \theta, \tau)) \\ &amp;= \sum_{i = 1}^n \left[ \frac{\theta_i \mathbf{y}_i - A(\theta_i)}{d(\tau_i)} + \log(h(\mathbf{y}_i, \tau_i))\right] \end{aligned} \end{equation}\] <p>Taking the first- and second-order derivatives of the log-likelihood with respect to the parameter $\theta$ gives us the gradient and Hessian:</p> \[\begin{equation} \label{eq:grad-hessian} \begin{aligned} \frac{\partial \ell(\mathbf{y}; \theta, \tau)}{\partial \theta} &amp;= \begin{bmatrix} \frac{\mathbf{y}_1 - \frac{d A(\theta_1)}{d \theta_1}}{d(\tau_1)} \\ \vdots \\ \frac{\mathbf{y}_n - \frac{d A(\theta_n)}{d \theta_1}}{d(\tau_n)} \\ \end{bmatrix} \\ &amp;= \text{diag}\left( \frac{1}{d(\tau)} \right) \left[ \mathbf{y} - \frac{\partial A(\theta)}{\partial \theta}\right] \\ \frac{\partial^2 \ell(\mathbf{y}; \theta, \tau)}{\partial \theta \partial \theta^\top} &amp;= \text{diag}\left( \frac{1}{d(\tau)} \right) \begin{bmatrix} - \frac{d^2(A(\theta_1))}{d \theta_1 d \theta_1} &amp; \dots &amp; - \frac{d^2(A(\theta_1))}{d \theta_1 d \theta_n} \\ \vdots &amp; \ddots &amp; \vdots \\ - \frac{d^2(A(\theta_n))}{d \theta_n d \theta_1} &amp; \dots &amp; - \frac{d^2(A(\theta_n))}{d \theta_n d \theta_n} \end{bmatrix} \\ &amp;= -\text{diag}\left( \frac{1}{d(\tau)} \right)\frac{\partial^2 A(\theta)}{\partial \theta \partial \theta^\top} \end{aligned} \end{equation}\] <p>where we let $\tau = (\tau_1, \dots, \tau_n)^\top$.</p> <ul id="mean-var-glm" class="tab" data-tab="a8ed1c84-3760-4d62-a2fa-93d29a90152f" data-name="mean-var-glm"> <li class="active" id="mean-var-glm-claim"> <a href="#">claim </a> </li> <li id="mean-var-glm-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="a8ed1c84-3760-4d62-a2fa-93d29a90152f" data-name="mean-var-glm"> <li class="active"> <p>Using properties of the likelihood function, we can derive the mean (denoted by $\mu$) and variance of $\mathbf{y}$ as:</p> \[\begin{equation} \label{eq:mean-var-glm} \begin{aligned} \mu &amp;= \mathbb{E}[\mathbf{y}] = \frac{\partial A(\theta)}{\partial \theta} \\ \text{Cov}(\mathbf{y}) &amp;= \text{diag}\left( d(\tau) \right) \underbrace{\frac{\partial^2 A(\theta)}{\partial \theta \partial \theta^\top}}_{= V(\mu)} \end{aligned} \end{equation}\] </li> <li> <p>Under certain regularity conditions, which we’ll assume hold here, the expectation of the score function (the gradient of the log-likelihood) should equal zero (see <a href="/blog/2025/likelihood-theory">my likelihood post</a> for a discussion on this). Thus:</p> \[\begin{aligned} &amp;\mathbb{E}\left[ \frac{\partial \ell(\mathbf{y}; \theta, \tau)}{\partial \theta} \right] = \mathbf{0} \\ \implies &amp;\text{diag}\left( \frac{1}{d(\tau)} \right) \left(\mathbb{E}[\mathbf{y}] - \frac{\partial A(\theta)}{\partial \theta} \right) = \mathbf{0} \\ \implies &amp;\mathbb{E}[\mathbf{y}] = \frac{\partial A(\theta)}{\partial \theta} \end{aligned}\] <p>Under these conditions, we also have that the Fisher information (the variance of the score) is the negative expectation of the Hessian. Thus:</p> \[\begin{aligned} &amp;\mathbb{E}\left[ \frac{\partial \ell(\mathbf{y}; \theta, \tau)}{\partial \theta} \frac{\partial \ell(\mathbf{y}; \theta, \tau)}{\partial \theta^\top} \right] = -\mathbb{E}\left[\frac{\partial^2 \ell(\mathbf{y}; \theta, \tau)}{\partial \theta \partial \theta^\top} \right] \\ \implies &amp;\mathbb{E}\left[ \text{diag}\left( \frac{1}{d^2(\tau)} \right) \left(\mathbf{y} - \frac{\partial A(\theta)}{\partial \theta}\right)\left(\mathbf{y} - \frac{\partial A(\theta)}{\partial \theta}\right)^\top\right] = - \mathbb{E}\left[ -\text{diag}\left( \frac{1}{d(\tau)} \right)\frac{\partial^2 A(\theta)}{\partial \theta \partial \theta^\top} \right] \\ \implies &amp; \text{diag}\left( \frac{1}{d^2(\tau)} \right)\mathbb{E}\left[ \left(\mathbf{y} -\mathbb{E}[\mathbf{y}]\right)\left(\mathbf{y} - \mathbb{E}[\mathbf{y}]\right)^\top\right] = \text{diag}\left( \frac{1}{d(\tau)} \right)\frac{\partial^2 A(\theta)}{\partial \theta \partial \theta^\top} \\ \implies &amp;\text{Cov}(\mathbf{y}) = \text{diag}\left(d(\tau)\right) \frac{\partial^2 A(\theta)}{\partial \theta \partial \theta^\top} \end{aligned}\] </li> </ul> <p>Eq. \eqref{eq:mean-var-glm} shows that the covariance matrix of $\mathbf{y}$ is the product of a function of the dispersion parameter and a function of the parameter vector $\theta$. In the literature, $\frac{\partial^2 A(\theta)}{\partial \theta \partial \theta^\top}$ is often referred to as the <i>variance function</i> and denoted by $V(\mu)$ (where $\mu$ is the mean $\mathbb{E}[\mathbf{y}]$) since a function of the parameters is just a function of the mean as $\mathbf{X}$ is fixed.</p> <p>Arguably more importantly, the righthand side of $\mu = \frac{\partial A(\theta)}{\partial \theta}$ is some function of $\theta$, which we’ll denote by $s^{-1}(\theta)$. This lets us link the mean with the canonical parameter through the expression:</p> \[\begin{equation} \label{eq:link-mean-param} \theta = s(\mu) \end{equation}\] <p>Recall that we assumed that $\mathbb{E}\left[ g(\mathbf{y}) \rvert \mathbf{X} \right] = \mathbf{X} \beta$, which is equivalent to the assumption that $\mu = g^{-1}(\mathbf{X}\beta)$. Thus, we can relate the likelihood parameter, $\theta$, to the regression parameters, $\beta$, by:</p> \[\begin{equation} \label{eq:link-param-beta} \theta = s(g^{-1}(\mathbf{X}\beta)) \end{equation}\] <p>Now we have everything we need for our generalized linear model!</p> <h3 id="estimation">Estimation</h3> <p>The parameter estimates for generalized linear models are generally found through maximum likelihood methods. Since we assume a distribution from an exponential family, we can (usually) write a likelihood function as in Eq. \eqref{eq:log-lik-glm}:</p> \[\ell(\mathbf{y}; \theta, \tau) = \sum_{i = 1}^n \left[ \frac{\theta_i^\top \mathbf{y}_i - A(\theta_i)}{d(\tau_i)} + \log(h(\mathbf{y}_i, \tau_i)) \right]\] <p>As we usually do, we want to take the gradient with respect to the parameters of interest, $\beta$. However, this expression is not in terms of $\beta$, so we use the chain rule. The maximum likelihood estimate of $\beta$ is a solution to:</p> \[\begin{equation} \label{eq:score-beta} \begin{aligned} \frac{\partial \ell(\mathbf{y}; \theta, \tau)}{\partial \beta} &amp;= \frac{\partial \theta}{\partial \beta} \frac{\partial \ell(\mathbf{y}; \theta, \tau)}{\partial \theta} = \mathbf{0} \\ \implies \frac{\partial \ell(\mathbf{y}; \theta, \tau)}{\partial \beta} &amp;= \text{diag}\left( d(\tau) \right)\frac{\partial \theta}{\partial \beta} \left(\mathbf{y} - \frac{\partial A(\theta)}{\partial \theta}\right) = \mathbf{0} &amp; \left(\text{Eq. }\eqref{eq:grad-hessian} \right) \\ \implies \frac{\partial \ell(\mathbf{y}; \theta, \tau)}{\partial \beta} &amp;= \text{diag}\left( d(\tau) \right)\frac{\partial \theta}{\partial \beta} \left(\mathbf{y} - g^{-1}(\mathbf{X} \beta) \right) = \mathbf{0} &amp; \left(\text{Eq. }\eqref{eq:mean-var-glm} \right) \end{aligned} \end{equation}\] <ul id="d-theta-d-beta" class="tab" data-tab="3b8a0284-cc60-46f0-9075-04c631f2eebc" data-name="d-theta-d-beta"> <li class="active" id="d-theta-d-beta-claim"> <a href="#">claim </a> </li> <li id="d-theta-d-beta-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="3b8a0284-cc60-46f0-9075-04c631f2eebc" data-name="d-theta-d-beta"> <li class="active"> <p>To find $\frac{\partial \theta}{\partial \beta}$, we use the chain rule again:</p> \[\begin{equation} \label{eq:d-theta-d-beta} \frac{\partial \theta}{\partial \beta} = \frac{\partial \eta}{\partial \beta} \frac{\partial \theta}{\partial \eta} = \text{diag}\left( \frac{1}{d(\tau)} \right)\mathbf{X}^\top \text{Cov}^{-1}(\mathbf{y})\left[ \frac{\partial g(\mu)}{\partial \mu} \right]^{-1} \end{equation}\] </li> <li> <p>The first derivative is easy:</p> \[\frac{\partial \eta}{\partial \beta} = \frac{\partial}{\partial \beta} [\mathbf{X} \beta] = \mathbf{X}^\top\] <p>The second is a bit trickier. By the chain rule:</p> \[\frac{\partial \theta}{\partial \eta} = \frac{\partial \theta}{\partial \mu} \frac{\partial \mu}{\partial \eta}\] <p>Using the fact that $\theta = s(\mu)$ and $\mu = \frac{\partial A(\theta)}{\partial \theta}$, we get:</p> \[\begin{aligned} &amp;\theta = s(\mu) \\ \implies &amp;\frac{\partial \theta}{\partial \theta} = \frac{\partial s(\mu)}{\partial \theta} = \frac{\partial s(\mu)}{\partial \mu} \frac{\partial \mu}{\partial \theta} \\ \implies &amp;\mathbb{I}_{n \times n} = \frac{\partial s(\mu)}{\partial \mu} \frac{\partial}{\partial \theta} \left[ \frac{\partial A(\theta)}{\partial \theta} \right] = \frac{\partial s(\mu)}{\partial \mu} \frac{\partial^2 A(\theta)}{\partial \theta \partial \theta^\top} \end{aligned}\] <p>Using Eq. \eqref{eq:mean-var-glm}, we get:</p> \[\begin{aligned} &amp;\mathbb{I}_{n \times n} = \frac{\partial s(\mu)}{\partial \mu} \\ \implies &amp;\text{diag}\left( \frac{1}{d(\tau)} \right) \text{Cov}(\mathbf{y}) \\ \implies &amp;\text{diag}\left( d(\tau) \right) \text{Cov}^{-1}(\mathbf{y}) = \frac{\partial s(\mu)}{\partial \mu} = \frac{\partial \theta}{\partial \mu} \end{aligned}\] <p>We can do the same sort of trick for $\frac{\partial \mu}{\partial \eta}$:</p> \[\begin{aligned} &amp;\mu = g^{-1}(\eta) \\ \implies &amp;\eta = g(\mu) \\ \implies &amp;\frac{\partial \eta}{\partial \eta} = \frac{\partial g(\mu)}{\partial \eta} = \frac{\partial g(\mu)}{\partial \mu} \frac{\partial \mu}{\partial \eta} \\ \implies &amp;\mathbb{I}_{n \times n} = \frac{\partial g(\mu)}{\partial \mu} \frac{\partial \mu}{\partial \eta} \\ \implies &amp;\frac{\partial \mu}{\partial \eta} = \left[ \frac{\partial g(\mu)}{\partial \mu} \right]^{-1} \end{aligned}\] <p>Putting the two results together, we get: \(\frac{\partial \theta}{\partial \eta} = \text{diag}\left( d(\tau) \right) \text{Cov}^{-1}(\mathbf{y}) \left[ \frac{\partial g(\mu)}{\mu} \right]^{-1}\)</p> </li> </ul> <p>Using the fact that each component of $g(\mu)$ uses only the corresponding component in $\mu$, we see that $\left[ \frac{\partial g(\mu)}{\partial \mu} \right]^{-1}$ is diagonal, so the inverse is the matrix where we take the reciprocal of the diagonal elements. The same goes for $\text{Cov}^{-1}(\mathbf{y})$ since we assumed independent data (i.i.d. errors).</p> <p>Plugging Eq. \eqref{eq:d-theta-d-beta} into Eq. \eqref{eq:score-beta}, our objective becomes solving:</p> \[\begin{equation} \label{eq:new-obj} \frac{\partial \ell(\mathbf{y}; \theta, \tau)}{\partial \beta} = \mathbf{X}^\top \text{Cov}^{-1}(\mathbf{y})\left[ \frac{\partial g(\mu)}{\partial \mu} \right]^{-1} \left(\mathbf{y} - g^{-1}(\mathbf{X} \beta) \right) = \mathbf{0} \end{equation}\] <p>Unfortunately, there may not be a closed form solution to the above equation since $g^{-1}(\cdot)$ may be non-linear. To get around this problem, we do a linear approximation of $g^{-1}(\mathbf{X}\beta)$ and optimize this instead.</p> <p>Let $\hat{\beta}^{(t)}$ be a guess for the optimal value of $\beta$. Also let \(\eta^{(t)} = \mathbf{X}\beta^{(t)}\) be the linear predictor evaluated at this value. We can then write the first order Taylor approximation of \(g^{-1}(\eta)\) as:</p> \[g^{-1}(\eta) \approx g^{-1}(\eta^{(t)}) + \frac{\partial g^{-1}(\eta)}{\partial \eta} \bigg\rvert_{\eta = \eta^{(t)}} (\eta - \eta^{(t)}) = g^{-1}(\eta^{(t)}) + \mathbf{W}^{(t)}(\eta - \eta^{(t)}) \label{eq:taylor-mu}\] <p>where $\mathbf{W}^{(t)} = \frac{\partial g^{-1}(\eta)}{\partial \eta} \bigg\rvert_{\eta = \eta^{(t)}}$ is the matrix of first order partial derivatives of $\mu$ with respect to $\eta$ evaluated at our guess for $\eta$, $\eta^{(t)}$. This matrix is diagonal since any component, $\mu_i$, of the mean vector is only a function of one corresponding component of the linear predictor $\eta_i$.</p> <p>We now use this approximation into Eq. \eqref{eq:new-obj}:</p> \[\begin{aligned} \frac{\partial \ell(\mathbf{y}; \theta, \tau)}{\partial \beta} &amp;\approx \mathbf{X}^\top \text{Cov}^{-1}(\mathbf{y})\left[ \frac{\partial g(\mu)}{\partial \mu} \right]^{-1}\bigg\rvert_{\eta = \eta^{(t)}} \left(\mathbf{y} - g^{-1}(\eta^{(t)}) + \mathbf{W}^{(t)} \left( \eta^{(t)} - \eta \right) \right) \\ &amp;= \mathbf{X}^\top \text{Cov}^{-1}(\mathbf{y})\left[ \frac{\partial g(\mu)}{\partial \mu} \right]^{-1}\bigg\rvert_{\eta = \eta^{(t)}} \mathbf{W}^{(t)} \left([\mathbf{W}^{(t)}]^{-1} (\mathbf{y} - g^{-1}(\eta^{(t)})) + \eta^{(t)} - \eta \right) \\ &amp;= \mathbf{X}^\top \text{Cov}^{-1}(\mathbf{y})\left[ \frac{\partial g(\mu)}{\partial \mu} \right]^{-2} \bigg\rvert_{\eta = \eta^{(t)}} \left(\frac{\partial g(\mu)}{\partial \mu}\bigg\rvert_{\eta = \eta^{(t)}} (\mathbf{y} - g^{-1}(\eta^{(t)})) + \eta^{(t)} - \eta \right) &amp; \left(\frac{\partial \mu}{\partial \eta} \bigg\rvert_{\eta = \eta^{(t)}} = \frac{\partial g^{-1}(\eta)}{\partial \eta} \bigg\rvert_{\eta = \eta^{(t)}} = \mathbf{W}^{(t)} = \left[\frac{\partial g(\mu)}{\partial \mu} \right]^{-1} \bigg\rvert_{\eta = \eta^{(t)}} \right) \end{aligned} \nonumber\] <p>Letting $\mathbf{z}^{(t)} = \frac{\partial g(\mu)}{\partial \mu} \bigg\rvert_{\eta = \eta^{(t)}} (\mathbf{y} - g^{-1}(\eta^{(t)})) + \eta^{(t)}$, and defining $\tilde{\mathbf{W}}^{(t)} = \text{Cov}^{-1}(\mathbf{y})\left[ \frac{\partial g(\mu)}{\partial \mu} \right]^{-2}\bigg\rvert_{\eta = \eta^{(t)}}$, we can rewrite this as:</p> \[\frac{\partial \ell(\mathbf{y}; \theta, \tau)}{\partial \beta} \approx \mathbf{X}^\top \tilde{\mathbf{W}}^{(t)} \left(\mathbf{z}^{(t)} - \eta \right)\] <p>Notice that setting the above equal to zero is exactly Eq. \eqref{eq:wls-obj} with $\mathbf{z}^{(t)}$ instead of $\mathbf{y}$, so we can use weighted least squares. Though we have a closed form solution, since we are using an approximation, we need to iterate the above process until convergence. So at iteration $t + 1$, we will set:</p> \[\begin{aligned} \eta^{(t+1)} &amp;= \mathbf{X} \beta^{(t)} \\ \mu^{(t+1)} &amp;= g^{-1} \eta^{(t+1)} \\ \mathbf{W}^{(t+1)} &amp;= \text{Cov}^{-1}(\mathbf{y}) \left[ \frac{\partial g(\mu)}{\partial \mu} \right]^{-2} \bigg\rvert_{\eta = \eta^{(t+1)}} \\ \mathbf{z}^{(t+1)} &amp;= \eta^{(t+1)} + \frac{ \partial g(\mu)}{\partial \mu}\bigg\rvert_{\eta = \eta^{(t+1)}}(\mathbf{y} - g^{-1}(\eta^{(t+1)})) \\ \beta^{(t+1)} &amp;= (\mathbf{X}^\top \tilde{\mathbf{W}}^{(t+1)} \mathbf{X})^{-1} \mathbf{X}^\top \tilde{\mathbf{W}}^{(t+1)} \mathbf{z}^{(t+1)} \end{aligned}\] <p>This process is called <a href="https://en.wikipedia.org/w/index.php?title=Iteratively_reweighted_least_squares&amp;oldid=1279139010">iteratively reweighted least squares</a>.</p> <h3 id="uncertainty-quantification-1">Uncertainty Quantification</h3> <p>Due to the iterative nature of the estimation procedure, it is not so clear how to quantify our uncertainty in our estimates. Since there is no closed form solution to the MLEs, <a href=" https://statisticaloddsandends.wordpress.com/2020/11/20/variance-of-coefficients-for-linear-models-and-generalized-linear-models/">there is also no closed form for the variance of the MLEs</a>.</p> <p>One can, instead, derive the asymptotic variance of the estimator using <a href="/posts/2025/02/03/likelihood-theory.html">likelihood theory</a>. As a maximum likelihood estimator, it should be asymptotically normal with mean $\beta$ and covariance matrix $-\left[ \mathbb{E}\left[ \frac{\partial^2 \ell(\mathbf{y}; \theta, \tau)}{\partial \beta \beta^\top} \right] \right]^{-1}$ <i>if the model is correct</i>.</p>]]></content><author><name></name></author><category term="regression"/><category term="likelihood"/><category term="models"/><summary type="html"><![CDATA[A Primer]]></summary></entry><entry><title type="html">Quasi-Likelihood</title><link href="https://aerosengart.github.io/blog/2025/quasi-likelihood/" rel="alternate" type="text/html" title="Quasi-Likelihood"/><published>2025-05-30T00:00:00+00:00</published><updated>2025-05-30T00:00:00+00:00</updated><id>https://aerosengart.github.io/blog/2025/quasi-likelihood</id><content type="html" xml:base="https://aerosengart.github.io/blog/2025/quasi-likelihood/"><![CDATA[<p>This post is a review of quasi-likelihood theory and mostly relies upon Wedderburn<d-cite key="wedderburn1974"></d-cite> and Breslow &amp; Clayton<d-cite key="breslow1993"></d-cite>. Quasi-likelihood functions provide an alternative and less demanding way of characterizing the distribution of observations compared to specifying a true likelihood function. In essence, we simply assume a particular relationship between the mean and the variance rather than a particular distributional family. Then a so-called <i>quasi-likelihood</i> function can be defined and used for parameter estimation.</p> <hr/> <h2 id="set-up">Set-Up</h2> <p>Let $x_1, \dots, x_n$ denote our (independent) observations. Denote the expectation and variance of an arbitrary $x_i$ with $\mathbb{E}[x_i] = \mu_i$ and $\text{var}(x_i) = \phi V(\mu_i)$ where $\phi &gt; 0$ is some scale parameter. $V(\mu_i)$ is some (known) function of the mean.</p> <p>It’s important to note that the expectations and variances need not be identical, but we enforce that the variances are proportional to some (shared) function of the expectations. We assume that $\mu_i = g(\beta_1, \dots, \beta_m)$. That is, the expectations are some (known) function of parameters $\beta_1, \dots, \beta_m$.</p> <p>The quasi-likelihood is easier to explain after defining the <i>quasi-score</i> function.</p> <div class="definition"> <body> <strong>Definition (Quasi-Score).</strong> The <i>quasi-score</i> of $x_i$ is given by: $$ \begin{equation} \label{eq:quasi-score} U(x_i; \mu_i) = \frac{x_i - \mu_i}{\phi V(\mu_i)} \end{equation} $$ </body> <br/> </div> <p>In likelihood theory, the score function is the gradient of the log-likelihood function with respect to the parameters. In a similar fashion, the quasi-score is the gradient of the quasi-likelihood function with respect to the mean.</p> <div class="definition"> <body> <strong>Definition (Quasi-Likelihood).</strong> The <i>quasi-likelihood</i> (or, more precisely, the <i>quasi log-likelihood</i>) of $x_i$ is given by: $$ \begin{equation} \label{eq:quasi-likelihood} \begin{aligned} \ell_q(x_i; \mu_i) &amp;= \int_{x_i}^{\mu_i} \frac{x_i - z}{\phi V(z)} dz \\ &amp;\iff \\ \frac{\partial}{\partial \mu_i} [\ell_q(x_i; \mu_i)] &amp;= U(x_i; \mu_i) \end{aligned} \end{equation} $$ </body> <br/> </div> <p>The quasi-score satisfies several of the properties that the score in likelihood theory satisfies, which justifies its name as a <i>quasi</i> score.</p> <div id="theorem-1"></div> <div class="theorem"> <strong>Theorem 1 (Wedderburn (1974)).</strong> <ul id="theorem-1-wedderburn" class="tab" data-tab="5faa61af-ca04-4f95-8bd6-0c08724ca00e" data-name="theorem-1-wedderburn"> <li class="active" id="theorem-1-wedderburn-theorem"> <a href="#">theorem </a> </li> <li id="theorem-1-wedderburn-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="5faa61af-ca04-4f95-8bd6-0c08724ca00e" data-name="theorem-1-wedderburn"> <li class="active"> <p>Let $x$ be some observation with expectation $\mu$ and variance $\phi V(\mu)$ for some $\phi &gt; 0$. Suppose $\mu = g(\beta_1, \dots, \beta_m)$ for some (continuous and differentiable) funciton $g(\cdot)$. The quasi-score and quasi-likelihood, as defined in Eqs. \eqref{eq:quasi-score} and \eqref{eq:quasi-likelihood}, satisfy the following properties:</p> <ol> <li>$\mathbb{E}\left[ U(x; \mu) \right] = 0$</li> <li>$\mathbb{E}\left[ \frac{\partial \ell_q(x; \mu) }{\partial \beta_i} \right] = 0$ for all $i \in [m]$</li> <li>$\text{Var}(U(x; \mu)) = - \mathbb{E}\left[ \frac{\partial^2 \ell_q(x;\mu)}{\partial \mu^2}\right] = \frac{1}{\phi V(\mu)}$</li> <li>$\mathbb{E}\left[ \frac{\partial \ell_q(x; \mu)}{\partial \beta_i} \frac{\partial \ell_q(x; \mu)}{\partial \beta_j}\right] = - \mathbb{E}\left[ \frac{\partial^2 \ell_q(x; \mu)}{\partial \beta_i \partial \beta_j} \right] = \frac{1}{\phi V(\mu)} \frac{\partial \mu}{\partial \beta_i} \frac{\partial \mu}{\partial \beta_j}$</li> </ol> </li> <li> <p>Proving (1) is simple:</p> \[\mathbb{E}\left[ U(x; \mu) \right] = \mathbb{E}\left[ \frac{x - \mu}{\phi V(\mu)} \right] = \frac{1}{\phi V(\mu)} \mathbb{E}\left[ x - \mu\right] = \frac{1}{\phi V(\mu)} (\mu - \mu) = 0\] <p>To show (2), we note that:</p> \[\frac{\partial \ell_q(x; \mu)}{\partial \beta_i} = \frac{\partial \ell_q(x; \mu)}{\mu} \frac{\partial \mu}{\beta_j} \hspace{5mm} \implies \hspace{5mm} \mathbb{E}\left[ \frac{\partial \ell_q(x; \mu) }{\partial \beta_i} \right] = \mathbb{E}\left[ \frac{\partial \ell_q(x; \mu)}{\partial \mu} \frac{\partial \mu}{\beta_i} \right] = \frac{\partial \mu}{\beta_i} \mathbb{E}\left[ U(x; \mu) \right] = 0 \nonumber\] <p>Showing (3) is also relatively easy. We first show that $\text{Var}(U(x;\mu)) = \frac{1}{\phi V(\mu)}$:</p> \[\text{Var}(U(x; \mu)) = \text{Var}\left(\frac{x - \mu}{\phi V(\mu)}\right) = \frac{1}{\phi^2 V^2(\mu)} \text{Var}(x - \mu) = \frac{1}{\phi^2 V^2(\mu)} \text{Var}(x) = \frac{\phi V(\mu)}{\phi^2 V^2(\mu)} = \frac{1}{\phi V(\mu)}\] <p>Next, we show that $- \mathbb{E}\left[ \frac{\partial^2 \ell_q(x;\mu)}{\partial \mu^2}\right] = \frac{1}{\phi V(\mu)}$:</p> \[- \mathbb{E}\left[ \frac{\partial^2 \ell_q(x;\mu)}{\partial \mu^2}\right] = - \mathbb{E}\left[ \frac{\partial U(x; \mu)}{\partial \mu} \right] = - \mathbb{E}\left[ \frac{\partial}{\partial \mu} \left[ \frac{x - \mu}{\phi V(\mu)}\right] \right] = - \mathbb{E}\left[ \frac{\phi V(\mu)(-1) - (x- \mu)(\phi V'(\mu))}{\phi^2 V^2(\mu)} \right] = \frac{1}{\phi V(\mu)} + \frac{(\mathbb{E}[x]- \mu)(\phi V'(\mu))}{\phi^2 V^2(\mu)} = \frac{1}{\phi V(\mu)}\] <p>For (4), we first show that $\mathbb{E}\left[ \frac{\partial \ell_q(x; \mu)}{\partial \beta_i} \frac{\partial \ell_q(x; \mu)}{\partial \beta_j}\right] = \frac{1}{\phi V(\mu)} \frac{\partial \mu}{\partial \beta_i} \frac{\partial \mu}{\partial \beta_j}$:</p> \[\begin{aligned} \mathbb{E}\left[ \frac{\partial \ell_q(x; \mu)}{\partial \beta_i} \frac{\partial \ell_q(x; \mu)}{\partial \beta_j}\right] &amp;= \mathbb{E}\left[ \frac{\partial \ell_q(x;\mu)}{\partial \mu} \frac{\partial \mu}{\partial \beta_i} \frac{\partial \ell_q(x;\mu)}{\partial \mu} \frac{\partial \mu}{\partial \beta_j}\right] \\ &amp;= \mathbb{E}\left[ \left( \frac{\partial \ell_q(x;\mu)}{\partial \mu} \right)^2 \right] \frac{\partial \mu}{\partial \beta_i} \frac{\partial \mu}{\partial \beta_j} \\ &amp;= \mathbb{E}\left[ U^2(x; \mu) \right] \frac{\partial \mu}{\partial \beta_i} \frac{\partial \mu}{\partial \beta_j} \\ &amp;= \mathbb{E}\left[ \frac{(x - \mu)^2}{\phi^2 V^2(\mu)} \right] \frac{\partial \mu}{\partial \beta_i} \frac{\partial \mu}{\partial \beta_j} \\ &amp;= \frac{\phi V(\mu)}{\phi^2 V(\mu)} \frac{\partial \mu}{\partial \beta_i} \frac{\partial \mu}{\partial \beta_j} \\ &amp;= \frac{1}{\phi V(\mu)} \frac{\partial \mu}{\partial \beta_i} \frac{\partial \mu}{\partial \beta_j} \end{aligned}\] <p>Then we show that $- \mathbb{E}\left[ \frac{\partial^2 \ell_q(x; \mu)}{\partial \beta_i \partial \beta_j} \right] = \frac{1}{\phi V(\mu)} \frac{\partial \mu}{\partial \beta_i} \frac{\partial \mu}{\partial \beta_j}$:</p> \[\begin{aligned} - \mathbb{E}\left[ \frac{\partial^2 \ell_q(x; \mu)}{\partial \beta_i \partial \beta_j} \right] &amp;= - \mathbb{E}\left[ \frac{\partial}{\partial \beta_j} \left[ \frac{\partial \ell_q(x; \mu)}{\partial \mu} \frac{\partial \mu}{\partial \beta_i}\right] \right] \\ &amp;= - \mathbb{E}\left[ \frac{\partial}{\partial \beta_j} \left[ U(x; \mu) \frac{\partial \mu}{\partial \beta_i}\right] \right] \\ &amp;= - \mathbb{E}\left[ \frac{\partial}{\partial \beta_j} \left[ \frac{x - \mu}{\phi V(\mu)} \frac{\partial \mu}{\partial \beta_i}\right] \right] \\ &amp;= - \mathbb{E}\left[ \frac{\partial}{\partial \beta_j} \left[\frac{x-\mu}{\phi V(\mu)} \right] \frac{\partial \mu}{\partial \beta_i} \right] + \underbrace{\mathbb{E}\left[ \frac{x - \mu}{\phi V(\mu)}\frac{\partial}{\partial \beta_j} \left[\frac{\partial \mu}{\partial \beta_i} \right] \right]}_{=0} \\ &amp;= - \mathbb{E}\left[ \frac{\phi V(\mu) \frac{\partial}{\partial \beta_j}[x - \mu] - (x-\mu) \frac{\partial}{\partial \beta_j}[\phi V(\mu)]}{\phi^2 V^(\mu)}\right] \frac{\partial \mu}{\partial \beta_i} \\ &amp;= \left(- \mathbb{E}\left[ - \frac{1}{\phi V(\mu)} \frac{\partial \mu}{\partial \beta_j} \right] + \underbrace{\mathbb{E}\left[ x- \mu\right]}_{=0} \frac{\frac{\partial}{\partial \beta_j}[\phi V(\mu)]}{\phi^2 V^2(\mu)} \right) \frac{\partial \mu}{\partial \beta_i} \\ &amp;= \frac{1}{\phi V(\mu)} \frac{\partial \mu}{\partial \beta_j} \frac{\partial \mu}{\partial \beta_i} \end{aligned}\] </li> </ul> </div> <hr/> <h2 id="connections-to-likelihood">Connections To Likelihood</h2> <p>Suppose that the distribution of $x$ is a function of $\mu$ such that a log-likelihood can be explicitly written. Let $\ell(z; \mu)$ denote this log-likelihood.</p> <ul id="claim-lik1" class="tab" data-tab="f6bc984f-fecf-4ded-9a9e-d410e9b2a245" data-name="claim-lik1"> <li class="active" id="claim-lik1-claim"> <a href="#">claim </a> </li> <li id="claim-lik1-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="f6bc984f-fecf-4ded-9a9e-d410e9b2a245" data-name="claim-lik1"> <li class="active"> <p>The following property is due to the above theorem:</p> \[\begin{equation} \label{eq:corollary-1} - \mathbb{E}\left[ \frac{\partial^2 \ell_q(x; \mu)}{\partial \mu^2} \right] \leq - \mathbb{E}\left[ \frac{\partial^2 \ell(x; \mu)}{\partial \mu^2} \right] \end{equation}\] </li> <li> <p>By (4) of <a href="#theorem-1">Theorem 1</a>:</p> \[- \mathbb{E}\left[ \frac{\partial^2 \ell_q(x; \mu)}{\partial \mu^2}\right] = \frac{1}{\phi V(\mu)}\] <p>Our problem then becomes showing that:</p> \[\frac{1}{\phi V(\mu)} \leq - \mathbb{E}\left[\frac{\partial^2 \ell(x; \mu)}{\partial \mu^2} \right] \hspace{5mm} \iff \hspace{5mm} \phi V(\mu) \geq - \frac{1}{\mathbb{E}\left[\frac{\partial^2 \ell(x; \mu)}{\partial \mu^2} \right]}\] <p>Under certain regularity conditions (see <a href="/posts/2025/02/03/likelihood-theory.html">my likelihood post</a>), we have that $-\mathbb{E}\left[\frac{\partial^2 \ell(x; \mu)}{\partial \mu^2} \right]$ is the Fisher information. The result follows directly from the <a href="https://en.wikipedia.org/wiki/Cramér–Rao_bound">Cramér-Rao bound</a>.</p> </li> </ul> <p>Wedderburn provides an additional connection between quasi-likelihood and likelihood functions for one-parameter distributions specified by the mean.</p> <div id="theorem-2"></div> <div class="theorem"> <strong>Theorem 2 (Wedderburn (1974)).</strong> <ul id="theorem-2-wedderburn" class="tab" data-tab="f4862352-2234-4772-b8e0-1132d33e02a7" data-name="theorem-2-wedderburn"> <li class="active" id="theorem-2-wedderburn-statement"> <a href="#">statement </a> </li> <li id="theorem-2-wedderburn-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="f4862352-2234-4772-b8e0-1132d33e02a7" data-name="theorem-2-wedderburn"> <li class="active"> <p>Let $x$ be some observation with expectation $\mu$ and variance $\phi V(\mu)$ for some $\phi &gt; 0$. Suppose $\mu = g(\beta_1, \dots, \beta_m)$ for some (continuous and differentiable) function $g(\cdot)$. The log-likelihood function, $\ell(x; \mu)$, for $x$ satisfies:</p> \[\begin{equation} \label{eq:ll-condition} \frac{\partial}{\partial \mu} \left[ \ell(x; \mu) \right] = \frac{x - \mu}{\phi V(\mu)} \end{equation}\] <p>if and only if the density function of $x$ can be written, with respect to some measure, as:</p> \[f_x = \exp\left( x \theta - h(\theta) \right)\] </li> <li> <p>We first prove the forwards direction. Assume the log-likelihood satisfies Eq. \eqref{eq:ll-condition}. We integrate with respect to $\mu$:</p> \[\begin{aligned} &amp;\int \frac{\partial}{\partial \mu} \left[ \ell(x; \mu) \right] d\mu = \int \frac{x - \mu}{\phi V(\mu)} d\mu \\ \implies &amp;\ell(x; \mu) = \frac{x}{\phi} \int \frac{1}{V(\mu)} d \mu - \frac{1}{\phi}\int \frac{\mu}{V(\mu)} d\mu \end{aligned}\] <p>Substituting in $\theta = \frac{1}{\phi} \int \frac{1}{V(\mu)} d\mu$:</p> \[\ell(x; \mu) = x \theta - ?\] </li> </ul> </div> <p>The theorem can be summarized quite nicely: the quasi-likelihood function will equal the log-likelihood function <i>if and only if</i> the distribution comes from an exponential family.</p> <p>Extending the previous corollary, we see that for a one-parameter exponential family, Eq. \eqref{eq:corollary-1} obtains equality. Under certain regularity conditions (see <a href="/blog/2025/likelihood-theory.html">my likelihood post</a>), $-\mathbb{E}\left[\frac{\partial^2 \ell(x; \mu)}{\partial \mu^2} \right]$ is the Fisher information, which describes the amount of information about $\mu$ that is held in $x$.</p> <p>Since equality is obtained, we can also think of $-\mathbb{E}\left[\frac{\partial^2 \ell_q(x; \mu)}{\partial \mu^2} \right]$ as describing the amount of information about $\mu$ that is held in $x$. In addition, the difference between the former and the latter can be thought of as the amount of information gained by knowing, specifically, the distribution of $z$.</p> <hr/> <h2 id="estimation">Estimation</h2> <p>Let $x_{1:n}= (x_1, \dots, x_n)$ for independent observations $x_1, \dots, x_n$, and let $\mu_{1:n} = (\mu_1, \dots, \mu_n)$. We’ll denote the gradient of the (full) quasi-likelihood with respect to the parameters $\beta_1, \dots, \beta_m$ with:</p> \[\mathbf{u} = \frac{\partial \ell_q(x_{1:n}; \mu_{1:n})}{\partial \beta} = \begin{bmatrix} \frac{\partial \ell_q(x_{1:n}; \mu_{1:n})}{\partial \beta_1} \\ \vdots \\ \frac{\partial \ell_q(x_{1:n}; \mu_{1:n})}{\partial \beta_m} \end{bmatrix} \label{eq:gradient-ql}\] <p>By <a href="#theorem-1">Theorem 1</a>, $\mathbf{u}$ has mean vector:</p> \[\begin{equation} \label{eq:u-mean} \begin{aligned} \mathbb{E}[\mathbf{u}] &amp;= \mathbb{E}\left[ \frac{\partial \ell_q(x_{1:n}; \mu_{1:n})}{\partial \beta} \right] \\ &amp;= \mathbb{E}\left[ \begin{bmatrix} \sum_{i = 1}^n \frac{\partial \ell_q(x_i; \mu_i)}{\partial \beta_1} \\ \vdots \\ \sum_{i = 1}^n \frac{\partial \ell_q(x_i; \mu_i)}{\partial \beta_m} \\ \end{bmatrix} \right] \\ &amp;= \sum_{i = 1}^n \mathbb{E}\left[ \begin{bmatrix} \frac{\partial \ell_q(x_i; \mu_i)}{\partial \beta_1} \\ \vdots \\ \frac{\partial \ell_q(x_i; \mu_i)}{\partial \beta_m} \\ \end{bmatrix} \right]\\ &amp;= \mathbf{0} \end{aligned} \end{equation}\] <p>and covariance matrix:</p> \[\begin{equation} \label{eq:u-cov} \begin{aligned} \text{Cov}(\mathbf{u}) &amp;= \mathbb{E}\left[ \frac{\partial \ell_q(x_{1:n}; \mu_{1:n})}{\partial \beta} \frac{\partial \ell_q(x_{1:n}; \mu_{1:n})}{\partial \beta^\top}\right] \\ &amp;= - \mathbb{E}\left[\frac{\partial^2 \ell_q(x_{1:n}; \mu_{1:n})}{\partial \beta \partial \beta^\top}\right] \end{aligned} \end{equation}\] <p>The <i>maximum quasi-likelihood estimates</i> of $\beta$, denoted by $\hat{\beta}$, are found by setting $\mathbf{u}$ equal to $\mathbf{0}$ and solving for $\beta$, just like we would do for maximum likelihood estimation.</p> <div id="theorem-3"></div> <div class="theorem"> <strong>Theorem 3 (Wedderburn (1974)).</strong> <ul id="theorem-3-wedderburn" class="tab" data-tab="0213f986-a541-4054-b1b8-13463f7fa44f" data-name="theorem-3-wedderburn"> <li class="active" id="theorem-3-wedderburn-theorem"> <a href="#">theorem </a> </li> <li id="theorem-3-wedderburn-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="0213f986-a541-4054-b1b8-13463f7fa44f" data-name="theorem-3-wedderburn"> <li class="active"> <p>Let $x$ be some observation with expectation $\mu$ and variance $\phi V(\mu)$ for some $\phi &gt; 0$. Suppose $\mu = g(\beta_1, \dots, \beta_m)$ for some (continuous and differentiable) function $g(\cdot)$.</p> <p>Denote the gradient of the (full) quasi-likelihood with respect to the parameters $\beta_1, \dots, \beta_m$ with $\mathbf{u}$, and let $\hat{\beta}$ be the maximum quasi-likelihood estimates of $\beta$. The mean of $\hat{\beta}$ is approximately $\mathbf{0}$, and the covariance of $\hat{\beta}$ is approximately:</p> \[\text{Cov}(\hat{\beta}) \approx \text{Cov}^{-1}(\mathbf{u}) = \left[ -\mathbb{E}\left[ \frac{\partial^2 \ell_q(x_{1:n}; \mu_{1:n})}{\partial \beta \partial \beta^\top} \right] \right]^{-1}\] <p>if $\phi = 1$.</p> </li> <li> <p>Let $\hat{\mathbf{u}}$ denote the gradient vector evaluated at the maximum quasi-likelihood estimate, $\hat{\beta}$. Since $\hat{\beta}$ is the value of $\beta$ such that $\mathbf{u}$ equals $\mathbf{0}$, a first-order Taylor approximation of $\mathbf{u}$ gives us:</p> \[\begin{aligned} \mathbf{u} &amp;\approx \hat{\mathbf{u}} + \frac{\partial \mathbf{u}}{\partial \beta} (\beta - \hat{\beta}) \\ &amp;= \frac{\partial^2 \ell_q(x_{1:n}; \mu_{1:n})}{\partial \beta \partial \beta^\top} (\beta - \hat{\beta}) \\ \implies \beta - \hat{\beta} &amp;\approx \left[ \frac{\partial^2 \ell_q(x_{1:n}; \mu_{1:n})}{\partial \beta \partial \beta^\top} \right]^{-1} \mathbf{u} \end{aligned}\] <p>If we approximate the inverted matrix by its expectation, whose elements are given in <a href="#theorem-1">Theorem 1</a>, we get:</p> \[&amp;\beta - \hat{\beta} \approx -\text{Cov}^{-1}(\mathbf{u})\mathbf{u} \\ \implies &amp;\hat{\beta} \approx \beta + \text{Cov}^{-1}(\mathbf{u})\mathbf{u}\] <p>Since $\mathbf{u}$ has expectation zero, it is clear that $\mathbb{E}[\hat{\beta}] \approx \mathbf{0}$ as well.</p> <p>The first term on the right-hand side of the above expression is fixed (the true parameter value), so the <i>approximate</i> covariance matrix of $\hat{\beta}$ is:</p> \[\begin{aligned} \text{Cov}(\hat{\beta}) &amp;\approx \text{Cov}\left( \beta + \text{Cov}^{-1}(\mathbf{u})\mathbf{u} \right) \\ &amp;= \mathbb{E}\left[ \text{Cov}^{-1}(\mathbf{u})\mathbf{u} \left(\text{Cov}^{-1}(\mathbf{u})\mathbf{u}\right)^\top \right] \\ &amp;= \text{Cov}^{-1}(\mathbf{u})\mathbb{E}\left[ \mathbf{u} \mathbf{u}^\top\right] \text{Cov}^{-1}(\mathbf{u}) \\ &amp;= \text{Cov}^{-1}(\mathbf{u})\] </li> </ul> </div> <p>The above theorem holds only for a scale parameter $\phi = 1$. If we relax this assumption to $\phi &gt; 0$, the expectation does not change, but we need to estimate $\phi$ before we can approximate the covariance of the maximum quasi-likelihood estimates.</p>]]></content><author><name></name></author><category term="glmm"/><category term="likelihood"/><category term="theory"/><summary type="html"><![CDATA[A Primer]]></summary></entry><entry><title type="html">One-Sided Score Test For Variance Components</title><link href="https://aerosengart.github.io/blog/2025/score-test-one-sided/" rel="alternate" type="text/html" title="One-Sided Score Test For Variance Components"/><published>2025-02-18T00:00:00+00:00</published><updated>2025-02-18T00:00:00+00:00</updated><id>https://aerosengart.github.io/blog/2025/score-test-one-sided</id><content type="html" xml:base="https://aerosengart.github.io/blog/2025/score-test-one-sided/"><![CDATA[<p>In many cases, we may want to test the null hypothesis that a parameter is zero against a one-sided alternative (e.g. the parameter is non-negative). In this setting, we are constraining the alternative parameter space, and for some parameters (such as variance components), the value of the parameter under the null may be on the boundary.</p> <p>In this post, I’ll cover some of the literature on tests against one-sided alternatives. We’ll mostly be in the mindset of tests of homogeneity in mixed models (i.e. testing whether the random effects variance is zero), but a lot of these results are generally applicable.</p> <hr/> <h2 id="some-intuition">Some Intuition</h2> <p>First, let’s discuss some of the intuition behind the score test (also called Rao’s test (after C. R. Rao) and the Lagrange multiplier test). Suppose we have some model or data-generating process parametrized by $\theta$. Our goal will be to test:</p> \[H_0: \theta = \theta_0\] <p>Let $\ell(\theta; \mathbf{X})$ denote the log-likelihood function for parameter $\theta$ given data $\mathbf{X}$. We can imagine that if our maximum likelihood estimate, $\hat{\theta}$, is far from $\theta_0$, then our data provide evidence against $H_0$.</p> <p>The score test uses the slope of the log-likelihood (i.e. the derivative), called the <i>score</i> to determine what it means for $\hat{\theta}$ to be far from $\theta_0$. If the derivative is quite large (in absolute value) at $\theta_0$, then that implies that we have moved quite far away from the root of the log-likelihood, $\hat{\theta}$.</p> <p>Under the assumption that the log-likelihood is partially differentiable w.r.t. each component of $\theta$ and the Fisher information exists and is invertible at $\theta_0$, the standard score test statistic for i.i.d. sample $\mathbf{X} = (\mathbf{X}_1, \dots, \mathbf{X}_n)$ is computed as:</p> \[t = \frac{1}{n} U^\top(\theta_0) \mathcal{I}^{-1} (\theta_0) U(\theta_0)\] <p>where</p> \[U(\theta^*) = \sum_{i = 1}^n \frac{\partial}{\partial \theta} \ell(\theta; \mathbf{X}_i) \rvert_{\theta = \theta^*}; \hspace{8mm} \mathcal{I}(\theta^*) = \mathbb{E}\left[ U(\theta) U^\top(\theta) \right] \bigg\rvert_{\theta = \theta^*}\] <ul id="claim-1" class="tab" data-tab="4dfe7180-973f-46b7-87ec-239221c1d88f" data-name="claim-1"> <li class="active" id="claim-1-claim"> <a href="#">claim </a> </li> <li id="claim-1-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="4dfe7180-973f-46b7-87ec-239221c1d88f" data-name="claim-1"> <li class="active"> <p>Assuming $U(\theta)$ has finite variance and mean zero (which it will under certain conditions — see <a href="/posts/2025/02/02/likelihood-theory.html">this post on likelihood theory</a>), it is asymptotically multivariate Gaussian when suitably centered and scaled (by the central limit theorem).<d-cite key="dasgupta2008"></d-cite></p> </li> <li> <p>Notice that $U(\theta_0)$ is the sum of (functions of) i.i.d. random variables. As we noted above, $U(\theta_0)$ has mean zero under certain regularity conditions. The CLT states that:</p> \[\frac{\sqrt{n}\left(\frac{1}{n} \sum_{i = 1}^n X_i - \mu \right) }{\sigma} \rightsquigarrow \mathcal{N}(0, 1) \hspace{4mm} \iff \hspace{4mm} (\frac{1}{n} \sum_{i = 1}^n X_i - \mu_X) \rightsquigarrow \mathcal{N}\left(0, \frac{\sigma^2}{n}\right) \nonumber\] <p>If $\mathcal{I}(\theta)$ is the covariance of $U(\theta)$, then $n^2 \mathcal{I}(\theta)$ is the covariance of $\frac{1}{n}U(\theta)$. Thus:</p> \[\sqrt{n} \left(\frac{1}{n} U^\top(\theta)\right) \rightsquigarrow \mathcal{N}\left(\mathbf{0}, \mathcal{I}(\theta) \right) \hspace{4mm} \iff \hspace{4mm} \sqrt{n} \left(\frac{1}{n} U^\top(\theta)\right)\mathcal{I}^{-1/2}(\theta) \rightsquigarrow \mathcal{N}\left(\mathbf{0}, \mathbb{I}\right) \nonumber\] <p>This can then be used to derive the asymptotic null distribution of the score test statistic, since the distribution of a squared Gaussian random variable is $\chi^2$:</p> \[\left( \sqrt{n} \left(\frac{1}{n} U^\top(\theta)\right)\mathcal{I}^{-1/2}(\theta)\right)^2 \rightsquigarrow \chi^2_k \hspace{4mm} \iff \hspace{4mm} \frac{1}{n} U^\top(\theta) \mathcal{I}^{-1}(\theta) U(\theta) \rightsquigarrow \chi^2_k\] <p>where $k$ is the dimension of $\theta$.</p> </li> </ul> <p>The alternative for the above test is implicitly two-sided: $H_1: \theta \neq \theta_0$.</p> <hr/> <h2 id="introduction">Introduction</h2> <p>In our setting, it is important to consider a one-sided alternative because variances are non-negative. As pointed out by Hall and Praestgaard<d-cite key="hall2001"></d-cite> , omnibus tests like that of Lin (1997)<d-cite key="lin1997"></d-cite> , which implicitly test against a two-sided alternative, have power against these impossible cases. They also explain that the claim that Lin’s global test for homogeneity is locally asymptotically most stringest does not hold in particular boundary cases. Thus, it seems worthwhile to pursue explicitly one-sided tests, most of which are related to cones.</p> <h3 id="all-about-cones">All About Cones</h3> <p>As an introduction into the theoretical/geometric setting we are interested in, I will cover some of the background and results in Shapiro<d-cite key="shapiro1988"></d-cite> This will give us a better foundation for what’s to come.</p> <p>Shapiro restricts his attention to closed and convex cones, so I’ll do the same for the rest of this section.</p> <aside><p>A set is <i>closed</i> if it contains all of its boundary points.</p></aside> <p>A <i>cone</i>, $C$, in $\mathbb{R}^m$ is defined as the set \(C := \\{ t\mathbf{x} \in C \rvert \mathbf{x} \in C \\}\) for any $t &gt; 0$. $C$ is called a <i>pointed cone</i> if $\mathbf{x} \in C$ and $-\mathbf{x} \in C$ implies $\mathbf{x} = \mathbf{0}$, the zero vector.</p> <aside><p>A set, $S$, is <i>convex</i> if $tx + (1-t)y \in S$ for $x,y \in S$ and $t \in [0, 1]$.</p></aside> <p>Shapiro denotes the orthogonal projection of a point onto $C$ with</p> \[P(\mathbf{x}, C) = \underset{\eta \in C}{\arg\min} \left \{ (\mathbf{x} - \eta)^\top \mathbf{U} (\mathbf{x} - \eta) \right \}\] <p>where $\mathbf{U}$ is any positive-definite matrix. This orthogonal projection maps the input point, $\mathbf{x}$, to the closest point on $C$, $\eta$, where closeness is defined by the norm associated with the matrix $\mathbf{U}$. We’ll denote this norm with $\rvert \rvert \mathbf{x} \rvert \rvert = \sqrt{\mathbf{x}^\top \mathbf{U} \mathbf{x}}$, and we’ll use $\langle\mathbf{x}, \mathbf{y}\rangle = \mathbf{x}^\top \mathbf{U}\mathbf{y}$ to denote the inner product of $\mathbf{x}$ and $\mathbf{y}$ associated with $\mathbf{U}$.</p> <p>This brings us to the <i>dual</i> cone, which I have very little intuition…</p> <div id="dual-cone"></div> <div class="definition"> <strong>Definition (Dual Cone).</strong> <body> The <i>dual cone</i> is the set $C^0= \\{ \mathbf{y} \rvert \langle \mathbf{x}, \mathbf{y} \rangle \leq 0 \hspace{2mm} \forall \mathbf{x} \in C \\}$. <br/> If $C$ is a vector subspace, then $C^0$ is the orthogonal complement of $C$, and if $C$ is closed and convex, then $(C^0)^0 = C$. </body> </div> <p>If we have another convex cone $K$, then:</p> \[\rvert \rvert \mathbf{x} - P(\mathbf{x}, C) \rvert \rvert^2 = \rvert \rvert \mathbf{x} - P(\mathbf{x}, K) \rvert \rvert^2 + \rvert \rvert P(\mathbf{x}, K) - P(\mathbf{x}, C) \rvert \rvert^2\] <p>if $C$ or $K$ is a lienar space and $C \subset K$.</p> <p>Furthermore, if $C$ is a vector subspace, then $\mathbf{x} - P(\mathbf{x}, C) = P(\mathbf{x}, C^0)$. That is, the difference between $\mathbf{x}$ and the orthogonal projection of $\mathbf{x}$ onto $C$ is equivalent to its orthogonal projection onto the dual cone $C^0$.</p> <div id="chi-bar-squared"></div> <div class="definition"> <strong>Definition ($\bar{\chi}^2$-Statistic).</strong> <body> Let $\mathbf{y} \sim \mathcal{N}(\mathbf{0}, \mathbf{V})$ be a Gaussian random vector of $m$ dimensions with some covariance matrix, $\mathbf{V}$, and let $C$ be a convex cone. A <i>$\bar{\chi}^2$-statistic</i> is given by the following: $$ \begin{aligned} \bar{\chi}^2 &amp;= \mathbf{y}^\top \mathbf{V}^{-1} \mathbf{y} - \underset{\eta \in C}{\min} \left\{ (\mathbf{y} - \eta)^\top \mathbf{V}^{-1}(\mathbf{y} - \eta) \right\} \\ &amp;= \mathbf{y}^\top \mathbf{V}^{-1} \mathbf{y} - (\mathbf{y} - P(\mathbf{y}, C))^\top \mathbf{V}^{-1}(\mathbf{y} - P(\mathbf{y}, C)) \\ &amp;= (\mathbf{y} - \mathbf{y} + P(\mathbf{y}, C))^\top \mathbf{V}^{-1}(\mathbf{y} - \mathbf{y} + P(\mathbf{y}, C)) \\ &amp;= \rvert \rvert P(\mathbf{y}, C) \rvert \rvert^2 \end{aligned} $$ where in the above, the inner products/norms are taken using the matrix $\mathbf{V}^{-1}$. </body> </div> <p>The chi-bar-squared statistic follows a mixture of $\chi^2$ distributions:</p> \[\mathbb{P}(\bar{\chi}^2 \geq c) = \sum_{i = 1}^m w_i \mathbb{P}(\chi_i^2 \geq c)\] <p>where $\chi_i^2$ is a $\chi^2$ random variable with $i$ degrees of freedom, and $w_i$ are individuals weights that sum to $1$. As is standard, we let $\chi^2_0$ be a point mass at $0$. We’ll denote this mixture distribution as $\mathcal{\bar{X}}^2(\mathbf{V}, C)$, since it depends on both $\mathbf{V}$ and the cone, $C$.</p> <p>Note that, if we have the dual cone to $C$, then we can ignore the first term in the test statistic equation to get:</p> \[\bar{\chi}^2 = \underset{\eta \in D}{\min} \left\{ (\mathbf{y} - \eta)^\top \mathbf{V}^{-1} (\mathbf{y} - \eta) \right\}\] <p>which follows a $\mathcal{\bar{X}}^2(\mathbf{V}, C^0)$ distribution. We can also note that the mixture’s weights satisfy:</p> \[w_i(m, \mathbf{V}, C^0) = w_{m - i}(m, \mathbf{V}, C) \hspace{15mm} i = 0, \dots, m\] <hr/> <h2 id="a-multivariate-one-sided-test">A Multivariate One-Sided Test</h2> <p>Some of the later literature is based upon mid-century work by Kudô (1963)<d-cite key="kudo1963"></d-cite> for testing the mean vector of a multivariate Gaussian distribution. Kudô’s work has some very nice geometric interpretations that permit the derivation of the limiting distribution of his test statistic, and all of it begins with the likelihood ratio.</p> <h3 id="set-up">Set-Up</h3> <p>We have a multivariate Gaussian population with mean vector $\theta^\top = (\theta_1, \dots, \theta_k)$ and known, non-singular, positive definite variance-covariance matrix $\Sigma$. Given a sample of $n$ i.i.d. observations $\mathbf{X} = { \mathbf{X}^{(1)}, \dots, \mathbf{X}^{(n)} }$, we wish to test:</p> \[H_0: \theta_i = 0 \hspace{2mm} i = 1, 2, \dots, k \hspace{8mm} \text{vs.} \hspace{8mm} H_1: \theta_i \geq 0 \hspace{2mm} i = 1, 2, \dots, k\] <p>where at least one inequality in the alternative hypothesis setting is strict.</p> <p>Letting $\bar{\mathbf{X}} = \frac{1}{n} \sum_{i = 1}^n \mathbf{X}^{(i)}$, the sample mean vector, we can rewrite the likelihood of the sample under $H_1$ as:</p> \[\begin{aligned} \mathcal{L}(\theta, \Sigma; \mathbf{X}) &amp;= \frac{1}{(\sqrt{2 \pi})^{kn} \rvert \Sigma \rvert^n} \exp\left(- \frac{1}{2} \sum_{i = 1}^n (\mathbf{X}^{(i)} - \theta)^\top \Sigma^{-1} (\mathbf{X}^{(i)} - \theta) \right) \\ &amp;= \frac{1}{(\sqrt{2 \pi})^{kn} \rvert \Sigma \rvert^n} \exp\left(- \frac{1}{2} \sum_{i = 1}^n \left[ (\mathbf{X}^{(i)} - \bar{\mathbf{X}})^\top \Sigma^{-1} (\mathbf{X}^{(i)} - \bar{\mathbf{X}}) + n(\bar{\mathbf{X}} - \theta)^\top \Sigma^{-1}(\bar{\mathbf{X}} - \theta) \right] \right) \\ \end{aligned}\] <p>The standard likelihood ratio test statistic is given by:</p> \[\begin{aligned} t_{LRT} &amp;= \frac{\underset{\theta_i = 0 \\ i = 1, \dots, k}{\max} \{ \mathcal{L}(\theta, \Sigma; \mathbf{X}) \}}{\underset{\theta_i \geq 0 \\ i = 1, \dots, k}{\max} \{ \mathcal{L}(\theta, \Sigma; \mathbf{X}) \}} = \frac{\exp\left(-\frac{1}{2} n \bar{\mathbf{X}}^\top \Sigma^{-1}\bar{\mathbf{X}}\right)}{\underset{\theta_i \geq 0 \\ i = 1, \dots, k}{\max} \{ \exp\left(-\frac{1}{2}n(\bar{\mathbf{X}} - \theta)^\top \Sigma^{-1}(\bar{\mathbf{X}} - \theta)\right) \}} \end{aligned}\] <ul id="claim-2" class="tab" data-tab="808efb54-67bf-43db-9a14-1a6267bedaee" data-name="claim-2"> <li class="active" id="claim-2-claim"> <a href="#">claim </a> </li> <li id="claim-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="808efb54-67bf-43db-9a14-1a6267bedaee" data-name="claim-2"> <li class="active"> <p>Notice that the argument maximum of the above is equivalent to:</p> \[\bar{\chi}^2 = n \left[ \bar{\mathbf{X}}^\top \Sigma^{-1}\bar{\mathbf{X}} - \underset{\theta_i \geq 0 \\ i = 1, \dots, k}{\arg\min} \left\{ (\bar{\mathbf{X}} - \theta)^\top \Sigma^{-1}(\bar{\mathbf{X}} - \theta) \right\} \right]\] </li> <li> \[\begin{aligned} \theta^* &amp;= \frac{\exp\left(-\frac{1}{2} n \bar{\mathbf{X}}^\top \Sigma^{-1}\bar{\mathbf{X}}\right)}{\underset{\theta_i \geq 0 \\ i = 1, \dots, k}{\arg\max} \{ \exp\left(-\frac{1}{2}n(\bar{\mathbf{X}} - \theta)^\top \Sigma^{-1}(\bar{\mathbf{X}} - \theta)\right) \}} \\ &amp;= \underset{\theta_i \geq 0 \\ i = 1, \dots, k}{\arg\min} \left\{ \frac{\exp\left(-\frac{1}{2} n \bar{\mathbf{X}}^\top \Sigma^{-1}\bar{\mathbf{X}}\right)}{\exp\left(-\frac{1}{2}n(\bar{\mathbf{X}} - \theta)^\top \Sigma^{-1}(\bar{\mathbf{X}} - \theta)\right)} \right\} &amp; \left(\text{maximize denom. = minimize quotient} \right) \\ &amp;= \underset{\theta_i \geq 0 \\ i = 1, \dots, k}{\arg\min} \left\{ \exp\left( -\frac{1}{2} n \bar{\mathbf{X}}^\top \Sigma^{-1}\bar{\mathbf{X}} + \frac{1}{2}n(\bar{\mathbf{X}} - \theta)^\top \Sigma^{-1}(\bar{\mathbf{X}} - \theta) \right) \right\} \\ &amp;= \underset{\theta_i \geq 0 \\ i = 1, \dots, k}{\arg\min} \left\{ -\frac{1}{2} n \bar{\mathbf{X}}^\top \Sigma^{-1}\bar{\mathbf{X}} + \frac{1}{2}n(\bar{\mathbf{X}} - \theta)^\top \Sigma^{-1}(\bar{\mathbf{X}} - \theta) \right\} &amp; \left(\exp \text{ monotonic} \right)\\ &amp;= n \underset{\theta_i \geq 0 \\ i = 1, \dots, k}{\arg\min} \left\{ -\bar{\mathbf{X}}^\top \Sigma^{-1}\bar{\mathbf{X}} + (\bar{\mathbf{X}} - \theta)^\top \Sigma^{-1}(\bar{\mathbf{X}} - \theta) \right\} \\ &amp;= n \underset{\theta_i \geq 0 \\ i = 1, \dots, k}{\arg\max} \left\{ \bar{\mathbf{X}}^\top \Sigma^{-1}\bar{\mathbf{X}} - (\bar{\mathbf{X}} - \theta)^\top \Sigma^{-1}(\bar{\mathbf{X}} - \theta) \right\} &amp; \left( \text{minimize = maximize negative} \right)\\ &amp;= n \left[ \bar{\mathbf{X}}^\top \Sigma^{-1}\bar{\mathbf{X}} - \underset{\theta_i \geq 0 \\ i = 1, \dots, k}{\arg\min} \left\{ (\bar{\mathbf{X}} - \theta)^\top \Sigma^{-1}(\bar{\mathbf{X}} - \theta) \right\} \right] &amp; \left(\text{ maximize quantity = minimize positive subtraction} \right) \end{aligned}\] </li> </ul> <hr/> <h2 id="a-one-sided-score-test">A One-Sided Score Test</h2> <p>I’ll next dip into the work of Silvapulle and Silvapulle<d-cite key="silvapulle1995"></d-cite>, who present a score-type test statistic for one-side alternative hypotheses based upon estimating functions instead of the true score function.</p> <h3 id="set-up-1">Set-Up</h3> <p>The authors present a fairly general setting where we do not assume we know the exact form of the distribution of the observations, only that they depend on some $k \times 1$-dimensional vector-valued parameter, $\theta$, that is partitioned into the nuisance parameters, $\lambda$, and the components of interest, $\psi$. We’ll write the partitioned parameter vector as $(\lambda : \psi) = (\lambda^\top, \psi^\top)^\top$ where $\lambda$ is $(k - q) \times 1$ and $\psi$ is $q \times 1$.</p> <p>We’re interested in testing hypotheses of the form:</p> \[H_0: \psi = \mathbf{0}_q \hspace{10mm} H_A: \psi \in \mathcal{C} \nonumber\] <p>where $\mathbf{0}_q$ is a $q$-dimensional vector of zeros and $\mathcal{C}$ is the $q$-dimensional Euclidean space, or a closed, convex cone in $q$-dimensional Euclidean space with its vertex at the origin. The latter case encompasses alternatives of the form $\psi \geq \mathbf{0}_q$, which is the alternative hypothesis I am interested in for variance component testing.</p> <p>Define $\mathbf{D}(\lambda)$ as some (fixed) matrix-valued function of the nuisance parameter, $\lambda$, that is independent of some (potentially vector-valued) random variable, $\delta$. Also define the $q \times 1$ vector, $\mathbf{U}_0$, as some function of the data.</p> <p>Suppose under the sequence of alternatives $K_n: \psi = n^{-1/2}\delta$, $\mathbf{U}_0$ satisfies:</p> <p>\begin{equation} \label{eq:U-condition} \mathbf{U}_0 \rightsquigarrow \mathcal{N}(\delta, \mathbf{D}(\lambda)) \end{equation}</p> <p>as our sample size $n \rightarrow \infty$. Notice that testing the alternative hypothesis that $\psi \geq 0$ is equivalent to testing $\delta \geq \mathbf{0}_q$, which implies that the null hypothesis is equivalent to $\delta = \mathbf{0}_q$.</p> <h3 id="a-general-test-statistic">A General Test Statistic</h3> <p>The authors define a very general test statistic as the following.</p> <div class="definition"> <strong>Definition (Test Statistic).</strong> Let $\tilde{\mathbf{D}}(\lambda)$ be any consistent estimator under the null hypothesis of $\mathbf{D}(\lambda)$, the asymptotic covariance matrix of $\mathbf{U}_0$. The test statistic for $H_0: \psi = \mathbf{0}$ against $H_A: \psi \in \mathcal{C}$ has the form: $$ T = \mathbf{U}_0^\top \tilde{\mathbf{D}}(\lambda)^{-1}\mathbf{U}_0 - \underset{\mathbf{b} \in \mathcal{C}}{\inf} \left\{ (\mathbf{U}_0 - \mathbf{b})^\top \tilde{\mathbf{D}}(\lambda)^{-1}(\mathbf{U}_0 - \mathbf{b})\right\} $$ </div> <p>A $p$-value for large sample sizes can be found by defining $\mathbf{Z} \sim \mathcal{N}(\mathbf{0}, \mathbf{D}(\lambda))$ and:</p> <p>\begin{equation} \label{eq:xi-defn} \xi(t, \mathbf{D}(\lambda), \mathcal{C}) = \mathbb{P}\left( \left[ \mathbf{Z}^\top \mathbf{D}(\lambda)^{-1} \mathbf{Z} - \underset{\mathbf{b} \in \mathcal{C}}{\inf} \left\{ (\mathbf{Z} - \mathbf{b})^\top \mathbf{D}(\lambda)^{-1}(\mathbf{Z} - \mathbf{b}) \right\} \right] \geq t \right) \end{equation}</p> <p>The quantity $1 - \xi(t, \mathbf{D}(\lambda), \mathcal{C})$ follows a chi-bar-squared distribution; that is, a mixture of chi-squared distributions as we introduced in the previous section. The weights for the mixture can be hard to find, but we can get around this using the fact that, for large enough $n$ and under $H_0$ (i.e. $\delta = \mathbf{0}$), $\mathbf{U}_0$ is approximately $\mathcal{N}(\mathbf{0}, \mathbf{D}(\lambda))$. Thus, $\mathbb{P}(T \geq t; \lambda) \approx \xi(t, \mathbf{D}(\lambda), \mathcal{C})$.</p> <p>Suppose we observe a value of $T$, $t^*$. Define \(\mathbf{D}^*(\lambda)\) as a consistent estimator of $\mathbf{D}(\lambda)$ for any $\lambda$. Then it follows that:</p> \[p \approx \underset{\lambda}{\sup} \left\{ \xi(t^*, \mathbf{D}^*(\lambda), \mathcal{C}) \right\} \nonumber\] <p>for large enough $n$ because $\lambda$ is a nuisance parameter, so we can take the “best” probability over all of its values.</p> <h3 id="use">Use</h3> <p>How do we use this test statistic in practice? This is pretty much just a question of what function, $\mathbf{U}_0$, of our data we want to pick. Silvapulle and Silvapulle explain how to construct a score-type test statistic using their general statistic.</p> <p>Let’s define $\mathbf{S}_n(\theta)$ as any $k \times 1$ vector estimating equation (so it should have expectation zero) for $\theta$ (e.g. the score function or something else). We need this vector to satisfy a couple of conditions.</p> <h4 id="conditions">Conditions</h4> <p>Suppose $\mathbf{S}_n(\theta)$ is such that there exist non-singular $\mathbf{G}(\theta)$ and $\mathbf{V}(\theta)$ satisfying for any $a &gt; 0$:</p> <p>\begin{equation} \label{eq:condition-a1} \frac{1}{\sqrt{n}}\mathbf{S}_n(\theta) \rightsquigarrow \mathcal{N}(\mathbf{0}, \mathbf{V}(\theta)) \end{equation}</p> <p>and</p> <p>\begin{equation} \label{eq:condition-a2} \underset{\rvert \rvert \mathbf{h} \rvert \rvert \leq a}{\sup} \left\{ \frac{1}{\sqrt{n}} \left( \mathbf{S}_n\left(\theta + \frac{1}{\sqrt{n}} \mathbf{h}\right) - \mathbf{S}_n(\theta) \right) + \mathbf{G}(\theta) \mathbf{h} \right\} = o_p(1) \end{equation}</p> <p>where $o_p(1)$ is stochastic order notation for convergence in probability to $0$.</p> <p>The first condition basically states that $\mathbf{S}_n(\theta)$ is asymptotically Gaussian when suitably scaled. Let’s take a closer look at the second condition (this will be kind of hand-wavy).</p> <p>Suppose we fix $\mathbf{h}$. The directional derivative of $\mathbf{S}_n(\theta)$ at $\theta$ along $\mathbf{h}$ is given by the limit:</p> \[\nabla_{\mathbf{h}} \mathbf{S}_n(\theta) = \underset{s \rightarrow 0}{\lim} \left[ \frac{\mathbf{S}_n(\theta + s \mathbf{h}) - \mathbf{S}_n(\theta)}{s \rvert \rvert \mathbf{h} \rvert \rvert} \right]\] <p>Technically, this is the definition for a scalar function, but we can just use the above notation to mean the limits are taken element-wise to get the result for a vector-valued function.</p> <p>If we let $s = \frac{1}{\sqrt{n}}$, then we can rewrite the above as the limit as $n \rightarrow \infty$:</p> \[\nabla_{\mathbf{h}} \mathbf{S}_n(\theta) = \underset{n \rightarrow \infty}{\lim} \left[ \frac{\sqrt{n}}{\rvert \rvert \mathbf{h} \rvert \rvert} \left( \mathbf{S}_n \left(\theta + \frac{1}{\sqrt{n}} \mathbf{h} \right) - \mathbf{S}_n(\theta) \right) \right]\] <p>Scaling by $- \frac{1}{n}$, we get:</p> \[\nabla_{\mathbf{h}} \left[ -\frac{1}{n} \mathbf{S}_n(\theta)\right] = -\frac{1}{\rvert \rvert \mathbf{h} \rvert \rvert} \underset{n \rightarrow \infty}{\lim} \left[ \frac{1}{\sqrt{n} } \left( \mathbf{S}_n \left(\theta + \frac{1}{\sqrt{n}} \mathbf{h} \right) - \mathbf{S}_n(\theta) \right) \right]\] <p>Recall that for differentiable functions, the directional derivative is equal to the dot product between the gradient and the normalized direction vector. That is:</p> \[\begin{aligned} &amp;\nabla_{\mathbf{h}} \left[ -\frac{1}{n} \mathbf{S}_n(\theta)\right] = \frac{\partial}{\partial \theta} \left[ - \frac{1}{n} \mathbf{S}_n(\theta) \right] \cdot \frac{\mathbf{h}}{\rvert \rvert \mathbf{h}\rvert \rvert} \\ \implies &amp;-\underset{n \rightarrow \infty}{\lim} \left[ \frac{1}{\sqrt{n} } \left( \mathbf{S}_n \left(\theta + \frac{1}{\sqrt{n}} \mathbf{h} \right) - \mathbf{S}_n(\theta) \right) \right] = \frac{\partial}{\partial \theta} \left[ - \frac{1}{n} \mathbf{S}_n(\theta) \right] \cdot \mathbf{h} \end{aligned}\] <p>Let’s subtract $\mathbf{G}(\theta) \mathbf{h}$ from both sides.</p> \[\begin{aligned} &amp;- \mathbf{G}(\theta) \mathbf{h} - \underset{n \rightarrow \infty}{\lim} \left[ \frac{1}{\sqrt{n} } \left( \mathbf{S}_n \left(\theta + \frac{1}{\sqrt{n}} \mathbf{h} \right) - \mathbf{S}_n(\theta) \right) \right] = \left( \frac{\partial}{\partial \theta} \left[ - \frac{1}{n} \mathbf{S}_n(\theta) \right] - \mathbf{G}(\theta) \right) \mathbf{h} \\ \implies &amp; - \left(\underset{n \rightarrow \infty}{\lim} \left[ \frac{1}{\sqrt{n} } \left( \mathbf{S}_n \left(\theta + \frac{1}{\sqrt{n}} \mathbf{h} \right) - \mathbf{S}_n(\theta) \right) \right] + \mathbf{G}(\theta) \mathbf{h} \right) = \left( \frac{\partial}{\partial \theta} \left[ - \frac{1}{n} \mathbf{S}_n(\theta) \right] - \mathbf{G}(\theta) \right) \mathbf{h} \end{aligned}\] <p>Notice that both $\mathbf{G}(\theta)$ and $\mathbf{h}$ are independent of $n$, so we can take them inside the limit:</p> \[- \left(\underset{n \rightarrow \infty}{\lim} \left[ \frac{1}{\sqrt{n} } \left( \mathbf{S}_n \left(\theta + \frac{1}{\sqrt{n}} \mathbf{h} \right) - \mathbf{S}_n(\theta) \right) + \mathbf{G}(\theta) \mathbf{h} \right] \right) = \left( \frac{\partial}{\partial \theta} \left[ - \frac{1}{n} \mathbf{S}_n(\theta) \right] - \mathbf{G}(\theta) \right) \mathbf{h}\] <p>Condition \eqref{eq:condition-a2} basically implies that the lefthand side will approach $0$, which itself implies that $\mathbf{G}(\theta) = \frac{\partial}{\partial \theta} \left[ - \frac{1}{n} \mathbf{S}_n(\theta) \right]$ in the limit.</p> <p>Let’s partition our vectors and matrices in the following ways:</p> \[\mathbf{S}_n(\theta) = \begin{bmatrix} \mathbf{S}^\top_{n, \lambda}(\theta) &amp; \mathbf{S}^\top_{n, \psi}(\theta) \end{bmatrix}^\top \hspace{2mm} \text{ and } \hspace{2mm} \mathbf{G}(\theta) = \begin{bmatrix} \mathbf{G}_{\lambda, \lambda}(\theta) &amp; \mathbf{G}_{\lambda, \psi}(\theta) \\ \mathbf{G}_{\psi, \lambda}(\theta) &amp; \mathbf{G}_{\psi, \psi}(\theta) \end{bmatrix} \hspace{2mm} \text{ and } \hspace{2mm} \mathbf{V}(\theta) = \begin{bmatrix} \mathbf{V}_{\lambda, \lambda}(\theta) &amp; \mathbf{V}_{\lambda, \psi}(\theta) \\ \mathbf{V}_{\psi, \lambda}(\theta) &amp; \mathbf{V}_{\psi, \psi}(\theta) \end{bmatrix}\] <p>Let \(\theta_0 = (\lambda : \mathbf{0})\) denote the value of $\theta$ under the null hypothesis, and suppose the null is true. Define the quantities:</p> \[\begin{aligned} \mathbf{Z}_n(\theta_0) &amp;= n^{-1/2} \left( \mathbf{S}_{n, \psi}(\theta_0) - \mathbf{G}_{\psi, \lambda}(\theta_0) \mathbf{G}_{\lambda, \lambda}^{-1}(\theta_0) \mathbf{S}_{n, \lambda}(\theta_0) \right) \\ \mathbf{C}(\theta_0) &amp;= \mathbf{V}_{\psi, \psi}(\theta_0) - \mathbf{G}_{\psi, \lambda}(\theta_0) \mathbf{G}^{-1}_{\lambda, \lambda}(\theta_0) \mathbf{V}_{\lambda, \psi}(\theta_0) - \left( \mathbf{V}_{\psi, \lambda}(\theta_0) - \mathbf{G}_{\psi, \lambda}(\theta_0) \mathbf{G}_{\lambda, \lambda}^{-1}(\theta_0) \mathbf{V}_{\lambda, \lambda}(\theta_0) \right)\left(\mathbf{G}^{-1}_{\lambda, \lambda}(\theta_0)\right)^\top \mathbf{G}^\top_{\psi, \lambda}(\theta_0) \end{aligned}\] <p>Since the above condition (Eq. \eqref{eq:condition-a1}) is assumed to be satisfied, and $\mathbf{Z}_n$ is just a function of $\mathbf{S}_n$:</p> \[n^{-1/2} \mathbf{S}_n(\theta_0) \rightsquigarrow \mathcal{N}(\mathbf{0}, \mathbf{V}(\theta_0)) \hspace{2mm} \implies \hspace{2mm} \mathbf{Z}_n(\theta_0) \rightsquigarrow \mathcal{N}(\mathbf{0}, \mathbf{C}(\theta_0))\] <p>Denote consistent estimators for $\mathbf{G}(\theta_0)$ and $\mathbf{V}(\theta_0)$ with $\tilde{\mathbf{G}}(\theta_0)$ and $\tilde{\mathbf{V}}(\theta_0)$, respectively. Furthermore, let $\tilde{\lambda}$ denote a “suitable” estimator for $\lambda$ (where suitable is not really specific, but examples are given in the text), and let $\tilde{\theta}_0 = (\tilde{\lambda} : \mathbf{0})$. Define:</p> \[\begin{aligned} \mathbf{G}^{\psi, \psi}(\theta) &amp;= \left( \mathbf{G}_{\psi, \psi}(\theta) - \mathbf{G}_{\psi, \lambda}(\theta) \mathbf{G}^{-1}_{\lambda, \lambda}(\theta) \mathbf{G}_{\lambda, \psi}(\theta) \right)^{-1} \\ \mathbf{A}(\theta) &amp;= \left( \mathbf{G}^\top(\theta) \mathbf{V}^{-1}(\theta)\mathbf{G}(\theta) \right)^{-1} \\ \tilde{\mathbf{Z}}_n(\tilde{\theta}_0) &amp;= n^{-1/2} \left( \mathbf{S}_{n, \psi}(\tilde{\theta}_0) - \tilde{\mathbf{G}}_{\psi, \lambda}(\tilde{\theta}_0) \tilde{\mathbf{G}}_{\lambda, \lambda}^{-1}(\tilde{\theta}_0) \mathbf{S}_{n, \lambda}(\tilde{\theta}_0) \right) \end{aligned}\] <p>Using these, define:</p> \[\mathbf{U}(\tilde{\theta}_0) = \tilde{\mathbf{G}}^{\psi, \psi}(\theta_0) \tilde{\mathbf{Z}}_n(\tilde{\theta}_0)\] <p>where $\tilde{\mathbf{G}}^{\psi, \psi}$ is a consistent estimator for $\mathbf{G}^{\psi, \psi}(\theta_0)$.</p> <p>Let’s partition $\mathbf{A}(\theta)$ in the same way that we did with $\mathbf{V}(\theta)$ and $\mathbf{G}(\theta)$. With some work, we can see that for a fixed \(\delta \in \mathcal{C}\), \(\mathbf{U}(\tilde{\theta}_0) \rightsquigarrow \mathcal{N}(\delta, \mathbf{A}_{\psi, \psi}(\theta_0))\) under the sequence of alternatives $H_n: \psi = n^{-1/2} \delta$ as we take $n \rightarrow \infty$. Thus, $\mathbf{U}$ is a function of the data satisfying the condition in Eq. \eqref{eq:U-condition} and can be used in our test statistic construction.</p> <div id="test-statistic"></div> <div class="definition"> <body> <strong>Definition (One-Sided Test Statistic).</strong> <br/> The test statistic for $H_0: \psi = \mathbf{0}$ against $H_A: \psi \in \mathcal{C}$ is given by: $$ T_s = \mathbf{U}^\top(\tilde{\theta}_0) \tilde{\mathbf{A}}_{\psi, \psi}^{-1}(\tilde{\theta}_0) \mathbf{U}(\tilde{\theta}_0) - \underset{\mathbf{b} \in \mathcal{C}}{\inf} \left\{ (\mathbf{U}(\tilde{\theta}_0) - \mathbf{b})^\top \tilde{\mathbf{A}}_{\psi, \psi}^{-1}(\tilde{\theta}_0) (\mathbf{U}(\tilde{\theta}_0) - \mathbf{b}) \right\} \label{eq:test-stat-2} $$ where $\tilde{\mathbf{A}}_{\psi, \psi}^{-1}(\tilde{\theta}_0)$ is the partition of $\mathbf{A}_{\psi, \psi}^{-1}(\tilde{\theta}_0)$ corresponding to $(\psi, \psi)$ and constructed using $\tilde{\mathbf{G}}(\tilde{\theta}_0)$ and $\tilde{\mathbf{V}}(\tilde{\theta}_0)$ (I think...the authors never define $\tilde{\mathbf{A}}$). </body> </div> <p>A large sample $p$-value can be obtained as:</p> \[p \approx \underset{\lambda}{\sup} \left\{ \xi(t^*, \mathbf{A}_{\psi, \psi}(\theta_0), \mathcal{C}) \right\}\] <p>where $\xi(\cdot, \cdot, \cdot)$ is defined as in Eq. \eqref{eq:xi-defn} and $t^*$ is the observed value of $T_s$.</p> <hr/> <h2 id="results">Results</h2> <div class="theorem"> <strong> Lemma 1 (Silvapulle and Silvapulle)</strong> <ul id="lemma-1" class="tab" data-tab="32b5cc1f-dd18-4f20-b4a9-1c8da6286de2" data-name="lemma-1"> <li class="active" id="lemma-1-statement"> <a href="#">statement </a> </li> <li id="lemma-1-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="32b5cc1f-dd18-4f20-b4a9-1c8da6286de2" data-name="lemma-1"> <li class="active"> <p>Let $\hat{\theta}$ be an estimator of $\theta$ using the entire parameter space (no restrictions imposed). Let $\mathcal{P}$ denote a closed and convex cone with its vertex at the origin. Let $\mathbf{B}$ be a positive definite matrix independent of $\theta$, and let \(\mathbf{B}_{\psi, \psi \cdot \lambda} = \mathbf{B}_{\psi, \psi} - \mathbf{B}_{\psi, \lambda} \mathbf{B}_{\lambda, \lambda}^{-1} \mathbf{B}_{\lambda, \psi}\).</p> <p>Note that:</p> \[\begin{aligned} (\hat{\theta} - \theta)^\top \mathbf{B} (\hat{\theta} - \theta) &amp;= \left( \begin{bmatrix} \hat{\psi} \\ \hat{\lambda} \end{bmatrix} - \begin{bmatrix} \psi \\ \lambda \end{bmatrix} \right)^\top \begin{bmatrix} \mathbf{B}_{\psi, \psi} &amp; \mathbf{B}_{\psi, \lambda} \\ \mathbf{B}_{\lambda, \psi} &amp; \mathbf{B}_{\lambda, \lambda} \end{bmatrix} \left( \begin{bmatrix} \hat{\psi} \\ \hat{\lambda} \end{bmatrix} - \begin{bmatrix} \psi \\ \lambda \end{bmatrix} \right) \end{aligned} \nonumber\] <p>The minimum of the above expression over just $\psi \in \mathcal{P}$ is equivalent to \(\underset{\psi \in \mathcal{P}}{\min} \left \{ (\hat{\psi} - \psi)^\top \mathbf{B}_{\psi, \psi \cdot \lambda}(\hat{\psi} - \psi) \right \}\) where we get \(\mathbf{B}_{\psi, \psi \cdot \lambda}\) by adjusting $\mathbf{B}_{\psi, \psi}$ for the uncertainty in $\hat{\lambda}$.</p> <p>Let:</p> \[(\bar{\psi} : \bar{\lambda}) := \underset{\psi \in \mathcal{P}}{\arg \min} \left \{ (\hat{\theta} - \theta)^\top \mathbf{B}(\hat{\theta} - \theta) \right \}\] <p>Then:</p> \[\bar{\lambda} = \hat{\lambda} + \mathbf{B}_{\lambda, \lambda}^{-1} \mathbf{B}_{\lambda, \psi}(\hat{\psi} - \bar{\psi})\] </li> <li> <p>There are two steps that go into the proof. We need to show that:</p> \[\underset{\psi \in \mathcal{P}}{\min} \left\{ (\hat{\theta} - \theta)^\top \mathbf{B} (\hat{\theta} - \theta) \right\} = \underset{\psi \in \mathcal{P}}{\min} \left\{ (\hat{\psi} - \psi)^\top \mathbf{B}_{\psi, \psi \cdot \lambda}(\hat{\psi} - \psi) \right\} \nonumber\] <p>We also need to show that \(\bar{\lambda} = \hat{\lambda} + \mathbf{B}_{\lambda, \lambda}^{-1} \mathbf{B}_{\lambda, \psi}(\hat{\psi} - \bar{\psi})\).</p> <p>The second claim can be shown by supposing we know $\bar{\psi}$ and finding $\bar{\lambda}$ by optimizing over $\lambda$. Plugging in our expression for $\bar{\lambda}$ into the objective function, we can decompose it into two pieces, one of which is the objective function in the RHS of the first claim. Then all we need to do is show that the minimizer of that piece is the same as $\bar{\psi}$.</p> <p>Since the minimization is over $\psi \in \mathcal{P}$, we’ll assume $\psi \in \mathcal{P}$. To save space, define $J(\theta) = (\hat{\theta} - \theta)^\top \mathbf{B}(\hat{\theta} - \theta)$.</p> <p>Notice that $\underset{\theta}{\min} \left{ J(\theta) \right}$ is the same as $\underset{\psi}{\min} \left{ J(\psi : \bar{\lambda}) \right}$ and also $\underset{\lambda}{\min} \left{ J(\bar{\psi} : \lambda) \right}$. That is, minimizing $J(\theta)$ over all values of $\theta$ is the same as if we plugged in the minimizing value of $\psi$ ($\bar{\psi}$) and then just minimized over values of $\lambda$ (and vice versa).</p> <p>Since $J(\bar{\psi} : \lambda)$ has a quadratic form, we can set the derivative equal to $0$ and solve to get a minimizing value for $\lambda$:</p> \[\begin{aligned} \frac{\partial}{\partial \lambda} \left[ J(\bar{\psi} : \lambda) \right] &amp;= \frac{\partial}{\partial \lambda} \left[ \left( \begin{bmatrix} \hat{\psi} \\ \hat{\lambda} \end{bmatrix} - \begin{bmatrix} \psi \\ \lambda \end{bmatrix} \right)^\top \begin{bmatrix} \mathbf{B}_{\psi, \psi} &amp; \mathbf{B}_{\psi, \lambda} \\ \mathbf{B}_{\lambda, \psi} &amp; \mathbf{B}_{\lambda, \lambda} \end{bmatrix} \left( \begin{bmatrix} \hat{\psi} \\ \hat{\lambda} \end{bmatrix} - \begin{bmatrix} \psi \\ \lambda \end{bmatrix} \right) \right] \\ &amp;= \frac{\partial}{\partial \lambda} \left[ \begin{bmatrix} (\hat{\psi} - \psi)^\top \mathbf{B}_{\psi, \psi} + (\hat{\lambda} - \lambda)^\top \mathbf{B}_{\lambda, \psi} \\ (\hat{\psi} - \psi)^\top \mathbf{B}_{\psi, \lambda} + (\hat{\lambda} - \lambda)^\top \mathbf{B}_{\lambda, \lambda} \end{bmatrix}^\top \left( \begin{bmatrix} \hat{\psi} \\ \hat{\lambda} \end{bmatrix} - \begin{bmatrix} \psi \\ \lambda \end{bmatrix} \right) \right] \\ &amp;= \frac{\partial}{\partial \lambda} \left[ (\hat{\psi} - \psi)^\top \mathbf{B}_{\psi, \psi}(\hat{\psi} - \psi) + (\hat{\lambda} - \lambda)^\top \mathbf{B}_{\lambda, \psi} (\hat{\psi} - \psi) + (\hat{\psi} - \psi)^\top \mathbf{B}_{\psi, \lambda}(\hat{\lambda} - \lambda) + (\hat{\lambda} - \lambda)^\top \mathbf{B}_{\lambda, \lambda} (\hat{\lambda} - \lambda) \right] \\ &amp;= 2(\hat{\lambda} - \lambda)^\top \mathbf{B}_{\lambda, \lambda} - (\hat{\psi} - \psi)^\top \mathbf{B}_{\psi, \lambda} - \left( \mathbf{B}_{\lambda, \psi} (\hat{\psi} - \psi) \right)^\top\\ &amp;= -2(\hat{\lambda} - \lambda)^\top \mathbf{B}_{\lambda, \lambda} - 2(\hat{\psi} - \psi)^\top \mathbf{B}_{\psi, \lambda} \end{aligned} \nonumber\] <p>where in the last line we assume the authors mean <i>symmetric</i> positive definite when they say positive definite. <br/></p> <p>Setting this equal to $0$ yields:</p> \[\begin{aligned} &amp;0 = \frac{\partial}{\partial \lambda} \left[ J(\bar{\psi} : \lambda) \right] \\ \implies &amp;0 = -2(\hat{\lambda} - \lambda)^\top \mathbf{B}_{\lambda, \lambda} - 2(\hat{\psi} - \bar{\psi})^\top \mathbf{B}_{\psi, \lambda} \\ \implies &amp;-(\hat{\psi} - \bar{\psi})^\top\mathbf{B}_{\psi, \lambda} = (\hat{\lambda} - \lambda)^\top \mathbf{B}_{\lambda, \lambda} \\ \implies &amp;\lambda^\top \mathbf{B}_{\lambda, \lambda} = (\hat{\psi} - \bar{\psi})^\top \mathbf{B}_{\psi, \lambda} + \hat{\lambda}^\top \mathbf{B}_{\lambda, \lambda} \\ \implies &amp;\lambda^\top = (\hat{\psi} - \bar{\psi})^\top\mathbf{B}_{\psi, \lambda} \mathbf{B}_{\lambda, \lambda}^{-1} + \hat{\lambda}^\top \\ \implies &amp;\bar{\lambda} = \hat{\lambda} + \mathbf{B}_{\lambda, \lambda}^{-1}\mathbf{B}_{\lambda, \psi} (\hat{\psi} - \bar{\psi}) \end{aligned} \nonumber\] <p>This shows that that there exists a point $\bar{\lambda}$ satisfying the form specified in the Lemma. Now we want to show that this corresponds to the minimum over $\psi \in \mathcal{P}$.</p> <p>Since this value of $\lambda$ is optimal for any fixed $\psi$, we can just plug this into $J(\psi : \bar{\lambda})$ and minimize to find a value for $\bar{\psi}$:</p> \[\begin{aligned} J(\psi : \bar{\lambda}) &amp;= \begin{bmatrix} \hat{\psi} - \psi \\ -\mathbf{B}_{\lambda, \lambda}^{-1} \mathbf{B}_{\lambda, \psi} (\hat{\psi} - \bar{\psi}) \end{bmatrix}^\top \begin{bmatrix} \mathbf{B}_{\psi, \psi} &amp; \mathbf{B}_{\psi, \lambda} \\ \mathbf{B}_{\lambda, \psi} &amp; \mathbf{B}_{\lambda, \lambda} \end{bmatrix} \begin{bmatrix} \hat{\psi} - \psi \\ -\mathbf{B}_{\lambda, \lambda}^{-1} \mathbf{B}_{\lambda, \psi} (\hat{\psi} - \bar{\psi}) \end{bmatrix} \\ &amp;= \begin{bmatrix} (\hat{\psi} - \psi)^\top \mathbf{B}_{\psi, \psi} - (\hat{\psi} - \bar{\psi})^\top \mathbf{B}_{\psi, \lambda} \mathbf{B}_{\lambda, \lambda}^{-1} \mathbf{B}_{\lambda, \psi} \\ (\hat{\psi} - \psi)^\top \mathbf{B}_{\psi, \lambda} - (\hat{\psi} - \bar{\psi})^\top \mathbf{B}_{\psi, \lambda} \mathbf{B}_{\lambda, \lambda}^{-1} \mathbf{B}_{\lambda, \lambda} \end{bmatrix}^\top \begin{bmatrix} \hat{\psi} - \psi \\ -\mathbf{B}_{\lambda, \lambda}^{-1} \mathbf{B}_{\lambda, \psi} (\hat{\psi} - \bar{\psi}) \end{bmatrix} \\ &amp;= (\hat{\psi} - \psi)^\top\mathbf{B}_{\psi, \psi}(\hat{\psi} - \psi) \underbrace{ - (\hat{\psi} - \bar{\psi})^\top \mathbf{B}_{\psi, \lambda} \mathbf{B}_{\lambda, \lambda}^{-1} \mathbf{B}_{\lambda, \psi}(\hat{\psi} - \psi) - (\hat{\psi} - \psi)^\top\mathbf{B}_{\psi, \lambda} \mathbf{B}_{\lambda, \lambda}^{-1} \mathbf{B}_{\lambda, \psi} (\hat{\psi} - \bar{\psi}) + (\hat{\psi} - \bar{\psi})^\top \mathbf{B}_{\psi, \lambda} \mathbf{B}_{\lambda, \lambda}^{-1} \mathbf{B}_{\lambda, \psi} (\hat{\psi} - \bar{\psi})}_{(a)}\\ &amp;= (\hat{\psi} - \psi)^\top\mathbf{B}_{\psi, \psi}(\hat{\psi} - \psi) - (\hat{\psi} - \psi)^\top \mathbf{B}_{\psi, \lambda} \mathbf{B}^{-1}_{\lambda, \lambda} \mathbf{B}_{\lambda, \psi}(\hat{\psi} - \psi) + (\bar{\psi} - \psi)^\top \mathbf{B}_{\psi, \lambda} \mathbf{B}^{-1}_{\lambda, \lambda} \mathbf{B}_{\lambda, \psi}(\bar{\psi} - \psi) \\ &amp;= \underbrace{(\hat{\psi} - \psi)^\top\left( \mathbf{B}_{\psi, \psi} - \mathbf{B}_{\psi, \lambda} \mathbf{B}^{-1}_{\lambda, \lambda} \mathbf{B}_{\lambda, \psi} \right) (\hat{\psi} - \psi)}_{=: f(\psi)} + \underbrace{(\bar{\psi} - \psi)^\top \mathbf{B}_{\psi, \lambda} \mathbf{B}^{-1}_{\lambda, \lambda} \mathbf{B}_{\lambda, \psi}(\bar{\psi} - \psi)}_{=: g(\psi)} \end{aligned} \nonumber\] <details> <summary>Details Of $(a)$.</summary> In $(a)$, we do the following simplification. Let $\bar{\mathbf{B}} = \mathbf{B}_{\psi, \lambda} \mathbf{B}^{-1}_{\lambda, \lambda} \mathbf{B}_{\lambda, \psi}$. Then: $$ \begin{aligned} (a) &amp;= -(\hat{\psi} - \bar{\psi})^\top \bar{\mathbf{B}} (\hat{\psi} - \psi) - (\hat{\psi} - \psi)^\top \bar{\mathbf{B}} (\hat{\psi} - \bar{\psi}) + (\hat{\psi} - \bar{\psi})^\top \bar{\mathbf{B}} (\hat{\psi} - \bar{\psi}) \\ &amp;= \left(-\hat{\psi}^\top\bar{\mathbf{B}}\hat{\psi}+ \bar{\psi}^\top \bar{\mathbf{B}} \hat{\psi} + \hat{\psi}^\top \bar{\mathbf{B}} \psi - \bar{\psi}^\top \bar{\mathbf{B}} \psi \right) + \left( - \hat{\psi}^\top \bar{\mathbf{B}} \hat{\psi} + \psi^\top \bar{\mathbf{B}} \hat{\psi} + \hat{\psi}^\top \bar{\mathbf{B}} \bar{\psi} - \psi^\top \bar{\mathbf{B}} \bar{\psi} \right) + \left( \hat{\psi}^\top \bar{\mathbf{B}} \hat{\psi} - \bar{\psi}^\top \bar{\mathbf{B}} \hat{\psi} - \hat{\psi}^\top \bar{\mathbf{B}} \bar{\psi} + \bar{\psi}^\top \bar{\mathbf{B}} \bar{\psi} \right) \\ &amp;= \underbrace{-\hat{\psi}^\top \bar{\mathbf{B}}\hat{\psi} + \hat{\psi}^\top \bar{\mathbf{B}} \psi + \psi^\top \bar{\mathbf{B}} \hat{\psi}}_{(i)} + \underbrace{\bar{\psi}^\top \bar{\mathbf{B}} \bar{\psi} -\bar{\psi}^\top \bar{\mathbf{B}} \psi - \psi^\top \bar{\mathbf{B}} \bar{\psi}}_{(ii)} \\ &amp;= \underbrace{(\bar{\psi} - \psi)^\top \bar{\mathbf{B}} (\bar{\psi} - \psi) - \psi^\top \bar{\mathbf{B}} \psi}_{=(ii)} - \underbrace{\left((\hat{\psi} - \psi)^\top \bar{\mathbf{B}}(\hat{\psi} - \psi) + \psi^\top \bar{\mathbf{B}}\psi\right)}_{=(i)} \\ &amp;= (\bar{\psi} - \psi)^\top \bar{\mathbf{B}} (\bar{\psi} - \psi) - (\hat{\psi} - \psi)^\top \bar{\mathbf{B}}(\hat{\psi} - \psi) \end{aligned} \nonumber $$ </details> <p>We can see that minimizing $J(\psi: \bar{\lambda})$ over values of $\psi$ is equivalent to minimizing $f(\psi) + g(\psi)$, where:</p> \[f(\psi) = (\hat{\psi} - \psi)^\top \mathbf{B}_{\psi, \psi \cdot \lambda} (\hat{\psi} - \psi)\hspace{25mm} g(\psi) = (\bar{\psi} - \psi)^\top \mathbf{B}_{\psi, \lambda} \mathbf{B}^{-1}_{\lambda, \lambda} \mathbf{B}_{\lambda, \psi}(\bar{\psi} - \psi) \nonumber\] <p>We also know that $f(\psi)$ and $g(\psi)$ are strictly convex and convex, respectively.</p> <details> <summary>Proofs Of Convexity.</summary> First, let's look at $f(\psi)$, in which the middle portion is the Schur complement of $\mathbf{B}_{\lambda, \lambda}$. The positive definiteness of $\mathbf{B}$ implies that the Schur complement is also positive definite (since we also assume $\mathbf{B}_{\lambda, \lambda}$ is invertible). Since $f(\psi)$ has a quadratic form associated with a positive definite matrix, it is strictly convex. <br/> <br/> Looking at $g(\psi)$, we see that the middle portion, $\mathbf{B}_{\lambda, \lambda}^{-1}$ is positive definite due to the fact that the principal sub-matrices of a positive definite matrix are also positive definite and the inverse of a positive definite matrix is also positive definite. Secondly, we know that pre- and post-multiplying a positive definite matrix by another matrix will yield a positive semi-definite matrix. Thus, $g(\psi)$ is positive semi-definite, which implies that it is convex since it is a quadratic form associated with a positive semi-definite matrix. </details> <p>Denote the minimizer of $f(\psi)$ with \(\psi^*\) and consider the line segment \([ \bar{\psi}, \psi^* ]\), which connects the minimizing value of $\psi$ for $J(\theta)$ with the minimizing value of $\psi$ for $f(\psi)$. <br/> Supposedly we can show that $\psi^* = \bar{\psi}$, which would complete the proof. However, I am unsure of how to show this. Here is my best attempt so far, which is an adaptation of <a href="https://math.stackexchange.com/questions/3108304/does-this-special-case-of-convex-quadratic-programming-have-a-partially-unique-s ">AlephZero’s proof</a> on Math StackExchange: <br/> Assume \((\psi^*, \bar{\lambda})\) is a solution to the minimization problem (I don’t know how to show this). Suppose \(\psi^* \neq \bar{\psi}\). Since both \(\bar{\psi}\) and \(\psi^*\) are within $\mathcal{P}$, which is closed and convex, the entire line segment is also contained in $\mathcal{P}$ and any convex combination of $\bar{\psi}$ and $\psi^*$ is also a solution. Let \(a := J(\bar{\psi} : \bar{\lambda}) = J(\psi^* : \bar{\lambda})\), and let $t \in (0, 1)$. Let $t(\psi : \lambda)$ denote the scaling of the respecting entries in the concatenated vector of parameter values. We then have:</p> \[\begin{aligned} J(t(\bar{\psi} : \bar{\lambda}) + (1 - t)(\psi^* : \bar{\lambda})) &amp;= J(t\bar{\psi} + (1-t)\psi^* : \bar{\lambda}) \\ &amp;= f(t\bar{\psi} + (1-t)\psi^*) + g(t\bar{\psi} + (1-t) \psi^*) \\ &amp;&lt; t f(\bar{\psi}) + (1-t)f(\psi^*) + tg(\bar{\psi}) + (1-t)g(\psi^*) \hspace{15mm} f \text{ is strictly convex and } g \text{ is convex} \\ &amp;= t J(\bar{\psi} : \bar{\lambda}) + (1 - t) J(\psi^* : \bar{\lambda}) \\ &amp;= a \end{aligned} \nonumber\] <p>This implies any convex combination of $\bar{\psi}$ and $\psi^<em>$ achieves a value smaller than the minimum, which is a contradiction. Thus, $\bar{\psi} = \psi^</em>$.</p> <p style="color:red;">TODO: FINISH PROOF</p> </li> </ul> </div> <p>With the above lemma, we can prove the following theorem.</p> <div class="theorem"> <strong>Theorem 1 (Silvapulle and Silvapulle)</strong> <ul id="theorem-1" class="tab" data-tab="9849cb2d-b32b-468c-bb43-38288dbd446a" data-name="theorem-1"> <li class="active" id="theorem-1-statement"> <a href="#">statement </a> </li> <li id="theorem-1-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="9849cb2d-b32b-468c-bb43-38288dbd446a" data-name="theorem-1"> <li class="active"> <p>Define $\mathbf{S}_n(\theta) = \frac{\partial}{\partial \theta} \left[ \ell(\theta) \right]$ as the score function (the derivative of the log-likelihood), and assume that it satisfies Condition \eqref{eq:condition-a1} and \eqref{eq:condition-a2}. Suppose we are testing $H_0: \psi = \mathbf{0}$ against $H_A: \psi \in \mathcal{C}$ for $\mathcal{C}$ as defined above. As $n \rightarrow \infty$, the likelihood ratio test statistic, $LR = -2 \left(\ell(\theta_0) - \ell(\hat{\theta}) \right)$ where $\hat{\theta}$ is the MLE of $\theta$ over the entire parameter space, satisfies:</p> \[LR = T_s + o_p(1) \nonumber\] <p>under the null hypothesis.</p> </li> <li> <p>We’ll assume we’re in the setting where the null hypothesis is true ($\theta_0 = (\lambda : \mathbf{0})$). Since we’ve taken our estimating equation, $\mathbf{S}_n(\theta)$, as the score, we can take $\mathbf{G}(\theta_0)$ to be the Fisher information under the null (WHY?).</p> <p>First, let’s rewrite the log-likelihood, $\ell(\theta)$, with a Taylor expansion about the unrestricted estimator, $\hat{\theta}$:</p> \[\ell(\theta) = \ell(\hat{\theta}) + \frac{n}{2}(\hat{\theta} - \theta)^\top \mathbf{G}(\theta_0) (\hat{\theta} -\theta) + \Delta(\theta) \nonumber\] <p>where, for any $a &gt; 0$, $\underset{\rvert \rvert \theta - \theta_0 \rvert \rvert \leq \frac{a}{\sqrt{n}}}{\sup} \left{ \rvert \Delta(\theta) \rvert \right} = o_p(1)$. This last term is just a bound on all of the terms in the Taylor expansion that are of a higher order than two, written with stochastic order notation. A related proof can be found in my post “A Score Test Primer”. The multiplication by $n$ is just because we assume we have $n$ i.i.d. observations, so we can just multiply the value for a single observation by the number we have.</p> <p>From this expansion, we can rewrite the likelihood ratio test statistic as:</p> \[LR = n\left[ \underset{\psi = \mathbf{0}}{\min} \left\{ (\hat{\theta} - \theta)^\top \mathbf{G}(\theta_0) (\hat{\theta} - \theta)\right\} - \underset{\psi \in \mathcal{C}}{\min}\left\{ (\hat{\theta} - \theta)^\top \mathbf{G}(\theta_0) (\hat{\theta} - \theta) \right\} \right] + o_p(1) \nonumber\] <p>Using Lemma 1, we can see that:</p> \[\begin{aligned} LR &amp;= n\left[ \underset{\psi = \mathbf{0}}{\min} \left\{ (\hat{\psi} - \psi)^\top \mathbf{G}_{\psi, \psi \cdot \lambda}(\theta_0) (\hat{\psi} - \psi) \right\} - \underset{\psi \in \mathcal{C}}{\min}\left\{(\hat{\psi} - \psi)^\top \mathbf{G}_{\psi, \psi \cdot \lambda}(\theta_0) (\hat{\psi} - \psi) \right\} \right] + o_p(1) \\ &amp;= n \left[ \underset{\psi = \mathbf{0}}{\min} \left\{ \hat{\psi}^\top \mathbf{G}_{\psi, \psi \cdot \lambda}(\theta_0) \hat{\psi}\right\} + \underset{\psi \in \mathcal{C}}{\min}\left\{(\hat{\psi} - \psi)^\top \mathbf{G}_{\psi, \psi \cdot \lambda}(\theta_0) (\hat{\psi} - \psi) \right\}\right] + o_p(1) \\ &amp;= n \left[ \hat{\psi}^\top \mathbf{G}_{\psi, \psi \cdot \lambda}(\theta_0) \hat{\psi} + \underset{\psi \in \mathcal{C}}{\inf}\left\{(\hat{\psi} - \psi)^\top \mathbf{G}_{\psi, \psi \cdot \lambda}(\theta_0) (\hat{\psi} - \psi) \right\}\right] + o_p(1) \hspace{15mm} \mathcal{C} \text{ is closed, so } \min \text{ is same as } \inf \\ \end{aligned} \nonumber\] <p>Notice that $\sqrt{n} \mathbf{G}(\theta_0) (\hat{\theta} - \theta_0) = \frac{1}{\sqrt{n}} \mathbf{S}_n(\theta_0) + o_p(1)$ because</p> \[T_s = \mathbf{U}^\top \tilde{\mathbf{A}}_{\psi, \psi}^{-1}\mathbf{U} - \underset{\mathbf{b} \in \mathcal{C}}{\inf} \left\{ (\mathbf{U} - \mathbf{b})^\top \tilde{\mathbf{A}}_{\psi, \psi}^{-1}(\mathbf{U} - \mathbf{b}) \right\}\] <p style="color:red;">TODO: FINISH PROOF</p> </li> </ul> </div>]]></content><author><name></name></author><category term="likelihood"/><category term="theory"/><category term="score-test"/><category term="testing"/><summary type="html"><![CDATA[In many cases, we may want to test the null hypothesis that a parameter is zero against a one-sided alternative (e.g. the parameter is non-negative). In this setting, we are constraining the alternative parameter space, and for some parameters (such as variance components), the value of the parameter under the null may be on the boundary.]]></summary></entry><entry><title type="html">Likelihood and Large-Sample Theory</title><link href="https://aerosengart.github.io/blog/2025/likelihood-theory/" rel="alternate" type="text/html" title="Likelihood and Large-Sample Theory"/><published>2025-02-02T00:00:00+00:00</published><updated>2025-02-02T00:00:00+00:00</updated><id>https://aerosengart.github.io/blog/2025/likelihood-theory</id><content type="html" xml:base="https://aerosengart.github.io/blog/2025/likelihood-theory/"><![CDATA[<p>The score test in non-standard conditions has been the motivation for much of my reading these past few months. However, it has led me to wonder about the small details of the test in standard conditions. What <i>exactly</i> are the regularity conditions and when are they usually satisfied? When can we appeal to large-sample theory for the score test? It is slightly more challenging than anticipated to get a straight answer to these questions, and this is the purpose of this post.</p> <p>This post relies on some measure theory, which I’ve covered in <a href="/posts/2025/measure-theory">another post</a>. Most of the content comes from Moran (1971)<d-cite key="moran1971"></d-cite> and Schervish (1995)<d-cite key="schervish1995"></d-cite>.</p> <hr/> <h2 id="background">Background</h2> <p>Suppose we have some probability space $(S, \mathcal{A}, \mu)$. A random variable is some function $X: S \rightarrow \mathcal{X}$ where $\mathcal{X}$ is the sample space (which we only require to be a Borel space but is usually a subset of Euclidean space) with $\sigma$-field $\mathcal{B}$. Individual elements of $\mathcal{X}$ are denoted with $x$.</p> <p>Let \(\mathcal{P}_\Theta\) be a family of distributions for $X$ parametrized by $\Theta: S \rightarrow \Omega$ where $\Omega$ is the parameter space with $\sigma$-field $\tau$. Individual elements of $\Omega$ are denoted with $\theta$. Denote the conditional distribution of $X$ given $\Theta = \theta$ with $P_\theta$ (which is a distribution on $(\mathcal{X},\mathcal{B})$).</p> <p>To make our notation match less measure theoretical texts, we’ll use $X$ to denote a random variable with realizations denoted with the lowercase $x$. A parameter will be denoted with $\Theta$ with particular values denoted by $\theta$ and its true value by $\theta^*$. The density of $X$ given parameter $\Theta$ evaluated at a particular $x$ and $\theta$ will be denoted by $f_{X \rvert \Theta}(x; \theta)$ or, more compactly, $f(x; \theta)$.</p> <hr/> <h2 id="score-and-information">Score and Information</h2> <p>We’ll first define a <strong>very</strong> important quantity in likelihood theory: the <i>score function</i>.</p> <div class="definition"> <strong>Definition (Score).</strong> <body> Suppose $\Theta$ is $k$-dimensional, let $f_{X \rvert \Theta}(x; \theta)$ denote the density of $X$ given $\Theta = \theta$. The <i>score function</i> (or <i>score statistic</i>) is given by: $$ \begin{equation} \label{eq:score} U_\Theta(x; \theta) = \frac{\partial \log f_{X \rvert \Theta}(x; \Theta)}{\partial \Theta} \bigg\rvert_{\Theta = \theta} = \frac{\partial \log f_{X \rvert \Theta}(x; \theta)}{\partial \theta} \end{equation} $$ </body> </div> <p>The score function is the gradient of the log density of the data with respect to the parameter. It describes the curvature of the log density at a particular value of the parameter $\Theta$.</p> <div class="definition"> <strong>Definition (Fisher Information).</strong> <body> Suppose $\Theta$ is $k$-dimensional, let $f_{X \rvert \Theta}(x; \theta)$ denote the density of $X$ given $\Theta = \theta$. The <i>Fisher information</i> is the expectation of the squared gradient of the log density of the data: $$ \begin{equation} \label{eq:information} \mathcal{I}_X(\theta) = \mathbb{E}_\Theta \left[ U_\Theta(x; \theta) U_\theta(x; \theta)^\top \right] \end{equation} $$ </body> </div> <p>The Fisher information describes the amount of information about $\Theta$ held by $X$. It is also the variance of the score function under conditions when the score has expectation zero.</p> <h3 id="fisher-information-conditions">Fisher Information Conditions</h3> <p>Schervish outlines several regularity conditions, which he terms the <i>Fisher Information (FI) conditions</i>, that are needed for the definition of the Fisher information and some nice results about the properties of the score.</p> <h4 id="condition-1">Condition 1</h4> <blockquote> <p>There exists a subset of the sample space, $B$, with measure $0$ (i.e. $\mu(B) = 0$) such that $\frac{\partial f_{X \rvert \Theta}(x; \theta)}{\partial \theta_i}$ exists for any $x \notin B$ and all values (and coordinates) of $\theta$.</p> </blockquote> <p>Condition 1 requires that the partial derivatives with respect to all coordinates of $\theta$ (for all values of $\theta$) exists almost surely.</p> <p>Intuitively (and a bit hand-wavily), this means that the derivatives must exist for all possible values of $\theta$ for pretty much any sample. This implies that log-likelihood functions that have cusps or points will not be differentiable at the particular value of $\theta$ where the feature occurs, implying a violation of this condition.</p> <div class="example"> <body> <strong>Example.</strong> Suppose we have $n$ i.i.d. samples $x_1, \dots, x_n \sim Unif(0, \theta)$ for some value $\theta$. The log-likelihood is given by: $$ \ell(\theta; x_1, \dots, x_n) = \log \left( \prod_{i = 1}^n \frac{1}{\theta} \mathbb{I}(0 \leq x_i \leq \theta) \right) = \log \left( \frac{1}{\theta^n} \mathbb{I}(0 \leq x_i \leq \theta; \forall i) \right) = \log \left(\frac{1}{\theta^n}\mathbb{I}(\min_i x_i \geq 0) \mathbb{I}(\max_i x_i \leq \theta) \right) \nonumber $$ The MLE is $\max_i x_i$. Clearly, the above is not differentiable at this point. </body> </div> <h4 id="condition-2">Condition 2</h4> <blockquote> <p>The order of integration and differentiation can be exchanged for all coordinates of $\theta$. That is: \(\frac{\partial}{\partial \theta_i} \int f_{X \rvert \Theta}(x; \theta) d\mu(x) = \int \frac{\partial f_X(x; \theta)}{\partial \theta_i} d \mu(x)\)</p> </blockquote> <p>Condition 2 states the order of integration of differentiation can be exchanged. Since differentiation is basically just a particular limit, we can use results about the interchanging of the integral and limit to get results about interchaing the integral with differentiation.</p> <div class="theorem"> <body> <strong>Theorem 1. (Dominated Convergence Theorem)</strong> <br/> For a sequence of measurable functions $\{f_n\}_{n = 1}^\infty$ and measurable functions $f$ and $g$ satisfying $f_n(x) \rightarrow f(x)$ almost everywhere, $\rvert f_n(x) \rvert \leq g(x)$ almost everywhere, and $\int g(x) d\mu(x) &lt; \infty$: $$ \underset{n \rightarrow \infty}{\lim} \int f_n(x) d\mu(x) = \int f(x) d\mu(x) \nonumber $$ The dominated convergence theorem states that the integral of the limit of a sequence of measurable functions equals the limit of the integral of each element in the sequence. </body> </div> <p>The Dominated Convergence Theorem states that we can interchange the order of limits and integrals for (certain) functions that are always smaller than (in absolute value) some other function with finite integral. If we define a function that mimics the form of the derivative as a limit (something along the lines of $h(x) = \frac{f(x + \delta) - f(x)}{\delta}$), then we can use this theorem to get results for derivatives and integrals.</p> <p>This is the basic idea of the Leibniz integral rule:</p> <div class="theorem"> <body> <strong>Theorem 2. (Leibniz Integral Rule)</strong> <br/> Let $\Omega$ be an open subset of $\mathbb{R}$ and $(S, \mathcal{A}, \mu) =: \mathcal{M}$ be a measure space. Let $f: \Omega \times \mathcal{M} \rightarrow \mathbb{R}$ be a function that satisfies: <ul> <li>$f(x, \theta)$ is Lebesgue-integrable in $x$ for all $\theta \in \Omega$</li> <li>$\frac{\partial f(x, \theta)}{\partial \theta}$ exists for all $\theta \in \Omega$ and for <i>almost all</i> $x \in \mathcal{M}$</li> <li>There exists integrable function $g: \mathcal{M} \rightarrow \mathbb{R}$ that is integrable and satisfies $\big\rvert \frac{\partial f(x, \theta)}{\partial \theta} \big\rvert \leq g(x)$ for all $\theta \in \Omega$ and almost every $x \in \mathcal{M}$</li> </ul> Then for all $\theta \in \Omega$: $$ \frac{d}{d\theta} \int_\mathcal{M} f(x, \theta) dx = \int_{\mathcal{M}} \frac{\partial}{\partial \theta} f(x, \theta) dx $$ </body> </div> <p>In summary, if our log-likelihood/density satisfies the (Lebesgue version of the) Leibniz Rule conditions, then it will satisfy Condition 2.</p> <h4 id="condition-3">Condition 3</h4> <blockquote> <p>The set \(C = \{ x: f_X(x \rvert \theta) &gt; 0 \}\) is the same $\forall \theta$.</p> </blockquote> <p>Condition 3 states that the support of $f_X(x; \theta)$ should not depend on $\theta$. This is fairly easy to verify because we usually assume we know the family of distributions that our data are drawn from. I won’t go into any more details than this.</p> <h3 id="implications">Implications</h3> <p>The FI conditions allow us to obtain the following results that are pretty fundamental for later likelihood theory.</p> <ul id="score-exp-0" class="tab" data-tab="5bd0c759-06be-47b8-8033-73d95945203f" data-name="score-exp-0"> <li class="active" id="score-exp-0-claim"> <a href="#">claim </a> </li> <li id="score-exp-0-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="5bd0c759-06be-47b8-8033-73d95945203f" data-name="score-exp-0"> <li class="active"> <p>When these conditions hold, the score has expectation $0$.</p> </li> <li> \[\begin{aligned} \mathbb{E}_{\Theta}\left[ U_\Theta(x; \theta) \right] &amp;= \int f_{X \rvert \Theta}(x; \theta) \frac{\partial \log f_{X \rvert \Theta}(x; \theta)}{\partial \Theta} d x \\ &amp;= \int \frac{\partial f_{X \rvert \Theta}(x; \theta)}{\partial \Theta} d x \\ &amp;= \frac{\partial}{\partial \Theta} \int f_{X \rvert \Theta}(x; \theta) d x \\ &amp;= \frac{\partial}{\partial \Theta} [1]\\ &amp;= 0 \end{aligned}\] </li> </ul> <p>Furthermore, we have the following result under additional constraints.</p> <ul id="fisher-info-1" class="tab" data-tab="555ed4f9-8a25-4a8f-a6f3-faeabaa33838" data-name="fisher-info-1"> <li class="active" id="fisher-info-1-claim"> <a href="#">claim </a> </li> <li id="fisher-info-1-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="555ed4f9-8a25-4a8f-a6f3-faeabaa33838" data-name="fisher-info-1"> <li class="active"> <p>If the log-likelihood is twice differentiable with respect to $\theta$, then the Fisher information is equal to:</p> \[\mathcal{I}_X(\theta) = - \mathbb{E}_\Theta \left[ \frac{\partial^2 \log f_{X \rvert \Theta}(x; \Theta)}{\partial \Theta \partial \Theta^\top} \bigg\rvert \theta \right]\] </li> <li> \[\begin{aligned} \frac{\partial^2}{\partial \Theta \partial \Theta^\top} \left[ \log f_{X \rvert \Theta}(x; \Theta)\right] &amp;= \frac{\partial}{\partial \Theta} \left[ \frac{1}{f_{X \rvert \Theta}(x; \Theta)} \frac{\partial}{\partial \Theta^\top} \left[ f_{X \rvert \Theta}(x; \Theta) \right] \right] \\ &amp;= \frac{\partial}{\partial \Theta} \left[ \frac{1}{f_{X\rvert \Theta}(x; \Theta)} \right] \frac{\partial}{\partial \Theta^\top} \left[ f_{X \rvert \Theta}(x; \Theta) \right] + \frac{1}{f_{X\rvert \Theta}(x; \Theta)} \frac{\partial^2}{\partial \Theta \partial \Theta^\top} \left[ f_{X \rvert \Theta}(x; \Theta) \right] \\ &amp;= - \frac{\frac{\partial}{\partial \Theta} \left[ f_{X \rvert \Theta}(x; \Theta) \right] \frac{\partial}{\partial \Theta^\top} \left[ f_{X \rvert \Theta}(x; \Theta) \right]}{f^2_{X \rvert \Theta}(x; \Theta)} + \frac{\frac{\partial^2}{\partial \Theta \partial \Theta^\top}[ f_{X \rvert \Theta}(x; \Theta)]}{f_{X \rvert \Theta}(x; \Theta)} \\ &amp;= \frac{\frac{\partial^2}{\partial \Theta \partial \Theta^\top}[ f_{X \rvert \Theta}(x; \Theta)]}{f_{X \rvert \Theta}(x; \Theta)} - \left(\frac{\partial}{\partial \Theta} [ \log f_{X \rvert \Theta}(x; \Theta)]\right)^2 \end{aligned}\] <p>Taking the expected value:</p> \[\begin{aligned} \mathbb{E}_\Theta \left[ \frac{\partial^2}{\partial \Theta \partial \Theta^\top} \left[ \log f_{X \rvert \Theta}(x; \Theta)\right] \bigg\rvert \theta \right] &amp;= \mathbb{E}_\Theta \left[ \frac{\frac{\partial^2}{\partial \theta \partial \theta^\top}[ f_{X \rvert \Theta}(x; \theta)]}{f_{X \rvert \Theta}(x; \theta)} - \left(\frac{\partial}{\partial \theta} [ \log f_{X \rvert \Theta}(x; \theta)]\right)^2 \right] \\ &amp;= \mathbb{E}_\Theta \left[ \frac{\frac{\partial^2}{\partial \theta \partial \theta^\top}[ f_{X \rvert \Theta}(x; \theta)]}{f_{X \rvert \Theta}(x; \theta)} \right] - \mathbb{E}_\Theta \left[ \frac{\partial}{\partial \theta} [ \log f_{X \rvert \Theta}(x; \theta)] \right] \\ &amp;= \mathbb{E}_\Theta \left[ \frac{\frac{\partial^2}{\partial \theta \partial \theta^\top}[ f_{X \rvert \Theta}(x; \theta)]}{f_{X \rvert \Theta}(x; \theta)} \right] - \mathcal{I}_X(\theta) \\ &amp;= \int_\mathbb{R} \left( \frac{\frac{\partial^2}{\partial \theta \partial \theta^\top}[ f_{X \rvert \Theta}(x; \theta)]}{f_{X \rvert \Theta}(x; \theta)} \right) f(x; \theta) dx - \mathcal{I}_X(\theta) \\ &amp;= \int_\mathbb{R} \left( \frac{\partial^2}{\partial \theta \partial \theta^\top}[ f_{X \rvert \Theta}(x; \theta)] \right) dx - \mathcal{I}_X(\theta) \\ &amp;\overset{(i)}{=} \frac{\partial^2}{\partial \theta \partial \theta^\top} \left[ \underbrace{\int_\mathbb{R} f_{X \rvert \Theta}(x; \theta) dx}_{= 1} \right] - \mathcal{I}_X \\ &amp;= - \mathcal{I}_X \end{aligned}\] <p>In $(i)$, we rely on the regularity conditions (specifically number 2 above) so we can interchange the order of differentiation and integration.</p> </li> </ul> <hr/> <h2 id="maximum-likelihood">Maximum Likelihood</h2> <p>If we consider the observations $x$ as fixed, then we can define the <i>likelihood function</i> as a function of $\Theta$:</p> \[\mathcal{L}(\theta; x) = f_{X \rvert \Theta}(x; \theta) \label{eq:lik-func}\] <p>One of the most common settings in which the likelihood function will be useful is in statistical inference. A good starting point is in point estimation. Intuitively, it seems reasonable to judge the quality of a parameter estimate by how probable it is one would observe the sample at hand under the assumption that the estimate is the true parameter value. Or, in another way, we might think that the best estimate we could come up with is the one that is most likely to result in the observations we have. Thus, maximum likelihood estimation is born.</p> <div class="definition"> <body> <strong>Definition. (Maximum Likelihood Estimator).</strong> Let $X$ be a random variable with density $f_{X \rvert \Theta}(x; \theta)$, and let $x$ be some realization of $X$. A <i>maximum likelihood estimator (MLE)</i> is any random quantity: $$ \hat{\theta} = \underset{\theta \in \Omega}{\arg\max}\left\{ f_{X \rvert \Theta}(x; \theta) \right\} = \underset{\theta \in \Omega}{\arg \max}\left\{ \mathcal{L}(\theta; x) \right\} $$ The MLE is a function $\hat{\theta}: \mathcal{X} \rightarrow \Omega$ mapping from the sample space to the parameter space. </body> </div> <p>If the parameter space $\Omega$ is compact and the likelihood function is continuous over $\Omega$, then maximum likelihood estimate will exist for a given sample (i.e. the supremum of the maximum likelihood estimator will be achieved in $\Omega$). If the parameter space is open, then the likelihood may increase and never reach a supremum.</p> <div class="example"> <body> <strong>Example.</strong> Consider $x$ distributed uniformly on the open interval $(0, \theta)$. The likelihood is: $$ \mathcal{L}(\theta; x) = \frac{1}{\theta} \mathbb{I}(x &gt; 0) \mathbb{I}(x &lt; \theta) \nonumber $$ This function is decreasing on the interval $(0, \theta)$, and the maximum is never achieved. </body> </div> <p>MLEs exhibit the <i>invariance property</i>, which is, in words, that a function of an MLE is the MLE of that function.</p> <div class="theorem"> <strong>Invariance Property of Maximum Likelihood Estimators.</strong> <ul id="invariance-prop" class="tab" data-tab="246b0df0-6ca5-4998-b3f7-e1f0eb94337c" data-name="invariance-prop"> <li class="active" id="invariance-prop-statement"> <a href="#">statement </a> </li> <li id="invariance-prop-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="246b0df0-6ca5-4998-b3f7-e1f0eb94337c" data-name="invariance-prop"> <li class="active"> <p>Let $\hat{\theta}$ be an MLE of $\Theta$, and let $g$ be some function of $\theta$. Then $g(\hat{\theta})$ is an MLE of $g(\Theta)$.</p> </li> <li> <p>Define the <i>induced likelihood function</i>:</p> \[\mathcal{L}^*(\eta; x) = \underset{\theta: g(\theta) = \eta}{\sup} \left\{ \mathcal{L}(\theta; x) \right\}\] <p>which is a function of $\eta$ equal to the maximum value of the likelihood function over all values of $\theta$ such that $g(\theta) = \eta$. Let:</p> \[\hat{\eta} = \underset{\eta}{\arg\sup}\left\{ \mathcal{L}^*(\eta; x) \right\}; \hspace{5mm} \hat{\theta} = \underset{\theta}{\arg\sup}\left\{ \mathcal{L}(\theta; x) \right\}\] <p>We have:</p> \[\begin{aligned} \mathcal{L}^*(\hat{\eta}; x) &amp;= \underset{\eta}{\sup}\left\{ \mathcal{L}^*(\eta; x) \right\} \\ &amp;= \underset{\eta}{\sup}\left\{ \underset{\theta: g(\theta) = \eta}{\sup} \left\{ \mathcal{L}(\theta; x) \right\} \right\} \\ &amp;= \underset{\theta}{\sup}\left\{ \mathcal{L}(\theta; x) \right\} \\ &amp;= \mathcal{L}(\hat{\theta}; x) \\ &amp;= \underset{\theta: g(\theta) = g(\hat{\theta})}{\sup} \left\{ \mathcal{L}(\theta; x) \right\} \\ &amp;= \mathcal{L}^*(\hat{\theta}; x) \end{aligned}\] </li> </ul> </div> <h3 id="finding-mles">Finding MLEs</h3> <p>The easiest way to find an MLE is to use set the log-likelihood equal to zero (since monotonic transformations will not affect the $\arg \max$ or $\arg \min$).</p> <div class="theorem"> <body> <strong>Theorem 3. (Fermat's Interior Extremem Theorem)</strong> <br/> For function $f: A \rightarrow \mathbb{R}$, let $x_0 \in A$ be a local extremum of $f$. If $f$ is differentiable at $x_0$, then $f'(x_0) = 0$. <br/> A simple corollary states that a global extremum $x_0$ of $f$ must fall into one of the following cases: (1) $x_0$ is on the boundary of $A$; (2) $f$ is not differentiable at $x_0$; (3) $x_0$ is a stationary point of $f$. </body> </div> <p>For Fermat’s Theorem to apply, the log-likelihood must be differentiable in $\Omega$. The previous example is one such where this is not the case. This theorem also implies that this $x_0$ occurs on the <em>interior</em> of the domain of $f$. We would have to also check the boundary points if $A$ were closed.</p> <h3 id="asymptotic-normality">Asymptotic Normality</h3> <p>The MLE is asymptotically normal under suitable conditions.</p> <div class="theorem"> <body> <strong>Theorem 4. (Asymptotic Normality of MLEs)</strong> <br/> Let $\Omega \subseteq \mathbb{R}^p$ and $\{ X_n \}_{i = 1}^\infty$ be conditionally i.i.d. random variables given $\Theta = \theta^*$ with distribution $P_{\theta^*}$. Let $\hat{\theta}_n$ be an MLE for $\Theta$ and assume $\hat{\theta}_n \overset{p}{\rightarrow} \theta^*$. <br/> Further assume that the second partial derivatives with respect to $\Theta$ of the densities are continuous and that the order of differentiation and integration can be exchanged. Suppose there exists function $H_r(x, \theta)$ such that, for $\theta^*$ in the interior of $\Omega$ and each $k,j$, the following is satisfied with $\underset{r \rightarrow 0}{\lim} \mathbb{E}_{\theta^*} \left[ H_r(X, \theta^*) \right] = 0$: $$ \underset{\rvert\rvert \theta - \theta^* \rvert \rvert \leq r}{\sup} \left\{ \frac{\partial^2 \ell(\theta^*; x)}{\partial \theta_k \partial \theta_j} - \frac{\partial^2 \ell(\theta; x)}{\partial \theta_k \partial \theta_j} \right\} \leq H_r(x, \theta^*) \nonumber $$ And finally, assume the Fisher information for a single data point, $\mathcal{I}_X(\theta)$, is finite and non-singular. Then, under $P_{\theta^*}$: $$ \sqrt{n}\left( \hat{\theta}_n - \theta^* \right) \rightsquigarrow \mathcal{N}\left(0, \mathcal{I}^{-1}_{X}(\theta^*) \right) \nonumber $$ In words, this theorem states that the MLE (suitably centered and scaled) is asymptotically normal. </body> </div> <p>The requirements are that the the true parameter value is on the interior of the parameter space (if it is restricted); the MLE is consistent; the density is nice enough (continuous second derivatives); the order of integration and differentiation can be exchanged; and that there is a function with finite mean that bounds the difference between the second derivatives of the log-likelihoods for two values of $\Theta$. This last condition is a uniform law of large numbers.</p> <p>We often supply \(\hat{\theta}_n\) for (usually unknown) \(\theta^*\) in the Fisher information, which yields the <em>expected Fisher information</em>. The expected Fisher information converges in probability to the Fisher Information given \(\Theta = \theta^*\).</p> <p>We could also instead use the scaled matrix of second partial derivaties of the log-likelihood function near to $\hat{\theta}_n$:</p> \[- \frac{1}{n} \left\{ \frac{\partial^2 \ell(\Theta; x)}{\partial \Theta_i \Theta_j} \bigg\rvert_{\Theta = \hat{\theta}_n} \right\}\] <p>which is called the <i>observed Fisher information</i>. It’s basically the sample analog of the expected information.</p>]]></content><author><name></name></author><category term="theory"/><category term="likelihood"/><summary type="html"><![CDATA[A Primer]]></summary></entry><entry><title type="html">Clustering Stability</title><link href="https://aerosengart.github.io/blog/2025/clustering-stability/" rel="alternate" type="text/html" title="Clustering Stability"/><published>2025-01-31T00:00:00+00:00</published><updated>2025-01-31T00:00:00+00:00</updated><id>https://aerosengart.github.io/blog/2025/clustering-stability</id><content type="html" xml:base="https://aerosengart.github.io/blog/2025/clustering-stability/"><![CDATA[<p>My last post related to clustering discussed how to describe a “good” clustering algorithm. One way to measure this is by <i>stability</i>, which I’ll define more rigorously later.</p> <p>The main paper I’ll be summarizing, <i>Sober Look at Clustering Stability</i>, by Shai Ben-David, Ulrike von Luxberg, and Dávid Pál<d-cite key="bendavid2006"></d-cite> takes a decision theoretic perspective. This allows the authors to derive some results about the behavior of (un)stable algorithms; however it limits their discussion to clustering algorithms that optimize some objective function. I’ll restrict myself to these as well.</p> <hr/> <h2 id="stability-conceptually">Stability (Conceptually)</h2> <p>Similar to the general idea of a robust estimation procedure, a <i>stable</i> clustering algorithm should produce similar results for small perturbations of the data. As Ben-David, von Luxburg, and Pál point out, stability is often used for tuning $k$, the number of clusters the algorithm should yield. Intuitively, choosing $k$ too large will result in arbitrary splitting of true clusters. On the other hand, a $k$ that is too small will do the opposite and arbitrarily merge true clusters.</p> <p>For optimization-based clustering algorithms, there are two sources of instability:</p> <ul> <li> <p><strong>Sampling</strong>: Variability in our observations will result in different values of our objective function. This, in turn, will influence the clustering.</p> </li> <li> <p><strong>Data Distribution</strong>: If the data distribution is such that there are multiple optimizers of the objective function, then there is ambiguity in which clustering is the best.</p> </li> </ul> <h2 id="background-and-notation">Background And Notation</h2> <p>I’ll be following the notation in Ben-David, von Luxburg, and Pál fairly closely throughout this post with a few slight tweaks to make it align better with my habits. Let’s do a quick run-down of their main definitions.</p> <p>Our observations $S = {x_1, \dots, x_n}$ will be assumed to be i.i.d. from some sample space, $X$, with probability measure $P$. If $X$ is a metric space, its metric will be denoted by $\ell$. $P_S$ denotes the uniform distribution over the finite sample $S$.</p> <div id="clustering"></div> <div class="definition"> <body> <strong>Definition (Clustering).</strong> Formally, a <i>clustering</i>, $\mathcal{C}: X \rightarrow \mathbb{N}$, of a set, $X$, is a finite partition. <i>Clusters</i> are the sets of data points in $X$ that are in the same group, and we denote these by $C_i := \{ x \in X; \mathcal{C}(x) = i \}$. A <i>clustering algorithm</i> is a function $A$ that outputs a clustering of $X$ given a finite sample $S \subset X$. </body> </div> <p>We’ll denote the cluster of point $x$ with $C(x)$. If $C(x) = C(y)$, then we write $x \underset{\mathcal{C}}{\sim} y$.</p> <div id="clustering-distance"></div> <div class="definition"> <body> <strong>Definition (Clustering Distance).</strong> For family of probability distributions $\mathcal{P}$ over some data space, $X$, and family of clusterings of $X$, $\mathcal{S}$, a <i>clustering distance</i> will be a function $d: \mathcal{P} \times \mathcal{S} \times \mathcal{S} \rightarrow [0, 1]$ that satisfies the following properties for any $P \in \mathcal{P}$ and $\mathcal{C}_1, \mathcal{C}_2, \mathcal{C}_3 \in \mathcal{S}$: <ul> <li>$d_P(\mathcal{C}_1, \mathcal{C}_1) = 0$</li> <li>$d_P(\mathcal{C}_1, \mathcal{C}_2) = d_P(\mathcal{C}_2, \mathcal{C}_1)$</li> <li>$d_P(\mathcal{C}_1, \mathcal{C}_3) \leq d_P(\mathcal{C}_1, \mathcal{C}_2) + d_P(\mathcal{C}_2, \mathcal{C}_3)$</li> </ul> </body> </div> <p>Note that the above definition differs from the traditional definition of a distance metric in that it is not required that if $d_P(\mathcal{C}_1, \mathcal{C}_2) = 0$ then $\mathcal{C}_1 = \mathcal{C}_2$.</p> <p>With these definitions out of the way, we can finally define stability formally.</p> <div id="stability"></div> <div class="definition"> <body> <strong>Definition (Stability).</strong> For probability distribution $P$ over data space $X$, clustering distance $d$, and clustering algorithm $A$, the <i>stability of $A$ for a sample of size $n$ with respect to $P$</i> is: $$ \text{stab}(A, P, n) = \underset{S_1 \sim P^n; \\ S_2 \sim P^n}{\mathbb{E}} \left[ d_P(A(S_1), A(S_2)) \right]\nonumber $$ The <i>stability of $A$ with respect to $P$</i> is: $$ \text{stab}(A, P) = \underset{n \rightarrow \infty}{\lim \sup} \left[ \text{stab}(A, P, n) \right] $$ where $\underset{n \rightarrow \infty}{\lim \sup} := \underset{n \rightarrow \infty}{\lim} \left[ \underset{m \geq n}{\sup} \text{stab}(A, P, n) \right]$. </body> </div> <p>In words, $\text{stab}(A, P, n)$ is the expected distance between the clusterings output by $A$ whe given two i.i.d. samples of size $n$ drawn from $P$.</p> <div id="r-minimizing"></div> <div class="definition"> <body> <strong>Definition ($R$-Minimizing).</strong> Let $opt(P) := \underset{\mathcal{C} \in \mathcal{S}}{\inf} \left[ R(P, \mathcal{C}) \right]$, the optimal risk achieved by clusterings in $\mathcal{S}$ for distribution $P$. For a sample $S \subseteq X$, the <i>empirical risk</i> is the risk over the empirical data distribution, $R(P_S, \mathcal{C})$. A clustering algorithm, $A$, is then called <i>$R$-minimizing</i> if $R(P_S, \mathcal{C}) = opt(P_S)$ for any $S \subseteq X$. </body> </div> <p>An $R$-minimizing algorithm will achieve empirical risk equal to the optimal risk (for that distribution).</p> <div id="risk-optimizing-converging"></div> <div class="definition"> <body> <strong>Definition (Risk Optimizing and Risk Converging).</strong> For some domain set, $X$, some set of clusterings of $X$, $\mathcal{S}$, some set of probability distributions on $X$, $\mathcal{P}$, a clustering algorithm is said to be _risk optimizing_ if its output clustering is chosen to minimize (or maximize if you switch the signs) an objective function (or risk function), $R: \mathcal{P} \times \mathcal{S} \rightarrow \mathbb{R}^+_0$. <br/> An $R$-minimizing algorithm $A$ is called <i>risk converging</i> if, for every $\epsilon &gt; 0$ and every $\delta \in (0, 1)$, $\exists n_0$ such that $\forall n &gt; n_0$: $$ \underset{S \sim P^n}{\mathbb{P}}\left( R(P, A(S)) &lt; opt(P) + \epsilon \right) &gt; 1 - \delta \nonumber $$ Essentially, a risk converging clustering algorithm will achieve risk within $\epsilon$ of the optimal risk with high probability for a large enough sample size. </body> </div> <p>For clustering distance, $d$, a probability distribution, $P$, has a <i>unique minimizer</i>, \(\mathcal{C}^*\) if for any $\eta &gt; 0$, there exists \(\epsilon &gt; 0\) such that if \(R(P, \mathcal{C}) &lt; opt(P) + \epsilon\), then \(d_P(\mathcal{C}^*, \mathcal{C}) &lt; \eta\). In words, this means that any clustering with risk that is within some $\epsilon$ of the risk achieved by the unique minimizer should be very close (by $\eta$) to the unique minimizer.</p> <p>$P$ is said to have $m$ distinct minimizers if there are \(\mathcal{C}^*_1, \dots, \mathcal{C}^*_m\) such that \(d_P(\mathcal{C}^*_i, \mathcal{C}^*_j) &gt; 0\) for all \(i \neq j\) such that for all \(\eta &gt; 0\), there exists \(\epsilon &gt; 0\) such that if \(R(P, \mathcal{C}) &lt; opt(P) + \epsilon\), then there exists \(1 \leq i \leq m\) such that \(d_P(\mathcal{C}^*_i, \mathcal{C}) &lt; \eta\).</p> <div id="measure-preserving-symmetry"></div> <div class="definition"> <body> <strong>Definition (Measure-Preserving Symmetry).</strong> For probability distribution, $P$, over $(X, \ell)$, a function $g: X \rightarrow X$ is a <i>$P$-preserving symmetry</i> of $(X, \ell)$ if: <ul> <li>$\mathbb{P}(A) = \mathbb{P}(g(A))$ for any $P$-measurable set $A \subseteq X$</li> <li>$\underset{x,y \sim P}{\mathbb{P}}(\ell(x,y) = \ell(g(x), g(y))) = 1$</li> </ul> </body> </div> <p>The above distribution gives us a way to characterize risk functions and clustering distances.</p> <div id="odd"></div> <div class="definition"> <body> <strong>Definition (Distance-Distribution Dependent Risk and Clustering Distance).</strong> A risk function, $R$ is called <i>ODD</i> if, for every distribution $P$, every $P$-preserving symmetry $g$, and every clustering $\mathcal{C}$, we have that $R(P, \mathcal{C}) = R(P, g(\mathcal{C}))$. <br/> A clustering distance, $d$, is called <i>ODD</i> if, for every distribution $P$, every $P$-preserving symmetry $g$, and any clusterings $\mathcal{C}_1, \mathcal{C}_2$, we have that $d_P(\mathcal{C}_1, \mathcal{C}_2) = d_P(g(\mathcal{C}_1), g(\mathcal{C}_2))$. <br/> Intuitively, the above state that ODD risks and distances only depend on distances and distributions. That is, transforming the points in ways that do not affect their probabilities or distances will not affect the risk or distance between the points. All common distances are ODD. </body> </div> <hr/> <h2 id="results">Results</h2> <p>Ben-David et al. present their first theorem as follows:</p> <div class="theorem"> <strong>Theorem 10 (Ben-David et al.)</strong> <ul id="theorem-10-bendavid" class="tab" data-tab="004c32e2-2852-4186-b88b-1eead755917a" data-name="theorem-10-bendavid"> <li class="active" id="theorem-10-bendavid-statement"> <a href="#">statement </a> </li> <li id="theorem-10-bendavid-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="004c32e2-2852-4186-b88b-1eead755917a" data-name="theorem-10-bendavid"> <li class="active"> <p>Any risk converging, $R$-minimizing clustering algorithm will be stable on distribution $P$ if $P$ has a unique minimizer.</p> </li> <li> <p>The basic idea is to show that, for any value $\xi &gt; 0$, the stability of an algorithm $A$ satisfying the stated properties is less than $\xi$ for sufficiently large sample size $m$. This implies that $A$ is stable on $P$.</p> <p>Define $A$ as a risk converging, $R$-minimizing algorithm, and fix some $\xi &gt; 0$. Let \(\mathcal{C}^*\) be the unique minimizer of $P$. First, set some $\delta \in (0, 1)$ and $\eta &gt; 0$ such that $2(\eta + \delta) &lt; \xi$.</p> <p>Since \(\mathcal{C}^*\) is the unique minimizer of $P$, we have that $\exists \epsilon &gt; 0$ such that:</p> \[R(P, \mathcal{C}) &lt; opt(P) + \epsilon \implies d_P(\mathcal{C}^*, \mathcal{C}) &lt; \eta \nonumber\] <p>by the definition of a unique minimizer for a distribution. Furthermore, by the definition of risk convergence, $\exists m_0$ such that for all $m &gt; m_0$ we have:</p> \[\underset{S \sim P^m}{\mathbb{P}} \left( R(P, A(S)) \geq opt(P) + \epsilon \right) &lt; \delta \nonumber\] <p>Consider $m &gt; m_0$ and $S \sim P^m$. If $R(P, A(S)) &lt; opt(P) + \epsilon$, then \(d_P(A(S), \mathcal{C}^*) &lt; \eta\). Since the implication is only one way, the probability of the $R(P, A(S)) &lt; opt(P) + \epsilon$ is less than or equal to the probability of \(d_P(A(S), \mathcal{C}^*) &lt; \eta\) (since there may be some cases where the latter holds without the former). Thus:</p> \[\underset{S \sim P^m}{\mathbb{P}}\left( d_P(A(S), \mathcal{C}^*) \geq \eta \right) \leq \underset{S \sim P^m}{\mathbb{P}}\left( R(P, A(S)) \geq opt(P) + \epsilon\right) &lt; \delta \nonumber\] <p>The bound then follows:</p> \[\begin{aligned} \text{stab}(A, P, m) &amp;= \underset{S_1, S_2 \sim P^m}{\mathbb{E}} \left[ d_P(A(S_1), A(S_2)) \right] \\ &amp;\leq \underset{S_1, S_2 \sim P^m}{\mathbb{E}} \left[ d_P(A(S_1), \mathcal{C}^*) + d_P(\mathcal{C}^*, A(S_2)) \right] \hspace{15mm} \text{(triangle ineq.)}\\ &amp;= 2 \underset{S \sim P^m}{\mathbb{E}}\left[ d_P(A(S), \mathcal{C}^*) \right] \hspace{15mm} \text{i.i.d. samples} \\ &amp;\overset{(i)}{\leq} 2\left( \eta \cdot \underset{S \sim P^m}{\mathbb{P}}\left( d_P(A(S), \mathcal{C}^*) &lt; \eta \right) + 1 \cdot \underset{S \sim P^m}{\mathbb{P}} \left( d_P(A(S), \mathcal{C}^*) \geq \eta \right) \right) \hspace{15mm} \text{defn. of expectation} \\ &amp;\leq 2\left( \eta + \underset{S \sim P^m}{\mathbb{P}} \left( R(P, A(S)) \geq opt(P) + \epsilon \right) \right) \\ &amp;\leq 2(\eta + \delta) \\ &amp;&lt; \xi \end{aligned} \nonumber\] <p>In $(i)$, I am not entirely sure of the choice of $1$. However, I think we can take $\xi$ to be arbitrarily small as $m \rightarrow \infty$?</p> </li> </ul> </div> <p>The authors emphasize a couple points (see the paper for illustrations):</p> <ul> <li> Algorithms (such as $k$-means and $k$-medians) can be stable for <i>any</i> choice of $k$ </li> <li> A stable algorithm does not mean the choice of number of clusters is correct </li> </ul> <p>With these points in mind, the authors prove a related theorem.</p> <div class="theorem"> <strong>Theorem 15 (Ben-David et al.).</strong> <ul id="theorem-15-bd" class="tab" data-tab="aaf2874a-a7a7-4da2-a9e0-1710f6c74d50" data-name="theorem-15-bd"> <li class="active" id="theorem-15-bd-statement"> <a href="#">statement </a> </li> <li id="theorem-15-bd-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="aaf2874a-a7a7-4da2-a9e0-1710f6c74d50" data-name="theorem-15-bd"> <li class="active"> <p>Suppose we have an ODD risk function, $R$, an ODD clustering distance $d$, a probability distribution, $P$, with some number $n$ distinct minimizers, and $P$-symmetry, $g$, such that \(d_P(\mathcal{C}^*, g(\mathcal{C}^*)) &gt; 0\) for every $R$-minimizer \(\mathcal{C}^*\). Any risk convergent, $R$-minimizing clustering algorithm is <i>unstable</i> on $P$.</p> </li> <li> <p>For the $R$-minimizers, denote the clustering they result in as \(\{ \mathcal{C}_1^*, \dots, \mathcal{C}_n^*\}\). Furthermore, let \(r := \underset{1 \leq i \leq n}{\min} \left( d_P(\mathcal{C}^*_i, g(\mathcal{C}^*_i)) \right)\).</p> <p>Choose $\epsilon &gt; 0$ such that a clustering achieving risk within $\epsilon$ of the best possible risk implies the existence of some $R$-minimizer that is close to $\mathcal{C}$ (less than $r/4$ distance between them). That is, $R(P, \mathcal{C}) &lt; opt(P) + \epsilon$ implies $\exists 1 \leq i \leq n$ such that $d_P(\mathcal{C}^*_i, \mathcal{C}) &lt; r/4$.</p> <p>Define $T = { S \in X^m \rvert R(P, A(S)) &lt; opt(P) + \epsilon }$, the set of samples from $X$ of size $m$ whose clustering using $A$ achieves risk within $\epsilon$ of $opt(P)$. $A$ is assumed to be risk convergent, so there exists $m_0$ such that for $m &gt; m_0$, $P(T) &gt; 0.9$ (by choosing $\delta = 0.1$).</p> <p>Now, for $1 \leq i \leq n$, define \(T_i = \{ S \in T \rvert d_p(\mathcal{C}^*_i, A(S)) \leq r/4 \}\), the samples in $T$ whose clusterings with $A$ are within $r/4$ distance of the $i$-th $R$-minimizing clustering, \(\mathcal{C}^*_i\). This step is just defining subsets of $T$ that we know exist since $P$ has $n$ distinct minimizers (which implies this $\epsilon$ exists).</p> <p>Since $P(T) &gt; 0.9$ for sufficiently large $m$, there must be some $i_0$ such that \(P(T_{i_0}) \geq 0.9/n\) (<i>why?</i>). Since $g$ is a $P$-preserving symmetry and $R$ is assumed to be ODD, we know that for any $S \in T$, $g(S) \in T$ (by definition of ODD). The fact that $g$ is a $P$-preserving symmetry also implies that \(P(g[T_{i_0}]) \geq 0.9/n\) since \(P(T_{i_0}) \geq 0.9/n\).</p> <p>By the definition of $r$, we have that \(d_P(\mathcal{C}^*_{i_0}, g(\mathcal{C}^*_{i_0})) \geq r\), and by the construction of $T_{i_0}$, we also have that for all $S \in T_{i_0}$, \(d_P(\mathcal{C}^*_{i_0}, A(S)) \leq r/4\). Recall that $d$ is ODD as well, so for all \(S \in T_{i_0}\), \(d_P(g(\mathcal{C}^*_{i_0}), A(g(S))) \leq r/4\).</p> <p>By our definition of clustering distances, $d$ must satisfy the triangle inequality. Thus, for any $S \in T_{i_0}$ and $S’ \in g[T_{i_0}]$:</p> \[d_P(A(S), A(S')) \leq d_P(\mathcal{C}^*_i, A(S)) + d_P(\mathcal{C}^*_i, A(S')) \leq \frac{r}{2} \nonumber\] <p>All of the above leads to the following. For any $m \geq m_0$:</p> \[\begin{aligned} stab(A, P, m) &amp;= \underset{S, S' \sim P^m}{\mathbb{E}} \left[ d_P(A(S), A(S')) \right] \\ &amp;\geq \frac{r}{2} \underset{S, S' \sim P^m}{\mathbb{P}} \left( d_P(A(S), A(S')) \geq \frac{r}{2} \right) \hspace{15mm} \text{ ignore other case (which must be non-negative)} \\ &amp;\geq \frac{r}{2}\underset{S, S' \sim P^m}{\mathbb{P}} \left( A \in T_{i_0} \cap S' \in g[T_{i_0}] \right) \\ &amp;= \frac{r}{2} \underset{S \sim P^m}{\mathbb{P}} \left( S \in T_{i_0} \right) \underset{S' \sim P^m}{\mathbb{P}}\left( S' \in g[T_{i_0}] \right) \hspace{15mm} S, S' \text{ independent } \\ &amp;\geq \frac{r0.9^2}{2 n^2} \end{aligned}\] <p>The stability is therefore always positive (regardless of $m$), which implies that $stab(A,P)$ is as well (since $r &gt; 0$). Thus, $A$ is unstable on $P$.</p> </li> </ul> </div>]]></content><author><name></name></author><category term="clustering"/><category term="philosophy"/><category term="paper-reviews"/><summary type="html"><![CDATA[What Does It Mean To Be Stable?]]></summary></entry><entry><title type="html">Clustering - An Axiomatic Approach</title><link href="https://aerosengart.github.io/blog/2025/impossibility-clustering/" rel="alternate" type="text/html" title="Clustering - An Axiomatic Approach"/><published>2025-01-13T00:00:00+00:00</published><updated>2025-01-13T00:00:00+00:00</updated><id>https://aerosengart.github.io/blog/2025/impossibility-clustering</id><content type="html" xml:base="https://aerosengart.github.io/blog/2025/impossibility-clustering/"><![CDATA[<p>Though the journey to this point is a bit confusing, I have recently become interesting in clustering metrics and evaluation. In this post, I’ll work through a couple papers on describing how good a clustering function is based upon a set of axioms. These include Kleinberg’s <i>An Impossibility Theorem for Clustering</i><d-cite key="kleinberg2002"></d-cite> and Ben-David and Ackerman’s <i>Measure of Clustering Quality</i><d-cite key="bendavid2008"></d-cite>.</p> <h2 id="background">Background</h2> <p>First, let’s more rigorously define what we mean by <i>clustering</i> and our problem setting as defined by Kleinberg.</p> <div id="clustering"></div> <div class="definition"> <strong>Definition (Clustering).</strong> <body> Suppose we have a set $S$ of $n \geq 2$ observations, which we notate as $S = \{ 1, 2, \dots, n\}$. We'll define a <i>distance function</i> $d: S \times S \rightarrow \mathbb{R}$ as a function satisfying, for all $i,j,k \in S$: <ul> <li>Non-Negativity: $d(i,j) \geq 0$</li> <li>Symmetry: $d(i,j) = d(j,i)$</li> <li>Identity of Indiscernibles: $d(i,j) = 0$ if, and only if, $i = j$</li> <li>Triangle Inequality: $d(i,k) \leq d(i,j) + d(j,k)$</li> </ul> The last condition (Triangle Inequality) is not necessary for the following discussion, but it is a nice property and requiring it makes a choice of $d$ a metric. <br/> A <i>clustering function</i>, $f$, is a function operating on a distance function $d$ on $S$ that outputs a partition $\Gamma$ of $S$. A partition is called <i>trivial</i> if each cluster contains one point or if it has only one cluster. </body> </div> <p>We can evaluate a clustering function with what the author terms a <i>clustering quality metric</i>.</p> <div id="cqm"></div> <div class="definition"> <strong>Definition (Clustering Quality Metric).</strong> <body> A <i>clustering-quality measure (CQM)</i> is a function operating on a partition $\Gamma$ of a set $S$ with respect to distance $d$ that outputs a non-negative real number which represents the "goodness" of the clustering $\Gamma$. That is, a CQM $m$ is the function $m: (\Gamma, S, d) \rightarrow \mathbb{R}^+_0$. </body> </div> <p>Finally, we’ll define some helpful terms and quantities that will be used later. Let $\alpha \cdot d$ denote scaling the distance function $d$ by $\alpha$. That is $\alpha \cdot d$ is the distance function who assigns distance $\alpha d(i,j)$ between points $i$ and $j$. We’ll denote all possible output partitions of $f$ as $\text{Range}(f)$. We denote the fact points $i$ and $j$ are in the same cluster in partition $\Gamma$ with $i \underset{\Gamma}{\sim} j$, and we use the notation $i \underset{\Gamma}{\not \sim} j$ if they are not in the same cluster.</p> <div id="rep-set"></div> <div class="definition"> <strong>Definition (Representative Set).</strong> <body> For $\Gamma = \{ \gamma_1, \dots, \gamma_g\}$, $G$ is a <i>representative set</i> of $\Gamma$ if $\rvert G \rvert = g$ and $G\cap \gamma_i \neq \emptyset$ \forall i$. </body> </div> <p>We call a set $G$ a <i>representative set</i> of $\Gamma$ if it contains a single observation from each cluster in $\Gamma$.</p> <div id="isomorphism"></div> <div class="definition"> <strong>Definition (Isomorphism).</strong> <body> Let $\Gamma$ and $\Gamma'$ be partitions of $S$ with $d$. We call $\Gamma$ and $\Gamma'$ <i>isomorphic</i>, denoted by $\Gamma \underset{d}{\approx} \Gamma'$ if there is a <i>distance-preserving isomorphism</i> $\phi: S \rightarrow S$ such that $\forall i,j \in S$, we have $i \underset{\Gamma}{\sim} j$ if, and only if, $\phi(i) \underset{\Gamma'}{\sim} \phi(j)$. </body> </div> <p>A <i>distance-preserving isomorphism</i> can be thought of as a mapping between our source set, $S$, and some target set $\phi(S)$ such that for any $i \in S$, there exists $\phi(i) \in \phi(S)$; for any $i, j \in S$, $d(i, j) = d(\phi(i), \phi(j))$; and there exists an inverse mapping, $\phi’$, such that $\phi’(\phi(i)) = i$.</p> <div id="gamma-transformation"></div> <div class="definition"> <strong>Definition ($\Gamma$-Transformation).</strong> <body> Let $d$ and $d'$ be distance functions on $S$, and let $\Gamma$ be some partition of $S$. We call $d'$ a <i>$\Gamma$-transformation</i> of $d$ if: $$ d'(i,j) \leq d(i,j) \hspace{2mm} \forall \hspace{1mm} i,j \in S \hspace{10mm} \text{s.t. } i \underset{\Gamma}{\sim} j $$ And: $$ d'(i,j) \geq d(i,j) \hspace{2mm} \forall \hspace{1mm} i,j \in S \hspace{10mm} \text{s.t. } i \underset{\Gamma}{\not \sim} j $$ </body> </div> <p>A $\Gamma$-transformation of a distance function $d$ will assign smaller distances to points in the same cluster and larger distances to points in different clusters.</p> <div id="refinement"></div> <div class="definition"> <strong>Definition (Refinement).</strong> <body> Let $\Gamma$ be some partition of $S$. We call another partition $\Gamma'$ of $S$ a <i>refinement</i> of $\Gamma$ if, for every cluster $C' \in \Gamma'$, there is a cluster $C \in \Gamma$ such that $C' \subseteq C$. </body> </div> <p>More intuitively, a refinement is just a finer partition, which means each cluster in $\Gamma$ is either also in $\Gamma’$ or is split into multiple smaller clusters in $\Gamma’$. We can also define the partial order $\Gamma’ \preccurlyeq \Gamma$ if $\Gamma’$ is a refinement of $\Gamma$.</p> <div id="antichain"></div> <div class="definition"> <strong>Definition (Antichain).</strong> <body> An <i>antichain</i> is a collection $\mathcal{A}$ of partitions of $S$ such that for all $\Gamma, \Gamma' \in \mathcal{A}$ where neither $\Gamma$ is a refinement of $\Gamma'$ nor is $\Gamma'$ a refinement of $\Gamma$. We also require that $\Gamma$ and $\Gamma'$ be different partitions. </body> </div> <p>One way I like to conceptualize an antichain is like a collection of subsets where no subset is a subset of any other…but instead of subsets we have partitions.</p> <div id="ab-conforming"></div> <div class="definition"> <strong>Definition ($(a,b)$-Conforming).</strong> <body> Let $\Gamma$ be a partition of $S$. A distance function $d$ is said to <i>$(a,b)$-conform</i> to $\Gamma$ if: $$ d(i,j) \leq a \hspace{2mm} \forall \hspace{1mm} i,j \in S \hspace{10mm} \text{s.t. } i \underset{\Gamma}{\sim} j $$ And: $$ d(i,j) \geq b \hspace{2mm} \forall \hspace{1mm} i,j \in S \hspace{10mm} \text{s.t. } i \underset{\Gamma}{\not \sim} j $$ </body> </div> <div id="gamma-forcing"></div> <div class="definition"> <strong>Definition ($\Gamma$-Forcing).</strong> For a partition $\Gamma$ of $S$ and clustering function $f$, a pair of real numbers $(a, b)$ is called <i>$\Gamma$-forcing</i> with respect to $f$ if $f(d) = \Gamma$ for all distance functions that are $(a,b)$-conforming to $\Gamma$. &lt;/body&gt; </div> <hr/> <h2 id="the-impossibility-theorem">The Impossibility Theorem</h2> <p>Kleinberg’s impossibility theorem is based upon a set of three axioms that characterize a “good” clustering function, $f$.</p> <div id="axioms"></div> <div class="definition"> <strong>Definition (Kleinberg's Axioms).</strong> <body> <ul> <li><i>Scale Invariance:</i> For any distance function $d$ and positive real $\alpha &gt; 0$, $f(d) = f(\alpha \cdot d)$. A scale-invariant clustering function should not change its output when all distances between points are changed by some factor. </li> <li><i>Richness:</i> $\text{Range}(f)$ should be all possible paritions of $S$. Richness implies that our clustering function is flexible enough that any partition of $S$ can be achieved if we find the right distance function.</li> <li><i>Consistency:</i> For any two distance functions $d$ and $d'$ such that $f(d) = \Gamma$ and $d'$ is a $\Gamma$-transformation of $d$, $f(d') = \Gamma$. A consistent clustering function will output the same partition if we make points within the same cluster closer together and make points in different clusters farther apart.</li> </ul> </body> </div> <p>Kleinberg goes on to show that these axioms imply a semi-surprising result.</p> <div class="theorem"> <strong>Theorem 2.1 (Impossibility Theorem).</strong> <body> <ul id="impossibility-theorem" class="tab" data-tab="8965c430-0008-43cf-b377-34dac3431563" data-name="impossibility-theorem"> <li class="active" id="impossibility-theorem-statement"> <a href="#">statement </a> </li> <li id="impossibility-theorem-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="8965c430-0008-43cf-b377-34dac3431563" data-name="impossibility-theorem"> <li class="active"> <p>For $n \geq 2$, there does not exists a clustering function $f$ that is scale-invariant, rich, and consistent.</p> </li> <li> <p>The proof of Theorem 2.1 rests upon the claim that a scale-invariant and consistent clustering fucntion $f$ cannot possibly be rich as $\text{Range(f)}$ forms an antichain (Theorem 3.1 in paper).</p> <p>First, suppose we have a consistent clustering function $f$, and let $\Gamma$ be some partition in $\text{Range}(f)$. Because $\Gamma \in \text{Range}(f)$, $\exists d$ such that $f(d) = \Gamma$ (by definition). Define $a’$ as the minimum distance between any two points in the same cluster over all clusters in $\Gamma$, and define $b’$ as the maximum distance between any two points in different clusters over all clusters in $\Gamma$.</p> <p>Select positive real numbers $a, b$ such that $a &lt; b$, $a \leq a’$, and $b \geq b’$. Notice that, for any distance function $d’$ that $(a,b)$-conforms to $\Gamma$ is a $\Gamma$-transformation of $d$. This is due to the fact that $d’(i,j) \leq a \leq a’ \leq d(i,j)$ for $i,j \in S$ such that $i \underset{\Gamma}{\sim} j$ and $d’(i,j) \geq b \geq b’ \geq d(i,j)$ for $i,j \in S$ such that $i \underset{\Gamma}{\not \sim} j$, which is precisely the definition of a $\Gamma$-transformation.</p> <p>Since $d’$ is a $\Gamma$-transformation of $d$ and $f$ is consistent by assumption, $(a,b)$ is $\Gamma$-forcing.</p> <p>Now we further assume that $f$ is scale-invariant and that $\exists \Gamma_0, \Gamma_1 \in \text{Range}(f)$ such that $\Gamma_0$ is a refinement of $\Gamma_1$. Define $(a_0, b_0)$ and $(a_1, b_1)$ be pairs of reals that are $\Gamma_0$-forcing and $\Gamma_1$-forcing, respectively, such that $a_0 &lt; b_0$ and $a_1 &lt; b_1$. Such pairs can be found by taking any two partitions in $\text{Range}(f)$ and setting $\Gamma_0$ and $\Gamma_1$ according to the argument above.</p> <p>Let $a_2$ be some number such that $a_2 \leq a_1$ and set $\epsilon$ such that $0 &lt; \epsilon &lt; \frac{a_0 a_2}{b_0}$. We now construct the distance function $d$ that satisfies the following:</p> <ul> <li><body>$d(i,j) \leq \epsilon$ for $i,j$ such that $\Gamma_0^i = \Gamma_0^j$</body></li> <li>$a_2 \leq d(i,j) \leq a_1$ for $i,j$ such that $\Gamma_1^i = \Gamma_1^j$ and $\Gamma_0^i \neq \Gamma_0^j$</li> <li>$d(i,j) \geq b_1$ for $i,j$ such that $\Gamma_1^i \neq \Gamma_1^j$</li> </ul> <p>According to the above conditions, $d$ $(a_1, b_1)$-conforms to $\Gamma_1$. Since $f$ is still assumed to be consistent, this implies $f(d) = \Gamma_1$.</p> <p>Let $\alpha = \frac{b_0}{a_2}$ and $d’ = \alpha \cdot d$. Since $f$ is scale-invariant, $f(d’) = f(d) = \Gamma_1$. However, for $i,j$ such that $\Gamma_0^i = \Gamma_0^j$, $d’(i,j) = \alpha d(i,j) \leq \alpha \epsilon &lt; \frac{a_0 a_2 b_0}{a_2 b_0} = a_0$. In addition, for $i,j$ such that $\Gamma_0^i \neq \Gamma_0^j$, $d’(i,j) = \alpha d(i,j) \geq \alpha a_2 = \frac{b_0 a_2}{a_2} = b_0$. This implies that $d’$ $(a_0, b_0)$-conforms to $\Gamma_0$, which implies that $f(d’) = \Gamma_0$.</p> <p>Since $\Gamma_0$ is a refinement of $\Gamma_1$, they are distinct partitions, so $\Gamma_0 \neq \Gamma_1$, and we arrive at a contradition.</p> </li> </ul> </body> </div> <p>Kleinberg proves an additional theorem that describes the partitions achievable by scale-invariant, consistent clustering functions $f$.</p> <div class="theorem"> <strong>Theorem 3.2 (Characterization Theorem).</strong> <body> <ul id="characterization-theorem" class="tab" data-tab="ef587f71-be4d-4d08-9dd1-e8c8bd303eaa" data-name="characterization-theorem"> <li class="active" id="characterization-theorem-statement"> <a href="#">statement </a> </li> <li id="characterization-theorem-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="ef587f71-be4d-4d08-9dd1-e8c8bd303eaa" data-name="characterization-theorem"> <li class="active"> <p>Let $\mathcal{A}$ be any antichain of partitions. There exists a scale-invariant, consistent clustering function $f$ such that $\text{Range}(f) = \mathcal{A}$.</p> </li> <li> <p>Kleinberg’s proof uses the <i>sum-of-pairs</i> clustering method which outputs the partition $\Gamma \in \mathcal{A}$ that minimizes $\Phi_d(\Gamma) = \sum_{(i,j) \sim \Gamma} d(i,j)$ where the notation $(i,j) \sim \Gamma$ indicates $i \underset{\Gamma}{\sim} j$.</p> <p>Notice that for any $\alpha &gt; 0$, $\Phi_{\alpha \cdot d}(\Gamma) = \sum_{(i,j) \sim \Gamma} \alpha d(i,j) = \alpha \sum_{(i,j) \sim \Gamma} d(i,j) = \alpha \Phi_{d}(\Gamma)$. Since $\alpha$ is positive, the argmin of $\Phi_{\alpha \cdot d}(\Gamma)$ is equivalent to the argmin of $\Phi_d(\Gamma)$, which implies $f(d) = f(\alpha \cdot d)$ so $f$ is scale-invariant.</p> <p>Fix some $\Gamma \in \mathcal{A}$. Let $d$ be a distance function satisfying:</p> <ul> <li>$d(i,j) &lt; \frac{1}{n^3}$ for $i,j$ such that $i \underset{\Gamma}{\sim} j$</li> <li>$d(i,j) \geq 1$ for $(i,j)$ such that $i \underset{\Gamma}{\not \sim} j$</li> </ul> <p>Notice that $\Phi_d(\Gamma) = \sum_{(i,j) \sim \Gamma} d(i,j) &lt; 1$ since the summation is only over $i,j$ in the same clusters of which there can be, at most, $n^2$ pairs (if there is only one cluster).</p> <p>Also notice that $\Phi_d(\Gamma’) &lt; 1$ only if $\Gamma’$ is a refinement of $\Gamma$. To see why, consider $\Gamma’$ that is <i>not</i> a refinement of $\Gamma$. This implies that there is some cluster $C’ \in \Gamma’$ such that $C’ \not\subseteq C$ for all $C \in \Gamma$. This implies that there exist points $i,j$ such that $i \underset{\Gamma’}{\sim} j$, so they are included in the summation in $\Phi_d(\Gamma’)$, but $i \underset{\Gamma}{\not \sim} j$, so $d(i,j) &gt; 1$. If $\Gamma’$ is a refinement of $\Gamma$, then $i \underset{\Gamma}{\sim} j$, so $d(i,j) &lt; \frac{1}{n^3}$.</p> <p>Because $\mathcal{A}$ is an antichain, there is no refinement $\Gamma’ \in \mathcal{A}$ of $\Gamma$. Thus, $\Gamma = \underset{\Gamma^* \in \mathcal{A}}{\arg\min} \Phi_d(\Gamma^*)$, which implies $f(d) = \Gamma$. Thus, $f$ is rich, since there exists a $d$ such that $f(d) = \Gamma$ for any $\Gamma \in \mathcal{A}$.</p> <p>Let $d$ be a distance function such that $f(d) = \Gamma$, and let $d’$ be a $\Gamma$-transformation of $d$. Furthermore, let $\Gamma’$ be some other partition and define $\Delta(\Gamma’) := \Phi_d(\Gamma’) - \Phi_{d’}(\Gamma’)$. We have that $\Delta(\Gamma) = \sum_{(i,j) \sim \Gamma}d(i,j) - \sum_{(i,j) \sim \Gamma} d’(i,j)$ and $\Delta(\Gamma) = \sum_{(i,j) \sim \Gamma’} d(i,j) - \sum_{(i,j) \sim \Gamma’} d’(i,j)$. Then:</p> \[\begin{aligned} \Delta(\Gamma) &amp;= \sum_{(i,j) \sim \Gamma} d(i,j) - \sum_{(i,j) \sim \Gamma} d'(i,j) \\ &amp;= \sum_{(i,j) \sim \Gamma} d(i,j) - d'(i,j) \\ &amp;\overset{(i)}{\geq} \sum_{(i,j) \sim \Gamma, (i,j) \sim \Gamma'} d(i,j) - d'(i,j) \\ &amp;\overset{(ii)}{\geq} \sum_{(i,j) \sim \Gamma'} d(i,j) - d'(i,j) \\ &amp;= \Delta(\Gamma') \end{aligned}\] <p>We know $d’$ is a $\Gamma$-transformation of $d$. Thus, $d’(i,j) \leq d(i,j)$ for $(i,j) \sim \Gamma$, implying that $d(i,j) - d’(i,j) \geq 0$ for all $(i,j) \sim \Gamma$. $(i)$ follows from the fact that $\rvert (i,j) \sim \Gamma \cap (i,j) \sim \Gamma’ \rvert \leq \rvert (i,j) \sim \Gamma \rvert$.</p> <p>Similarly, $(ii)$ is equivalent to the summation in $(i)$ plus $(i,j) \sim \Gamma’$ such that $(i,j) \not\sim \Gamma$. For these pairs, $d’(i,j) \geq d(i,j)$, since $d’$ is a $\Gamma$-transformation of $d$. This means $d(i,j) - d’(i,j) \leq 0$ for these pairs, which implies $(ii)$.</p> <p>The above argument implies $\Delta(\Gamma) \geq \Delta(\Gamma’)$ for any $\Gamma’ \in \mathcal{A}$. This implies $\Phi_d(\Gamma) - \Phi_{d’}(\Gamma) \geq \Phi_d(\Gamma’) - \Phi_{d’}(\Gamma’)$, which implies $\Phi_d(\Gamma) - \Phi_d(\Gamma’) \geq \Phi_{d’}(\Gamma) - \Phi_{d’}(\Gamma’)$.</p> <p>Since $\Gamma’$ minimizes $\Phi_{d’}$ and $\Gamma$ minimizs $\Phi_{d}$, we have that $\Phi_d(\Gamma) - \Phi_{d’}(\Gamma) \leq 0$. Chaining this together with the previous yields $0 \geq \Phi_d’(\Gamma) - \Phi_{d’}(\Gamma’)$, which implies $\Phi_{d’}(\Gamma’) \geq \Phi_{d’}(\Gamma)$. If $\Gamma’$ minimizes, $\Phi_{d’}$, then it must be the case that $\Gamma’ = \Gamma$. Thus, $f(d’) = \Gamma$, and therefore $f$ is consistent.</p> </li> </ul> </body> </div> <hr/> <h2 id="relaxations">Relaxations</h2> <p>Due to the impossibility theorem, it might be worthwhile to look into easing up the conditions in the axioms. Kleinberg provides a few examples.</p> <h4 id="relaxing-richness">Relaxing Richness</h4> <p>Theorem 3.2 is an example of a relaxation of the richness property. If we were satisfied with a clustering function that is scale-invariant, consistent, but only achieves an antichain as its range, then the sum-of-pairs method will work.</p> <h4 id="relaxing-consistency">Relaxing Consistency</h4> <p>Kleinberg proposes <i>Refinement-Consistency</i>, in which a $\Gamma$-transformation should output a refinement of $\Gamma$. Unfortunately, this is not yet enough; a scale-invariant, rich, and refinement-consistent clustering function does not exist. However, if one also relaxes richness to say that all but one (trivial) partition can be achieved by $f$ — termed <i>Near-Richness</i> —, then Theorem 2.1 does not hold.</p> <p>An alternative is to relax consistency to something I’ll call <i>Weak Consistency</i>, which is where if $d’$ is a $f(d)$-transformation of $d$, then either $f(d’)$ is a refinement of $f(d)$ or $f(d)$ is a refinement of $f(d’)$. There do exist clustering functions that satisfy all three of scale-invariance, richness, and weak consistency.</p> <p>In some ways, this relaxation may be more reasonable. For example, consider some partition that results in four clusters arranged in a square. Now, construct a distance function that just puts more distance between the left clusters and the right clusters. Although this new distance function is a $\Gamma$-transformation, it might be better to combine the left clusters and right clusters to have a partition with two groups. Ackerman and Ben-David provide this example as an illustration in Figure 1 of their paper.</p> <h4 id="relaxing-scale-invariance">Relaxing Scale-Invariance</h4> <p>Kleinberg does not discuss a relaxation of scale-invariance at length. He mentions that <i>single-linkage clustering</i> where we stop combining clusters when their distances exceed some value $r$ satisfies consistency and richness. It satisfies a weaker scale-invariance where we let $f(\alpha \cdot d)$ be a refinement of $f(d)$ when $\alpha &gt; 1$. Besides this, I don’t see why relaxing scale-invariance any more than this would be an attractive property of a clustering function.</p> <aside><p>Single-linkage clustering is an agglomerative method in which we start by letting each observation be its own cluster. Then we iteratively combine clusters based upon distances until some stopping criterion is met.</p></aside> <hr/> <h2 id="the-consistency-theorem">The Consistency Theorem</h2> <p>Kleinberg’s main result is a theorem (and its proof) stating that there does not exist a clustering function that satisfies three simple and desirable properties. This seems quite disappointing as clustering is a popular subtopic in unsupervised learning, and it seems to imply that clustering is at least impossibly difficult to define precisely if not simply impossible to actually do.</p> <p>Ackerman and Ben-David push back against this interpretation and state that the way in which Kleinberg axiomatized clustering was part of the reason for the impossibility result. They provide a slightly different perspective and provide a set of axioms for the function used to assess clustering quality rather than the clustering function itself.</p> <p>Ackerman and Ben-David adjust Steinberg’s work to apply to CQMs rather than clustering functions, which results in a consistent set of axioms. They also add one additional property in order to make their set of axioms satisfy two properties (<i>soundness</i> and <i>completeness</i>, which I won’t go into here) that make it more useful for defining what methods should be used for clustering.</p> <div id="axioms2"></div> <div class="definition"> <strong>Definition (Ackerman and Ben-David Axioms).</strong> <body> <ul> <li><strong>Scale Invariance:</strong> A CQM $m$ is called <i>scale-invariant</i> if for every partition $\Gamma$ of set $S$ with respect to distance $d$ and ever $\alpha &gt; 0$, we have $m(\Gamma, S, d) = m(\Gamma, S, \alpha \cdot d)$. </li> <li><strong>Richness:</strong> A CQM $m$ is called _rich_ if for each non-trivial partitions $\Gamma$ of $S$, there exists distance function $d$ such that $\Gamma = \underset{\Gamma'}{\arg \max} \left[ m(\Gamma', S, d) \right]$. A CQM will satisfy the richness property if, for each non-trivial $\Gamma$, we have $m(\Gamma, S, d) \geq m(\Gamma', S, d)$ for all possible partitions $\Gamma'$ of $S$ with $d$ (which may be chosen for each $\Gamma$). The $\max$ becomes a $\min$, and the inequality is reversed for CQMs that assign lower values to better clusterings.</li> <li><strong>Consistency:</strong> A CQM $m$ is called <i>consistent</i> if for every $\Gamma$ of $S$ with $d$, $m(\Gamma, X, d') \geq m(\Gamma, S, d)$ for any $\Gamma$-transformation, $d'$, of $d$. This condition is weaker than the consistency defined by Kleinberg as it does not penalize clustering functions that are only weakly consistent (recall that this means $f(d)$ and $f(d')$ are permitted to be refinements of each other).</li> <li><strong>Isomorphism Invariance:</strong> A CQM $m$ is called <i>isomorphism-invariant</i> if, for all $\Gamma$, $\Gamma'$ of $S$ with $d$ such that $\Gamma$ and $\Gamma'$ are isomorphic, we have $m(\Gamma, S, d) = m(\Gamma', S, d)$. This condition basically states that if we have two clusterings that would be the same if we swapped the points around (in a special way...), they should have the same score according to the CQM.</li> </ul> </body> </div> <p>Ackerman and Ben-David then prove that consistent CQMs exist.</p> <div class="theorem"> <strong> Theorem 2 and 3 (Ackerman and Ben-David).</strong> <body> <ul id="theorem-2-3" class="tab" data-tab="f661a2b4-14ee-43bd-b126-cbae6174cc63" data-name="theorem-2-3"> <li class="active" id="theorem-2-3-statement"> <a href="#">statement </a> </li> <li id="theorem-2-3-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="f661a2b4-14ee-43bd-b126-cbae6174cc63" data-name="theorem-2-3"> <li class="active"> <p>There exists a clustering quality measure that satisfies all four of scale-invariance, richness, consistency. and isomorphism invariance. That is, the four properties comprise a consistent set of axioms.</p> </li> <li> <p>It suffices to construct a CQM and prove it satisfies the three axioms.</p> <p>First, we define the <i>Relative Point Margin</i>. For distance $d$ and clustering $\Gamma$ containing $k$ clusters, the <i>$G$-Relative Point Margin</i> is defined as $G$-\(RM_{S, d}(x) = \frac{d(x, g_x)}{d(x, g'_{x})}\) where $g_x \in G$ is the closest cluster center to $x$, \(g'_{x} \in G\) is the second closest cluster center to $x$, and $G \subseteq S$.</p> <p>Next, define the <i>Relative Margin</i>. Let $\mathcal{G}$ denote the set of all possible representative sets of $\Gamma$. Then the relative margin of $\Gamma$ over $(S, d)$ is $RM_{S, d}(\Gamma) = \underset{G \in \mathcal{G}}{\min} \left{ \underset{x \in S \setminus G}{\textit{avg}} G\text{-}RM_{S, d}(x) \right}$. This is the representative set that achieves the minimum average relative point margin where the average is taken over all points not in the representative set. The relative margin assigns lower values to better clusterings (so the inequalities in the richness and consistency definitions will be reversed).</p> <p>Let $\Gamma$ be an arbitrary clustering of the set $S$ with distance function $d$ in the following.</p> <p><strong>Scale-Invariance.</strong> Let $d’$ be a distance function satisfying $d’(i,j) = \alpha d(i, j)$ for all $i,j \in S$ and all $\alpha &gt; 0$. For any $i,j,k \in S$, we have:</p> \[\frac{d'(i,j)}{d'(i,k)} = \frac{\alpha d(i,j)}{\alpha d(i,k)} = \frac{d(i,j)}{d(i,k)} \implies \frac{d'(i, g_i)}{d'(i, g'_i)} = \frac{d(i, g_i)}{d(i, g'_i)}\] <p>since scaling all distances will result in the same centers. This implies that $RM_{S, d’}(\Gamma) = RM_{S, d}(\Gamma)$.</p> <p><strong>Consistency.</strong> Let $d’$ be a $\Gamma$-transformation of $d$. We have:</p> \[\begin{aligned} \begin{cases} d'(i,j) \leq d(i,j) &amp; \text{for } i \underset{\Gamma}{\sim} j \\ d'(i,j) \geq d(i,j) &amp; \text{for } i \underset{\Gamma}{\not \sim} j \end{cases} &amp;\implies \frac{d'(i,j)}{d'(i,k)} \leq \frac{d(i,j)}{d(i,k)} \text{ for } i \underset{\Gamma}{\sim} j; i \underset{\Gamma}{\not \sim} k \\ &amp;\implies G\text{-}RM_{S,d'}(i) \leq G\text{-}RM_{S,d}(i) \text{ for any } G \in \mathcal{G} \end{aligned}\] <p>The first implication follows from the fact that the closest cluster center to $i$ will be a point in the same cluster, and the second closest cluster center to $i$ will be a point in a different cluster (by the definition of a representative set).</p> <p><strong>Richness.</strong> Let $\Gamma$ be an arbitrary non-trivial clustering of $S$ with $d$. Define the distance function $d$ as:</p> \[\begin{cases} d(i,j) = 1 &amp; \text{for } i \underset{\Gamma}{\sim} j \\ d(i,j) = 10 &amp; \text{for } i \underset{\Gamma}{\not \sim} j \end{cases} \nonumber\] <p>It follows that:</p> \[\begin{aligned} \underset{\Gamma'}{\arg \min} \left\{ RM_{S, d}(\Gamma') \right\} &amp;= \underset{\Gamma'}{\arg \min} \left\{ \underset{G \in \mathcal{G}}{\min} \left[ \underset{x \in S \setminus G}{\textit{avg}} G\text{-}RM_{S,d}(x) \right] \right\} \\ &amp;= \underset{\Gamma'}{\arg \min} \left\{ \underset{G \in \mathcal{G}}{\min} \left[ \frac{1}{\rvert S \setminus G \rvert} \sum_{x \in S \setminus G} \frac{d(x, g_x)}{d(x, g'_x)} \right] \right\} \\ &amp;= \underset{\Gamma'}{\arg \min} \left\{ \underset{G \in \mathcal{G}}{\min} \left[ \frac{1}{\rvert S \setminus G \rvert} \sum_{x \in S \setminus G} \frac{1}{10} \right] \right\} \\ &amp;= \underset{\Gamma'}{\arg \min} \left\{ \underset{G \in \mathcal{G}}{\min} \left[ \frac{1}{10} \right] \right\} \end{aligned}\] <p>We arrive at the fact that, for this choice of $d$, the minimum $RM_{S,d}(\Gamma’)$ is achieved by every non-trivial partition of $S$. Thus, $\Gamma = \underset{\Gamma’}{\arg \min} \left{ RM_{S, d}(\Gamma’) \right}$.</p> <p><strong>Isomorphism Invariance.</strong> Let $\Gamma’$ be a partition such that $\Gamma \underset{d}{\approx} \Gamma’$. Since they are isomorphic, there exists a distance-preserving isomorphism $\phi$. Let $G’ := { \phi(x): x \in G }$, and let $\mathcal{G}’$ be the set of all $G’$. Thus:</p> \[\begin{aligned} \frac{d(i, g_i)}{d(i, g'_i)} = \frac{d(\phi, g_{\phi(i)})}{d(\phi, g'_{\phi(i)})} &amp;\implies G\text{-}RM_{S,d}(i) = G'\text{-}RM_{S,d}(i) \\ &amp;\implies \underset{i \in S \setminus G}{avg} \left\{ G\text{-}RM_{S,d}(i)\right\} = \underset{i \in S \setminus G'}{avg} \left\{ G'\text{-}RM_{S,d}(i) \right\} \\ &amp;\implies \underset{G \in \mathcal{G}}{\min} \left[ \underset{i \in S \setminus G}{avg} \left\{ G\text{-}RM_{S,d}(i)\right\}\right] = \underset{G \in \mathcal{G}}{\min} \left[ \underset{i \in S \setminus G'}{avg} \left\{ G'\text{-}RM_{S,d}(i)\right\}\right] \\ &amp;\implies RM_{S,d}(\Gamma) = RM_{S,d}(\Gamma') \end{aligned}\] </li> </ul> </body> </div> <h2 id="examples-and-extensions">Examples and Extensions</h2> <p>Ackerman and Ben-David provide a few different examples of CQMs that satisfy their axioms.</p> <h4 id="weakest-link">Weakest Link</h4> <p>Suppose we are in a linkage-based regime. Define the <i>Weakest Link Between Points</i> as the following. Let $\gamma_k$ be the $k$-th cluster in $\Gamma$. For $\Gamma$ over $S$ with $d$:</p> \[\Gamma\text{-}WL_{S, d}(i, j) = \underset{x \in \gamma_k \forall k}{\min} \left\{ \max \left[ d(i, x), d(i, j) \right] \right\}\] <p>The <i>Weakest Link</i> of $\Gamma$ over $S$ with $d$ is:</p> \[WL(\Gamma) = \frac{\underset{i \underset{\Gamma}{\sim} j}{\max} \Gamma\text{-}WL_{S,d}(i,j)}{\underset{i \underset{\Gamma}{\not \sim}}{\min} d(i,j)}\] <p>This is calculable in $O(n^3)$ time.</p> <h4 id="additive-margin">Additive Margin</h4> <p>Suppose we are in a center-based clustering (like $k$-means). Define the <i>Additive Point Margin</i> as:</p> \[G\text{-}AM_{S, d}(i) = d(i, \gamma'_i) - d(i, \gamma_i)\] <p>The <i>Additive Margin</i> of $\Gamma$ over $S$ with $d$ is:</p> \[AM_{S,d}(\Gamma) = \underset{G \in \mathcal{G}}{\min} \left\{ \frac{ \frac{1}{\rvert S \rvert} \sum_{i \in S} G\text{-}AM_{S, d}(i) }{ \frac{1}{\rvert \left\{ \{ i, j\} \subseteq S \rvert i \underset{\Gamma}{\sim} j \right\}} \sum_{i \underset{\Gamma}{\sim} j} d(i, j) } \right\}\] <p>This is calculable in $O(n^{k+1})$ time.</p> <h4 id="functions-of-cqms">Functions of CQMs</h4> <p>One can also use functions of clustering-quality measures to create a new one. If one had a CQM defined for partitions of $k$ clusters, one could consider taking the minimum, maximum, or average over all subsets of size $k$ for a clustering of arbitrary size greater than $k$.</p> <h4 id="cluster-number-dependence">Cluster Number Dependence</h4> <p>The above CQMs do not depend on the number of clusters in a partition, which makes it easy to compare clusterings that are of different sizes. Ackerman and Ben-David extend their framework to CQMs that <strong>do</strong> depend on cluster number, such as those that are based upon the objective functions of some clustering methods (such as $k$-means).</p> <p>In order to make these CQMs compliant with the scale-invariance property, the quality scores must be normalized in some way. An example is <i>$\mathcal{L}$-normalization</i>, which scales the loss of a clustering by the loss of a trivial clustering that has a single cluster of all observations.</p> <p>Loss-based clustering functions also tend to either reward or punish more clusters. Ackerman and Ben-David term CQMs based on a loss function that “prefers” more clusters as <i>refinement-preferring</i> and those based on losses that prefer fewer clusters as <i>coarsening-preferring</i>. More explicitly, a refinement-preferring CQM will assign a better quality score to refinements of $\Gamma$ than to $\Gamma$, and a coarsening-preferring CQM will assign better quality scores to $\Gamma$ than to its refinements. These CQMs do not satisfy the richness property.</p>]]></content><author><name></name></author><category term="clustering"/><category term="philosophy"/><category term="paper-reviews"/><summary type="html"><![CDATA[Though the journey to this point is a bit confusing, I have recently become interesting in clustering metrics and evaluation. In this post, I’ll work through a couple papers on describing how good a clustering function is based upon a set of axioms. These include Kleinberg’s An Impossibility Theorem for Clustering and Ben-David and Ackerman’s Measure of Clustering Quality.]]></summary></entry></feed>