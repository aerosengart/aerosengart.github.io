<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://aerosengart.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://aerosengart.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-06T21:55:24+00:00</updated><id>https://aerosengart.github.io/feed.xml</id><title type="html">Anna Rosengart</title><entry><title type="html">Score Calcuations</title><link href="https://aerosengart.github.io/blog/2025/score-test-neg-binom/" rel="alternate" type="text/html" title="Score Calcuations"/><published>2025-12-11T00:00:00+00:00</published><updated>2025-12-11T00:00:00+00:00</updated><id>https://aerosengart.github.io/blog/2025/score-test-neg-binom</id><content type="html" xml:base="https://aerosengart.github.io/blog/2025/score-test-neg-binom/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>This post is just a catch-all for my derivations for my score test project with negative binomial outcomes. Our set-up is as follows. We have $n$ observations coming from $k$ different clusters, each of size $n_t$ for $t \in [k]$. The full data will be denoted by $\mathbf{y}$. Though $\mathbf{y}$ is a vector, we’ll denote the $j$-th observation from cluster $i$ with $\mathbf{y}_{i,j}$. For example, \(\mathbf{y}_{i,j}\) denotes element \(\sum_{l = 1}^{i - 1} n_l + j\) of $\mathbf{y}$. We’ll also denote the $n_i$-dimensional vector of responses for cluster $i$ with $\mathbf{y}_i$.</p> <p>For each observation, we will have $p$ fixed effect covariates arranged in a $p$-dimensional vector, \(\mathbf{x}_{i, j}\), and $q$ random effects covariates in a $q$-dimensional vector, \(\mathbf{z}_{i,j}\). We’ll assume that the observations within the same cluster are independent.</p> <p>Our model comes in the form of a specification of the conditional mean, $\mu_{i,j} = \mathbb{E}[\mathbf{y}_{i,j} \rvert \beta_i]$ (where we suppress the addition conditioning on the covariates themselves). For a monotonic and differentiable link function (e.g. $\log(\cdot)$ or $\text{logit}(\cdot)$), the conditional mean of the $j$-th observation in group $i$ is assumed to be given by:</p> \[\mu_{i,j} = g^{-1}\left(\alpha^\top \mathbf{x}_{i,j} + \beta_i^\top \mathbf{z}_{i,j} \right) \label{eq:glmm}\] <p>We then assume that the observations themselves follow some exponential family distribution with measurement errors, $\epsilon_{i,j}$, which is the deviation of the response from its (unit-specific) conditional mean. These errors are assumed to have mean zero and be independent of each other and of the random effects. We further assume the responses, $\mathbf{y}_{i,j}$, conditional on the random effects (and the covariates), are independent with variances equal to some function of the conditional mean.</p> <p>In general, we will assume that:</p> \[\beta_i \overset{iid}{\sim} \mathcal{N}\left(\mathbf{0}_q, D(\tau^2) \right)\] <p>for some variance component, $\tau^2$. We’ll use $[\cdot] \rvert_{H_0}$ to denote evaluation of the function in brackets when setting $\beta$ equal to $\beta_0$. We’ll also use a superscript $0$ (e.g. $\mu^0$, $\eta^0$, etc.) to denote the quantity under the null hypothesis (i.e. $\tau^2 = \mathbf{0} \implies \beta = \mathbf{0}$).</p> <hr/> <h2 id="negative-binomial-case">Negative Binomial Case</h2> <h3 id="set-up">Set-Up</h3> <p>In this example, we’ll let the responses be negative binomial. To keep things simple, we’ll say we only have a single fixed intercept and a single random slope. We let $\phi &gt; 0$ denote the <i>unknown</i> dispersion parameter and assume the conditional mean to be given by:</p> \[\mu_{i,j} = \exp\left( \alpha_i + \beta_i \mathbf{z}_{i,j} \right) \label{eq:neg-bin-mean}\] <p>The likelihood based on a single observation, $\mathbf{y}_{i,j}$, is given by:</p> \[\mathcal{L}(\mathbf{y}_{i,j}; \alpha_i, \tau^2 \rvert \beta_i) = \frac{\Gamma\left(\mathbf{y}_{i,j} + \frac{1}{\phi}\right)}{\Gamma(\mathbf{y}_{i,j} + 1) \Gamma\left(\frac{1}{\phi} \right)}\left(\frac{1}{1 + \phi \mathbf{y}_{i,j}}\right)^{\frac{1}{\phi}} \left( \frac{\phi \mu_{i,j}}{1 + \phi \mu_{i,j}} \right)^{\mathbf{y}_{i,j}} \label{eq:neg-bin-single-lik}\] <p>where $\Gamma(\cdot)$ is the gamma function:</p> \[\Gamma(x) = \int_0^\infty t^{x - 1} \exp(-t) dt\] <p>The above parametrization of the likelihood implies that the conditional variance of the responses is given by:</p> \[V(\mu_{i,j}) = \mu_{i,j} + \frac{1}{\phi} \mu_{i,j}^2\] <p>The conditional log-likelihood based on cluster $i$ is:</p> \[\ell(\mathbf{y}_i; \alpha_i, \tau^2 \rvert \beta_i) = \sum_{j = 1}^{n_i} \left[ \log \Gamma \left( \mathbf{y}_{i,j} + \frac{1}{\phi} \right) - \log \Gamma\left(\mathbf{y}_{i,j} + 1\right) - \log\Gamma\left(\frac{1}{\phi} \right) - \frac{1}{\phi} \log\left(1 + \phi \mathbf{y}_{i,j} \right) + \mathbf{y}_{i,j} \left( \log(\phi \mu_{i,j}) - \log(1 + \phi \mu_{i,j}) \right) \right] \label{eq:neg-bin-full-cond-ll}\] <p>We assume to have the following generalized linear mixed model:</p> \[\begin{equation} \label{eq:glmm-y} \begin{aligned} \mathbf{y}_{i,j} \rvert \beta_i &amp;\sim \text{NegBin}(\mu_{i,j}, \phi) \\ \mu_{i,j} &amp;= \exp\left(\eta_{i,j}\right) = \exp\left(\alpha_i + \beta_i \mathbf{z}_{i,j}\right) \end{aligned} \end{equation}\] <h3 id="approximation">Approximation</h3> <p>We follow a pseudo-likelihood approach (see <a href="/blog/2025/glmm.html">here</a>) which permits a Gaussian approximation of the outcome distribution via a linearization. Suppose we find the MLEs of the $\alpha_i$ and $\phi$ terms under $H_0$, which we can do with <a href="/blog/2025/glm.html#weighted-least-squares">iteratively reweighted least squares</a> or some comparable algorithm (basically just fitting the null model). Denote these estimates with $\tilde{\alpha_i}$ and $\tilde{\phi}$, respectively. We can compute our <i>working</i> responses and errors as:</p> \[\begin{equation} \label{eq:working-responses} \mathbf{y}^\star_{i,j} = \alpha_i + \beta_i \mathbf{z}_{i,j} + \epsilon^\star_{i,j}; \hspace{10mm} \epsilon^\star_{i,j} \sim \mathcal{N}\left(0, \frac{V(\tilde{\mu}_{i,j})}{\delta^2(\tilde{\eta}_{i,j})}\right) \end{equation}\] <p>where</p> \[\tilde{\eta}_{i,j} = \tilde{\alpha}_i; \hspace{10mm} \delta(\tilde{\eta}_{i,j}) = \frac{\partial g^{-1}(\eta_{i,j})}{\partial \eta_{i,j}}\bigg\rvert_{\eta_{i,j} = \tilde{\eta}_{i,j}}\] <p>Since $g(\cdot) = \log(\cdot)$, we have that \(\delta(\tilde{\eta}_{i,j}) = \exp(\tilde{\eta}_{i,j})\), implying that the working error variances are:</p> \[\frac{V(\tilde{\mu}_{i,j})}{\delta^2(\tilde{\eta}_{i,j})} = \frac{\exp(\tilde{\eta}_{i,j}) + \frac{1}{\phi}\exp(\tilde{\eta}_{i,j})}{\exp^2(\tilde{\eta}_{i,j})} = \frac{1}{\exp(\tilde{\eta}_{i,j})}\left(1 + \frac{1}{\tilde{\phi}}\right)\] <p>where we recall that $\tilde{\eta}_{i,j} = \tilde{\alpha}_i$ under $H_0$. Eq. \eqref{eq:working-responses} describes a linear mixed model (i.e. Gaussian outcomes), defined in terms of the working responses and errors. The only big difference is that we force the error variances to all be different and fix them at:</p> \[\sigma_{i,j}^2 = \frac{V(\tilde{\mu}_{i,j})}{\delta^2(\tilde{\eta}_{i,j})} = \frac{1}{\exp(\tilde{\alpha}_i)} \left(1 + \frac{1}{\tilde{\phi}} \right)\] <p>We have simplified some of the calculations since we can now use the Gaussian likelihood instead:</p> \[\begin{aligned} \mathcal{L}(\theta; \mathbf{y}^\star) &amp;= \prod_{i = 1}^k (2 \pi)^{-\frac{n}{2}} \rvert \Sigma_{y_i} \rvert^{-\frac{1}{2}} \exp\left(- \frac{1}{2} (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \Sigma_{y_i}^{-1} (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right) \\ \ell(\theta; \mathbf{y}^\star) &amp;= \sum_{i = 1}^k \left[ -\frac{n}{2} \log(2 \pi) - \frac{1}{2}\log(\rvert \Sigma_{y^\star_i} \rvert) - \frac{1}{2} (\mathbf{y}^\star_i - \alpha_i \mathbf{1}_n)^\top \Sigma_{y^\star_i}^{-1} (\mathbf{y}^\star_i - \alpha_i \mathbf{1}_n) \right] \end{aligned}\] <p>In the rest of the post, I’ll drop the $\star$ superscript for readability. Just be sure to remember that we are not dealing with the original observations but with their transformations.</p> <h3 id="score">Score</h3> <p>The marginal covariance matrix is very similar to the Gaussian outcome model above. The only thing that has changed is that each error has its own variance:</p> \[\Sigma_{y_i} = \text{diag}\left(\begin{bmatrix} \sigma^2_{i,1} &amp; \dots &amp; \sigma^2_{i, n}\end{bmatrix}\right) + \tau^2 \mathbf{z}_i \mathbf{z}_i^\top\] <p>Its inverse, $\Sigma^{-1}_{y_i}$, is:</p> <ul id="sigma-inv-2" class="tab" data-tab="33d3c0cb-b372-4f1e-becc-25a4f8b3344f" data-name="sigma-inv-2"> <li class="active" id="sigma-inv-2-equation"> <a href="#">equation </a> </li> <li id="sigma-inv-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="33d3c0cb-b372-4f1e-becc-25a4f8b3344f" data-name="sigma-inv-2"> <li class="active"> \[\Sigma^{-1}_{y_i} = \text{diag}\left( \begin{bmatrix} \frac{1}{\sigma^2_{i, 1}} &amp; \dots &amp; \frac{1}{\sigma^2_{i,n}} \end{bmatrix} \right) - \text{diag}\left( \begin{bmatrix} \frac{\tau^2}{(\sigma_{i,1}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)} &amp; \dots &amp; \frac{\tau^2}{(\sigma_{i,n}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)} \end{bmatrix} \right) \mathbf{z}_i \mathbf{z}_i^\top\] </li> <li> <p>First, let’s let \(\mathbf{w}_i = \tau^2 \mathbf{z}_i\), which is the $n$-vector $\mathbf{z}_i$ where each coordinate has been multiplied by $\tau^2$. We’ll also let \(\sigma^2 = (\sigma^2_{i,1}, \dots, \sigma^2_{i, n})^\top\), the vector of the error variances for cluster $i$, and $\frac{1}{\sigma^2}$ will be the vector of the reciprocals of the coordinates of $\sigma^2$. Using the <a href="https://en.wikipedia.org/wiki/Sherman–Morrison_formula">Sherman-Morrison formula</a>, we have:</p> \[\begin{aligned} \Sigma^{-1}_{y_i} &amp;= \left(\text{diag}(\sigma^2) - \mathbf{w}_i \mathbf{v}_i^\top\right)^{-1} \\ &amp;= \text{diag}^{-1}(\sigma^2) - \frac{\text{diag}^{-1}(\sigma^2) \mathbf{w}_i \mathbf{z}_i^\top \text{diag}^{-1}(\sigma^2)}{1 + \mathbf{z}_i^\top \text{diag}^{-1}(\sigma^2) \mathbf{w}_i} \\ &amp;= \text{diag}\left(\frac{1}{\sigma^2}\right) - \frac{\text{diag}\left(\frac{\tau^2}{(\sigma^2)^2}\right) \mathbf{z}_i \mathbf{z}_i^\top}{1 + \mathbf{z}_i^\top \text{diag}\left(\frac{\tau^2}{\sigma^2}\right) \mathbf{z}_i} \\ &amp;= \text{diag}\left(\begin{bmatrix} \frac{1}{\sigma^2_{i, 1}} &amp; \dots &amp; \frac{1}{\sigma^2_{i,n}} \end{bmatrix} \right) - \frac{\text{diag}\left( \begin{bmatrix} \frac{\tau^2}{(\sigma_{i,1}^2)^2} &amp; \dots &amp; \frac{\tau^2}{(\sigma_{i,n}^2)^2} \end{bmatrix} \right) \mathbf{z}_i \mathbf{z}_i^\top}{1 + \mathbf{z}_i^\top \text{diag}\left( \left[ \frac{\tau^2}{\sigma_{i,1}^2}, \dots, \frac{\tau^2}{\sigma_{i,n}^2} \right] \right) \mathbf{z}_i} \\ &amp;= \text{diag}\left( \begin{bmatrix} \frac{1}{\sigma^2_{i, 1}} &amp; \dots &amp; \frac{1}{\sigma^2_{i,n}} \end{bmatrix} \right) - \frac{\text{diag}\left( \begin{bmatrix} \frac{\tau^2}{(\sigma_{i,1}^2)^2} &amp; \dots &amp; \frac{\tau^2}{(\sigma_{i,n}^2)^2} \end{bmatrix} \right) \mathbf{z}_i \mathbf{z}_i^\top}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}} \\ &amp;= \text{diag}\left( \begin{bmatrix} \frac{1}{\sigma^2_{i, 1}} &amp; \dots &amp; \frac{1}{\sigma^2_{i,n}} \end{bmatrix} \right) - \text{diag}\left( \begin{bmatrix} \frac{\tau^2}{(\sigma_{i,1}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)} &amp; \dots &amp; \frac{\tau^2}{(\sigma_{i,n}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)} \end{bmatrix} \right) \mathbf{z}_i \mathbf{z}_i^\top \end{aligned}\] <p>We also have:</p> \[[\Sigma^{-1}_{y_i}]_{j,j'} = - \frac{\tau^2 \mathbf{z}_{i,j} \mathbf{z}_{i,j'}}{(\sigma_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)} \hspace{10mm} \text{and} \hspace{10mm} [\Sigma^{-1}_{y_i}]_{j,j} = \frac{1}{\sigma^2_{i,j}} - \frac{\tau^2 \mathbf{z}_{i,j}^2}{(\sigma_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)}\] </li> </ul> <h4 id="derivatives">Derivatives</h4> <p>We first find the derivative with respect to $\tau^2$:</p> <ul id="deriv-tau-2" class="tab" data-tab="7f1ae526-ea51-49a2-9b9f-b0ffcdd4ed79" data-name="deriv-tau-2"> <li class="active" id="deriv-tau-2-equation"> <a href="#">equation </a> </li> <li id="deriv-tau-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="7f1ae526-ea51-49a2-9b9f-b0ffcdd4ed79" data-name="deriv-tau-2"> <li class="active"> \[\frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} = - \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{j = 1}^n \left[ \frac{\mathbf{z}_{i,j}^2}{\sigma^2_{i,j}} - \sum_{h = 1}^n \frac{\tau^2}{(\sigma^2_{i,j})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)} \mathbf{z}_{i,j}^2 \mathbf{z}^2_{i,h}\right] - \sum_{j = 1}^n \sum_{h = 1}^n (\mathbf{y}_{i,j} - \alpha_i)(\mathbf{y}_{i,h} - \alpha_i) \frac{\mathbf{z}_{i,j} \mathbf{z}_{i,h}}{(\sigma^2_{i,j})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^2} \right]\] </li> <li> <p>First, the log determinant term:</p> \[\begin{aligned} \frac{\partial}{\partial \tau^2} \left[ \log (\rvert \Sigma_{y_i} \rvert) \right] &amp;= \text{tr}\left[ \Sigma^{-1}_{y_i} \frac{\partial}{\partial \tau^2} \left[ \Sigma_{y_i} \right] \right] \\ &amp;= \text{tr} \left[ \left( \text{diag}\left( \begin{bmatrix} \frac{1}{\sigma^2_{i, 1}} &amp; \dots &amp; \frac{1}{\sigma^2_{i,n}} \end{bmatrix} \right) - \text{diag}\left( \begin{bmatrix} \frac{\tau^2}{(\sigma_{i,1}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)} &amp; \dots &amp; \frac{\tau^2}{(\sigma_{i,n}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)} \end{bmatrix} \right) \mathbf{z}_i \mathbf{z}_i^\top \right) \mathbf{z}_i \mathbf{z}_i^\top \right] \\ &amp;= \text{tr} \left[ \text{diag}\left( \begin{bmatrix} \frac{1}{\sigma^2_{i, 1}} &amp; \dots &amp; \frac{1}{\sigma^2_{i,n}} \end{bmatrix} \right) \mathbf{z}_{i} \mathbf{z}_i^\top \right] - \text{tr} \left[ \text{diag}\left( \begin{bmatrix} \frac{\tau^2}{(\sigma_{i,1}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)} &amp; \dots &amp; \frac{\tau^2}{(\sigma_{i,n}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)} \end{bmatrix} \right) \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \\ &amp;= \sum_{j = 1}^n \sum_{k = 1}^n \left( \text{diag}\left( \begin{bmatrix} \frac{1}{\sigma^2_{i, 1}} &amp; \dots &amp; \frac{1}{\sigma^2_{i,n}} \end{bmatrix} \right) \right)_{j,k} \left(\mathbf{z}_i \mathbf{z}_{i}^\top \right)_{k,j} - \sum_{j = 1}^n \sum_{k = 1}^n \left( \text{diag}\left( \begin{bmatrix} \frac{\tau^2}{(\sigma_{i,1}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)} &amp; \dots &amp; \frac{\tau^2}{(\sigma_{i,n}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)} \end{bmatrix} \right) \mathbf{z}_i \mathbf{z}_i^\top \right)_{j,k} \left( \mathbf{z}_i \mathbf{z}_i^\top \right)_{k,j} \\ &amp;= \sum_{j = 1}^n \frac{\mathbf{z}_{i,j}^2}{\sigma^2_{i,j}} - \sum_{j = 1}^n \sum_{k = 1}^n \frac{\tau^2}{(\sigma^2_{i,j})^2 \left( 1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)} \mathbf{z}_{i,j} \mathbf{z}_{i,k} \mathbf{z}_{i,k} \mathbf{z}_{i,j} \\ &amp;= \sum_{j = 1}^n \left[ \frac{\mathbf{z}_{i,j}^2}{\sigma^2_{i,j}} - \sum_{k = 1}^n \frac{\tau^2}{(\sigma^2_{i,j})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)} \mathbf{z}_{i,j}^2 \mathbf{z}^2_{i,k}\right] \end{aligned}\] <p>Next, the quadratic term. We will compute the derivative of $\Sigma^{-1}_{y_i}$ with respect to $\tau^2$ element-wise:</p> \[\begin{aligned} \frac{\partial}{\partial \tau^2}\left[ [\Sigma^{-1}_{y_i}]_{j,j'}\right] &amp;= \frac{\partial}{\partial \tau^2} \left[ - \frac{\tau^2 \mathbf{z}_{i,j} \mathbf{z}_{i,j'}}{(\sigma_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)} \right] \\ &amp;= -\frac{(\sigma_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)(\mathbf{z}_{i,j}\mathbf{z}_{i,j'}) - (\tau^2 \mathbf{z}_{i,j}\mathbf{z}_{i,j'})\left( (\sigma_{i,j}^2)^2\sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)}{(\sigma_{i,j}^2)^4 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^2} \\ &amp;= -\frac{\mathbf{z}_{i,j}\mathbf{z}_{i,j'} \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}- \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)}{(\sigma_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^2} \\ &amp;= -\frac{\mathbf{z}_{i,j}\mathbf{z}_{i,j'}}{(\sigma_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^2} \\ \frac{\partial}{\partial \tau^2}\left[ [\Sigma^{-1}_{y_i}]_{j,j}\right] &amp;= \frac{\partial}{\partial \tau^2} \left[ \frac{1}{\sigma^2_{i,j}} - \frac{\tau^2 \mathbf{z}_{i,j}^2}{(\sigma_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)} \right] \\ &amp;= - \frac{(\sigma_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)\mathbf{z}_{i,j}^2 - \tau^2 \mathbf{z}_{i,j}^2 (\sigma_{i,j}^2)^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}}{(\sigma_{i,j}^2)^4 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^2} \\ &amp;= - \frac{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)\mathbf{z}_{i,j}^2 - \tau^2 \mathbf{z}_{i,j}^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}}{(\sigma_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^2} \\ &amp;= - \frac{\mathbf{z}_{i,j}^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}- \tau^2\sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)}{(\sigma_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^2} \\ &amp;= - \frac{\mathbf{z}_{i,j}^2}{(\sigma_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^2} \\ \end{aligned}\] <p>In matrix notation, we have:</p> \[\frac{\partial}{\partial \tau^2}\left[ \Sigma^{-1}_{y_i} \right] = \text{diag}\left(\left[ -\frac{1}{(\sigma^2_{i,1})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^2}, \dots, -\frac{1}{(\sigma^2_{i,n})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^2} \right] \right) \mathbf{z}_i \mathbf{z}_i^\top\] <p>Then:</p> \[\begin{aligned} (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \frac{\partial}{\partial \tau^2} \left[ \Sigma^{-1}_{y_i} \right](\mathbf{y}_i - \alpha_i \mathbf{1}_n) &amp;= (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ \text{diag}\left(\left[ -\frac{1}{(\sigma^2_{i,1})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^2}, \dots, -\frac{1}{(\sigma^2_{i,n})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^2} \right] \right) \mathbf{z}_i \mathbf{z}_i^\top \right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ &amp;= \sum_{j = 1}^n \sum_{h = 1}^n (\mathbf{y}_{i,j} - \alpha_i)(\mathbf{y}_{i,h} - \alpha_i) \left[ \text{diag}\left(\left[ -\frac{1}{(\sigma^2_{i,1})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^2}, \dots, -\frac{1}{(\sigma^2_{i,n})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^2} \right] \right) \mathbf{z}_i \mathbf{z}_i^\top \right]_{j,k} \\ &amp;= - \sum_{j = 1}^n \sum_{h = 1}^n (\mathbf{y}_{i,j} - \alpha_i)(\mathbf{y}_{i,h} - \alpha_i) \frac{\mathbf{z}_{i,j} \mathbf{z}_{i,h}}{(\sigma^2_{i,j})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^2} \end{aligned}\] <p>And thus:</p> \[\begin{aligned} \frac{\partial}{\partial \tau^2} \left[ \ell(\theta; \mathbf{y}) \right] &amp;= \frac{\partial}{\partial \tau^2} \left[ \sum_{i = 1}^k \left[ -\frac{n}{2} \log(2 \pi) - \frac{1}{2}\log(\rvert \Sigma_{y_i} \rvert) - \frac{1}{2} (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \Sigma_{y_i}^{-1} (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \right] \\ &amp;= - \frac{1}{2} \sum_{i = 1}^k \left[ \frac{\partial}{\partial \tau^2} \left[ \log(\rvert \Sigma_{y_i} \rvert) \right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \frac{\partial}{\partial \tau^2} \left[ \Sigma_{y_i}^{-1} \right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right]\\ &amp;= - \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{j = 1}^n \left[ \frac{\mathbf{z}_{i,j}^2}{\sigma^2_{i,j}} - \sum_{k = 1}^n \frac{\tau^2}{(\sigma^2_{i,j})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)} \mathbf{z}_{i,j}^2 \mathbf{z}^2_{i,k}\right] - \sum_{j = 1}^n \sum_{k = 1}^n (\mathbf{y}_{i,j} - \alpha_i)(\mathbf{y}_{i,k} - \alpha_i) \frac{\mathbf{z}_{i,j} \mathbf{z}_{i,k}}{(\sigma^2_{i,j})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^2} \right] \end{aligned}\] </li> </ul> <p>Next, we find the derivative with respect to $\phi$:</p> <ul id="deriv-phi-1" class="tab" data-tab="89469e5b-b156-4987-9c7e-785e22fbb176" data-name="deriv-phi-1"> <li class="active" id="deriv-phi-1-equation"> <a href="#">equation </a> </li> <li id="deriv-phi-1-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="89469e5b-b156-4987-9c7e-785e22fbb176" data-name="deriv-phi-1"> <li class="active"> <p>Coming soon…</p> </li> <li> <p>First, let’s find the derivative of the diagonal components of $\Sigma_{y_i}$:</p> \[\begin{aligned} \frac{\partial}{\partial \phi} \left[ (\Sigma_{y_i})_{j,j} \right] &amp;= \frac{\partial}{\partial \phi} \left[ \sigma^2_{i,j} + \tau^2 \mathbf{z}_{i,j}^2 \right] \\ &amp;= \frac{\partial}{\partial \phi} \left[ \frac{1}{\tilde{\mu}_{i,j}} \left(1 + \frac{1}{\phi} \right) + \tau^2 \mathbf{z}_{i,j}^2 \right] \\ &amp;= \frac{1}{\tilde{\mu}_{i,j}} \frac{\partial}{\partial \phi} \left[ \frac{1}{\phi} \right] \\ &amp;= -\frac{1}{\tilde{\mu}_{i,j} \phi^2 } \end{aligned}\] <p>This implies that \(\frac{\partial}{\partial \phi}\left[ \Sigma_{y_i} \right] = \text{diag}\left( \begin{bmatrix} -\frac{1}{\tilde{\mu}_{i,1} \phi^2 } &amp; \dots &amp; -\frac{1}{\tilde{\mu}_{i,n} \phi^2 } \end{bmatrix} \right)\).</p> \[\begin{aligned} \frac{\partial}{\partial \phi} \left[ \log(\rvert \Sigma_{y_i} \rvert) \right] &amp;= \text{tr} \left[ \Sigma_{y_i}^{-1} \frac{\partial}{\partial \tau^2} \left[ \Sigma_{y_i} \right] \right] \\ &amp;= \text{tr} \left[ \left( \text{diag}\left( \begin{bmatrix} \frac{1}{\sigma^2_{i, 1}} &amp; \dots &amp; \frac{1}{\sigma^2_{i,n}} \end{bmatrix} \right) - \text{diag}\left( \begin{bmatrix} \frac{\tau^2}{(\sigma_{i,1}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)} &amp; \dots &amp; \frac{\tau^2}{(\sigma_{i,n}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)} \end{bmatrix} \right) \mathbf{z}_i \mathbf{z}_i^\top \right) \text{diag}\left( \begin{bmatrix} -\frac{1}{\tilde{\mu}_{i,1} \phi^2 } &amp; \dots &amp; -\frac{1}{\tilde{\mu}_{i,n} \phi^2 } \end{bmatrix} \right) \right] \\ &amp;= \text{tr} \left[ \text{diag}\left(\begin{bmatrix} - \frac{1}{\tilde{\mu}_{i,1} \sigma^2_{i,1} \phi^2} &amp; \dots &amp; - \frac{1}{\tilde{\mu}_{i,n} \sigma^2_{i,n} \phi^2} \end{bmatrix}\right) \right] - \text{tr} \left[ \text{diag}\left( \begin{bmatrix} \frac{\tau^2}{(\sigma_{i,1}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)} &amp; \dots &amp; \frac{\tau^2}{(\sigma_{i,n}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)} \end{bmatrix} \right) (\mathbf{z}_i \mathbf{z}_i^\top) \text{diag}\left( \begin{bmatrix} -\frac{1}{\tilde{\mu}_{i,1} \phi^2 } &amp; \dots &amp; -\frac{1}{\tilde{\mu}_{i,n} \phi^2 } \end{bmatrix} \right) \right] \\ &amp;= \sum_{j = 1}^n \sum_{h = 1}^n \left[ \text{diag}\left(\begin{bmatrix} - \frac{1}{\tilde{\mu}_{i,1} \sigma^2_{i,1} \phi^2} &amp; \dots &amp; - \frac{1}{\tilde{\mu}_{i,n} \sigma^2_{i,n} \phi^2} \end{bmatrix}\right)\right]_{j,h} - \text{tr}\left[ \mathbf{z}_i^\top \text{diag}\left( \begin{bmatrix} -\frac{1}{\tilde{\mu}_{i,1} \phi^2 } &amp; \dots &amp; -\frac{1}{\tilde{\mu}_{i,n} \phi^2 } \end{bmatrix} \right) \text{diag}\left( \begin{bmatrix} \frac{\tau^2}{(\sigma_{i,1}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)} &amp; \dots &amp; \frac{\tau^2}{(\sigma_{i,n}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)} \end{bmatrix} \right) \mathbf{z}_i \right] \\ &amp;= \sum_{j = 1}^n -\frac{1}{\tilde{\mu}_{i,j} \sigma^2_{i,j} \phi^2} - \mathbf{z}_i^\top \text{diag}\left( \begin{bmatrix} -\frac{1}{\tilde{\mu}_{i,1} \phi^2 } &amp; \dots &amp; -\frac{1}{\tilde{\mu}_{i,n} \phi^2 } \end{bmatrix} \right) \text{diag}\left( \begin{bmatrix} \frac{\tau^2}{(\sigma_{i,1}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)} &amp; \dots &amp; \frac{\tau^2}{(\sigma_{i,n}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)} \end{bmatrix} \right) \mathbf{z}_i \\ &amp;= \sum_{j = 1}^n -\frac{1}{\tilde{\mu}_{i,j} \sigma^2_{i,j} \phi^2} - \mathbf{z}_{i}^\top \text{diag} \left( \begin{bmatrix} - \frac{\tau^2}{\tilde{\mu}_{i,1} \phi^2 (\sigma^2_{i,1})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)} &amp; \dots &amp; - \frac{\tau^2}{\tilde{\mu}_{i,n} \phi^2 (\sigma^2_{i,n})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)} \end{bmatrix} \right) \mathbf{z}_i \\ &amp;= - \sum_{j = 1}^n \frac{1}{\tilde{\mu}_{i,j} \left(\frac{1}{\tilde{\mu}_{i,j}} \left(1 + \frac{1}{\phi}\right) \right) \phi^2} - \sum_{j = 1}^n \sum_{h = 1}^n \mathbf{z}_{i,j} \mathbf{z}_{i,h} \left[ \text{diag} \left( \begin{bmatrix} - \frac{\tau^2}{\tilde{\mu}_{i,1} \phi^2 (\sigma^2_{i,1})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)} &amp; \dots &amp; - \frac{\tau^2}{\tilde{\mu}_{i,n} \phi^2 (\sigma^2_{i,n})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)} \end{bmatrix} \right) \right]_{j,h} \\ &amp;= - \sum_{j = 1}^n \frac{1}{\phi^2 + \phi} - \sum_{j = 1}^n \mathbf{z}_{i,j}^2 \left(- \frac{\tau^2}{\tilde{\mu}_{i,j} \phi^2 (\sigma^2_{i,j})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)} \right) \\ &amp;= - \sum_{j = 1}^n \left[ \frac{1}{\phi^2 + \phi} - \frac{\tau^2 \mathbf{z}_{i,j}^2 }{\tilde{\mu}_{i,n} \phi^2 \left( \frac{1}{\tilde{\mu}_{i,j}} \left(1 + \frac{1}{\phi}\right) \right)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right) }\right] \\ &amp;= - \sum_{j = 1}^n \left[ \frac{1}{\phi^2 + \phi} - \frac{\tau^2 \mathbf{z}_{i,j}^2 }{ \sigma^2_{i,j} (\phi^2 + \phi) \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right) }\right] \\ &amp;= -\sum_{j = 1}^n \frac{\sigma^2_{i,j} \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right) - \tau^2 \mathbf{z}_{i,j}^2 }{\sigma^2_{i,j} (\phi^2 + \phi) \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right) } &amp;= \sum_{j = 1}^n \frac{\tau^2 \mathbf{z}_{i,j}^2 - \left( \sigma^2_{i,j} + \sigma^2_{i,j} \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)}{\sigma^2_{i,j} (\phi^2 + \phi) \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right) } \end{aligned}\] <p>Next, the quadratic term. This requires the following:</p> \[\begin{aligned} \frac{\partial}{\partial \phi} \left[ [ \Sigma^{-1}_{y_i}]_{j,j'} \right] &amp;= \frac{\partial}{\partial \phi} \left[ - \frac{\tau^2 \mathbf{z}_{i,j} \mathbf{z}_{i,j'}}{(\sigma^2_{i,j})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)}\right] \\ &amp;= - \tau^2 \mathbf{z}_{i,j} \mathbf{z}_{i,j'} \frac{\partial}{\partial \phi} \left[ (\sigma^2_{i,j})^{-2} \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^{-1} \right] \\ &amp;= - \tau^2 \mathbf{z}_{i,j} \mathbf{z}_{i,j'} \left[ -2(\sigma^2_{i,j})^{-3} \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^{-1} \frac{\partial}{\partial \phi} \left[ \sigma^2_{i,j} \right] - \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^{-1} (\sigma^2_{i,j})^{-2} \frac{\partial}{\partial \phi} \left[ 1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right] \right] \\ &amp;= - \tau^2 \mathbf{z}_{i,j} \mathbf{z}_{i,j'} \left[ -2(\sigma^2_{i,j})^{-3} \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^{-1} \frac{\partial}{\partial \phi} \left[ \frac{1}{\tilde{\mu}_{i,j}}\left(1 + \frac{1}{\phi} \right) \right] - \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^{-1} (\sigma^2_{i,j})^{-2} \tau^2 \sum_{l = 1}^n \mathbf{z}_{i,l}^2 \frac{\partial}{\partial \phi} \left[ \left( \sigma^2_{i,l} \right)^{-1}\right] \right] \\ &amp;= - \tau^2 \mathbf{z}_{i,j} \mathbf{z}_{i,j'} \left[ 2(\sigma^2_{i,j})^{-3} \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^{-1} \tilde{\mu}^{-1}_{i,j} \phi^{-2} - \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^{-1} (\sigma^2_{i,j})^{-2} \tau^2 \sum_{l = 1}^n - \mathbf{z}^2_{i,l} (\sigma^2_{i,l})^{-2} \frac{\partial}{\partial \phi} \left[ \sigma^2_{i,l} \right] \right] \\ &amp;= - \tau^2 \mathbf{z}_{i,j} \mathbf{z}_{i,j'} \left[ 2(\sigma^2_{i,j})^{-3} \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^{-1} \tilde{\mu}^{-1}_{i,j} \phi^{-2} + \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^{-1} (\sigma^2_{i,j})^{-2} \tau^2 \sum_{l = 1}^n \mathbf{z}^2_{i,l} (\sigma^2_{i,l})^{-2} \tilde{\mu}_{i,l}^{-1} \phi^{-2} \right] \\ &amp;= - \tau^2 \mathbf{z}_{i,j} \mathbf{z}_{i,j'} \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^{-1} (\sigma^2_{i,j})^{-2} \phi^{-2} \left[ 2(\sigma^2_{i,j})^{-1} \tilde{\mu}^{-1}_{i,j} + \tau^2 \sum_{l = 1}^n \mathbf{z}^2_{i,l} (\sigma^2_{i,l})^{-2} \tilde{\mu}_{i,l}^{-1} \right] \\ &amp;= - \frac{\tau^2 \mathbf{z}_{i,j} \mathbf{z}_{i,j'}}{(\sigma^2_{i,j})^{2} \phi^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right) } \left[ \frac{2}{1 + \frac{1}{\phi}} + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}^2_{i,l}}{ \sigma^2_{i,l} \left(1 + \frac{1}{\phi} \right)} \right] \\ &amp;= - \frac{\tau^2 \mathbf{z}_{i,j} \mathbf{z}_{i,j'}}{(\sigma^2_{i,j})^{2} \phi^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right) \left(1 + \frac{1}{\phi} \right)} \left[ 2 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}^2_{i,l}}{ \sigma^2_{i,l} } \right] \\ \end{aligned}\] <p>And:</p> \[\begin{aligned} \frac{\partial}{\partial \phi} \left[ [ \Sigma^{-1}_{y_i}]_{j,j} \right] &amp;= \frac{\partial}{\partial \phi} \left[ \frac{1}{\sigma^2_{i,j}} - \frac{\tau^2 \mathbf{z}_{i,j} \mathbf{z}_{i,j'}}{(\sigma^2_{i,j})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)}\right] \\ &amp;= -\frac{1}{(\sigma^2_{i,j})^2 } \frac{\partial}{\partial\phi} \left[ \sigma^2_{i,j} \right] - \frac{\partial}{\partial \phi} \left[ \frac{\tau^2 \mathbf{z}_{i,j} \mathbf{z}_{i,j'}}{(\sigma^2_{i,j})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)}\right] \\ &amp;= \frac{1}{(\sigma^2_{i,j})^2 \tilde{\mu}_{i,j} \phi^2} - \frac{\tau^2 \mathbf{z}_{i,j} \mathbf{z}_{i,j'}}{(\sigma^2_{i,j})^{2} \phi^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right) \left(1 + \frac{1}{\phi} \right)} \left[ 2 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}^2_{i,l}}{ \sigma^2_{i,l} } \right] \\ &amp;= \frac{1}{\sigma^2_{i,j} \phi^2 \left(1 + \frac{1}{\phi} \right)} - \frac{\tau^2 \mathbf{z}_{i,j} \mathbf{z}_{i,j'}}{(\sigma^2_{i,j})^{2} \phi^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right) \left(1 + \frac{1}{\phi} \right)} \left[ 2 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}^2_{i,l}}{ \sigma^2_{i,l} } \right] \\ &amp;= \frac{1}{\sigma^2_{i,j}\left( \phi^2 + \phi\right)} - \frac{\tau^2 \mathbf{z}_{i,j} \mathbf{z}_{i,j'}}{(\sigma^2_{i,j})^{2} \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right) \left( \phi^2 + \phi\right)} \left[ 2 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}^2_{i,l}}{ \sigma^2_{i,l} } \right] \end{aligned}\] <p>In matrix notation:</p> \[\begin{aligned} \frac{\partial}{\partial \phi} \left[ \Sigma^{-1}_{y_i} \right] &amp;= \text{diag}\left( \begin{bmatrix} \frac{1}{\sigma^2_{i,1} (\phi^2 + \phi)} &amp; \dots &amp; \frac{1}{\sigma^2_{i,n} (\phi^2 + \phi)} \end{bmatrix} \right) - \text{diag} \left( \begin{bmatrix} \frac{ \tau^2 \left(2 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)}{(\sigma^2_{i,1})^2 (\phi^2 + \phi) \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)} &amp; \dots &amp; \frac{ \tau^2 \left(2 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)}{(\sigma^2_{i,n})^2 (\phi^2 + \phi) \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)} \end{bmatrix} \right) \mathbf{z}_i \mathbf{z}_i^\top \end{aligned}\] </li> </ul> <p>And now we find the gradient with respect to $\alpha$:</p> <ul id="deriv-alpha-2" class="tab" data-tab="acf892c9-72f4-4ba0-b9bb-5e636843bc9a" data-name="deriv-alpha-2"> <li class="active" id="deriv-alpha-2-equation"> <a href="#">equation </a> </li> <li id="deriv-alpha-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="acf892c9-72f4-4ba0-b9bb-5e636843bc9a" data-name="deriv-alpha-2"> <li class="active"> \[\frac{\partial}{\partial \alpha}[ \ell(\theta; \mathbf{y})] = \begin{bmatrix} (\mathbf{y}_1 - \alpha_1 \mathbf{1}_n)^\top \Sigma_{y_1}^{-1} \mathbf{1}_n \\ \vdots \\ (\mathbf{y}_k - \alpha_k \mathbf{1}_n)^\top \Sigma_{y_k}^{-1} \mathbf{1}_n \end{bmatrix}\] </li> <li> <p>We do this component-wise:</p> \[\begin{aligned} \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} &amp;= \frac{\partial}{\partial \alpha_j} \left[ \sum_{i = 1}^k \left[ -\frac{n}{2} \log(2 \pi) - \frac{1}{2}\log(\rvert \Sigma_{y_i} \rvert) - \frac{1}{2} (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \Sigma_{y_i}^{-1} (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \right] \\ &amp;= - \frac{1}{2} \frac{\partial}{\partial \alpha_j} \left[(\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \Sigma_{y_j}^{-1} (\mathbf{y}_j - \alpha_j \mathbf{1}_n) \right] \\ &amp;= - \frac{1}{2}\left(2 (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \Sigma_{y_j}^{-1} (-\mathbf{1}_n) \right) \\ &amp;= (\mathbf{y}_i - \alpha_j \mathbf{1}_n)^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ \implies \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha} &amp;= \begin{bmatrix} (\mathbf{y}_1 - \alpha_1 \mathbf{1}_n)^\top \Sigma_{y_1}^{-1} \mathbf{1}_n \\ \vdots \\ (\mathbf{y}_k - \alpha_k \mathbf{1}_n)^\top \Sigma_{y_k}^{-1} \mathbf{1}_n \end{bmatrix} \end{aligned}\] </li> </ul> <h4 id="mles">MLEs</h4> <p>We can find $\hat{\theta}$ by setting the above equal to zero and substitute $\tau^2 = 0$. We get:</p> <ul id="mle-2" class="tab" data-tab="89f7ed5c-85de-4371-aabf-aad24bb1e199" data-name="mle-2"> <li class="active" id="mle-2-equation"> <a href="#">equation </a> </li> <li id="mle-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="89f7ed5c-85de-4371-aabf-aad24bb1e199" data-name="mle-2"> <li class="active"> \[\hat{\theta} = \begin{bmatrix} \left(\sum_{l' = 1}^n \frac{1}{\sigma^2_{1,l'}} \right)^{-1} \sum_{l' = 1}^n \frac{\mathbf{y}_{1,l'}}{\sigma^2_{1,l'}} \\ \vdots \\ \left(\sum_{l' = 1}^n \frac{1}{\sigma^2_{k,l'}} \right)^{-1} \sum_{l' = 1}^n \frac{\mathbf{y}_{k,l'}}{\sigma^2_{k,l'}} \\ 0 \end{bmatrix}\] </li> <li> <p>We only need to deal with $\alpha$, which we can again do component-wise. First notice that:</p> \[\begin{aligned} \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} &amp;= (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ &amp;= (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left( \text{diag}\left(\left[ \frac{1}{\sigma^2_{j, 1}}, \dots, \frac{1}{\sigma^2_{j,n}} \right]\right) - \frac{\text{diag}\left( \left[ \frac{\tau^2}{(\sigma_{j,1}^2)^2}, \dots, \frac{\tau^2}{(\sigma_{j,n}^2)^2} \right] \right) \mathbf{z}_j \mathbf{z}_j^\top}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\sigma^2_{j,l}}} \right) \mathbf{1}_n \\ &amp;= (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left(\begin{bmatrix} \frac{1}{\sigma^2_{j, 1}} \\ \vdots \\ \frac{1}{\sigma^2_{j,n}} \end{bmatrix} - \left( \frac{1}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\sigma_{j,l}^2}} \right)\text{diag}\left( \left[ \frac{\tau^2}{(\sigma_{j,1}^2)^2}, \dots, \frac{\tau^2}{(\sigma_{j,n}^2)^2} \right] \right) \begin{bmatrix} \sum_{l = 1}^n \mathbf{z}_{j,1} \mathbf{z}_{j,l} \\ \vdots \\ \sum_{l = 1}^n \mathbf{z}_{j,n} \mathbf{z}_{j,l} \end{bmatrix} \right) \\ &amp;= \left( \sum_{l = 1}^n \frac{\mathbf{y}_{j,l} - \alpha_j}{\sigma^2_{j,l}} \right)- \left( \frac{1}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\sigma_{j,l}^2}} \right) (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \begin{bmatrix} \frac{\tau^2}{(\sigma_{j,1}^2)^2} \sum_{l = 1}^n \mathbf{z}_{j,1} \mathbf{z}_{j,l} \\ \vdots \\ \frac{\tau^2}{(\sigma_{j,n}^2)^2} \sum_{l = 1}^n \mathbf{z}_{j,n} \mathbf{z}_{j,l} \end{bmatrix} \\ &amp;= \left( \sum_{l = 1}^n \frac{\mathbf{y}_{j,l} - \alpha_j}{\sigma^2_{j,l}} \right)- \left( \frac{1}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\sigma_{j,l}^2}} \right) \sum_{l' = 1}^n (\mathbf{y}_{j,l'} - \alpha_j) \frac{\tau^2}{(\sigma_{j,l'}^2)^2} \sum_{l = 1}^n \mathbf{z}_{j,l'} \mathbf{z}_{j,l} \\ &amp;= \left( \sum_{l = 1}^n \frac{\mathbf{y}_{j,l} - \alpha_j}{\sigma^2_{j,l}} \right)- \left( \frac{\sum_{l = 1}^n \mathbf{z}_{j,l} }{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\sigma_{j,l}^2}} \right) \left( \sum_{l' = 1}^n \frac{\tau^2 (\mathbf{y}_{j,l'} - \alpha_j) \mathbf{z}_{j,l'} }{(\sigma_{j,l'}^2)^2} \right) \end{aligned}\] <p>So then we solve for $\alpha_j$ in:</p> \[\begin{aligned} 0 &amp;= (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ \implies 0 &amp;= \left( \sum_{l = 1}^n \frac{\mathbf{y}_{j,l} - \alpha_j}{\sigma^2_{j,l}} \right)- \left( \frac{\sum_{l = 1}^n \mathbf{z}_{j,l} }{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\sigma_{j,l}^2}} \right) \left( \sum_{l' = 1}^n \frac{\tau^2 (\mathbf{y}_{j,l'} - \alpha_j) \mathbf{z}_{j,l'} }{(\sigma_{j,l'}^2)^2} \right) \\ \implies 0 &amp;= \sum_{l = 1}^n \frac{\mathbf{y}_{j,l}}{\sigma^2_{j,l}} - \alpha_j \sum_{l = 1}^n \frac{1}{\sigma^2_{j,l}} - \left( \frac{\sum_{l = 1}^n \mathbf{z}_{j,l} }{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\sigma_{j,l}^2}} \right) \left[ \sum_{l' = 1}^n \frac{\tau^2 \mathbf{y}_{j,l'} \mathbf{z}_{j,l'}}{(\sigma^2_{j,l'})^2} - \alpha_j \sum_{l' =1}^n \frac{\tau^2 \mathbf{z}_{j,l'}}{(\sigma^2_{j,l'})} \right] \\ \implies \alpha_j \left[ \sum_{l = 1}^n \frac{1}{\sigma^2_{j,l}} - \sum_{l = 1}^n \frac{\tau^2 \mathbf{z}_{j,l'}}{(\sigma^2_{j,l'})^2}\right] &amp;= \sum_{l = 1}^n \frac{\mathbf{y}_{j,l}}{\sigma^2_{j,l}} - \left( \frac{\sum_{l = 1}^n \mathbf{z}_{j,l} }{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\sigma_{j,l}^2}} \right) \sum_{l' = 1}^n\frac{\tau^2 \mathbf{y}_{j,l'} \mathbf{z}_{j,l'}}{(\sigma^2_{j,l'})^2} \\ \implies \alpha_j \left(\sum_{l' = 1}^n \frac{1}{\sigma^2_{j,l'}} \left[1 - \frac{\tau^2 \mathbf{z}_{j,l'}}{\sigma^2_{j,l'}} \right]\right) &amp;= \sum_{l' = 1}^n \frac{\mathbf{y}_{j,l'}}{\sigma^2_{j,l'}} \left[ 1 - \left( \frac{\sum_{l = 1}^n \mathbf{z}_{j,l} }{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\sigma_{j,l}^2}} \right) \frac{\tau^2 \mathbf{z}_{j,l'}}{\sigma^2_{j,l'}} \right] \\ \implies \alpha_j &amp;= \left(\sum_{l' = 1}^n \frac{1}{\sigma^2_{j,l'}} \left[1 - \frac{\tau^2 \mathbf{z}_{j,l'}}{\sigma^2_{j,l'}} \right]\right)^{-1} \sum_{l' = 1}^n \frac{\mathbf{y}_{j,l'}}{\sigma^2_{j,l'}} \left[ 1 - \left( \frac{\sum_{l = 1}^n \mathbf{z}_{j,l} }{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\sigma_{j,l}^2}} \right) \frac{\tau^2 \mathbf{z}_{j,l'}}{\sigma^2_{j,l'}} \right] \end{aligned}\] <p>Under $H_0$, $\tau^2 = 0$, so we get:</p> \[\hat{\alpha}_j = \left(\sum_{l' = 1}^n \frac{1}{\sigma^2_{j,l'}} \right)^{-1} \sum_{l' = 1}^n \frac{\mathbf{y}_{j,l'}}{\sigma^2_{j,l'}}\] </li> </ul> <p>Thus, the score evaluated at $\theta = \hat{\theta}$ is then:</p> \[U_{\theta}(\hat{\theta}) = \begin{bmatrix} \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha} \bigg\rvert_{\theta = \hat{\theta}} \\ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \bigg\rvert_{\theta = \hat{\theta}} \end{bmatrix} = \begin{bmatrix} \left(\sum_{l' = 1}^n \frac{1}{\sigma^2_{1,l'}} \right)^{-1} \sum_{l' = 1}^n \frac{\mathbf{y}_{1,l'}}{\sigma^2_{1,l'}} \\ \vdots \\ \left(\sum_{l' = 1}^n \frac{1}{\sigma^2_{k,l'}} \right)^{-1} \sum_{l' = 1}^n \frac{\mathbf{y}_{k,l'}}{\sigma^2_{k,l'}} \\ - \frac{1}{2}\sum_{i = 1}^k \left[ \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma_{i,l}^2} + \sum_{l = 1}^n \sum_{l' = 1}^n \frac{\mathbf{z}_{i,l}\mathbf{z}_{i,l'}(\mathbf{y}_{i,l} - \hat{\alpha}_i)(\mathbf{y}_{i,l'} - \hat{\alpha}_i)}{\sigma^2_{i,l} \sigma^2_{i,l'}}\right] \end{bmatrix}\] <h3 id="information">Information</h3> <p>As before, to find the information, we need to compute the second-order derivatives of the log-likelihood, take the expectation under $H_0$ of minus those quantities, and evaluate them by plugging in $\hat{\theta}$.</p> <h4 id="derivatives-1">Derivatives</h4> <p>We’ll take all of the derivatives component-wise. We’ll start with those with respect to $\tau^2$.</p> <ul id="deriv-theta-tau-2" class="tab" data-tab="adbf28de-2ce5-4084-9680-c499da97806f" data-name="deriv-theta-tau-2"> <li class="active" id="deriv-theta-tau-2-equation"> <a href="#">equation </a> </li> <li id="deriv-theta-tau-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="adbf28de-2ce5-4084-9680-c499da97806f" data-name="deriv-theta-tau-2"> <li class="active"> \[\begin{aligned} \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \tau^2 \partial \theta} &amp;= \begin{bmatrix} - \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{1,l}}{\sigma^2_{1,l}}\right)^2} \left(\sum_{l' = 1}^n \frac{\mathbf{z}_{1,l'}(\mathbf{y}_{1,l'} - \alpha_1)}{(\sigma^2_{1,l'})^2}\right)\left(\sum_{l'' = 1}^n \mathbf{z}_{1,l''}\right) \\ \vdots \\ - \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{k,l}}{\sigma^2_{k,l}}\right)^2} \left(\sum_{l' = 1}^n \frac{\mathbf{z}_{k,l'}(\mathbf{y}_{k,l'} - \alpha_k)}{(\sigma^2_{k,l'})^2}\right)\left(\sum_{l'' = 1}^n \mathbf{z}_{k,l''}\right) \\ \frac{1}{2} \sum_{i = 1}^k \left[ \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^2} \left( \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^2 - \frac{2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^3} \left( \sum_{a = 1}^n (\mathbf{y}_{i,a} - \alpha_i) \frac{\mathbf{z}_{i,a}}{\sigma^2_{i,a}} \right)^2 \right] \end{bmatrix} \end{aligned} \label{eq:info-tau-tau}\] </li> <li> \[\begin{aligned} \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial (\tau^2)^2} &amp;= \frac{\partial}{\partial \tau^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] \\ &amp;= \frac{\partial}{\partial \tau^2} \left[ - \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} - \sum_{l = 1}^n \sum_{j = 1}^n \frac{\tau^2 \mathbf{z}_{i,l}^2 \mathbf{z}_{i,j}^2}{(\sigma_{i,l}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)} - \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \frac{\mathbf{z}_{i,a} \mathbf{z}_{i,b}}{(\sigma^2_{i,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^2} \right] \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[\sum_{l = 1}^n \sum_{j = 1}^n \frac{\partial}{\partial \tau^2} \left[ \frac{\tau^2 \mathbf{z}_{i,l}^2 \mathbf{z}_{i,j}^2}{(\sigma_{i,l}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)} \right] + \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \frac{\partial}{\partial \tau^2} \left[ \frac{\mathbf{z}_{i,a} \mathbf{z}_{i,b}}{(\sigma^2_{i,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^2} \right] \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \left(\frac{\mathbf{z}_{i,l}^2 \mathbf{z}_{i,j}^2 (\sigma^2_{i,l})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right) - \tau^2 \mathbf{z}_{i,l}^2 \mathbf{z}_{i,j}^2 (\sigma^2_{i,l})^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} }{\left((\sigma^2_{i,l})^2 \left( 1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)\right)^2}\right) + \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \left(\frac{-2\mathbf{z}_{i,a}\mathbf{z}_{i,b}(\sigma^2_{i,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma_{i,l}}\right) \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma_{i,l}^2}}{\left((\sigma^2_{i,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}} \right)^2\right)^2}\right) \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\sigma^2_{i,l})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^2} - 2 \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \left(\frac{\mathbf{z}_{i,a}\mathbf{z}_{i,b}(\sigma^2_{i,a})^2 \left(\sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}} \right)}{(\sigma^2_{i,a})^4 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}} \right)^3}\right) \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[ \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^2} \left( \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{(\sigma^2_{i,l})^2} \right) \left( \sum_{j = 1}^n \mathbf{z}_{i,j}^2 \right) - \frac{2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^3} \left( \sum_{a = 1}^n (\mathbf{y}_{i,a} - \alpha_i) \frac{\mathbf{z}_{i,a}}{(\sigma^2_{i,a})^2} \right) \left( \sum_{b = 1}^n (\mathbf{y}_{i,b} - \alpha_i) \mathbf{z}_{i,b} \right) \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[ \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^2} \left( \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{(\sigma^2_{i,l})^2} \right) \left( \sum_{l = 1}^n \mathbf{z}_{i,l}^2 \right) - \frac{2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^3} \left( \sum_{a = 1}^n (\mathbf{y}_{i,a} - \alpha_i) \frac{\mathbf{z}_{i,a}}{\sigma^2_{i,a}} \right) \left( \sum_{a = 1}^n (\mathbf{y}_{i,a} - \alpha_i) \frac{\mathbf{z}_{i,a}}{\sigma^2_{i,a}} \right) \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[ \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^2} \left( \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^2 - \frac{2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^3} \left( \sum_{a = 1}^n (\mathbf{y}_{i,a} - \alpha_i) \frac{\mathbf{z}_{i,a}}{\sigma^2_{i,a}} \right)^2 \right] \end{aligned}\] <p>Next, we take the derivative (with respect to $\tau^2$) of the derivative with respect to $\alpha_j$:</p> \[\begin{aligned} \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \tau^2 \partial \alpha_j } &amp;= \frac{\partial}{\partial \tau^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] \\ &amp;= \frac{\partial}{\partial \tau^2} \left[ (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \right] \\ &amp;= \begin{bmatrix} (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left[ \text{diag}\left( \begin{bmatrix} -\frac{1}{(\sigma^2_{j,1})^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\sigma^2_{j,l}}\right)^2} &amp; \dots &amp; -\frac{1}{(\sigma^2_{j,n})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\sigma^2_{j,l}}\right)^2} \end{bmatrix} \right) \mathbf{z}_{j} \mathbf{z}_j^\top \right] \mathbf{1}_n \end{bmatrix} \\ &amp;= \begin{bmatrix} -\frac{\mathbf{y}_{j, 1} - \alpha_j}{(\sigma^2_{j,1})^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\sigma^2_{j,l}}\right)^2} &amp; \dots &amp; -\frac{\mathbf{y}_{j,n} - \alpha_j}{(\sigma^2_{j,n})^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\sigma^2_{j,l}}\right)^2} \end{bmatrix} \mathbf{z}_j \mathbf{z}_j^\top \mathbf{1}_n \\ &amp;= \left(- \sum_{l' = 1}^n \frac{\mathbf{z}_{j,l'}(\mathbf{y}_{j,l'} - \alpha_j)}{(\sigma^2_{j,l'})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\sigma^2_{j,l}}\right)^2 }\right)\left(\sum_{l'' = 1}^n \mathbf{z}_{j,l''}\right) \\ &amp;= - \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\sigma^2_{j,l}}\right)^2} \left(\sum_{l' = 1}^n \frac{\mathbf{z}_{j,l'}(\mathbf{y}_{j,l'} - \alpha_j)}{(\sigma^2_{j,l'})^2}\right)\left(\sum_{l'' = 1}^n \mathbf{z}_{j,l''}\right) \end{aligned}\] <p>Putting the two together into a matrix yield:</p> \[\begin{aligned} \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \tau^2 \partial \theta } &amp;= \frac{\partial}{\partial \tau^2} \left[ \begin{bmatrix} \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha} \\ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \end{bmatrix} \right] \\ &amp;= \begin{bmatrix} - \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{1,l}}{\sigma^2_{1,l}}\right)^2} \left(\sum_{l' = 1}^n \frac{\mathbf{z}_{1,l'}(\mathbf{y}_{1,l'} - \alpha_1)}{(\sigma^2_{1,l'})^2}\right)\left(\sum_{l'' = 1}^n \mathbf{z}_{1,l''}\right) \\ \vdots \\ - \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{k,l}}{\sigma^2_{k,l}}\right)^2} \left(\sum_{l' = 1}^n \frac{\mathbf{z}_{k,l'}(\mathbf{y}_{k,l'} - \alpha_k)}{(\sigma^2_{k,l'})^2}\right)\left(\sum_{l'' = 1}^n \mathbf{z}_{k,l''}\right) \\ \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\sigma^2_{i,l})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^2} - 2 \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \left(\frac{\mathbf{z}_{i,a}\mathbf{z}_{i,b}(\sigma^2_{i,a})^2 \left(\sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}} \right)}{(\sigma^2_{i,l})^4 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}} \right)^3}\right) \right] \end{bmatrix} \end{aligned}\] </li> </ul> <p>We then do the same but with respect to $\alpha$.</p> <ul id="deriv-theta-alpha-2" class="tab" data-tab="025764c5-6053-4481-94b3-2fd8559ffc3f" data-name="deriv-theta-alpha-2"> <li class="active" id="deriv-theta-alpha-2-equation"> <a href="#">equation </a> </li> <li id="deriv-theta-alpha-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="025764c5-6053-4481-94b3-2fd8559ffc3f" data-name="deriv-theta-alpha-2"> <li class="active"> \[\frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j \partial \theta} = \begin{bmatrix} 0 \\ \vdots \\ - \mathbf{1}_n^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ \vdots \\ 0 \\ -\frac{1}{\left( 1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\sigma^2_{j,l}}\right)^2 } \sum_{a = 1}^n \frac{\mathbf{z}_{j,a}(\mathbf{y}_{j, a} - \alpha_j)}{(\sigma_{j,a}^2)^2} \left(\sum_{b = 1}^n \mathbf{z}_{j,b} \right) \end{bmatrix}\] </li> <li> <p>First, we find the gradient (with respect to $\alpha$) of the derivative with respect to $\tau^2$. This should be equivalent to the corresponding components of Eq. \eqref{eq:info-tau-tau}.</p> \[\begin{aligned} \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j \partial \tau^2 } &amp;= \frac{\partial}{\partial \alpha_j} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] \\ &amp;= \frac{\partial}{\partial \alpha_j} \left[ - \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} - \sum_{l = 1}^n \sum_{j = 1}^n \frac{\tau^2 \mathbf{z}_{i,l}^2 \mathbf{z}_{i,j}^2}{(\sigma_{i,l}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}}\right)} - \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \frac{\mathbf{z}_{i,a} \mathbf{z}_{i,b}}{(\sigma^2_{i,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^2} \right] \right] \\ &amp;= \frac{\partial}{\partial \alpha_j} \left[ \frac{1}{2} \sum_{i = 1}^k \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \frac{\mathbf{z}_{i,a} \mathbf{z}_{i,b}}{(\sigma^2_{i,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}}\right)^2} \right] \\ &amp;= \frac{1}{2} \sum_{a = 1}^n \sum_{b = 1}^n \frac{\partial}{\partial \alpha_j} \left[ (\mathbf{y}_{j,a} - \alpha_j)(\mathbf{y}_{j,b} - \alpha_j) \frac{\mathbf{z}_{j,a} \mathbf{z}_{j,b}}{(\sigma^2_{j,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\sigma^2_{j,l}}\right)^2} \right] \\ &amp;= \frac{1}{2} \sum_{a = 1}^n \sum_{b = 1}^n \frac{\mathbf{z}_{j,a} \mathbf{z}_{j,b}}{(\sigma^2_{j,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\sigma^2_{j,l}}\right)^2} \frac{\partial}{\partial \alpha_j} \left[ \mathbf{y}_{j,a} \mathbf{y}_{j,b} - \alpha_j \mathbf{y}_{j,a} - \alpha \mathbf{y}_{j,b} + \alpha_j^2 \right] \\ &amp;= -\frac{1}{2 \left( 1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\sigma^2_{j,l}}\right)^2 } \sum_{a = 1}^n \sum_{b = 1}^n \left[ \frac{\mathbf{z}_{j,a}\mathbf{z}_{j,b}(\mathbf{y}_{j, a} - \alpha_j)}{(\sigma_{j,a}^2)^2} + \frac{\mathbf{z}_{j,a}\mathbf{z}_{j,b}(\mathbf{y}_{j,b} - \alpha_j)}{(\sigma_{j,a}^2)^2} \right] \\ &amp;= -\frac{1}{2 \left( 1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\sigma^2_{j,l}}\right)^2 } \left[ \sum_{a = 1}^n \frac{\mathbf{z}_{j,a}(\mathbf{y}_{j, a} - \alpha_j)}{(\sigma_{j,a}^2)^2} \sum_{b = 1}^n \mathbf{z}_{j,b} + \sum_{b = 1}^n \frac{\mathbf{z}_{j,b}(\mathbf{y}_{j,b} - \alpha_j)}{(\sigma_{j,b}^2)^2} \sum_{a = 1}^n \mathbf{z}_{j,a}\right] \\ &amp;= -\frac{1}{\left( 1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\sigma^2_{j,l}}\right)^2 } \sum_{a = 1}^n \frac{\mathbf{z}_{j,a}(\mathbf{y}_{j, a} - \alpha_j)}{(\sigma_{j,a}^2)^2} \left(\sum_{b = 1}^n \mathbf{z}_{j,b} \right) \end{aligned}\] <p>Now, we find the vector of second derivatives of the log-likelihood with respect to the components of $\alpha$:</p> \[\begin{aligned} \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j^2} &amp;= \frac{\partial}{\partial \alpha_j} \left[ (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \Sigma_{y_j}^{-1} \mathbf{1}_n\right] \\ &amp;= - \mathbf{1}_n^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ \frac{\partial^2 \ell(\theta; \mathbf{y})}{ \partial \alpha_{j'} \partial \alpha_j} &amp;= \frac{\partial}{\partial \alpha_{j'}} \left[ (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \Sigma_{y_j}^{-1} \mathbf{1}_n\right] \\ &amp;= 0 \\ \implies \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j \partial \alpha} &amp;= \begin{bmatrix} 0 \\ \vdots \\ - \mathbf{1}_n^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ \vdots \\ 0 \end{bmatrix} \end{aligned}\] <p>Putting the two above results together gives us:</p> \[\frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j \partial \theta} = \begin{bmatrix} 0 \\ \vdots \\ - \mathbf{1}_n^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ \vdots \\ 0 \\ -\frac{1}{\left( 1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\sigma^2_{j,l}}\right)^2 } \sum_{a = 1}^n \frac{\mathbf{z}_{j,a}(\mathbf{y}_{j, a} - \alpha_j)}{(\sigma_{j,a}^2)^2} \left(\sum_{b = 1}^n \mathbf{z}_{j,b} \right) \end{bmatrix}\] </li> </ul> <h4 id="expectations">Expectations</h4> <p>We now take the expectation of the above vectors. We’ll evaluate the second order partial derivatives with respect to $\tau^2$ first.</p> <ul id="info-tau-tau-2" class="tab" data-tab="ad8817d5-cf2f-4f34-9c1c-cc1cadeadcc1" data-name="info-tau-tau-2"> <li class="active" id="info-tau-tau-2-equation"> <a href="#">equation </a> </li> <li id="info-tau-tau-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="ad8817d5-cf2f-4f34-9c1c-cc1cadeadcc1" data-name="info-tau-tau-2"> <li class="active"> \[\mathbb{E}\left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \tau^2 \partial \theta } \right] = \begin{bmatrix} 0 \\ \vdots \\ 0 \\ \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\sigma^2_{i,l})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^2} \right] \end{bmatrix}\] </li> <li> \[\begin{aligned} \mathbb{E}\left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial (\tau^2)^2 }\right] &amp;= \mathbb{E}\left[ \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\sigma^2_{i,l})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^2} - 2 \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \left(\frac{\mathbf{z}_{i,a}\mathbf{z}_{i,b}(\sigma^2_{i,a})^2 \left(\sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}} \right)}{(\sigma^2_{i,l})^4 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}} \right)^3}\right) \right] \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\sigma^2_{i,l})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^2} - 2 \sum_{a = 1}^n \sum_{b = 1}^n \mathbb{E}\left[ (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i)\right] \left(\frac{\mathbf{z}_{i,a}\mathbf{z}_{i,b}(\sigma^2_{i,a})^2 \left(\sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}} \right)}{(\sigma^2_{i,l})^4 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}} \right)^3}\right) \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\sigma^2_{i,l})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^2} - 2 \sum_{a = 1}^n \sum_{b = 1}^n \text{Cov}\left(\mathbf{y}_{i,a}, \mathbf{y}_{i,b} \right) \left(\frac{\mathbf{z}_{i,a}\mathbf{z}_{i,b}(\sigma^2_{i,a})^2 \left(\sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}} \right)}{(\sigma^2_{i,l})^4 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\sigma^2_{i,l}} \right)^3}\right) \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\sigma^2_{i,l})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^2} \right] &amp; \left(\text{obs. ind. under } H_0 \right) \\ \mathbb{E}\left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \tau^2 \partial \alpha_j}\right] &amp;= \mathbb{E}\left[ - \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\sigma^2_{j,l}}\right)^2} \left(\sum_{l' = 1}^n \frac{\mathbf{z}_{j,l'}(\mathbf{y}_{j,l'} - \alpha_j)}{(\sigma^2_{j,l'})^2}\right)\left(\sum_{l'' = 1}^n \mathbf{z}_{j,l''}\right) \right] \\ &amp;= - \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\sigma^2_{j,l}}\right)^2} \left(\sum_{l' = 1}^n \frac{\mathbf{z}_{j,l'} \mathbb{E}\left[ (\mathbf{y}_{j,l'} - \alpha_j)\right] }{(\sigma^2_{j,l'})^2}\right)\left(\sum_{l'' = 1}^n \mathbf{z}_{j,l''}\right) \\ &amp;= 0 \end{aligned}\] </li> </ul> <p>And then we do the same for $\alpha_j$:</p> <ul id="info-alpha-alpha-2" class="tab" data-tab="c7b55190-1014-4f2c-af72-e41f1d6895a8" data-name="info-alpha-alpha-2"> <li class="active" id="info-alpha-alpha-2-equation"> <a href="#">equation </a> </li> <li id="info-alpha-alpha-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="c7b55190-1014-4f2c-af72-e41f1d6895a8" data-name="info-alpha-alpha-2"> <li class="active"> \[\mathbb{E}\left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j \partial \theta } \right] = \begin{bmatrix} 0 \\ \vdots \\ - \mathbf{1}_n^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ \vdots \\ 0 \\ 0 \end{bmatrix}\] </li> <li> \[\begin{aligned} \mathbb{E}\left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j \partial \tau^2} \right] &amp;= \mathbb{E}\left[ -\frac{1}{\left( 1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\sigma^2_{j,l}}\right)^2 } \sum_{a = 1}^n \frac{\mathbf{z}_{j,a}(\mathbf{y}_{j, a} - \alpha_j)}{(\sigma_{j,a}^2)^2} \left(\sum_{b = 1}^n \mathbf{z}_{j,b} \right) \right] \\ &amp;= -\frac{1}{\left( 1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\sigma^2_{j,l}}\right)^2 } \sum_{a = 1}^n \frac{\mathbf{z}_{j,a} \mathbb{E}\left[\mathbf{y}_{j, a} - \alpha_j\right] }{(\sigma_{j,a}^2)^2} \left(\sum_{b = 1}^n \mathbf{z}_{j,b} \right) \\ &amp;= 0 \\ \mathbb{E}\left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j^2} \right] &amp;= \mathbb{E}\left[ - \mathbf{1}_n^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \right] \\ &amp;= - \mathbf{1}_n^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ \mathbb{E}\left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j \partial \alpha_{j'}} \right] &amp;= 0 \end{aligned}\] </li> </ul> <p>We then evaluate the Fisher information at the MLE:</p> <ul id="info-2" class="tab" data-tab="49d95ab8-4b6d-48c4-bbff-538e2f05d796" data-name="info-2"> <li class="active" id="info-2-equation"> <a href="#">equation </a> </li> <li id="info-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="49d95ab8-4b6d-48c4-bbff-538e2f05d796" data-name="info-2"> <li class="active"> \[\mathcal{I}_{\theta, \theta}(\hat{\theta}) = \begin{bmatrix} - \mathbf{1}_n^\top\text{diag}\left(\left[ \frac{1}{\sigma^2_{1,1}}, \dots, \frac{1}{\sigma^2_{1,n}}\right]\right) \mathbf{1}_n &amp; 0 &amp; \dots &amp; 0 \\ 0 &amp; - \mathbf{1}_n^\top \text{diag}\left(\left[ \frac{1}{\sigma^2_{2,1}}, \dots, \frac{1}{\sigma^2_{2,n}}\right]\right) \mathbf{1}_n &amp; \dots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \dots &amp; \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\sigma^2_{i,l})^2} \right] \end{bmatrix}\] </li> <li> <p>Note that:</p> \[\Sigma_{y_i}^{-1} \bigg\rvert_{\theta = \hat{\theta}} = \text{diag}\left(\left[ \frac{1}{\sigma^2_{i,1}}, \dots, \frac{1}{\sigma^2_{i,n}}\right]\right)\] <p>Then we have:</p> \[\begin{aligned} - \mathbb{E}\left. \left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \tau^2 \partial \theta} \right] \right\rvert_{\theta = \hat{\theta}} &amp;= - \mathbb{E} \left.\left[ \begin{bmatrix} 0 \\ \vdots \\ 0 \\ \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\sigma^2_{i,l})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\sigma^2_{i,l}} \right)^2} \right] \end{bmatrix} \right] \right\rvert_{\theta = \hat{\theta}} \\ &amp;= \begin{bmatrix} 0 \\ \vdots \\ 0 \\ \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\sigma^2_{i,l})^2} \right] \end{bmatrix} \\ - \mathbb{E} \left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j \partial \theta} \right] \bigg\rvert_{\theta = \hat{\theta}} &amp;= - \mathbb{E} \left. \left[ \begin{bmatrix} 0 \\ \vdots \\ - \mathbf{1}_n^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ \vdots \\ 0 \\ 0 \end{bmatrix} \right] \right\rvert_{\theta = \hat{\theta}} \\ &amp;= \begin{bmatrix} 0 \\ \vdots \\ - \mathbf{1}_n^\top \text{diag}\left(\left[ \frac{1}{\sigma^2_{j,1}}, \dots, \frac{1}{\sigma^2_{j,n}}\right]\right) \mathbf{1}_n \\ \vdots \\ 0 \\ 0 \end{bmatrix} \end{aligned}\] <p>Putting these together into a big matrix:</p> \[\begin{aligned} \mathcal{I}_{\theta, \theta}(\hat{\theta}) &amp;= - \mathbb{E}\left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \theta \partial \theta^\top} \right] \bigg\rvert_{\theta = \hat{\theta}} \\ &amp;= \begin{bmatrix} - \mathbf{1}_n^\top\text{diag}\left(\left[ \frac{1}{\sigma^2_{1,1}}, \dots, \frac{1}{\sigma^2_{1,n}}\right]\right) \mathbf{1}_n &amp; 0 &amp; \dots &amp; 0 \\ 0 &amp; - \mathbf{1}_n^\top \text{diag}\left(\left[ \frac{1}{\sigma^2_{2,1}}, \dots, \frac{1}{\sigma^2_{2,n}}\right]\right) \mathbf{1}_n &amp; \dots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \dots &amp; \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\sigma^2_{i,l})^2} \right] \end{bmatrix} \end{aligned}\] </li> </ul> <hr/> ]]></content><author><name></name></author><category term="glmm"/><category term="glmm"/><category term="information"/><category term="score"/><summary type="html"><![CDATA[The Negative Binomial Case]]></summary></entry><entry><title type="html">Score and Information</title><link href="https://aerosengart.github.io/blog/2025/score-info/" rel="alternate" type="text/html" title="Score and Information"/><published>2025-10-29T00:00:00+00:00</published><updated>2025-10-29T00:00:00+00:00</updated><id>https://aerosengart.github.io/blog/2025/score-info</id><content type="html" xml:base="https://aerosengart.github.io/blog/2025/score-info/"><![CDATA[<p>This post is just a catch-all for my derivations for my score test project. Our set-up is as follows. We have $n$ observations coming from $k$ different clusters, each of size $n_t$ for $t \in [k]$. The full data will be denoted by $\mathbf{y}$. Though $\mathbf{y}$ is a vector, we’ll denote the $j$-th observation from cluster $i$ with $\mathbf{y}_{i,j}$. For example, \(\mathbf{y}_{i,j}\) denotes element \(\sum_{l = 1}^{i - 1} n_l + j\) of $\mathbf{y}$. We’ll also denote the $n_i$-dimensional vector of responses for cluster $i$ with $\mathbf{y}_i$.</p> <p>For each observation, we will have $p$ fixed effect covariates arranged in a $p$-dimensional vector, \(\mathbf{x}_{i, j}\), and $q$ random effects covariates in a $q$-dimensional vector, \(\mathbf{z}_{i,j}\). We’ll assume that the observations within the same cluster are independent.</p> <p>Our model comes in the form of a specification of the conditional mean, $\mu_{i,j} = \mathbb{E}[\mathbf{y}_{i,j} \rvert \beta_i]$ (where we suppress the addition conditioning on the covariates themselves). For a monotonic and differentiable link function (e.g. $\log(\cdot)$ or $\text{logit}(\cdot)$), the conditional mean of the $j$-th observation in group $i$ is assumed to be given by:</p> \[\mu_{i,j} = g^{-1}\left(\alpha^\top \mathbf{x}_{i,j} + \beta_i^\top \mathbf{z}_{i,j} \right) \label{eq:glmm}\] <p>We then assume that the observations themselves follow some exponential family distribution with measurement errors, $\epsilon_{i,j}$, which is the deviation of the response from its (unit-specific) conditional mean. These errors are assumed to have mean zero and be independent of each other and of the random effects. We further assume the responses, $\mathbf{y}_{i,j}$, conditional on the random effects (and the covariates), are independent with variances equal to some function of the conditional mean.</p> <p>In general, we will assume that:</p> \[\beta_i \overset{iid}{\sim} \mathcal{N}\left(\mathbf{0}_q, D(\tau^2) \right)\] <p>for some variance component, $\tau^2$. We’ll use $[\cdot] \rvert_{H_0}$ to denote evaluation of the function in brackets when setting $\beta$ equal to $\beta_0$. We’ll also use a superscript $0$ (e.g. $\mu^0$, $\eta^0$, etc.) to denote the quantity under the null hypothesis (i.e. $\tau^2 = \mathbf{0} \implies \beta = \mathbf{0}$).</p> <hr/> <h2 id="gaussian-case">Gaussian Case</h2> <p>In this example, we’ll have the simple setting of a Gaussian response, which means $g(\cdot)$ is the identity function. We will have a fixed (but cluster-specific) intercept and a single random slope. We will have $k$ clusters and $n$ observations per cluster. We assume:</p> \[\mathbf{y}_{i, j} = \alpha_i + \beta_i \mathbf{z}_{i,j} + \epsilon_{i,j}, \hspace{8mm} \epsilon_{i,j} \overset{iid}{\sim} \mathcal{N}(0, \sigma^2), \hspace{5mm} \beta_i \overset{iid}{\sim} \mathcal{N}(0, \tau^2)\] <p>where we also assume the random effects and errors are independent. \(\mathbf{z}_i \in \mathbb{R}^n\) is the vector of covariate values for the $n$ samples in cluster $i$. We’ll denote the vector of responses for cluster $i$ with $\mathbf{y}_i$ so that \(\mathbf{y}_{i,j}\) denotes the $j$-th component of said vector. Marginally, the response vector $\mathbf{y}_i$ has mean \(\alpha_i \mathbb{1}_n\) and variance-covariance matrix:</p> <ul id="covar" class="tab" data-tab="4fb9c608-8f13-4d73-8f4c-08121c9cb3e0" data-name="covar"> <li class="active" id="covar-equation"> <a href="#">equation </a> </li> <li id="covar-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="4fb9c608-8f13-4d73-8f4c-08121c9cb3e0" data-name="covar"> <li class="active"> \[\Sigma_{y_i} = \sigma^2 \mathbb{I}_{n \times n} + \tau^2 \mathbf{z}_i \mathbf{z}_i^\top\] </li> <li> <p>For a single cluster:</p> \[\begin{aligned} \mathbb{E}\left[ (\mathbf{y}_{i,j} - \alpha_i)^2\right] &amp;= \mathbb{E}\left[ (\beta_i \mathbf{z}_{i,j} + \epsilon_{i,j})^2 \right] \\ &amp;= \mathbb{E}\left[\beta_i^2 \mathbf{z}_{i,j}^2 \right] + 2 \mathbb{E}\left[ \beta_i \mathbf{z}_{i,j} \epsilon_{i,j} \right] + \mathbb{E}\left[ \epsilon_{i,j}^2 \right] \\ &amp;= \tau^2 \mathbf{z}_{i,j}^2 + \sigma^2 \\ \mathbb{E}\left[ (\mathbf{y}_{i,j} - \alpha_i)(\mathbf{y}_{i,j'} - \alpha_i) \right] &amp;= \mathbb{E}\left[ (\beta_i \mathbf{z}_{i,j} + \epsilon_{i,j})(\beta_i \mathbf{z}_{i,j'} + \epsilon_{i,j'})\right] \\ &amp;= \mathbb{E}\left[ \beta_i^2 \mathbf{z}_{i,j} \mathbf{z}_{i,j'} \right] + \mathbb{E}\left[ \beta_i \mathbf{z}_{i,j} \epsilon_{i,j'}\right] + \mathbb{E}\left[ \beta_i \mathbf{z}_{i,j'} \epsilon_{i,j}\right] + \mathbb{E}\left[ \epsilon_{i,j} \epsilon_{i,j'}\right] \\ &amp;= \tau^2 \mathbf{z}_{i,j} \mathbf{z}_{i,j'} \nonumber \end{aligned}\] <p>Thus, the variance-covariance matrix for $\mathbf{y}_i$:</p> \[\Sigma_{y_i} = \begin{bmatrix} \sigma^2 + \tau^2 \mathbf{z}_{i,1}^2 &amp; \dots &amp; \tau^2 \mathbf{z}_{i,1} \mathbf{z}_{i,n} \\ \vdots &amp; \ddots &amp; \vdots \\ \tau^2 \mathbf{z}_{i,n} \mathbf{z}_{i, 1} &amp; \dots &amp; \sigma^2 + \tau^2 \mathbf{z}_{i, n}^2 \end{bmatrix} = \sigma^2 \mathbb{I}_{n \times n} + \tau^2 \mathbf{z}_i \mathbf{z}_i^\top \nonumber\] </li> </ul> <p>Since the $\beta_i$ are independent, observations from different clusters have covariance zero. Let $\mathbf{y} = (\mathbf{y}_1, \dots, \mathbf{y}_k)$ denote the full data, $\alpha = \begin{bmatrix} \alpha_1 &amp; \dots &amp; \alpha_k\end{bmatrix}^\top$, $\beta = \begin{bmatrix} \beta_1 &amp; \dots &amp; \beta_k\end{bmatrix}^\top$, and $\theta = (\alpha, \beta)$. The complete, marginal likelihood and log-likelihood are:</p> \[\begin{aligned} \mathcal{L}(\theta; \mathbf{y}) &amp;= \prod_{i = 1}^k (2 \pi)^{-\frac{n}{2}} \rvert \Sigma_{y_i} \rvert^{-\frac{1}{2}} \exp\left(- \frac{1}{2} (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \Sigma_{y_i}^{-1} (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right) \\ \ell(\theta; \mathbf{y}) &amp;= \sum_{i = 1}^k \left[ -\frac{n}{2} \log(2 \pi) - \frac{1}{2}\log(\rvert \Sigma_{y_i} \rvert) - \frac{1}{2} (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \Sigma_{y_i}^{-1} (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \end{aligned}\] <h3 id="score">Score</h3> <p>We first find the gradient of the log-likelihood with respect to $\theta$ parameter-wise. Using the Sherman-Morrison formula, we can find $\Sigma_{y_i}^{-1}$ to be:</p> <ul id="sigma-inv" class="tab" data-tab="79ce0d6f-e38e-4d30-8849-d5b9b2fdc566" data-name="sigma-inv"> <li class="active" id="sigma-inv-equation"> <a href="#">equation </a> </li> <li id="sigma-inv-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="79ce0d6f-e38e-4d30-8849-d5b9b2fdc566" data-name="sigma-inv"> <li class="active"> \[\Sigma_{y_i}^{-1} = \frac{1}{\sigma^2} \mathbb{I}_{n \times n} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top\] </li> <li> \[\begin{aligned} \Sigma_{y_i}^{-1} &amp;= \left[ \sigma^2 \mathbb{I}_{n \times n} + \mathbf{z}_i [\tau^2 \mathbb{I}_{n \times n}] \mathbf{z}_i^\top \right]^{-1} \\ &amp;= \frac{1}{\sigma^2} \mathbb{I}_{n \times n} - \left(1 + \tau^2 \mathbf{z}_i^\top \left[ \frac{1}{\sigma^2} \mathbb{I}_{n \times n} \right]\mathbf{z}_i \right)^{-1} \left( \left(\frac{1}{\sigma^2} \mathbb{I}_{n \times n}\right) \left(\tau^2 \mathbf{z}_{i} \mathbf{z}_i^\top \right) \left(\frac{1}{\sigma^2} \mathbb{I}_{n \times n}\right) \right) \\ &amp;= \frac{1}{\sigma^2} \mathbb{I}_{n \times n} - \left(1 + \frac{\tau^2}{\sigma^2} \mathbf{z}_i^\top \mathbf{z}_i \right)^{-1}\left(\frac{\tau^2}{(\sigma^2)^2} \mathbf{z}_i \mathbf{z}_i^\top \right) \\ &amp;= \frac{1}{\sigma^2} \mathbb{I}_{n \times n} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \end{aligned} \nonumber\] </li> </ul> <h4 id="derivatives">Derivatives</h4> <p>Let’s find the derivative with respect to $\sigma^2$:</p> <ul id="ell-deriv-1" class="tab" data-tab="29a8f01f-e670-49ec-a63c-decf35da2dac" data-name="ell-deriv-1"> <li class="active" id="ell-deriv-1-equation"> <a href="#">equation </a> </li> <li id="ell-deriv-1-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="29a8f01f-e670-49ec-a63c-decf35da2dac" data-name="ell-deriv-1"> <li class="active"> \[\frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} = -\frac{1}{2}\sum_{i = 1}^k \left[ \frac{n}{\sigma^2} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \text{tr}\left[\mathbf{z}_i \mathbf{z}_i^\top\right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2)^2} \mathbb{I}_{n \times n} + \frac{2\tau^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top - \frac{(\tau^2)^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top\right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right]\] </li> <li> \[\begin{aligned} \frac{\partial}{\partial \sigma^2} \left[ \log(\rvert \Sigma_{y_i} \rvert) \right] &amp;= \text{tr}\left[ \Sigma_{y_i}^{-1} \frac{\partial}{\partial \sigma^2} \left[\Sigma_{y_i}\right] \right] \\ &amp;= \text{tr}\left[ \Sigma_{y_i}^{-1} \mathbb{I}_{n \times n} \right] \\ &amp;= \text{tr}\left[ \Sigma^{-1} \right] \\ &amp;= \text{tr}\left[\frac{1}{\sigma^2} \mathbb{I}_{n \times n} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \right] \\ &amp;=\text{tr}\left[ \frac{1}{\sigma^2}\mathbb{I}_{n \times n} \right] \text{tr}\left[- \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top\right] \\ &amp;=\frac{n}{\sigma^2} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \text{tr}\left[\mathbf{z}_i \mathbf{z}_i^\top\right] \end{aligned} \nonumber\] \[\begin{aligned} \frac{\partial}{\partial \sigma^2} \left[ \Sigma_{y_i}^{-1} \right] &amp;= - \Sigma_{y_i}^{-1} \frac{\partial}{\partial \sigma^2} \left[ \Sigma_{y_i}\right] \Sigma_{y_i}^{-1} \\ &amp;= -\left[ \frac{1}{\sigma^2} \mathbb{I}_{n \times n} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \right] \mathbb{I}_{n \times n} \left[\frac{1}{\sigma^2} \mathbb{I}_{n \times n} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \right] \\ &amp;= - \left[ \frac{1}{(\sigma^2)^2} \mathbb{I}_{n \times n} - \frac{2\tau^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top + \frac{(\tau^2)^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \\ &amp;= - \frac{1}{(\sigma^2)^2} \mathbb{I}_{n \times n} + \frac{2\tau^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top - \frac{(\tau^2)^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \end{aligned}\] <p>The above imply:</p> \[\begin{aligned} \frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} &amp;= \frac{\partial}{\partial \sigma^2} \left[ \sum_{i = 1}^k - \frac{1}{2} \log(\rvert \Sigma_{y_i} \rvert) - \frac{1}{2}(\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \Sigma_{y_i}^{-1} (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \\ &amp;= -\frac{1}{2}\sum_{i = 1}^k \left[ \frac{n}{\sigma^2} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \text{tr}\left[\mathbf{z}_i \mathbf{z}_i^\top\right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2)^2} \mathbb{I}_{n \times n} + \frac{2\tau^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top - \frac{(\tau^2)^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top\right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \end{aligned}\] </li> </ul> <p>We do the same with $\tau^2$:</p> <ul id="ell-tau-1" class="tab" data-tab="30acb617-898e-47c9-a2cc-bcac7535e3d1" data-name="ell-tau-1"> <li class="active" id="ell-tau-1-equation"> <a href="#">equation </a> </li> <li id="ell-tau-1-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="30acb617-898e-47c9-a2cc-bcac7535e3d1" data-name="ell-tau-1"> <li class="active"> \[\frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} = - \frac{1}{2} \sum_{i = 1}^k \left[ \frac{1}{\sigma^2} \text{tr}[\mathbf{z}_i \mathbf{z}_i^\top] - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top\right] + (\mathbf{y}_i - \alpha_i\mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2)^2} \mathbf{z}_i \mathbf{z}_i^\top + \frac{2\tau^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{(\tau^2)^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top\mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}^\top \mathbf{z}_i \mathbf{z}_i^\top \right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right]\] </li> <li> \[\begin{aligned} \frac{\partial}{\partial \tau^2} \left[ \log(\rvert \Sigma_{y_i} \rvert) \right] &amp;= \text{tr}\left[ \Sigma_{y_i}^{-1} \frac{\partial}{\partial \tau^2} \left[\Sigma_{y_i}\right] \right] \\ &amp;= \text{tr}\left[ \Sigma_{y_i}^{-1} \mathbf{z}_i \mathbf{z}_i^\top \right] \\ &amp;= \text{tr} \left[ \left( \frac{1}{\sigma^2} \mathbb{I}_{n \times n} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top\right) \mathbf{z}_i \mathbf{z}_i^\top\right] \\ &amp;= \frac{1}{\sigma^2} \text{tr}[\mathbf{z}_i \mathbf{z}_i^\top] - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top\right] \end{aligned} \nonumber\] \[\begin{aligned} \frac{\partial}{\partial \tau^2} \left[ \Sigma_{y_i}^{-1} \right] &amp;= - \Sigma_{y_i}^{-1} \frac{\partial}{\partial \tau^2} \left[ \Sigma_{y_i}\right] \Sigma_{y_i}^{-1} \\ &amp;= -\left[ \frac{1}{\sigma^2} \mathbb{I}_{n \times n} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \right] \left[ \mathbf{z}_i \mathbf{z}_i^\top \right] \left[\frac{1}{\sigma^2} \mathbb{I}_{n \times n} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \right] \\ &amp;= - \frac{1}{(\sigma^2)^2} \mathbf{z}_i \mathbf{z}_i^\top + \frac{2\tau^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{(\tau^2)^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top\mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}^\top \mathbf{z}_i \mathbf{z}_i^\top \end{aligned} \nonumber\] \[\begin{aligned} \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} &amp;= \frac{\partial}{\partial \tau^2} \left[ \sum_{i = 1}^k - \frac{1}{2} \log(\rvert \Sigma_{y_i} \rvert) - \frac{1}{2}(\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \Sigma_{y_i}^{-1} (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \\ &amp;= - \frac{1}{2} \sum_{i = 1}^k \left[ \frac{1}{\sigma^2} \text{tr}[\mathbf{z}_i \mathbf{z}_i^\top] - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top\right] + (\mathbf{y}_i - \alpha_i\mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2)^2} \mathbf{z}_i \mathbf{z}_i^\top + \frac{2\tau^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{(\tau^2)^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top\mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}^\top \mathbf{z}_i \mathbf{z}_i^\top \right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \end{aligned}\] </li> </ul> <p>And then take the gradient with respect to $\alpha$:</p> <ul id="ell-alpha-1" class="tab" data-tab="a85fb320-19b8-4c81-b83d-034394be3ce5" data-name="ell-alpha-1"> <li class="active" id="ell-alpha-1-equation"> <a href="#">equation </a> </li> <li id="ell-alpha-1-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="a85fb320-19b8-4c81-b83d-034394be3ce5" data-name="ell-alpha-1"> <li class="active"> \[\frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha} = \begin{bmatrix} (\mathbf{y}_1 - \alpha_1 \mathbf{1}_n)^\top \Sigma_{y_i}^{-1} \mathbf{1}_n \\ \vdots \\ (\mathbf{y}_k - \alpha_k \mathbf{1}_n)^\top \Sigma_{y_i}^{-1} \mathbf{1}_n \end{bmatrix}\] </li> <li> <p>We do the computations component-wise: \(\begin{aligned} \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} &amp;= \sum_{i = 1}^k - \frac{1}{2} \frac{\partial}{\partial \alpha_j} \left[ (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \Sigma^{-1}_{y_i} (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \\ &amp;= - \frac{1}{2} \left(2 (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \Sigma_{y_i}^{-1}(- \mathbf{1}_n) \right) \\ &amp;= (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \Sigma_{y_i}^{-1} \mathbf{1}_n \end{aligned}\) So then: \(\begin{aligned} \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha} &amp;= \begin{bmatrix} (\mathbf{y}_1 - \alpha_1 \mathbf{1}_n)^\top \Sigma_{y_i}^{-1} \mathbf{1}_n \\ \vdots \\ (\mathbf{y}_k - \alpha_k \mathbf{1}_n)^\top \Sigma_{y_i}^{-1} \mathbf{1}_n \end{bmatrix} \end{aligned}\)</p> </li> </ul> <h4 id="mles">MLEs</h4> <p>We can then find the MLE vector, $\hat{\theta}$, by setting the above equations equal to zero, substituting $\tau^2 = 0$, and solving. The MLE vector, $\hat{\theta}$ is:</p> <ul id="mle-1" class="tab" data-tab="62f9fcff-02e5-45b8-a8ef-0bc22c7c84fa" data-name="mle-1"> <li class="active" id="mle-1-equation"> <a href="#">equation </a> </li> <li id="mle-1-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="62f9fcff-02e5-45b8-a8ef-0bc22c7c84fa" data-name="mle-1"> <li class="active"> \[\hat{\theta} = \begin{bmatrix} \frac{1}{n} \sum_{j = 1}^n \mathbf{y}_{1,j} \\ \vdots \\ \frac{1}{n} \sum_{j = 1}^n \mathbf{y}_{k,j} \\ \frac{1}{nk} \sum_{i = 1}^k (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ 0 \end{bmatrix}\] </li> <li> <p>We set the derivative with respect to $\sigma^2$ equal to zero, substitute $\tau^2 = 0$ (under $H_0$), and solve for $\sigma^2$:</p> \[\begin{aligned} 0 &amp;= \frac{\partial}{\partial \sigma^2} \left[ \ell(\theta; \mathbf{y}) \right] \bigg\rvert_{\theta = \theta_0}\\ 0 &amp;= -\frac{1}{2}\sum_{i = 1}^k \left[ \frac{n}{\sigma^2} - \frac{0}{\sigma^2(\sigma^2 + 0 \cdot\mathbf{z}_i^\top \mathbf{z}_i)} \text{tr}\left[\mathbf{z}_i \mathbf{z}_i^\top\right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2)^2} \mathbb{I}_{n \times n} + \frac{2\cdot 0}{(\sigma^2)^2(\sigma^2 + 0 \cdot \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top + \frac{(0)^2}{(\sigma^2)^2(\sigma^2 - 0 \cdot \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top\right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \\ 0 &amp;= -\frac{1}{2}\sum_{i = 1}^k \left[ \frac{n}{\sigma^2} + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2)^2} \mathbb{I}_{n \times n} \right](\mathbf{y}_i - \alpha_i \mathbf{1}_n)\right] \\ 0 &amp;= - \frac{nk}{2 \sigma^2} - \frac{1}{2} \sum_{i = 1}^k - \frac{1}{(\sigma^2)^2} (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ 0 &amp;= - \frac{nk}{2\sigma^2} + \frac{1}{2 (\sigma^2)^2} \sum_{i = 1}^k (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ \frac{nk}{2 \sigma^2} &amp;= \frac{1}{2(\sigma^2)^2} \sum_{i = 1}^k (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ \sigma^2 n k &amp;= \sum_{i = 1}^k (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ \sigma^2 &amp;= \frac{1}{nk} \sum_{i = 1}^k (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \end{aligned}\] <p>We set the gradient w.r.t $\alpha$ equal to zero, substitute $\tau^2 = 0$ (under $H_0$), and solve for $\alpha$.</p> \[\begin{aligned} \mathbf{0} &amp;= \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha} \bigg\rvert_{\theta = \theta_0} \\ \mathbf{0} &amp;= \begin{bmatrix} (\mathbf{y}_1 - \alpha_1 \mathbf{1}_n)^\top \left[\frac{1}{\sigma^2}\mathbb{I}_{n \times n} \right] \mathbf{1}_n \\ \vdots \\ (\mathbf{y}_k - \alpha_k \mathbf{1}_n)^\top \left[\frac{1}{\sigma^2}\mathbb{I}_{n \times n} \right] \mathbf{1}_n \end{bmatrix} \end{aligned}\] <p>Since each entry of the gradient only has one component of $\alpha$, we can solve then all separately:</p> \[\begin{aligned} 0 &amp;= (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ \frac{1}{\sigma^2} \mathbb{I}_{n \times n} \right] \mathbf{1}_n \\ 0 &amp;= \frac{1}{\sigma^2} (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \mathbf{1}_n \\ 0 &amp;= \frac{1}{\sigma^2} \sum_{j = 1}^n (\mathbf{y}_{i,j} - \alpha_i) \\ 0 &amp;= \frac{1}{\sigma^2} \left( \sum_{j =1 }^n \mathbf{y}_{i,j} - n \alpha_i \right) \\ n \alpha_i &amp;= \sum_{j =1 }^n \mathbf{y}_{i,j} \\ \alpha_i &amp;= \frac{1}{n} \sum_{j = 1}^n \mathbf{y}_{i,j} \end{aligned}\] </li> </ul> <p>It is easiest to write the score after evaluating it at the MLE of the parameter vector under $H_0$, which we denote with $\hat{\theta}$ (uncollapse the proof to see all of the details). The MLE is given by:</p> \[\hat{\theta} = \begin{bmatrix} \frac{1}{n} \sum_{j = 1}^n \mathbf{y}_{1,j} \\ \vdots \\ \frac{1}{n} \sum_{j = 1}^n \mathbf{y}_{k,j} \\ \frac{1}{nk} \sum_{i = 1}^k (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ 0 \end{bmatrix}\] <p>Thus, the score evaluated at $\theta = \hat{\theta}$ is:</p> \[\begin{aligned} U_\theta (\hat{\theta}) &amp;= \begin{bmatrix} \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha} \bigg\rvert_{\theta = \hat{\theta}} \\ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} \bigg\rvert_{\theta = \hat{\theta}} \\ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \bigg\rvert_{\theta = \hat{\theta}} \end{bmatrix} = \begin{bmatrix} \frac{1}{\hat{\sigma}^2} (\mathbf{y}_1 - \hat{\alpha}_1 \mathbf{1}_n)^\top \mathbf{1}_n \\ \vdots \\ \frac{1}{\hat{\sigma}^2} (\mathbf{y}_k - \hat{\alpha}_k \mathbf{1}_n)^\top \mathbf{1}_n \\ - \frac{1}{2} \sum_{i = 1}^k \left[ \frac{n}{\hat{\sigma}^2} - \frac{1}{(\hat{\sigma}^2)^2} (\mathbf{y}_i - \hat{\alpha}_i \mathbf{1}_n)^\top (\mathbf{y}_i - \hat{\alpha}_i \mathbf{1}_n) \right] \\ -\frac{1}{2} \sum_{i = 1}^k \left[ \frac{\text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right]}{\hat{\sigma}^2} + \frac{1}{(\hat{\sigma}^2)^2}(\mathbf{y}_i - \hat{\alpha}_i \mathbf{1}_n)^\top \mathbf{z}_i \mathbf{z}_i^\top (\mathbf{y}_i - \hat{\alpha}_i \mathbf{1}_n) \right] \end{bmatrix} \end{aligned}\] <h3 id="information">Information</h3> <p>To find the information, we need to compute the second-order derivatives of the log-likelihood, take the expectation under $H_0$ of minus those quantities, and evaluate them by plugging in $\hat{\theta}$.</p> <h4 id="derivatives-1">Derivatives</h4> <p>We start by taking the derivative with respect to $\theta$ (component-wise) of the first derivative with respect to $\sigma^2$:</p> <ul id="deriv-theta-sigma" class="tab" data-tab="137b1ceb-467e-4aa8-a44c-fe6c50cf9129" data-name="deriv-theta-sigma"> <li class="active" id="deriv-theta-sigma-equation"> <a href="#">equation </a> </li> <li id="deriv-theta-sigma-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="137b1ceb-467e-4aa8-a44c-fe6c50cf9129" data-name="deriv-theta-sigma"> <li class="active"> \[\begin{aligned} \frac{\partial}{\partial \sigma^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} \right] &amp;= - \frac{1}{2}\sum_{i = 1}^k -\frac{n}{(\sigma^2)^2} + \frac{\tau^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top\right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ \frac{2}{(\sigma^2)^3} \mathbb{I}_{n \times n} - \frac{2\tau^2(3\sigma^2 + 2\tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2}\mathbf{z}_i \mathbf{z}_i^\top + \frac{2(\tau^2)^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3 (\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z})^3}\mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right](\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ \frac{\partial}{\partial \sigma^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] &amp;= -\frac{1}{2}\sum_{i = 1}^k -\frac{1}{(\sigma^2)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] - \frac{-\tau^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ \frac{2}{(\sigma^2)^3} \mathbf{z}_i \mathbf{z}_i^\top + \frac{-2\tau^2(2\sigma^2 + 3\tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{-2(\tau^2)^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3 (\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^3} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ \frac{\partial}{\partial \sigma^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] &amp;= (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left[ -\frac{1}{(\sigma^2)^2} \mathbb{I}_{n \times n} + \frac{\tau^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^2 (\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \right] \mathbf{1}_n \end{aligned}\] </li> <li> \[\begin{aligned} \frac{\partial}{\partial \sigma^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} \right] &amp;= -\frac{1}{2}\sum_{i = 1}^k \frac{\partial}{\partial \sigma^2}\left[ \frac{n}{\sigma^2} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \text{tr}\left[\mathbf{z}_i \mathbf{z}_i^\top\right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2)^2} \mathbb{I}_{n \times n} + \frac{2\tau^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top - \frac{(\tau^2)^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top\right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \\ &amp;= - \frac{1}{2}\sum_{i = 1}^k -\frac{n}{(\sigma^2)^2} - \frac{-\tau^2(2\sigma^2+\tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top\right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ \frac{2}{(\sigma^2)^3} \mathbb{I}_{n \times n} + \frac{-2\tau^2(3\sigma^2 + 2\tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2}\mathbf{z}_i \mathbf{z}_i^\top - \frac{-2(\tau^2)^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3 (\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z})^3}\mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right](\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ &amp;= - \frac{1}{2}\sum_{i = 1}^k -\frac{n}{(\sigma^2)^2} + \frac{\tau^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top\right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ \frac{2}{(\sigma^2)^3} \mathbb{I}_{n \times n} - \frac{2\tau^2(3\sigma^2 + 2\tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2}\mathbf{z}_i \mathbf{z}_i^\top + \frac{2(\tau^2)^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3 (\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z})^3}\mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right](\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ \frac{\partial}{\partial \sigma^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] &amp;= - \frac{1}{2} \sum_{i = 1}^k \frac{\partial}{\partial \sigma^2} \left[ \frac{1}{\sigma^2} \text{tr}[\mathbf{z}_i \mathbf{z}_i^\top] - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top\right] + (\mathbf{y}_i - \alpha_i\mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2)^2} \mathbf{z}_i \mathbf{z}_i^\top + \frac{2\tau^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{(\tau^2)^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top\mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}^\top \mathbf{z}_i \mathbf{z}_i^\top \right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \\ &amp;= -\frac{1}{2}\sum_{i = 1}^k -\frac{1}{(\sigma^2)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] - \frac{-\tau^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ \frac{2}{(\sigma^2)^3} \mathbf{z}_i \mathbf{z}_i^\top + \frac{-2\tau^2(2\sigma^2 + 3\tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{-2(\tau^2)^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3 (\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^3} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ \frac{\partial}{\partial \sigma^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] &amp;= \frac{\partial}{\partial \sigma^2} \left[ (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left[ \frac{1}{\sigma^2} \mathbb{I}_{n \times n} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \right] \mathbf{1}_n \right] \\ &amp;= (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left[ -\frac{1}{(\sigma^2)^2} \mathbb{I}_{n \times n} + \frac{\tau^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^2 (\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \right] \mathbf{1}_n \end{aligned}\] </li> </ul> <p>Next, we do the same with the derivative with respect to $\tau^2$:</p> <ul id="deriv-theta-tau" class="tab" data-tab="9d60f587-fb4b-4dcd-b739-f92760b6c9d6" data-name="deriv-theta-tau"> <li class="active" id="deriv-theta-tau-equation"> <a href="#">equation </a> </li> <li id="deriv-theta-tau-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="9d60f587-fb4b-4dcd-b739-f92760b6c9d6" data-name="deriv-theta-tau"> <li class="active"> \[\begin{aligned} \frac{\partial}{\partial \tau^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} \right] &amp;= -\frac{1}{2}\sum_{i = 1}^k - \frac{1}{(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[\frac{2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top - \frac{2\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^3} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ \frac{\partial}{\partial \tau^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] &amp;= -\frac{1}{2}\sum_{i = 1}^k - \frac{1}{(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ \frac{2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2}\mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{2\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^3} \mathbf{z}_i \mathbf{z}_i^\top\mathbf{z}_i \mathbf{z}_i^\top\mathbf{z}_i \mathbf{z}_i^\top\right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ \frac{\partial}{\partial \tau^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] &amp;= (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \right] \mathbf{1}_n \\ \end{aligned}\] </li> <li> \[\begin{aligned} \frac{\partial}{\partial \tau^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} \right] &amp;= -\frac{1}{2}\sum_{i = 1}^k \frac{\partial}{\partial \tau^2}\left[ \frac{n}{\sigma^2} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \text{tr}\left[\mathbf{z}_i \mathbf{z}_i^\top\right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2)^2} \mathbb{I}_{n \times n} + \frac{2\tau^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top - \frac{(\tau^2)^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top\right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \\ &amp;= -\frac{1}{2}\sum_{i = 1}^k 0 - \frac{1}{(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[0 + \frac{2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top - \frac{2\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^3} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ &amp;= -\frac{1}{2}\sum_{i = 1}^k - \frac{1}{(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[\frac{2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top - \frac{2\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^3} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ \frac{\partial}{\partial \tau^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] &amp;= - \frac{1}{2} \sum_{i = 1}^k \frac{\partial}{\partial \tau^2} \left[ \frac{1}{\sigma^2} \text{tr}[\mathbf{z}_i \mathbf{z}_i^\top] - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top\right] + (\mathbf{y}_i - \alpha_i\mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2)^2} \mathbf{z}_i \mathbf{z}_i^\top + \frac{2\tau^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{(\tau^2)^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top\mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}^\top \mathbf{z}_i \mathbf{z}_i^\top \right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \\ &amp;= -\frac{1}{2}\sum_{i = 1}^k 0 - \frac{1}{(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ 0 + \frac{2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2}\mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{2\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^3} \mathbf{z}_i \mathbf{z}_i^\top\mathbf{z}_i \mathbf{z}_i^\top\mathbf{z}_i \mathbf{z}_i^\top\right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ &amp;= -\frac{1}{2}\sum_{i = 1}^k - \frac{1}{(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ \frac{2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2}\mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{2\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^3} \mathbf{z}_i \mathbf{z}_i^\top\mathbf{z}_i \mathbf{z}_i^\top\mathbf{z}_i \mathbf{z}_i^\top\right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ \frac{\partial}{\partial \tau^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] &amp;= \frac{\partial}{\partial \tau^2} \left[ (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left[ \frac{1}{\sigma^2} \mathbb{I}_{n \times n} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \right] \mathbf{1}_n \right] \\ &amp;= (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left[ 0 - \frac{1}{(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \right] \mathbf{1}_n \\ &amp;= (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \right] \mathbf{1}_n \end{aligned}\] </li> </ul> <p>And finally with $\alpha_j$:</p> <ul id="deriv-theta-alpha" class="tab" data-tab="4cdf9af7-0420-4885-a379-0e21bbad0e9f" data-name="deriv-theta-alpha"> <li class="active" id="deriv-theta-alpha-equation"> <a href="#">equation </a> </li> </ul> <ul class="tab-content" id="4cdf9af7-0420-4885-a379-0e21bbad0e9f" data-name="deriv-theta-alpha"> <li class="active"> \[\begin{aligned} \frac{\partial}{\partial \alpha_j} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} \right] &amp;= (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2)^2} \mathbb{I}_{n \times n} + \frac{2\tau^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_j^\top \mathbf{z}_j)} \mathbf{z}_j \mathbf{z}_j^\top - \frac{(\tau^2)^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_j^\top \mathbf{z}_j)^2} \mathbf{z}_j \mathbf{z}_j^\top \mathbf{z}_j \mathbf{z}_j^\top \right]\mathbf{1}_n \\ \frac{\partial}{\partial \alpha} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] &amp;= (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2)^2} \mathbf{z}_i \mathbf{z}_i^\top + \frac{2\tau^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{(\tau^2)^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top\mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \mathbf{1}_n \\ \frac{\partial}{\partial \alpha_j} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] &amp;= - \mathbf{1}_n^\top \left[ \frac{1}{\sigma^2} \mathbb{I}_{n \times n} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \right] \mathbf{1}_n \\ \frac{\partial}{\partial \alpha_{j'}} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] &amp;= 0 \end{aligned}\] </li> </ul> <h4 id="expectations">Expectations</h4> <p>We take the expectation under the null of all of the terms we found in the previous section. First we do the $\sigma^2$ terms:</p> <ul id="expectation-sigma-1" class="tab" data-tab="cb9ac289-691b-4137-b2b5-dd665e6103cc" data-name="expectation-sigma-1"> <li class="active" id="expectation-sigma-1-equation"> <a href="#">equation </a> </li> <li id="expectation-sigma-1-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="cb9ac289-691b-4137-b2b5-dd665e6103cc" data-name="expectation-sigma-1"> <li class="active"> \[\begin{aligned} \mathbb{E}\left[ \frac{\partial}{\partial \sigma^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} \right] \right] &amp;= - \frac{1}{2}\sum_{i = 1}^k -\frac{n}{(\sigma^2)^2} + \frac{\tau^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top\right] + \text{tr}\left[ \left[ \frac{2}{(\sigma^2)^3} \mathbb{I}_{n \times n} - \frac{2\tau^2(3\sigma^2 + 2\tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2}\mathbf{z}_i \mathbf{z}_i^\top + \frac{2(\tau^2)^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3 (\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z})^3}\mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \Sigma_{y_i} \right] \\ \mathbb{E}\left[ \frac{\partial}{\partial \sigma^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] \right] &amp;= -\frac{1}{2}\sum_{i = 1}^k -\frac{1}{(\sigma^2)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] - \frac{-\tau^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] + \text{tr}\left[ \left[ \frac{2}{(\sigma^2)^3} \mathbf{z}_i \mathbf{z}_i^\top + \frac{-2\tau^2(2\sigma^2 + 3\tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{-2(\tau^2)^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3 (\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^3} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \Sigma_{y_i} \right] \\ \mathbb{E}\left[ \frac{\partial}{\partial \sigma^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] \right] &amp;= 0 \end{aligned}\] </li> <li> \[\begin{aligned} \mathbb{E}\left[ \frac{\partial}{\partial \sigma^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} \right] \right] &amp;= \mathbb{E}\left[ - \frac{1}{2}\sum_{i = 1}^k -\frac{n}{(\sigma^2)^2} + \frac{\tau^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top\right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ \frac{2}{(\sigma^2)^3} \mathbb{I}_{n \times n} - \frac{2\tau^2(3\sigma^2 + 2\tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2}\mathbf{z}_i \mathbf{z}_i^\top + \frac{2(\tau^2)^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3 (\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z})^3}\mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right](\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \\ &amp;= - \frac{1}{2}\sum_{i = 1}^k -\frac{n}{(\sigma^2)^2} + \frac{\tau^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top\right] + \mathbb{E}\left[ (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ \frac{2}{(\sigma^2)^3} \mathbb{I}_{n \times n} - \frac{2\tau^2(3\sigma^2 + 2\tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2}\mathbf{z}_i \mathbf{z}_i^\top + \frac{2(\tau^2)^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3 (\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z})^3}\mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right](\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \\ &amp;= - \frac{1}{2}\sum_{i = 1}^k -\frac{n}{(\sigma^2)^2} + \frac{\tau^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top\right] + \text{tr}\left[ \left[ \frac{2}{(\sigma^2)^3} \mathbb{I}_{n \times n} - \frac{2\tau^2(3\sigma^2 + 2\tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2}\mathbf{z}_i \mathbf{z}_i^\top + \frac{2(\tau^2)^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3 (\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z})^3}\mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \mathbb{E}\left[ (\mathbf{y}_i - \alpha_i \mathbf{1}_n) (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \right] \right] \\ &amp;= - \frac{1}{2}\sum_{i = 1}^k -\frac{n}{(\sigma^2)^2} + \frac{\tau^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top\right] + \text{tr}\left[ \left[ \frac{2}{(\sigma^2)^3} \mathbb{I}_{n \times n} - \frac{2\tau^2(3\sigma^2 + 2\tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2}\mathbf{z}_i \mathbf{z}_i^\top + \frac{2(\tau^2)^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3 (\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z})^3}\mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \Sigma_{y_i} \right] \\ \mathbb{E}\left[ \frac{\partial}{\partial \sigma^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] \right] &amp;= -\frac{1}{2}\sum_{i = 1}^k -\frac{1}{(\sigma^2)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] - \frac{-\tau^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] + \text{tr}\left[ \left[ \frac{2}{(\sigma^2)^3} \mathbf{z}_i \mathbf{z}_i^\top + \frac{-2\tau^2(2\sigma^2 + 3\tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{-2(\tau^2)^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3 (\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^3} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \Sigma_{y_i} \right] \\ \mathbb{E}\left[ \frac{\partial}{\partial \sigma^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] \right] &amp;= 0 \end{aligned}\] </li> </ul> <p>And the $\tau^2$ terms:</p> \[\begin{aligned} \mathbb{E}\left[ \frac{\partial}{\partial \tau^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} \right] \right] &amp;= -\frac{1}{2}\sum_{i = 1}^k - \frac{1}{(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] + \text{tr} \left[ \left[ \frac{2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top - \frac{2\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^3} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \Sigma_{y_i} \right] \\ \mathbb{E}\left[ \frac{\partial}{\partial \tau^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] \right] &amp;= -\frac{1}{2}\sum_{i = 1}^k - \frac{1}{(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] +\text{tr} \left[\left[ \frac{2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2}\mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{2\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^3} \mathbf{z}_i \mathbf{z}_i^\top\mathbf{z}_i \mathbf{z}_i^\top\mathbf{z}_i \mathbf{z}_i^\top\right] \Sigma_{y_i} \right] \\ \mathbb{E}\left[ \frac{\partial}{\partial \tau^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] \right] &amp;= 0 \end{aligned}\] <p>And finally the $\alpha_j$ ones:</p> <ul id="expectation-alpha-1" class="tab" data-tab="a564a0ec-0bd0-4f45-abcf-8add8e758de7" data-name="expectation-alpha-1"> <li class="active" id="expectation-alpha-1-equation"> <a href="#">equation </a> </li> <li id="expectation-alpha-1-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="a564a0ec-0bd0-4f45-abcf-8add8e758de7" data-name="expectation-alpha-1"> <li class="active"> \[\begin{aligned} \mathbb{E}\left[ \frac{\partial}{\partial \alpha_j} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} \right] \right] &amp;= 0 \\ \mathbb{E}\left[ \frac{\partial}{\partial \alpha} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] \right] &amp;= 0 \\ \mathbb{E}\left[ \frac{\partial}{\partial \alpha_j} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] \right] &amp;= - \mathbf{1}_n^\top \left[ \frac{1}{\sigma^2} \mathbb{I}_{n \times n} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \right] \mathbf{1}_n \\ \mathbb{E}\left[\frac{\partial}{\partial \alpha_{j'}} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] \right] &amp;= 0 \end{aligned}\] </li> <li> \[\begin{aligned} \mathbb{E}\left[ \frac{\partial}{\partial \alpha_j} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} \right] \right] &amp;= \mathbb{E}\left[(\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2)^2} \mathbb{I}_{n \times n} + \frac{2\tau^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_j^\top \mathbf{z}_j)} \mathbf{z}_j \mathbf{z}_j^\top - \frac{(\tau^2)^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_j^\top \mathbf{z}_j)^2} \mathbf{z}_j \mathbf{z}_j^\top \mathbf{z}_j \mathbf{z}_j^\top \right]\mathbf{1}_n \right] \\ &amp;= 0 \\ \mathbb{E}\left[ \frac{\partial}{\partial \alpha} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] \right] &amp;= \mathbb{E}\left[(\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2)^2} \mathbf{z}_i \mathbf{z}_i^\top + \frac{2\tau^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{(\tau^2)^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top\mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \mathbf{1}_n \right] \\ &amp;= 0 \\ \mathbb{E}\left[ \frac{\partial}{\partial \alpha_j} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] \right] &amp;= - \mathbf{1}_n^\top \left[ \frac{1}{\sigma^2} \mathbb{I}_{n \times n} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \right] \mathbf{1}_n \\ \mathbb{E}\left[\frac{\partial}{\partial \alpha_{j'}} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] \right] &amp;= 0 \end{aligned}\] </li> </ul> <p>We then evaluate the Fisher information at the MLEs:</p> <ul id="fisher-1" class="tab" data-tab="3b049325-4fa0-4b63-9e2d-06725e300112" data-name="fisher-1"> <li class="active" id="fisher-1-equation"> <a href="#">equation </a> </li> <li id="fisher-1-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="3b049325-4fa0-4b63-9e2d-06725e300112" data-name="fisher-1"> <li class="active"> \[\begin{aligned} \mathcal{I}_{\theta, \theta} (\hat{\theta}) &amp;= -\mathbb{E}\left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \theta \partial \theta^\top}\right]\bigg\rvert_{\theta = \hat{\theta}} \\ &amp;= \begin{bmatrix} \frac{n}{\hat{\sigma}^2} &amp; \dots &amp; 0 &amp; 0 &amp; 0\\ \vdots &amp; \ddots &amp; \vdots &amp; \vdots &amp; \vdots \\ 0 &amp; \dots &amp; \frac{n}{\hat{\sigma}^2} &amp; 0 &amp; 0 \\ 0 &amp; \dots &amp; 0 &amp; \frac{nk}{2\hat{\sigma}^2} &amp; \frac{1}{2(\hat{\sigma}^2)^2} \sum_{i = 1}^k \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right]\\ 0 &amp; \dots &amp; 0 &amp; \frac{1}{2(\hat{\sigma}^2)^2} \sum_{i = 1}^k \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] &amp; \frac{1}{2(\hat{\sigma}^2)^2} \sum_{i = 1}^k \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \end{bmatrix} \end{aligned}\] </li> <li> <p>Note that:</p> \[\begin{aligned} \Sigma_{y_i} \bigg\rvert_{\theta = \hat{\theta}} &amp;= \hat{\sigma}^2 \mathbb{I}_{n \times n} \end{aligned}\] <p>Thus:</p> \[\begin{aligned} -\mathbb{E}\left[ \frac{\partial}{\partial \sigma^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} \right] \right] \bigg\rvert_{\theta = \hat{\theta}} &amp;= \frac{1}{2}\sum_{i = 1}^k -\frac{n}{(\hat{\sigma}^2)^2} + \frac{0 \cdot (2\hat{\sigma}^2 + 0)}{(\hat{\sigma}^2)^2(\hat{\sigma}^2 + 0)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top\right] + \text{tr}\left[ \left[ \frac{2}{(\hat{\sigma}^2)^3} \mathbb{I}_{n \times n} - 0 \cdot \mathbf{z}_i \mathbf{z}_i^\top + 0 \cdot \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \Sigma_{y_i}\rvert_{\theta = \hat{\theta}} \right] \\ &amp;= \frac{1}{2}\sum_{i = 1}^k - \frac{n}{(\hat{\sigma}^2)^2} + \text{tr}\left[ \left[ \frac{2}{(\hat{\sigma}^2)^3} \mathbb{I}_{n \times n} \right] \left[ \hat{\sigma}^2 \mathbb{I}_{n \times n} \right] \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[ -\frac{n}{(\hat{\sigma}^2)^2} + \frac{2}{\hat{\sigma}^2} \text{tr}\left[ \mathbb{I}_{n \times n} \right]\right] \\ &amp;= \frac{1}{2}\sum_{i = 1}^k \left[ -\frac{n}{(\hat{\sigma}^2)^2} + \frac{2n}{\hat{\sigma}^2} \right] \\ &amp;= \sum_{i = 1}^k \frac{n}{2 \hat{\sigma}^2} \\ &amp;= \frac{nk}{2\hat{\sigma}^2} \\ -\mathbb{E}\left[ \frac{\partial}{\partial \sigma^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] \right] \bigg\rvert_{\theta = \hat{\theta}} &amp;=\frac{1}{2}\sum_{i = 1}^k -\frac{1}{(\hat{\sigma}^2)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] - \frac{-0 \cdot (2\hat{\sigma}^2 + 0 \cdot \mathbf{z}_i^\top \mathbf{z}_i)}{(\hat{\sigma}^2)^2(\hat{\sigma}^2 + 0 \cdot \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] + \text{tr}\left[ \left[ \frac{2}{(\hat{\sigma}^2)^3} \mathbf{z}_i \mathbf{z}_i^\top + \frac{-0 \cdot (2\hat{\sigma}^2 + 0 \cdot \mathbf{z}_i^\top \mathbf{z}_i)}{(\hat{\sigma}^2)^3(\hat{\sigma}^2 + 0 \cdot \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{-2(0)^2(2\hat{\sigma}^2 + 0 \cdot \mathbf{z}_i^\top \mathbf{z}_i)}{(\hat{\sigma}^2)^3 (\hat{\sigma}^2 + 0 \cdot \mathbf{z}_i^\top \mathbf{z}_i)^3} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \Sigma_{y_i} \rvert_{\theta = \hat{\theta}} \right] \\ &amp;= \frac{1}{2}\sum_{i = 1}^k -\frac{1}{(\hat{\sigma}^2)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] + \text{tr}\left[ \left[ \frac{2}{(\hat{\sigma}^2)^3} \mathbf{z}_i \mathbf{z}_i^\top \right] \left[ \hat{\sigma}^2 \mathbb{I}_{n \times n} \right] \right] \\ &amp;= \frac{1}{2}\sum_{i = 1}^k \left( - \frac{1}{(\hat{\sigma}^2)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top\right] + \frac{2}{(\hat{\sigma}^2)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] \right) \\ &amp;= \sum_{i = 1}^k \frac{1}{2(\hat{\sigma}^2)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] \\ &amp;= \frac{1}{2(\hat{\sigma}^2)^2} \sum_{i = 1}^k \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] \\ -\mathbb{E}\left[ \frac{\partial}{\partial \sigma^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha} \right] \right] \bigg\rvert_{\theta = \hat{\theta}} &amp;= 0 \\ - \mathbb{E}\left[ \frac{\partial}{\partial \tau^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} \right] \right]\bigg\rvert_{\theta = \hat{\theta}} &amp;= \frac{1}{2}\sum_{i = 1}^k - \frac{1}{(\hat{\sigma}^2 + 0 \cdot \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] + \text{tr} \left[ \left[\frac{2}{\hat{\sigma}^2(\hat{\sigma}^2 + 0 \cdot \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top - \frac{2 \cdot 0 }{\hat{\sigma}^2(\hat{\sigma}^2 + 0 \cdot \mathbf{z}_i^\top \mathbf{z}_i)^3} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \Sigma_{y_i}\rvert_{\theta = \hat{\theta}} \right] \\ &amp;= \frac{1}{2}\sum_{i = 1}^k -\frac{1}{(\hat{\sigma}^2)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top\right] + \text{tr}\left[ \left[\frac{2}{(\hat{\sigma}^2)^3} \mathbf{z}_i \mathbf{z}_i^\top \right] \left[ \hat{\sigma}^2 \mathbb{I}_{n \times n} \right]\right] \\ &amp;= \frac{1}{2}\sum_{i =1 }^k \left( -\frac{1}{(\hat{\sigma}^2)^2} + \frac{2}{(\hat{\sigma}^2)^2} \right) \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] \\ &amp;= \frac{1}{2(\hat{\sigma}^2)^2} \sum_{i = 1}^k \text{tr} \left[ \mathbf{z}_i \mathbf{z}_i^\top \right] \\ -\mathbb{E}\left[ \frac{\partial}{\partial \tau^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] \right] &amp;= \frac{1}{2}\sum_{i = 1}^k - \frac{1}{(\hat{\sigma}^2 + 0 \cdot\mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] +\text{tr} \left[\left[ \frac{2}{\hat{\sigma}^2(\hat{\sigma}^2 + 0 \cdot \mathbf{z}_i^\top \mathbf{z}_i)^2}\mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{2\cdot 0}{\hat{\sigma}^2(\hat{\sigma}^2 + 0 \cdot \mathbf{z}_i^\top \mathbf{z}_i)^3} \mathbf{z}_i \mathbf{z}_i^\top\mathbf{z}_i \mathbf{z}_i^\top\mathbf{z}_i \mathbf{z}_i^\top\right] \Sigma_{y_i} \rvert_{\theta = \hat{\theta}} \right] \\ &amp;= \frac{1}{2}\sum_{i = 1}^k - \frac{1}{(\hat{\sigma}^2)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] +\text{tr} \left[\left[ \frac{2}{(\hat{\sigma}^2)^3}\mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \left[\hat{\sigma}^2 \mathbb{I}_{n \times n} \right] \right] \\ &amp;= \frac{1}{2}\sum_{i = 1}^k \left( -\frac{1}{(\hat{\sigma}^2)^2} + \frac{2}{(\hat{\sigma}^2)^2} \right)\text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \\ &amp;= \frac{1}{2(\hat{\sigma}^2)^2} \sum_{i = 1}^k \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \\ -\mathbb{E}\left[ \frac{\partial}{\partial \tau^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha} \right] \right] \bigg\rvert_{\theta = \hat{\theta}} &amp;= 0 \\ -\mathbb{E}\left[ \frac{\partial}{\partial \alpha_j} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} \right] \right] \bigg\rvert_{\theta = \hat{\theta}} &amp;= 0 \\ -\mathbb{E}\left[ \frac{\partial}{\partial \alpha} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] \right] \bigg\rvert_{\theta = \hat{\theta}} &amp;= 0 \\ - \mathbb{E}\left[ \frac{\partial}{\partial \alpha_j} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] \right] &amp;= \mathbf{1}_n^\top \left[ \frac{1}{\hat{\sigma}^2} \mathbb{I}_{n \times n} - \frac{0}{\hat{\sigma}^2(\hat{\sigma}^2 + 0 \cdot \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \right] \mathbf{1}_n \\ &amp;= \frac{n}{\hat{\sigma}^2} \\ - \mathbb{E}\left[\frac{\partial}{\partial \alpha_{j'}} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] \right] \bigg\rvert_{\theta = \hat{\theta}} &amp;= 0 \end{aligned}\] </li> </ul> <hr/> <h2 id="negative-binomial-case">Negative Binomial Case</h2> <p>In this example, we’ll let the responses be negative binomial. To keep things simple, we’ll say we only have a single fixed intercept and a single random effect. We let $\phi &gt; 0$, denote the <i>known</i> dispersion parameter and assume the conditional mean to be given by:</p> \[\mu_{i,j} = \exp\left( \alpha_i + \beta_i \mathbf{z}_{i,j} \right) \label{eq:neg-bin-mean}\] <p>The likelihood based on a single observation, $\mathbf{y}_{i,j}$, is given by:</p> \[\mathcal{L}(\mathbf{y}_{i,j}; \alpha_i, \tau^2 \rvert \beta_i) = \frac{\Gamma\left(\mathbf{y}_{i,j} + \frac{1}{\phi}\right)}{\Gamma(\mathbf{y}_{i,j} + 1) \Gamma\left(\frac{1}{\phi} \right)}\left(\frac{1}{1 + \phi \mathbf{y}_{i,j}}\right)^{\frac{1}{\phi}} \left( \frac{\phi \mu_{i,j}}{1 + \phi \mu_{i,j}} \right)^{\mathbf{y}_{i,j}} \label{eq:neg-bin-single-lik}\] <p>where $\Gamma(\cdot)$ is the gamma function:</p> \[\Gamma(x) = \int_0^\infty t^{x - 1} \exp(-t) dt\] <p>The above parametrization of the likelihood implies that the conditional variance of the responses is given by:</p> \[V(\mu_{i,j}) = \mu_{i,j} + \frac{1}{\phi} \mu_{i,j}^2\] <p>The conditional log-likelihood based on cluster $i$ is:</p> \[\ell(\mathbf{y}_i; \alpha_i, \tau^2 \rvert \beta_i) = \sum_{j = 1}^{n_i} \left[ \log \Gamma \left( \mathbf{y}_{i,j} + \frac{1}{\phi} \right) - \log \Gamma\left(\mathbf{y}_{i,j} + 1\right) - \log\Gamma\left(\frac{1}{\phi} \right) - \frac{1}{\phi} \log\left(1 + \phi \mathbf{y}_{i,j} \right) + \mathbf{y}_{i,j} \left( \log(\phi \mu_{i,j}) - \log(1 + \phi \mu_{i,j}) \right) \right] \label{eq:neg-bin-full-cond-ll}\] <p>We follow a pseudo-likelihood approach (see <a href="/posts/2025/06/04/glmm.html">here</a>). We assume to have the following generalized linear mixed model:</p> \[\mathbf{y}_{i,j} \rvert \beta_i \sim \text{NegBin}(\mu_{i,j}, \phi); \hspace{10mm} \mu_{i,j} = \exp\left(\eta_{i,j}\right) = \exp\left(\alpha_i + \beta_i \mathbf{z}_{i,j}\right) \label{eq:glmm-y}\] <p>We’ll use a superscript $\star$ to denote a quantity evaluated at the parameter estimates made under $H_0$ (i.e. $\tau^2 = \mathbf{0}$). Our <i>working</i> responses and errors are:</p> \[\mathbf{y}^\star_{i,j} = \alpha_i + \beta_i \mathbf{z}_{i,j} + \epsilon^\star_{i,j}; \hspace{10mm} \epsilon^\star_{i,j} \sim \mathcal{N}\left(0, \frac{V(\hat{\mu}_{i,j})}{\delta^2(\hat{\eta}_{i,j})}\right)\] <p>where \(\delta(\hat{\eta}_{i,j}) = \frac{\partial g^{-1}(\eta_{i,j})}{\partial \eta_{i,j}}\bigg\rvert_{\eta_{i,j} = \hat{\eta}_{i,j}}\). We can then just apply all of the results we found in the previous section to this case but make \(\hat{\sigma}^2\) different for each observation, where \(\hat{\sigma}^2_{i,j} = \text{Var}(\epsilon_{i,j})\).</p> <p>To do so, we need an estimate of $\alpha_i$ under $H_0$, which we can do with <a href="/posts/2025/06/03/glm.html#weighted-least-squares">iteratively reweighted least squares</a> or some comparable algorithm. With these estimates, we can compute \(\hat{\sigma}_{i,j}^2\) and \(\mathbf{y}^\star_{i,j}\) and proceed as in the Gaussian case but without $\sigma^2$ in the parameter vector since we assume them to be fixed at \(\hat{\sigma}^2_{i,j}\).</p> <p>Since $g(\cdot) = \log(\cdot)$, we have that \(\delta(\hat{\eta}_{i,j}) = \exp(\hat{\eta}_{i,j})\), implying that the working error variances are:</p> \[\frac{V(\hat{\mu}_{i,j})}{\delta^2(\hat{\eta}_{i,j})} = \frac{\exp(\hat{\eta}_{i,j}) + \frac{1}{\phi}\exp(\hat{\eta}_{i,j})}{\exp^2(\hat{\eta}_{i,j})} = \frac{1}{\exp(\hat{\eta}_{i,j})}\left(1 + \frac{1}{\phi}\right)\] <p>where we recall that we assume $\phi$ is know and $\hat{\eta}_{i,j} = \hat{\alpha}_i$ under $H_0$. Dropping the star superscript, the likelihood and log-likelihood functions we will work with are giving by:</p> \[\begin{aligned} \mathcal{L}(\theta; \mathbf{y}) &amp;= \prod_{i = 1}^k (2 \pi)^{-\frac{n}{2}} \rvert \Sigma_{y_i} \rvert^{-\frac{1}{2}} \exp\left(- \frac{1}{2} (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \Sigma_{y_i}^{-1} (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right) \\ \ell(\theta; \mathbf{y}) &amp;= \sum_{i = 1}^k \left[ -\frac{n}{2} \log(2 \pi) - \frac{1}{2}\log(\rvert \Sigma_{y_i} \rvert) - \frac{1}{2} (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \Sigma_{y_i}^{-1} (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \end{aligned}\] <p>but using the working responses and their respective covariances and whatnot.</p> <h3 id="score-1">Score</h3> <p>The marginal covariance matrix is very similar to the Gaussian outcome model above. The only thing that has changed is that each error has its own variance:</p> \[\Sigma_{y_i} = \text{diag}([\hat{\sigma}^2_{i,1}, \dots, \hat{\sigma}^2_{i, n}]) + \tau^2 \mathbf{z}_i \mathbf{z}_i^\top\] <p>Its inverse, $\Sigma^{-1}_{y_i}$, is:</p> <ul id="sigma-inv-2" class="tab" data-tab="5a83ab2c-f01c-45db-a9a4-59eccac69fc2" data-name="sigma-inv-2"> <li class="active" id="sigma-inv-2-equation"> <a href="#">equation </a> </li> <li id="sigma-inv-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="5a83ab2c-f01c-45db-a9a4-59eccac69fc2" data-name="sigma-inv-2"> <li class="active"> \[\Sigma^{-1}_{y_i} = \text{diag}\left(\left[ \frac{1}{\hat{\sigma}^2_{i, 1}}, \dots, \frac{1}{\hat{\sigma}^2_{i,n}} \right]\right) - \frac{\text{diag}\left( \left[ \frac{\tau^2}{(\hat{\sigma}_{i,1}^2)^2}, \dots, \frac{\tau^2}{(\hat{\sigma}_{i,n}^2)^2} \right] \right) \mathbf{z}_i \mathbf{z}_i^\top}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}}\] </li> <li> <p>First, let’s let \(\mathbf{w}_i = \tau^2 \mathbf{z}_i\), which is the $n$-vector $\mathbf{z}_i$ where each coordinate has been multiplied by $\tau^2$. We’ll also let \(\hat{\sigma}^2 = (\hat{\sigma}^2_{i,1}, \dots, \hat{\sigma}^2_{i, n})^\top\), the vector of the error variances for cluster $i$, and $\frac{1}{\hat{\sigma}^2}$ will be the vector of the reciprocals of the coordinates of $\hat{\sigma}^2$. Using the <a href="https://en.wikipedia.org/wiki/Sherman–Morrison_formula">Sherman-Morrison formula</a>, we have:</p> \[\begin{aligned} \Sigma^{-1}_{y_i} &amp;= \left(\text{diag}(\hat{\sigma}^2) - \mathbf{w}_i \mathbf{v}_i^\top\right)^{-1} \\ &amp;= \text{diag}^{-1}(\hat{\sigma}^2) - \frac{\text{diag}^{-1}(\hat{\sigma}^2) \mathbf{w}_i \mathbf{z}_i^\top \text{diag}^{-1}(\hat{\sigma}^2)}{1 + \mathbf{z}_i^\top \text{diag}^{-1}(\hat{\sigma}^2) \mathbf{w}_i} \\ &amp;= \text{diag}\left(\frac{1}{\hat{\sigma}^2}\right) - \frac{\text{diag}\left(\frac{\tau^2}{(\hat{\sigma}^2)^2}\right) \mathbf{z}_i \mathbf{z}_i^\top}{1 + \mathbf{z}_i^\top \text{diag}\left(\frac{\tau^2}{\hat{\sigma}^2}\right) \mathbf{z}_i} \\ &amp;= \text{diag}\left(\left[ \frac{1}{\hat{\sigma}^2_{i, 1}}, \dots, \frac{1}{\hat{\sigma}^2_{i,n}} \right]\right) - \frac{\text{diag}\left( \left[ \frac{\tau^2}{(\sigma_{i,1}^2)^2}, \dots, \frac{\tau^2}{(\sigma_{i,n}^2)^2} \right] \right) \mathbf{z}_i \mathbf{z}_i^\top}{1 + \mathbf{z}_i^\top \text{diag}\left( \left[ \frac{\tau^2}{\sigma_{i,1}^2}, \dots, \frac{\tau^2}{\sigma_{i,n}^2} \right] \right) \mathbf{z}_i} \\ &amp;= \text{diag}\left(\left[ \frac{1}{\hat{\sigma}^2_{i, 1}}, \dots, \frac{1}{\hat{\sigma}^2_{i,n}} \right]\right) - \frac{\text{diag}\left( \left[ \frac{\tau^2}{(\sigma_{i,1}^2)^2}, \dots, \frac{\tau^2}{(\sigma_{i,n}^2)^2} \right] \right) \mathbf{z}_i \mathbf{z}_i^\top}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}} \end{aligned}\] <p>We also have:</p> \[[\Sigma^{-1}_{y_i}]_{j,j'} = - \frac{\tau^2 \mathbf{z}_{i,j} \mathbf{z}_{i,j'}}{(\hat{\sigma}_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)} \hspace{10mm} \text{and} \hspace{10mm} [\Sigma^{-1}_{y_i}]_{j,j} = \frac{1}{\hat{\sigma}^2_{i,j}} - \frac{\tau^2 \mathbf{z}_{i,j}^2}{(\hat{\sigma}_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)}\] </li> </ul> <h4 id="derivatives-2">Derivatives</h4> <p>We first find the derivative with respect to $\tau^2$:</p> <ul id="deriv-tau-2" class="tab" data-tab="a1b34ae2-86cd-4b2a-b30f-9072e7b3ec9d" data-name="deriv-tau-2"> <li class="active" id="deriv-tau-2-equation"> <a href="#">equation </a> </li> <li id="deriv-tau-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="a1b34ae2-86cd-4b2a-b30f-9072e7b3ec9d" data-name="deriv-tau-2"> <li class="active"> \[\frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} = - \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} - \sum_{l = 1}^n \sum_{j = 1}^n \frac{\tau^2 \mathbf{z}_{i,l}^2 \mathbf{z}_{i,j}^2}{(\hat{\sigma}_{i,l}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}\right)} - \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \frac{\mathbf{z}_{i,a} \mathbf{z}_{i,b}}{(\hat{\sigma}^2_{i,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \right]\] </li> <li> <p>First, the log determinant term:</p> \[\begin{aligned} \frac{\partial}{\partial \tau^2} \left[ \log (\rvert \Sigma_{y_i} \rvert) \right] &amp;= \text{tr}\left[ \Sigma^{-1}_{y_i} \frac{\partial}{\partial \tau^2} \left[ \Sigma_{y_i} \right] \right] \\ &amp;= \text{tr} \left[ \left( \text{diag}\left(\left[ \frac{1}{\hat{\sigma}^2_{i, 1}}, \dots, \frac{1}{\hat{\sigma}^2_{i,n}} \right]\right) - \frac{\text{diag}\left( \left[ \frac{\tau^2}{(\hat{\sigma}_{i,1}^2)^2}, \dots, \frac{\tau^2}{(\hat{\sigma}_{i,n}^2)^2} \right] \right) \mathbf{z}_i \mathbf{z}_i^\top}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}} \right) \mathbf{z}_i \mathbf{z}_i^\top \right] \\ &amp;= \sum_{l = 1}^n \sum_{j = 1}^n \left( \text{diag}\left(\left[ \frac{1}{\hat{\sigma}^2_{i, 1}}, \dots, \frac{1}{\hat{\sigma}^2_{i,n}} \right]\right) - \frac{\text{diag}\left( \left[ \frac{\tau^2}{(\hat{\sigma}_{i,1}^2)^2}, \dots, \frac{\tau^2}{(\hat{\sigma}_{i,n}^2)^2} \right] \right) \mathbf{z}_i \mathbf{z}_i^\top}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}} \right)_{l,j} \left(\mathbf{z}_i \mathbf{z}_i^\top \right)_{l,j} \\ &amp;= \sum_{l = 1}^n \left( \text{diag}\left(\left[ \frac{1}{\hat{\sigma}^2_{i, 1}}, \dots, \frac{1}{\hat{\sigma}^2_{i,n}} \right]\right) - \frac{\text{diag}\left( \left[ \frac{\tau^2}{(\hat{\sigma}_{i,1}^2)^2}, \dots, \frac{\tau^2}{(\hat{\sigma}_{i,n}^2)^2} \right] \right) \mathbf{z}_i \mathbf{z}_i^\top}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}} \right)_{l,l} \left(\mathbf{z}_i \mathbf{z}_i^\top \right)_{l,l} + \sum_{l = 1}^n \sum_{j \neq l} \left( \text{diag}\left(\left[ \frac{1}{\hat{\sigma}^2_{i, 1}}, \dots, \frac{1}{\hat{\sigma}^2_{i,n}} \right]\right) - \frac{\text{diag}\left( \left[ \frac{\tau^2}{(\hat{\sigma}_{i,1}^2)^2}, \dots, \frac{\tau^2}{(\hat{\sigma}_{i,n}^2)^2} \right] \right) \mathbf{z}_i \mathbf{z}_i^\top}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}} \right)_{l,j} \left(\mathbf{z}_i \mathbf{z}_i^\top \right)_{l,j} \\ &amp;= \sum_{l = 1}^n \left( \frac{1}{\hat{\sigma}^2_{i,l}} - \frac{\frac{\tau^2}{(\hat{\sigma}_{i,l}^2)^2}\mathbf{z}_{i,l}^2}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}} \right) \mathbf{z}_{i,l}^2 - \sum_{l = 1}^n \sum_{j \neq l} \left( \frac{\frac{\tau^2}{(\hat{\sigma}^2_{i,l})^2} \mathbf{z}_{i,l} \mathbf{z}_{i,j}}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}} \right) \mathbf{z}_{i,l} \mathbf{z}_{i,j} \\ &amp;= \sum_{l = 1}^n \left( \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} - \frac{\frac{\tau^2}{(\hat{\sigma}_{i,l}^2)^2}\mathbf{z}_{i,l}^4}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}} \right) - \sum_{l = 1}^n \sum_{j \neq l} \left( \frac{\frac{\tau^2}{(\hat{\sigma}^2_{i,l})^2} \mathbf{z}_{i,l}^2 \mathbf{z}_{i,j}^2}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}} \right) \\ &amp;= \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} - \sum_{l = 1}^n \sum_{j = 1}^n \frac{\frac{\tau^2}{(\hat{\sigma}_{i,l}^2)^2}\mathbf{z}_{i,l}^2 \mathbf{z}_{i,j}^2}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}} \end{aligned}\] <p>Next, the quadratic term. We will compute the derivative of $\Sigma^{-1}_{y_i}$ with respect to $\tau^2$ element-wise:</p> \[\begin{aligned} \frac{\partial}{\partial \tau^2}\left[ [\Sigma^{-1}_{y_i}]_{j,j'}\right] &amp;= \frac{\partial}{\partial \tau^2} \left[ - \frac{\tau^2 \mathbf{z}_{i,j} \mathbf{z}_{i,j'}}{(\hat{\sigma}_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)} \right] \\ &amp;= -\frac{(\hat{\sigma}_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)(\mathbf{z}_{i,j}\mathbf{z}_{i,j'}) - (\tau^2 \mathbf{z}_{i,j}\mathbf{z}_{i,j'})\left( (\hat{\sigma}_{i,j}^2)^2\sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)}{(\hat{\sigma}_{i,j}^2)^4 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \\ &amp;= -\frac{\mathbf{z}_{i,j}\mathbf{z}_{i,j'} \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}- \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)}{(\hat{\sigma}_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \\ &amp;= -\frac{\mathbf{z}_{i,j}\mathbf{z}_{i,j'}}{(\hat{\sigma}_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \\ \frac{\partial}{\partial \tau^2}\left[ [\Sigma^{-1}_{y_i}]_{j,j}\right] &amp;= \frac{\partial}{\partial \tau^2} \left[ \frac{1}{\hat{\sigma}^2_{i,j}} - \frac{\tau^2 \mathbf{z}_{i,j}^2}{(\hat{\sigma}_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)} \right] \\ &amp;= - \frac{(\hat{\sigma}_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)\mathbf{z}_{i,j}^2 - \tau^2 \mathbf{z}_{i,j}^2 (\hat{\sigma}_{i,j}^2)^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}}{(\hat{\sigma}_{i,j}^2)^4 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \\ &amp;= - \frac{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)\mathbf{z}_{i,j}^2 - \tau^2 \mathbf{z}_{i,j}^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}}{(\hat{\sigma}_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \\ &amp;= - \frac{\mathbf{z}_{i,j}^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}- \tau^2\sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)}{(\hat{\sigma}_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \\ &amp;= - \frac{\mathbf{z}_{i,j}^2}{(\hat{\sigma}_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \\ \end{aligned}\] <p>In matrix notation, we have:</p> \[\frac{\partial}{\partial \tau^2}\left[ \Sigma^{-1}_{y_i} \right] = \text{diag}\left(\left[ -\frac{1}{(\hat{\sigma}^2_{i,1})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2}, \dots, -\frac{1}{(\hat{\sigma}^2_{i,n})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \right] \right) \mathbf{z}_i \mathbf{z}_i^\top\] <p>Then:</p> \[\begin{aligned} (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \frac{\partial}{\partial \tau^2} \left[ \Sigma^{-1}_{y_i} \right](\mathbf{y}_i - \alpha_i \mathbf{1}_n) &amp;= (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ \text{diag}\left(\left[ -\frac{1}{(\hat{\sigma}^2_{i,1})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2}, \dots, -\frac{1}{(\hat{\sigma}^2_{i,n})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \right] \right) \mathbf{z}_i \mathbf{z}_i^\top \right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ &amp;= \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \left[ \text{diag}\left(\left[ -\frac{1}{(\hat{\sigma}^2_{i,1})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2}, \dots, -\frac{1}{(\hat{\sigma}^2_{i,n})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \right] \right) \mathbf{z}_i \mathbf{z}_i^\top \right]_{a,b} \\ &amp;= - \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \frac{\mathbf{z}_{i,a} \mathbf{z}_{i,b}}{(\hat{\sigma}^2_{i,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \end{aligned}\] <p>And thus:</p> \[\begin{aligned} \frac{\partial}{\partial \tau^2} \left[ \ell(\theta; \mathbf{y}) \right] &amp;= \frac{\partial}{\partial \tau^2} \left[ \sum_{i = 1}^k \left[ -\frac{n}{2} \log(2 \pi) - \frac{1}{2}\log(\rvert \Sigma_{y_i} \rvert) - \frac{1}{2} (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \Sigma_{y_i}^{-1} (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \right] \\ &amp;= - \frac{1}{2} \sum_{i = 1}^k \left[ \frac{\partial}{\partial \tau^2} \left[ \log(\rvert \Sigma_{y_i} \rvert) \right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \frac{\partial}{\partial \tau^2} \left[ \Sigma_{y_i}^{-1} \right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right]\\ &amp;= - \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} - \sum_{l = 1}^n \sum_{j = 1}^n \frac{\frac{\tau^2}{(\hat{\sigma}_{i,l}^2)^2}\mathbf{z}_{i,l}^2 \mathbf{z}_{i,j}^2}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}} - \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \frac{\mathbf{z}_{i,a} \mathbf{z}_{i,b}}{(\hat{\sigma}^2_{i,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \right] \end{aligned}\] </li> </ul> <p>And now we find the gradient with respect to $\alpha$:</p> <ul id="deriv-alpha-2" class="tab" data-tab="3e012524-673a-48d5-aff4-db5603d90e74" data-name="deriv-alpha-2"> <li class="active" id="deriv-alpha-2-equation"> <a href="#">equation </a> </li> <li id="deriv-alpha-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="3e012524-673a-48d5-aff4-db5603d90e74" data-name="deriv-alpha-2"> <li class="active"> \[\frac{\partial}{\partial \alpha}[ \ell(\theta; \mathbf{y})] = \begin{bmatrix} (\mathbf{y}_1 - \alpha_1 \mathbf{1}_n)^\top \Sigma_{y_1}^{-1} \mathbf{1}_n \\ \vdots \\ (\mathbf{y}_k - \alpha_k \mathbf{1}_n)^\top \Sigma_{y_k}^{-1} \mathbf{1}_n \end{bmatrix}\] </li> <li> <p>We do this component-wise:</p> \[\begin{aligned} \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} &amp;= \frac{\partial}{\partial \alpha_j} \left[ \sum_{i = 1}^k \left[ -\frac{n}{2} \log(2 \pi) - \frac{1}{2}\log(\rvert \Sigma_{y_i} \rvert) - \frac{1}{2} (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \Sigma_{y_i}^{-1} (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \right] \\ &amp;= - \frac{1}{2} \frac{\partial}{\partial \alpha_j} \left[(\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \Sigma_{y_j}^{-1} (\mathbf{y}_j - \alpha_j \mathbf{1}_n) \right] \\ &amp;= - \frac{1}{2}\left(2 (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \Sigma_{y_j}^{-1} (-\mathbf{1}_n) \right) \\ &amp;= (\mathbf{y}_i - \alpha_j \mathbf{1}_n)^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ \implies \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha} &amp;= \begin{bmatrix} (\mathbf{y}_1 - \alpha_1 \mathbf{1}_n)^\top \Sigma_{y_1}^{-1} \mathbf{1}_n \\ \vdots \\ (\mathbf{y}_k - \alpha_k \mathbf{1}_n)^\top \Sigma_{y_k}^{-1} \mathbf{1}_n \end{bmatrix} \end{aligned}\] </li> </ul> <h4 id="mles-1">MLEs</h4> <p>We can find $\hat{\theta}$ by setting the above equal to zero and substitute $\tau^2 = 0$. We get:</p> <ul id="mle-2" class="tab" data-tab="3c9b0b88-ffff-43f6-a2cb-7b5db0724294" data-name="mle-2"> <li class="active" id="mle-2-equation"> <a href="#">equation </a> </li> <li id="mle-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="3c9b0b88-ffff-43f6-a2cb-7b5db0724294" data-name="mle-2"> <li class="active"> \[\hat{\theta} = \begin{bmatrix} \left(\sum_{l' = 1}^n \frac{1}{\hat{\sigma}^2_{1,l'}} \right)^{-1} \sum_{l' = 1}^n \frac{\mathbf{y}_{1,l'}}{\hat{\sigma}^2_{1,l'}} \\ \vdots \\ \left(\sum_{l' = 1}^n \frac{1}{\hat{\sigma}^2_{k,l'}} \right)^{-1} \sum_{l' = 1}^n \frac{\mathbf{y}_{k,l'}}{\hat{\sigma}^2_{k,l'}} \\ 0 \end{bmatrix}\] </li> <li> <p>We only need to deal with $\alpha$, which we can again do component-wise. First notice that:</p> \[\begin{aligned} \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} &amp;= (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ &amp;= (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left( \text{diag}\left(\left[ \frac{1}{\hat{\sigma}^2_{j, 1}}, \dots, \frac{1}{\hat{\sigma}^2_{j,n}} \right]\right) - \frac{\text{diag}\left( \left[ \frac{\tau^2}{(\hat{\sigma}_{j,1}^2)^2}, \dots, \frac{\tau^2}{(\hat{\sigma}_{j,n}^2)^2} \right] \right) \mathbf{z}_j \mathbf{z}_j^\top}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\hat{\sigma}^2_{j,l}}} \right) \mathbf{1}_n \\ &amp;= (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left(\begin{bmatrix} \frac{1}{\hat{\sigma}^2_{j, 1}} \\ \vdots \\ \frac{1}{\hat{\sigma}^2_{j,n}} \end{bmatrix} - \left( \frac{1}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\hat{\sigma}_{j,l}^2}} \right)\text{diag}\left( \left[ \frac{\tau^2}{(\hat{\sigma}_{j,1}^2)^2}, \dots, \frac{\tau^2}{(\hat{\sigma}_{j,n}^2)^2} \right] \right) \begin{bmatrix} \sum_{l = 1}^n \mathbf{z}_{j,1} \mathbf{z}_{j,l} \\ \vdots \\ \sum_{l = 1}^n \mathbf{z}_{j,n} \mathbf{z}_{j,l} \end{bmatrix} \right) \\ &amp;= \left( \sum_{l = 1}^n \frac{\mathbf{y}_{j,l} - \alpha_j}{\hat{\sigma}^2_{j,l}} \right)- \left( \frac{1}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\hat{\sigma}_{j,l}^2}} \right) (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \begin{bmatrix} \frac{\tau^2}{(\hat{\sigma}_{j,1}^2)^2} \sum_{l = 1}^n \mathbf{z}_{j,1} \mathbf{z}_{j,l} \\ \vdots \\ \frac{\tau^2}{(\hat{\sigma}_{j,n}^2)^2} \sum_{l = 1}^n \mathbf{z}_{j,n} \mathbf{z}_{j,l} \end{bmatrix} \\ &amp;= \left( \sum_{l = 1}^n \frac{\mathbf{y}_{j,l} - \alpha_j}{\hat{\sigma}^2_{j,l}} \right)- \left( \frac{1}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\hat{\sigma}_{j,l}^2}} \right) \sum_{l' = 1}^n (\mathbf{y}_{j,l'} - \alpha_j) \frac{\tau^2}{(\hat{\sigma}_{j,l'}^2)^2} \sum_{l = 1}^n \mathbf{z}_{j,l'} \mathbf{z}_{j,l} \\ &amp;= \left( \sum_{l = 1}^n \frac{\mathbf{y}_{j,l} - \alpha_j}{\hat{\sigma}^2_{j,l}} \right)- \left( \frac{\sum_{l = 1}^n \mathbf{z}_{j,l} }{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\hat{\sigma}_{j,l}^2}} \right) \left( \sum_{l' = 1}^n \frac{\tau^2 (\mathbf{y}_{j,l'} - \alpha_j) \mathbf{z}_{j,l'} }{(\hat{\sigma}_{j,l'}^2)^2} \right) \end{aligned}\] <p>So then we solve for $\alpha_j$ in:</p> \[\begin{aligned} 0 &amp;= (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ \implies 0 &amp;= \left( \sum_{l = 1}^n \frac{\mathbf{y}_{j,l} - \alpha_j}{\hat{\sigma}^2_{j,l}} \right)- \left( \frac{\sum_{l = 1}^n \mathbf{z}_{j,l} }{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\hat{\sigma}_{j,l}^2}} \right) \left( \sum_{l' = 1}^n \frac{\tau^2 (\mathbf{y}_{j,l'} - \alpha_j) \mathbf{z}_{j,l'} }{(\hat{\sigma}_{j,l'}^2)^2} \right) \\ \implies 0 &amp;= \sum_{l = 1}^n \frac{\mathbf{y}_{j,l}}{\hat{\sigma}^2_{j,l}} - \alpha_j \sum_{l = 1}^n \frac{1}{\hat{\sigma}^2_{j,l}} - \left( \frac{\sum_{l = 1}^n \mathbf{z}_{j,l} }{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\hat{\sigma}_{j,l}^2}} \right) \left[ \sum_{l' = 1}^n \frac{\tau^2 \mathbf{y}_{j,l'} \mathbf{z}_{j,l'}}{(\hat{\sigma}^2_{j,l'})^2} - \alpha_j \sum_{l' =1}^n \frac{\tau^2 \mathbf{z}_{j,l'}}{(\hat{\sigma}^2_{j,l'})} \right] \\ \implies \alpha_j \left[ \sum_{l = 1}^n \frac{1}{\hat{\sigma}^2_{j,l}} - \sum_{l = 1}^n \frac{\tau^2 \mathbf{z}_{j,l'}}{(\hat{\sigma}^2_{j,l'})^2}\right] &amp;= \sum_{l = 1}^n \frac{\mathbf{y}_{j,l}}{\hat{\sigma}^2_{j,l}} - \left( \frac{\sum_{l = 1}^n \mathbf{z}_{j,l} }{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\hat{\sigma}_{j,l}^2}} \right) \sum_{l' = 1}^n\frac{\tau^2 \mathbf{y}_{j,l'} \mathbf{z}_{j,l'}}{(\hat{\sigma}^2_{j,l'})^2} \\ \implies \alpha_j \left(\sum_{l' = 1}^n \frac{1}{\hat{\sigma}^2_{j,l'}} \left[1 - \frac{\tau^2 \mathbf{z}_{j,l'}}{\hat{\sigma}^2_{j,l'}} \right]\right) &amp;= \sum_{l' = 1}^n \frac{\mathbf{y}_{j,l'}}{\hat{\sigma}^2_{j,l'}} \left[ 1 - \left( \frac{\sum_{l = 1}^n \mathbf{z}_{j,l} }{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\hat{\sigma}_{j,l}^2}} \right) \frac{\tau^2 \mathbf{z}_{j,l'}}{\hat{\sigma}^2_{j,l'}} \right] \\ \implies \alpha_j &amp;= \left(\sum_{l' = 1}^n \frac{1}{\hat{\sigma}^2_{j,l'}} \left[1 - \frac{\tau^2 \mathbf{z}_{j,l'}}{\hat{\sigma}^2_{j,l'}} \right]\right)^{-1} \sum_{l' = 1}^n \frac{\mathbf{y}_{j,l'}}{\hat{\sigma}^2_{j,l'}} \left[ 1 - \left( \frac{\sum_{l = 1}^n \mathbf{z}_{j,l} }{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\hat{\sigma}_{j,l}^2}} \right) \frac{\tau^2 \mathbf{z}_{j,l'}}{\hat{\sigma}^2_{j,l'}} \right] \end{aligned}\] <p>Under $H_0$, $\tau^2 = 0$, so we get:</p> \[\hat{\alpha}_j = \left(\sum_{l' = 1}^n \frac{1}{\hat{\sigma}^2_{j,l'}} \right)^{-1} \sum_{l' = 1}^n \frac{\mathbf{y}_{j,l'}}{\hat{\sigma}^2_{j,l'}}\] </li> </ul> <p>Thus, the score evaluated at $\theta = \hat{\theta}$ is then:</p> \[U_{\theta}(\hat{\theta}) = \begin{bmatrix} \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha} \bigg\rvert_{\theta = \hat{\theta}} \\ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \bigg\rvert_{\theta = \hat{\theta}} \end{bmatrix} = \begin{bmatrix} \left(\sum_{l' = 1}^n \frac{1}{\hat{\sigma}^2_{1,l'}} \right)^{-1} \sum_{l' = 1}^n \frac{\mathbf{y}_{1,l'}}{\hat{\sigma}^2_{1,l'}} \\ \vdots \\ \left(\sum_{l' = 1}^n \frac{1}{\hat{\sigma}^2_{k,l'}} \right)^{-1} \sum_{l' = 1}^n \frac{\mathbf{y}_{k,l'}}{\hat{\sigma}^2_{k,l'}} \\ - \frac{1}{2}\sum_{i = 1}^k \left[ \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}_{i,l}^2} + \sum_{l = 1}^n \sum_{l' = 1}^n \frac{\mathbf{z}_{i,l}\mathbf{z}_{i,l'}(\mathbf{y}_{i,l} - \hat{\alpha}_i)(\mathbf{y}_{i,l'} - \hat{\alpha}_i)}{\hat{\sigma}^2_{i,l} \hat{\sigma}^2_{i,l'}}\right] \end{bmatrix}\] <h3 id="information-1">Information</h3> <p>As before, to find the information, we need to compute the second-order derivatives of the log-likelihood, take the expectation under $H_0$ of minus those quantities, and evaluate them by plugging in $\hat{\theta}$.</p> <h4 id="derivatives-3">Derivatives</h4> <p>We’ll take all of the derivatives component-wise. We’ll start with those with respect to $\tau^2$.</p> <ul id="deriv-theta-tau-2" class="tab" data-tab="54b971d9-0911-48a2-b935-f4aa91b7d8ff" data-name="deriv-theta-tau-2"> <li class="active" id="deriv-theta-tau-2-equation"> <a href="#">equation </a> </li> <li id="deriv-theta-tau-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="54b971d9-0911-48a2-b935-f4aa91b7d8ff" data-name="deriv-theta-tau-2"> <li class="active"> \[\begin{aligned} \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \tau^2 \partial \theta} &amp;= \begin{bmatrix} - \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{1,l}}{\hat{\sigma}^2_{1,l}}\right)^2} \left(\sum_{l' = 1}^n \frac{\mathbf{z}_{1,l'}(\mathbf{y}_{1,l'} - \alpha_1)}{(\hat{\sigma}^2_{1,l'})^2}\right)\left(\sum_{l'' = 1}^n \mathbf{z}_{1,l''}\right) \\ \vdots \\ - \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{k,l}}{\hat{\sigma}^2_{k,l}}\right)^2} \left(\sum_{l' = 1}^n \frac{\mathbf{z}_{k,l'}(\mathbf{y}_{k,l'} - \alpha_k)}{(\hat{\sigma}^2_{k,l'})^2}\right)\left(\sum_{l'' = 1}^n \mathbf{z}_{k,l''}\right) \\ \frac{1}{2} \sum_{i = 1}^k \left[ \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} \right)^2} \left( \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} \right)^2 - \frac{2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^3} \left( \sum_{a = 1}^n (\mathbf{y}_{i,a} - \alpha_i) \frac{\mathbf{z}_{i,a}}{\hat{\sigma}^2_{i,a}} \right)^2 \right] \end{bmatrix} \end{aligned} \label{eq:info-tau-tau}\] </li> <li> \[\begin{aligned} \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial (\tau^2)^2} &amp;= \frac{\partial}{\partial \tau^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] \\ &amp;= \frac{\partial}{\partial \tau^2} \left[ - \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} - \sum_{l = 1}^n \sum_{j = 1}^n \frac{\tau^2 \mathbf{z}_{i,l}^2 \mathbf{z}_{i,j}^2}{(\hat{\sigma}_{i,l}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}\right)} - \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \frac{\mathbf{z}_{i,a} \mathbf{z}_{i,b}}{(\hat{\sigma}^2_{i,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \right] \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[\sum_{l = 1}^n \sum_{j = 1}^n \frac{\partial}{\partial \tau^2} \left[ \frac{\tau^2 \mathbf{z}_{i,l}^2 \mathbf{z}_{i,j}^2}{(\hat{\sigma}_{i,l}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}\right)} \right] + \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \frac{\partial}{\partial \tau^2} \left[ \frac{\mathbf{z}_{i,a} \mathbf{z}_{i,b}}{(\hat{\sigma}^2_{i,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \right] \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \left(\frac{\mathbf{z}_{i,l}^2 \mathbf{z}_{i,j}^2 (\hat{\sigma}^2_{i,l})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}\right) - \tau^2 \mathbf{z}_{i,l}^2 \mathbf{z}_{i,j}^2 (\hat{\sigma}^2_{i,l})^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} }{\left((\hat{\sigma}^2_{i,l})^2 \left( 1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}\right)\right)^2}\right) + \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \left(\frac{-2\mathbf{z}_{i,a}\mathbf{z}_{i,b}(\hat{\sigma}^2_{i,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}_{i,l}}\right) \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}_{i,l}^2}}{\left((\hat{\sigma}^2_{i,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}} \right)^2\right)^2}\right) \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\hat{\sigma}^2_{i,l})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} \right)^2} - 2 \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \left(\frac{\mathbf{z}_{i,a}\mathbf{z}_{i,b}(\hat{\sigma}^2_{i,a})^2 \left(\sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}} \right)}{(\hat{\sigma}^2_{i,a})^4 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}} \right)^3}\right) \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[ \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} \right)^2} \left( \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{(\hat{\sigma}^2_{i,l})^2} \right) \left( \sum_{j = 1}^n \mathbf{z}_{i,j}^2 \right) - \frac{2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^3} \left( \sum_{a = 1}^n (\mathbf{y}_{i,a} - \alpha_i) \frac{\mathbf{z}_{i,a}}{(\hat{\sigma}^2_{i,a})^2} \right) \left( \sum_{b = 1}^n (\mathbf{y}_{i,b} - \alpha_i) \mathbf{z}_{i,b} \right) \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[ \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} \right)^2} \left( \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{(\hat{\sigma}^2_{i,l})^2} \right) \left( \sum_{l = 1}^n \mathbf{z}_{i,l}^2 \right) - \frac{2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^3} \left( \sum_{a = 1}^n (\mathbf{y}_{i,a} - \alpha_i) \frac{\mathbf{z}_{i,a}}{\hat{\sigma}^2_{i,a}} \right) \left( \sum_{a = 1}^n (\mathbf{y}_{i,a} - \alpha_i) \frac{\mathbf{z}_{i,a}}{\hat{\sigma}^2_{i,a}} \right) \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[ \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} \right)^2} \left( \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} \right)^2 - \frac{2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^3} \left( \sum_{a = 1}^n (\mathbf{y}_{i,a} - \alpha_i) \frac{\mathbf{z}_{i,a}}{\hat{\sigma}^2_{i,a}} \right)^2 \right] \end{aligned}\] <p>Next, we take the derivative (with respect to $\tau^2$) of the derivative with respect to $\alpha_j$:</p> \[\begin{aligned} \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \tau^2 \partial \alpha_j } &amp;= \frac{\partial}{\partial \tau^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] \\ &amp;= \frac{\partial}{\partial \tau^2} \left[ (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \right] \\ &amp;= \begin{bmatrix} (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left[ \text{diag}\left( \begin{bmatrix} -\frac{1}{(\hat{\sigma}^2_{j,1})^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2} &amp; \dots &amp; -\frac{1}{(\hat{\sigma}^2_{j,n})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2} \end{bmatrix} \right) \mathbf{z}_{j} \mathbf{z}_j^\top \right] \mathbf{1}_n \end{bmatrix} \\ &amp;= \begin{bmatrix} -\frac{\mathbf{y}_{j, 1} - \alpha_j}{(\hat{\sigma}^2_{j,1})^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2} &amp; \dots &amp; -\frac{\mathbf{y}_{j,n} - \alpha_j}{(\hat{\sigma}^2_{j,n})^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2} \end{bmatrix} \mathbf{z}_j \mathbf{z}_j^\top \mathbf{1}_n \\ &amp;= \left(- \sum_{l' = 1}^n \frac{\mathbf{z}_{j,l'}(\mathbf{y}_{j,l'} - \alpha_j)}{(\hat{\sigma}^2_{j,l'})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2 }\right)\left(\sum_{l'' = 1}^n \mathbf{z}_{j,l''}\right) \\ &amp;= - \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2} \left(\sum_{l' = 1}^n \frac{\mathbf{z}_{j,l'}(\mathbf{y}_{j,l'} - \alpha_j)}{(\hat{\sigma}^2_{j,l'})^2}\right)\left(\sum_{l'' = 1}^n \mathbf{z}_{j,l''}\right) \end{aligned}\] <p>Putting the two together into a matrix yield:</p> \[\begin{aligned} \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \tau^2 \partial \theta } &amp;= \frac{\partial}{\partial \tau^2} \left[ \begin{bmatrix} \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha} \\ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \end{bmatrix} \right] \\ &amp;= \begin{bmatrix} - \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{1,l}}{\hat{\sigma}^2_{1,l}}\right)^2} \left(\sum_{l' = 1}^n \frac{\mathbf{z}_{1,l'}(\mathbf{y}_{1,l'} - \alpha_1)}{(\hat{\sigma}^2_{1,l'})^2}\right)\left(\sum_{l'' = 1}^n \mathbf{z}_{1,l''}\right) \\ \vdots \\ - \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{k,l}}{\hat{\sigma}^2_{k,l}}\right)^2} \left(\sum_{l' = 1}^n \frac{\mathbf{z}_{k,l'}(\mathbf{y}_{k,l'} - \alpha_k)}{(\hat{\sigma}^2_{k,l'})^2}\right)\left(\sum_{l'' = 1}^n \mathbf{z}_{k,l''}\right) \\ \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\hat{\sigma}^2_{i,l})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} \right)^2} - 2 \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \left(\frac{\mathbf{z}_{i,a}\mathbf{z}_{i,b}(\hat{\sigma}^2_{i,a})^2 \left(\sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}} \right)}{(\hat{\sigma}^2_{i,l})^4 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}} \right)^3}\right) \right] \end{bmatrix} \end{aligned}\] </li> </ul> <p>We then do the same but with respect to $\alpha$.</p> <ul id="deriv-theta-alpha-2" class="tab" data-tab="c323d426-83a9-4b78-8ada-61851222f315" data-name="deriv-theta-alpha-2"> <li class="active" id="deriv-theta-alpha-2-equation"> <a href="#">equation </a> </li> <li id="deriv-theta-alpha-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="c323d426-83a9-4b78-8ada-61851222f315" data-name="deriv-theta-alpha-2"> <li class="active"> \[\frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j \partial \theta} = \begin{bmatrix} 0 \\ \vdots \\ - \mathbf{1}_n^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ \vdots \\ 0 \\ -\frac{1}{\left( 1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2 } \sum_{a = 1}^n \frac{\mathbf{z}_{j,a}(\mathbf{y}_{j, a} - \alpha_j)}{(\hat{\sigma}_{j,a}^2)^2} \left(\sum_{b = 1}^n \mathbf{z}_{j,b} \right) \end{bmatrix}\] </li> <li> <p>First, we find the gradient (with respect to $\alpha$) of the derivative with respect to $\tau^2$. This should be equivalent to the corresponding components of Eq. \eqref{eq:info-tau-tau}.</p> \[\begin{aligned} \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j \partial \tau^2 } &amp;= \frac{\partial}{\partial \alpha_j} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] \\ &amp;= \frac{\partial}{\partial \alpha_j} \left[ - \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} - \sum_{l = 1}^n \sum_{j = 1}^n \frac{\tau^2 \mathbf{z}_{i,l}^2 \mathbf{z}_{i,j}^2}{(\hat{\sigma}_{i,l}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}\right)} - \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \frac{\mathbf{z}_{i,a} \mathbf{z}_{i,b}}{(\hat{\sigma}^2_{i,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \right] \right] \\ &amp;= \frac{\partial}{\partial \alpha_j} \left[ \frac{1}{2} \sum_{i = 1}^k \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \frac{\mathbf{z}_{i,a} \mathbf{z}_{i,b}}{(\hat{\sigma}^2_{i,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \right] \\ &amp;= \frac{1}{2} \sum_{a = 1}^n \sum_{b = 1}^n \frac{\partial}{\partial \alpha_j} \left[ (\mathbf{y}_{j,a} - \alpha_j)(\mathbf{y}_{j,b} - \alpha_j) \frac{\mathbf{z}_{j,a} \mathbf{z}_{j,b}}{(\hat{\sigma}^2_{j,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2} \right] \\ &amp;= \frac{1}{2} \sum_{a = 1}^n \sum_{b = 1}^n \frac{\mathbf{z}_{j,a} \mathbf{z}_{j,b}}{(\hat{\sigma}^2_{j,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2} \frac{\partial}{\partial \alpha_j} \left[ \mathbf{y}_{j,a} \mathbf{y}_{j,b} - \alpha_j \mathbf{y}_{j,a} - \alpha \mathbf{y}_{j,b} + \alpha_j^2 \right] \\ &amp;= -\frac{1}{2 \left( 1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2 } \sum_{a = 1}^n \sum_{b = 1}^n \left[ \frac{\mathbf{z}_{j,a}\mathbf{z}_{j,b}(\mathbf{y}_{j, a} - \alpha_j)}{(\hat{\sigma}_{j,a}^2)^2} + \frac{\mathbf{z}_{j,a}\mathbf{z}_{j,b}(\mathbf{y}_{j,b} - \alpha_j)}{(\hat{\sigma}_{j,a}^2)^2} \right] \\ &amp;= -\frac{1}{2 \left( 1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2 } \left[ \sum_{a = 1}^n \frac{\mathbf{z}_{j,a}(\mathbf{y}_{j, a} - \alpha_j)}{(\hat{\sigma}_{j,a}^2)^2} \sum_{b = 1}^n \mathbf{z}_{j,b} + \sum_{b = 1}^n \frac{\mathbf{z}_{j,b}(\mathbf{y}_{j,b} - \alpha_j)}{(\hat{\sigma}_{j,b}^2)^2} \sum_{a = 1}^n \mathbf{z}_{j,a}\right] \\ &amp;= -\frac{1}{\left( 1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2 } \sum_{a = 1}^n \frac{\mathbf{z}_{j,a}(\mathbf{y}_{j, a} - \alpha_j)}{(\hat{\sigma}_{j,a}^2)^2} \left(\sum_{b = 1}^n \mathbf{z}_{j,b} \right) \end{aligned}\] <p>Now, we find the vector of second derivatives of the log-likelihood with respect to the components of $\alpha$:</p> \[\begin{aligned} \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j^2} &amp;= \frac{\partial}{\partial \alpha_j} \left[ (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \Sigma_{y_j}^{-1} \mathbf{1}_n\right] \\ &amp;= - \mathbf{1}_n^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ \frac{\partial^2 \ell(\theta; \mathbf{y})}{ \partial \alpha_{j'} \partial \alpha_j} &amp;= \frac{\partial}{\partial \alpha_{j'}} \left[ (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \Sigma_{y_j}^{-1} \mathbf{1}_n\right] \\ &amp;= 0 \\ \implies \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j \partial \alpha} &amp;= \begin{bmatrix} 0 \\ \vdots \\ - \mathbf{1}_n^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ \vdots \\ 0 \end{bmatrix} \end{aligned}\] <p>Putting the two above results together gives us:</p> \[\frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j \partial \theta} = \begin{bmatrix} 0 \\ \vdots \\ - \mathbf{1}_n^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ \vdots \\ 0 \\ -\frac{1}{\left( 1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2 } \sum_{a = 1}^n \frac{\mathbf{z}_{j,a}(\mathbf{y}_{j, a} - \alpha_j)}{(\hat{\sigma}_{j,a}^2)^2} \left(\sum_{b = 1}^n \mathbf{z}_{j,b} \right) \end{bmatrix}\] </li> </ul> <h4 id="expectations-1">Expectations</h4> <p>We now take the expectation of the above vectors. We’ll evaluate the second order partial derivatives with respect to $\tau^2$ first.</p> <ul id="info-tau-tau-2" class="tab" data-tab="a7d45863-1bd8-4620-ae30-4c78cb68e194" data-name="info-tau-tau-2"> <li class="active" id="info-tau-tau-2-equation"> <a href="#">equation </a> </li> <li id="info-tau-tau-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="a7d45863-1bd8-4620-ae30-4c78cb68e194" data-name="info-tau-tau-2"> <li class="active"> \[\mathbb{E}\left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \tau^2 \partial \theta } \right] = \begin{bmatrix} 0 \\ \vdots \\ 0 \\ \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\hat{\sigma}^2_{i,l})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} \right)^2} \right] \end{bmatrix}\] </li> <li> \[\begin{aligned} \mathbb{E}\left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial (\tau^2)^2 }\right] &amp;= \mathbb{E}\left[ \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\hat{\sigma}^2_{i,l})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} \right)^2} - 2 \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \left(\frac{\mathbf{z}_{i,a}\mathbf{z}_{i,b}(\hat{\sigma}^2_{i,a})^2 \left(\sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}} \right)}{(\hat{\sigma}^2_{i,l})^4 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}} \right)^3}\right) \right] \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\hat{\sigma}^2_{i,l})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} \right)^2} - 2 \sum_{a = 1}^n \sum_{b = 1}^n \mathbb{E}\left[ (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i)\right] \left(\frac{\mathbf{z}_{i,a}\mathbf{z}_{i,b}(\hat{\sigma}^2_{i,a})^2 \left(\sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}} \right)}{(\hat{\sigma}^2_{i,l})^4 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}} \right)^3}\right) \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\hat{\sigma}^2_{i,l})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} \right)^2} - 2 \sum_{a = 1}^n \sum_{b = 1}^n \text{Cov}\left(\mathbf{y}_{i,a}, \mathbf{y}_{i,b} \right) \left(\frac{\mathbf{z}_{i,a}\mathbf{z}_{i,b}(\hat{\sigma}^2_{i,a})^2 \left(\sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}} \right)}{(\hat{\sigma}^2_{i,l})^4 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}} \right)^3}\right) \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\hat{\sigma}^2_{i,l})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} \right)^2} \right] &amp; \left(\text{obs. ind. under } H_0 \right) \\ \mathbb{E}\left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \tau^2 \partial \alpha_j}\right] &amp;= \mathbb{E}\left[ - \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2} \left(\sum_{l' = 1}^n \frac{\mathbf{z}_{j,l'}(\mathbf{y}_{j,l'} - \alpha_j)}{(\hat{\sigma}^2_{j,l'})^2}\right)\left(\sum_{l'' = 1}^n \mathbf{z}_{j,l''}\right) \right] \\ &amp;= - \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2} \left(\sum_{l' = 1}^n \frac{\mathbf{z}_{j,l'} \mathbb{E}\left[ (\mathbf{y}_{j,l'} - \alpha_j)\right] }{(\hat{\sigma}^2_{j,l'})^2}\right)\left(\sum_{l'' = 1}^n \mathbf{z}_{j,l''}\right) \\ &amp;= 0 \end{aligned}\] </li> </ul> <p>And then we do the same for $\alpha_j$:</p> <ul id="info-alpha-alpha-2" class="tab" data-tab="079ab571-4464-498d-8f9b-9fd6789dba80" data-name="info-alpha-alpha-2"> <li class="active" id="info-alpha-alpha-2-equation"> <a href="#">equation </a> </li> <li id="info-alpha-alpha-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="079ab571-4464-498d-8f9b-9fd6789dba80" data-name="info-alpha-alpha-2"> <li class="active"> \[\mathbb{E}\left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j \partial \theta } \right] = \begin{bmatrix} 0 \\ \vdots \\ - \mathbf{1}_n^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ \vdots \\ 0 \\ 0 \end{bmatrix}\] </li> <li> \[\begin{aligned} \mathbb{E}\left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j \partial \tau^2} \right] &amp;= \mathbb{E}\left[ -\frac{1}{\left( 1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2 } \sum_{a = 1}^n \frac{\mathbf{z}_{j,a}(\mathbf{y}_{j, a} - \alpha_j)}{(\hat{\sigma}_{j,a}^2)^2} \left(\sum_{b = 1}^n \mathbf{z}_{j,b} \right) \right] \\ &amp;= -\frac{1}{\left( 1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2 } \sum_{a = 1}^n \frac{\mathbf{z}_{j,a} \mathbb{E}\left[\mathbf{y}_{j, a} - \alpha_j\right] }{(\hat{\sigma}_{j,a}^2)^2} \left(\sum_{b = 1}^n \mathbf{z}_{j,b} \right) \\ &amp;= 0 \\ \mathbb{E}\left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j^2} \right] &amp;= \mathbb{E}\left[ - \mathbf{1}_n^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \right] \\ &amp;= - \mathbf{1}_n^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ \mathbb{E}\left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j \partial \alpha_{j'}} \right] &amp;= 0 \end{aligned}\] </li> </ul> <p>We then evaluate the Fisher information at the MLE:</p> <ul id="info-2" class="tab" data-tab="2440a7e3-cf48-465c-994d-123f82a0b709" data-name="info-2"> <li class="active" id="info-2-equation"> <a href="#">equation </a> </li> <li id="info-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="2440a7e3-cf48-465c-994d-123f82a0b709" data-name="info-2"> <li class="active"> \[\mathcal{I}_{\theta, \theta}(\hat{\theta}) = \begin{bmatrix} - \mathbf{1}_n^\top\text{diag}\left(\left[ \frac{1}{\hat{\sigma}^2_{1,1}}, \dots, \frac{1}{\hat{\sigma}^2_{1,n}}\right]\right) \mathbf{1}_n &amp; 0 &amp; \dots &amp; 0 \\ 0 &amp; - \mathbf{1}_n^\top \text{diag}\left(\left[ \frac{1}{\hat{\sigma}^2_{2,1}}, \dots, \frac{1}{\hat{\sigma}^2_{2,n}}\right]\right) \mathbf{1}_n &amp; \dots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \dots &amp; \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\hat{\sigma}^2_{i,l})^2} \right] \end{bmatrix}\] </li> <li> <p>Note that:</p> \[\Sigma_{y_i}^{-1} \bigg\rvert_{\theta = \hat{\theta}} = \text{diag}\left(\left[ \frac{1}{\hat{\sigma}^2_{i,1}}, \dots, \frac{1}{\hat{\sigma}^2_{i,n}}\right]\right)\] <p>Then we have:</p> \[\begin{aligned} - \mathbb{E}\left. \left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \tau^2 \partial \theta} \right] \right\rvert_{\theta = \hat{\theta}} &amp;= - \mathbb{E} \left.\left[ \begin{bmatrix} 0 \\ \vdots \\ 0 \\ \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\hat{\sigma}^2_{i,l})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} \right)^2} \right] \end{bmatrix} \right] \right\rvert_{\theta = \hat{\theta}} \\ &amp;= \begin{bmatrix} 0 \\ \vdots \\ 0 \\ \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\hat{\sigma}^2_{i,l})^2} \right] \end{bmatrix} \\ - \mathbb{E} \left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j \partial \theta} \right] \bigg\rvert_{\theta = \hat{\theta}} &amp;= - \mathbb{E} \left. \left[ \begin{bmatrix} 0 \\ \vdots \\ - \mathbf{1}_n^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ \vdots \\ 0 \\ 0 \end{bmatrix} \right] \right\rvert_{\theta = \hat{\theta}} \\ &amp;= \begin{bmatrix} 0 \\ \vdots \\ - \mathbf{1}_n^\top \text{diag}\left(\left[ \frac{1}{\hat{\sigma}^2_{j,1}}, \dots, \frac{1}{\hat{\sigma}^2_{j,n}}\right]\right) \mathbf{1}_n \\ \vdots \\ 0 \\ 0 \end{bmatrix} \end{aligned}\] <p>Putting these together into a big matrix:</p> \[\begin{aligned} \mathcal{I}_{\theta, \theta}(\hat{\theta}) &amp;= - \mathbb{E}\left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \theta \partial \theta^\top} \right] \bigg\rvert_{\theta = \hat{\theta}} \\ &amp;= \begin{bmatrix} - \mathbf{1}_n^\top\text{diag}\left(\left[ \frac{1}{\hat{\sigma}^2_{1,1}}, \dots, \frac{1}{\hat{\sigma}^2_{1,n}}\right]\right) \mathbf{1}_n &amp; 0 &amp; \dots &amp; 0 \\ 0 &amp; - \mathbf{1}_n^\top \text{diag}\left(\left[ \frac{1}{\hat{\sigma}^2_{2,1}}, \dots, \frac{1}{\hat{\sigma}^2_{2,n}}\right]\right) \mathbf{1}_n &amp; \dots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \dots &amp; \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\hat{\sigma}^2_{i,l})^2} \right] \end{bmatrix} \end{aligned}\] </li> </ul> <hr/> ]]></content><author><name></name></author><category term="glmm"/><category term="glmm"/><category term="information"/><category term="score"/><summary type="html"><![CDATA[Calculations for GLMMs]]></summary></entry><entry><title type="html">Variant Deconvolution</title><link href="https://aerosengart.github.io/blog/2025/variant-deconvolution/" rel="alternate" type="text/html" title="Variant Deconvolution"/><published>2025-07-14T00:00:00+00:00</published><updated>2025-07-14T00:00:00+00:00</updated><id>https://aerosengart.github.io/blog/2025/variant-deconvolution</id><content type="html" xml:base="https://aerosengart.github.io/blog/2025/variant-deconvolution/"><![CDATA[<p>In the case of a rapidly mutating virus such as SARS-CoV-2, the genetic material in a wastewater sample will be a mixture from different variants. More specifically, the sample will contain small segments of viral RNA, and these small segments will be from some unknown mixture of genomes where the mixture proportions are determined by the prevalence of the different variants in the population served by that sewer system. The goal of variant deconvolution is to estimate these proportions in order to detect the presence of a new or rising variant in an area or to aid in assessing variant fitness.</p> <p>One idea of doing this estimation is to use the number of times one sees different variant-defining mutations in the sample. However, an added layer of difficult is that there may be new variants (called “cryptic” variants), and their defining mutations may be unknown as well. Several, relatively new, methods have been proposed to tackle the problem of variant deconvolution in an unsupervised manner by using mutation frequencies. Though this does not allow for the identification of the variants, it does allow for cryptic variants. One could also simply match the variants to the frequent mutations to “annotate” the results.</p> <p>One problem that arises with wastewater samples is that the sequencing is performed on the amplified genetic material. Since the amount of viral RNA is quite small compared to the other things in the sample, tiling amplicon PCR is used to increase the quantity. However, this procedure could be biased towards or against particular sequences, which could lead to overrepresentation of certain regions of the genome or missing coverage of others. Another issue is that the genetic material is fragmented, which means that it is no longer possible to identify which mutations occured on the same molecule. Moreover, the sample preparation process can introduce additional bias and errors. Variant deconvolution methods are often adjusted to account for one or more of these effects.<d-cite key="dreifuss2022"></d-cite></p> <p>In this post, I’m going to go through a brief literature review of variant deconvolution.</p> <hr/> <h2 id="virpool-model-based-estimation-ofsars-cov-2-variant-proportions-inwastewater-samples">VirPool, Model-Based Estimation of SARS-CoV-2 Variant Proportions in Wastewater Samples</h2> <p>In the work of Gafurov et al.<d-cite key="gafurov2022"></d-cite> the variant proportions are estimated using a mixture model. This method is supervised in that the number and “identity” of the variants must be specified in the model.</p> <h3 id="variant-profiles">Variant Profiles</h3> <p>The identity is determined by a <i>variant profile</i> where the variant profile for variant $k$ is denoted by $P_k(i, a)$ and represents the probability of it having nucleotide $a$ at position $i$ (in the reference genome).</p> <p>These profiles are constructed for the variants of interest using data from GISAID. Each variant is associated with a Pango lineage identifier, and each sample in the GISAID database can be assigned to the closest ancestor clade of the lineages of our variants of interest (or assigned to an “other” category).</p> <p>Due to minimal coverage at certain positions caused by deletions, the relative frequency needed to calculate $P_k(i, a)$ must be adjusted, leading to the form:</p> \[P_k(i, a) = \frac{C_k(i, a)}{\max \left\{ \gamma_k, \sum_{b} C_k(i, b) \right\}}\] <p>where $\gamma_k$ is a coverage threshold for variant $k$, and $C_k(i, a)$ is the number of times we observe base $a$ at position $i$ in the genomes (i.e. GISAID samples) associated with variant $k$.</p> <h3 id="model">Model</h3> <p>Let $R = (r_1, r_2, \dots, r_{\rvert R \rvert})$ be a sequencing read. Suppose it starts at position $s$ and comes from variant $k$. We define the probability of this read as:</p> \[\mathbb{P}(R \rvert k) = \prod{i = 1}^{\rvert R \rvert} P_k(s + i - 1, r_i)\] <p>Let $\rho = (R_1, R_2, \dots, R_m)$ denote a sequence of $m$ reads. Assuming independence of reads and given variant weights $w_1, \dots, w_K$, the model assigns a likelihood to the probability of observing $\rho$:</p> \[\mathcal{L}(w_1, \dots, w_K; \rho) = \mathbb{P}\left( \rho \rvert w_1, \dots, w_K \right) = \prod_{i = 1}^m \left[ \sum_{k = 1}^k w_k \mathbb{P}(R_i \rvert k) \right]\] <p>where $w_i$ reflect the proportion of reads from variant $i$ in the sample. The weights $W = (w_1, \dots, w_K)$ for reads $R = (R_1, \dots, R_m)$ are then estimated as those values that maximize this likelihood as:</p> \[W^* = \text{softmax} \left\{ \underset{(\xi_1, \dots, \xi_K) \in \mathbb{R}^K}{\arg \min} \left\{ - \sum_{i = 1}^m \log\left( \sum_{k = 1}^K \frac{\exp(\xi_k)}{\sum_{j = 1}^K \exp(\xi_j)} \mathbb{P}(R_i \rvert k) \right) \right\} \right\}, \hspace{5mm} w_i = \frac{\exp(\xi_i)}{\sum_{j = 1}^K \exp(\xi_j)}\] <p>The model is also extend to include uniformly at random sequencing error (though it does not address insertions). Let $\epsilon \geq 0$ be the error rate. The model becomes:</p> \[\mathbb{P}\left( R \rvert k, \epsilon \right) = \prod_{i = 1}^{\rvert R \rvert} \left[ (1 - \epsilon) P_k(s + i - 1, r_i) + \frac{\epsilon}{3} \sum_{ a \neq r_i} P_k(s + i - 1, a) \right]\] <p>From what I can tell, they do not justify the factor $\frac{1}{3}$ in the second term…</p> <hr/> <h2 id="estimating-the-relative-proportions-of-sars-cov-2-haplotypes-from-wastewater-samples">Estimating the Relative Proportions of SARS-CoV-2 Haplotypes from Wastewater Samples</h2> <p>Pipes et al.<d-cite key="pipes2022"></d-cite> take a similar approach and construct a likelihood function of haplotype proportions that they maximize using an expectation-maximization algorithm. Since a haplotype is just a collection of mutations that are usually inherited together, we can think of them as representative of a variant (since the genomes associated with a given variant are highly similar).</p> <h3 id="model-1">Model</h3> <p>Let $r$ denote the number of sequencing reads in the experiment, and let $g$ denote the number of different genomes comprising the constructed reference database (i.e. a collection of samples from GISAID). The authors first define an $r \times g$ matrix, $D$, where the $(i,j)$-th element, $D_{i,j}$, denotes the number of sequences mismatches between sequencing read $i$ and genome $j$. (These mismatches can be found because the reads should be aligned to the same reference genome as the sequences in the MSA used to form the reference database.)</p> <p>Let $q_{i,j}$ denote the probability of seeing read $i$ given it is from haplotype $j$. This is assumed to have the form:</p> \[\begin{equation} \label{eq:q-defn} q_{i,j} = \alpha^{D_{i,j}} \times (1 - \alpha)^{n_j - D_{i,j}} \end{equation}\] <p>where $\alpha \geq 0$ is some (chosen) error rate, and $n_j$ is the length of read $j$. Assuming independence of the reads, the log-likelihood can be written as:</p> \[\ell(p_1, \dots, p_k) = \sum_{j = 1}^r \log\left( \sum_{i = 1}^k q_{i,j} p_{i} \right)\] <p>where $p_i$ denotes the relative proportion of haplotype $i$. Haplotypes with identical values for $q_{i,j}$ over all $j \in [n]$ are collapsed into a single group to keep the model identifiable.</p> <h3 id="imputation">Imputation</h3> <p>Pipes et al. also introduce a tree-based method of imputing missing nucleotides via phylogenetic data. Their imputation method is based on the Jukes-Cantor model<d-cite key="jukes1969"></d-cite> (also called the JC69 model) for DNA evolution, which itself relies on some Markov chain theory (see below).</p> <details> <summary>Markov Chain Review.</summary> We'll restrict our discussion to continuous-time Markov chains. <div id="markov-chain"></div> <div class="definition"> <strong>Definition (Continuous-Time Markov Chain).<d-cite key="anderson1991"></d-cite></strong> <br/> Let $(\Omega, \mathcal{A}, P)$ be a probability space, and let $S$ be a countable, non-empty set (called the <i>state space</i>). A stochastic process, $\{ X(t), t \in [0, \infty) \}$, defined on $(\Omega, \mathcal{A}, P)$ is called a <i>continuous-time parameter Markov chain</i> if, for any set $\{ t_i \}_{i = 1}^{n+1}$ such that $0 \leq t_1 &lt; t_2 &lt; \dots &lt; t_n &lt; t_{n+1}$ with corresponding set of states $\{ s_i \}_{i = 1}^{n+1}$ in $S$ such that: $$ \mathbb{P}\left(X(t_n) = s_n, X(t_{n-1}) = X_{s_{n-1}}, \dots, X(t_1) = s_1 \right) &gt; 0 \nonumber $$ satisfies the Markov property. That is: $$ \mathbb{P}\left(X(t_{n+1}) = s_{n+1} \rvert X(t_n) = s_n, X(t_{n-1}) = s_{n-1}, \dots, X(t_1) = s_1\right) \mathbb{P}\left(X(t_{n+1}) = s_{n+1} \rvert X(t_n) = s_n \right) \nonumber $$ </div> A continuous-time Markov chain can be described by its <i>transition function</i>, which is essentially the probability of transitioning between states in $S$. <div id="markov-chain"></div> <div class="definition"> <strong>Definition (Transition Function).&lt;d-cite key=anderson1991&lt;/d-cite&gt;</strong> <br/> Let $S$ be a state space. A <i>transition function</i>, denoted by $Q_{i,j}(t)$ for $i,j \in S$ and $t \geq 0$, is a function defined on $S$ that satisfies: <ol> <li> $Q_{i,j}(t) \geq 0$ for all $t \geq 0$ and $i,j \in S$. </li> <li> $Q_{i,j}(0) = \delta_{i,j}$ for any $i,j \in S$ where $\delta_{i,j} = 1$ if $i = j$ and $0$ otherwise. </li> <li> $\sum_{j \in S} Q_{i,j}(t) \leq 1$ for all $t \geq 0$ and for all $i \in S$. </li> <li> $P_{i,j}(t + r) = \sum_{k \in S} P_{i,k}(t) P_{k,j}(r)$ for all $t, r \geq 0$ and for all $i,j \in S$. </li> </ol> In Property (3) above, if $\sum_{j \in S} Q_{i,j}(t) = 1$, then the transition function is called <i>honest</i>. Property $(4)$ is called the <i>Chapman-Kolmogorov equation</i>. <br/> If the transition function additionally satisfies: $$ \underset{t \rightarrow 0}{\lim} Q_{i,i}(t) = 1 $$ for all $i \in S$, then it is called <i>standard</i>. This property implies that $Q_{i,j}(t) \rightarrow \delta_{i,j}$ for all $i,j \in S$. </div> <br/> Since $Q_{i,j}(t)$ is just a function mapping from $S^2$ to $\mathbb{R}$, we can use it to define a (possibly infinitely dimensional) matrix whose $(i,j)$-th entry corresponds to the value $Q_{i,j}(t)$ and represents the probability of the process transitioning from state $i$ to state $j$ at time $t$. <br/> If we define $\pi^(0)$ to be the vector of initial state probabilities (i.e. the probability of the Markov chain starting in each state $s \in S$), we can define $\pi^(n)$ (the $n$-step transition probabilities) which is the vector of state probabilities at time $n$. $\pi^{(n)}_{i,j}$ represents the probability of the Markov chain being in state $j$ after $n$ transitions when starting in state $i$. <br/> It follows that: $$ \pi^{(n)} = \pi^{(0)} Q^n, \hspace{5mm} \pi^{(n+1)} = \pi^{(n)} Q \nonumber $$ We now cover several important properties of Markov chain states: <ul> <li> <i>Communicable</i>: A state $j \in S$ is called <i>accessible</i> (denoted $i \rightarrow j$) from state $j \in S$ if $\pi_{i,j}^{(n)}(t) &gt; 0$. States $i,j \in S$ can <i>communicate</i> (denoted $i \leftrightarrow j$) if they are accessible from each other. </li> <li> <i>Periodicity</i>: Let $k = \text{GCD}\left\{ n &gt; 0 \rvert \mathbb{P}\left(X(n) = s \rvert X(0) = s \right) &gt; 0 \right\}$ for some state $s \in S$. That is, $k$ is the greatest common divisor of the number of transitions needed for the Markov chain to start at state $s$ and return to $s$. $k$ is called the <i>period</i> of state $s$. If $k &gt; 1$, then $s$ is called <i>periodic</i> (and <i>aperiodic</i> if $k = 1$). </li> <li> <i>Absorbing</i>: A state $s \in S$ is called <i>absorbing</i> if the probability of transitioning out of $s$ is zero. </li> <li> <i>Recurrence</i>: A state $s \in S$ is called <i>recurrent</i> if the probability of the process never returning to $s$ is zero. Otherwise, it is called <i>transient</i>. <ul> <li> The <i>hitting time</i>, $H_A$, of a set $A \subset S$ is the random variable representing the first time at which the Markov chain transitions to a state in $A$: $$ H_A = \min\left\{ t \geq 0 \rvert X(t) \in A \right\} \nonumber $$ </li> <li> The <i>hitting probability</i>, $h_{s, A}$, of a set $A \subset S$ is the probability from starting state $s$ is the probability of the Markov chain ever transitioning to a state in $A$ when its starting state is $s$: $$ h_{s, A} = \mathbb{P}\left( X(t) \in A \hspace{2mm} \text{ for some } t \geq 0 \rvert X(0) = s \right) = \mathbb{P}\left( H_A &lt; \infty \rvert X(0) = s \right) \nonumber $$ </li> <li> The <i>expected hitting time</i>, $\eta_{s, A}$, is the expected hitting time of set $A \subset S$ from state $s$: $$ \eta_{s, A} = \mathbb{E}\left[ H_A \rvert X(0) = s \right] \nonumber $$ $\eta_{i, A} &lt; \infty$ only if $h_{s, A} = 1$. </li> </ul> A state $s$ is called <i>positive recurrent</i> if $\eta_{s, A} &lt; 1$. That is, if the expected time to return to state $s$ is finite. Otherwise, $s$ is called <i>null recurrent</i>. </li> <li> A state $s \in S$ is called <i>ergodic</i> if it is aperiodic and positive recurrent. </li> </ul> Similar properties can be defined for the Markov chain itself: <ul> <li> <i>Irreducibility</i>: A Markov chain is called <i>irreducible</i> if, and only if, $i \leftrightarrow j$ for all $i,j \in S$. </li> <li> <i>Ergodic</i>: A Markov chain is called <i>ergodic</i> if it is irreducible and all of its states are ergodic. </li> <li> <i>Regular</i>: A Markov chain is called <i>regular</i> if, for some power $n$, $Q^n$ has all positive entries. A regular Markov chain can be represented by a fully connected graph. </li> <li> <i>Homogeneous</i>: A Markov chain is called <i>homogeneous</i> if, and only if, the transition probabilities are independent of time. In this case, $Q^(n) = Q^(n-1)$ for all $n$. </li> <li> <i>Stationary</i>: A Markov chain is called <i>stationary</i> if it has a stationary distribution, defined as a probability vector, $pi$, such that $\pi Q = \pi$. If the Markov chain is irreducible and aperiodic, then $\pi$ is unique. </li> </ul> </details> <details> <summary>Jukes-Cantor Model.</summary> The basic idea is to model the substitution of base pairs with a continuous-time Markov model. Suppose we have an ancestor sequence and a descendant sequence of this ancestor; in other words, we have observe a Markov process at time $0$ and some later time $n$. If we know the starting state at a position $i$ in the ancestor sequence (i.e. the initial state), the transition rate matrix ($R$ below), and the expected number of changes (<i>branch length</i>) between ancestor and descendant, then we can recover the probability of observing the descendant (i.e. the transition probability matrix). <br/> Since this model is for DNA evolution, there are four states, each corresponding to one of the nucelotide bases: $S = \{A, G, C, T\}$. We define the following transition matrix: $$ Q(t) = \begin{bmatrix} Q_{A,A}(t) &amp; Q_{A,G}(t) &amp; Q_{A,C}(t) &amp; Q_{A,T}(t) \\ Q_{G,A}(t) &amp; Q_{G,G}(t) &amp; Q_{G,C}(t) &amp; Q_{G,T}(t) \\ Q_{C,A}(t) &amp; Q_{C,G}(t) &amp; Q_{C,C}(t) &amp; Q_{C,T}(t) \\ Q_{T,A}(t) &amp; Q_{T,G}(t) &amp; Q_{T,C}(t) &amp; Q_{T,T}(t) \end{bmatrix} \nonumber $$ We define the <i>transition rate</i> matrix as: $$ R = \begin{bmatrix} - \mu_A &amp; \mu_{A, G} &amp; \mu_{A, C} &amp; \mu_{A, T} \\ \mu_{G, A} &amp; -\mu_{G} &amp; \mu_{G, C} &amp; \mu_{G, T} \\ \mu_{C, A} &amp; \mu_{C, G} &amp; -\mu_{C} &amp; \mu_{C, T} \\ \mu_{T, A} &amp; \mu_{T, G} &amp; \mu_{T, C} &amp; -\mu_{T} \end{bmatrix} \nonumber $$ where $\mu_{i, j}$ is the instantaneous rate of change from state $i$ to state $j$. The diagonal elements, $\mu_i$, represent the total rate of change to state $i$. <br/> The Jukes-Cantor model makes a few simplifying assumptions. First, the state probabilities are assumed to be equal and unchanging over time, so $\pi(t) = \left(\frac{1}{4}, \frac{1}{4}, \frac{1}{4}, \frac{1}{4} \right)$. The model is parametrized by $\mu$, which represents the substitution rate in the genome. Additionally, the transition probability matrix is assumed to be multiplicative, so the transition probability at time $t + t'$ is equal to the product of the probabilities at time $t$ and time $t'$. Thus, the transition rate and transition probability matrices are given by: $$ R = \begin{bmatrix} -3\mu &amp; \mu &amp; \mu &amp; \mu \\ \mu &amp; - 3\mu &amp; \mu &amp; \mu \\ \mu &amp; \mu &amp; -3\mu &amp; \mu \\ \mu &amp; \mu &amp; \mu &amp; -3\mu \end{bmatrix}, \hspace{5mm} Q(t) = \begin{bmatrix} r_t &amp; q_t &amp; q_t &amp; q_t \\ q_t &amp; r_t &amp; q_t &amp; q_t \\ q_t &amp; q_t &amp; r_t &amp; q_t \\ q_t &amp; q_t &amp; q_t &amp; r_t \end{bmatrix} \nonumber $$ Finding $Q(t)$ is our goals. Notice that, for some very short period of time, $\epsilon &gt; 0$: $$ Q(\epsilon) = \mathbb{I}_{4 \times 4} + R\epsilon = \begin{bmatrix} 1 - 3 \mu \epsilon &amp; \mu \epsilon &amp; \mu \epsilon &amp; \mu_\epsilon \\ \mu_\epsilon &amp; 1 - 3 \mu \epsilon &amp; \mu_\epsilon &amp; \mu_\epsilon \\ \mu_\epsilon &amp; \mu_\epsilon &amp; 1 - 3 \mu \epsilon &amp; \mu_\epsilon \\ \mu_\epsilon &amp; \mu_\epsilon &amp; \mu_\epsilon &amp; 1 - 3 \mu \epsilon \end{bmatrix} \nonumber $$ We can use the multiplicativity property to see: $$ \begin{aligned} &amp;Q(t + \epsilon) = Q(t) Q(\epsilon) = Q(t) (\mathbb{I}_{4 \times 4} + R \epsilon) \\ \implies &amp;Q(t + \epsilon) - Q(t) = Q(t) (\mathbb{I}_{4 \times 4} + R \epsilon) - Q(t) \\ \implies &amp;Q(t + \epsilon) - Q(t) = Q(t) R \epsilon \\ \implies &amp;\frac{1}{\epsilon} \left[ Q(t + \epsilon) - Q(t) \right] = Q(t) R \end{aligned} \nonumber $$ Taking $\epsilon \rightarrow 0$, we get the differential equations: $$ \begin{aligned} \frac{\partial}{\partial t} Q(t) &amp;= Q(t) R \\ &amp;= \begin{bmatrix} r_t &amp; q_t &amp; q_t &amp; q_t \\ q_t &amp; r_t &amp; q_t &amp; q_t \\ q_t &amp; q_t &amp; r_t &amp; q_t \\ q_t &amp; q_t &amp; q_t &amp; r_t \end{bmatrix} \begin{bmatrix} -3\mu &amp; \mu &amp; \mu &amp; \mu \\ \mu &amp; - 3\mu &amp; \mu &amp; \mu \\ \mu &amp; \mu &amp; -3\mu &amp; \mu \\ \mu &amp; \mu &amp; \mu &amp; -3\mu \end{bmatrix} \\ &amp;= \begin{bmatrix} -3 \mu r_t + 3 \mu q_t &amp; \mu r_t - \mu q_t &amp; \mu r_t - \mu q_t &amp; \mu r_t - \mu q_t \\ \mu r_t - \mu q_t &amp; -3 \mu r_t + 3 \mu q_t &amp; \mu r_t - \mu q_t &amp; \mu r_t - \mu q_t \\ \mu r_t - \mu q_t &amp; \mu r_t - \mu q_t &amp; -3 \mu r_t + 3 \mu q_t &amp; \mu r_t - \mu q_t \\ \mu r_t - \mu q_t &amp; \mu r_t - \mu q_t &amp; \mu r_t - \mu q_t &amp; -3 \mu r_t + 3 \mu q_t \end{bmatrix} \end{aligned} \nonumber $$ Put simply, we have the system: $$ \frac{d}{dt}[r_t] = -3 \mu r_t + 3 \mu q_t \hspace{4mm} \text{and} \hspace{4mm} \frac{d}{dt}[q_t] = \mu r_t + \mu q_t \nonumber $$ If we further assume that $r_t = q_t = \frac{1}{4}$ as $t \rightarrow \infty$, then we get the unique solutions: $$ r_t = \frac{1}{4}(1 + 3 \exp(-4 \mu t)) \hspace{4mm} \text{and} \hspace{4mm} q_t = \frac{1}{4}(1 - \exp(-4 \mu t)) \nonumber $$ One last concept we need to introduce is <i>branch length</i>. In this context, it is a quantification of the difference between a parent and child node in a phylogenetic tree (i.e. an ancestor and descendant) based upon the number of substitutions between the two sequences. Usually, this is normalized to the sequence lengths and also a value representing the <i>expected</i> number or proportion of substitutions. </details> <p>The general idea is that the missing nucleotides are imputed as the value maximizing the posterior probability of the descendant sequence using the Jukes-Cantor model as the probabilistic framework. There are some more details involving computational efficiency and details. Briefly, they do some preprocessing of the phylogenetic tree, and they split the entire tree into three parts to improve computational feasibility. The result of this step is a multiple sequence alignment table that has no gaps (missing bases in non-reference sequences are imputed, and positions corresponding to gaps in the reference strain are removed) and can serve as the <i>reference database</i>.</p> <h3 id="estimation">Estimation</h3> <p>An expectation-maximization (EM) algorithm is used to estimatem $q_{i,j}$ in Eq. \eqref{eq:q-defn}. Given $q_{i,j}$ for all $i, j$, we first initialize the relative haplotype proportions as \(p^0_i \sim \text{Unif}(0, 1)\) for all $i \in [k]$ and divide by \(\sum_{i = 1}^k p^0_i\) so the vector sums to $1$. Then for $t = 1, 2, \dots$ until convergence:</p> <ol> <li> Compute the relative haplotype proportions as $p^t_i = \frac{1}{n} \sum_{j = 1}^n \frac{p_i^{t-1}q_{i,j}}{\sum_{l = 1}^k p_l^{t-1}q_{i,j}}$. </li> <li> Compute the log-likelihood at step $t$ as $\ell^t(p_1, \dots, p_k) = \sum_{j = 1}^r \log\left( \sum_{i = 1}^k q_{i,j} p^t_{t} \right)$ </li> </ol> <p>There is an unidentifiability issue in the case that there exist haplotypes $i$ and $i’$ such that $q_{i,j} = q_{i’, j}$ for all reads $j \in [n]$. To overcome this, the authors group the haplotypes with identical $q$ values across all reads into a single “haplotype group”.</p> <hr/> <h2 id="unsupervised-detection-and-fitness-estimation-of-emerging-sars-cov-2-variants">Unsupervised Detection and Fitness Estimation of Emerging SARS-CoV-2 Variants</h2> <p>Lefebvre et al.<d-cite key="lefebvre2025"></d-cite> use a clustering-based approach to solve the variant deconvolution problem. By leveraging the fact that observations from the same sewer system are correlated over time, they learn which mutations belong to the same latent group based upon their frequencies.</p> <p>Since their method simultaneously estimates variant fitness, they use some terminology that is not really relevant to our discussion (e.g. variants under positive/negative selection). I’ll just use it to keep this post consistent with the original manuscript.</p> <h3 id="model-2">Model</h3> <p>Let $K \geq 0$ be the number of latent groups (i.e. variants) that are under some selective pressure (either positive or negative), which the authors call “non-neutral groups”. The model always includes a neutral group that is under no selective pressure, and we’ll use a group number of $0$ for this one. Let $G_k$ for $k \in { 0, \dots, K }$ denote group number.</p> <p>Let $n$ be the total (over all samples) number of mutations found in the dataset (compared to some reference sequence). We use $Z = { Z_1, \dots, Z_n } \in { 0, \dots, K }^n$ to be the group assignments for all mutations, where $Z_i$ for $i \in { 1, \dots, n}$ is the group assignment of the $i$-th mutation.</p> <p>Let $t_0, \dots, t_m$ denote the timepoints at which the data are collected, and let $\mathcal{T} = (t_0 - t_0 = 0, t_1 - t_0, \dots, t_m - t_0 = T)$ be the time differences between the observations. We will use $X_{i, t}$ to denote the count for mutation $i$ at time $t \in \mathcal{T}$, and we’ll use $d_{i, t}$ to denote the read depth for the genome position of mutation $i$ at time $t \in \mathcal{T}$.</p> <p>Let $\pi = { \pi_0, \dots, \pi_K }$ such that $\sum_{i = 0}^K \pi_i = 1$ where $\pi_k$ is the probability of a mutation belonging to group $k$ for $k \in { 0, \dots, K }$. We assume a multinomial distribution for the elements of $Z$.</p> <p>The authors choose a generalized linear model for the mutation counts conditional on non-neutral group:</p> \[X_{i,t} \rvert Z_i = k, k \neq 0 \sim \text{Binom}\left(d_{i,t}, \text{logit}^{-1}(\exp(\mu_k + s_k t))\right)\] <p>where $\mu_k$ and $s_k$ are the intercept and slope for group $k$ when $k$ is not the neutral group. The authors call $s_k$ a “selection coefficient” because a negative/positive value represents the decrease/increase in the mutation frequency.</p> <p>For the neutral group, they use a Beta-binomial distribution:</p> \[X_{i,t} \rvert Z_i = 0 \sim \text{Binom}(d_{i,t}, u), \hspace{5mm} u \sim \text{Beta}(\alpha, \beta)\] <p>for some learned hyperparameters, $\alpha$ and $\beta$. There is no selective pressure, so it seems reasonable to allow $s_0 = 0$ in this way. However, I’m not convinced one needs to learn $\alpha$ and $\beta$ over just $\mu_0$…</p> <p>The parameter vector is denoted by \(\theta = (\pi, \mu, s, \alpha, \beta)\), where \(\pi = (\pi_0, \dots, \pi_K) \in [0, 1]^{K + 1}\), \(\mu = (\mu_1, \dots, \mu_K) \in \mathbb{R}^K\), \(s = (s_1, \dots, s_K) \in \mathbb{R}^K_{\geq 0}\), and \(\alpha, \beta \in \mathbb{R}_{&gt; 0}\). One can then write the joint distribution of \(X = \{ X_1, \dots, X_n \}\) with \(X_i = \{ X_{i,t} \}_{t = 0, \dots, T}\) and $Z$, conditional on $\theta$, as:</p> \[\mathbb{P}(X, Z \rvert \theta) = \prod_{i = 1}^n \prod_{t \in \mathcal{T}} \mathbb{P}(Z_i \rvert \pi)\mathbb{P}\left(X_{i,t} \rvert Z_i; \mu, s, \alpha, \beta \right)\] <p>Lefebvre et al. use an expectation-maximization algorithm to estimate the parameter values.</p> <hr/> <h2 id="tracking-sars-cov-2-genomic-variants-in-wastewater-sequencing-data-with-lollipop">Tracking SARS-CoV-2 Genomic Variants in Wastewater Sequencing Data with LolliPop</h2> <p>Dreifuss et al.<d-cite key="dreifuss2022"></d-cite> use a regression-based framework to perform variant deconvolution. Their novelty comes from an additional regularization step that imposes a kind of “smoothness over time”, making their method particularly suited to time series analysis.</p> <h3 id="model-3">Model</h3> <p>We assume to have $V$ possible variants, each of which is characterized by some subset of the mutation set, $S_M = { 1, \dots, M}$, defined with respect to a single, fixed reference strain. The characteristic mutations are collected into a <i>profile</i> matrix, $\mathbf{X} \in {0, 1}^{M \times V}$, where $\mathbf{X}_{i,j} = 1$ if mutation $i \in S_M$ is characteristic of variant $j \in [V]$ and $0$ otherwise.</p> <p>We let \(\mathbf{y}_t \in [0, 1]^M\) denote the <i>mutation frequency vector</i> at time \(t \in [T]\) where \(\mathbf{y}_{t, i}\) is the observed frequency of mutation $i$ at time $t$ (computed as the proportion of reads with mutation $i$ from a sample taken at time $t$). The goal is to estimate the <i>relative variant abundance vector</i>, \(\mathbf{b}_t \in [0, 1]^V\), where \(\mathbf{b}_{t, j}\) denotes the relative proportion of variant $j \in [V]$ in the sample. We enforce that \(\rvert \rvert \mathbf{b}_t \rvert \rvert_1 = 1\).</p> <p>The model is a linear regression framework. Under the assumption that:</p> \[\begin{equation} \label{eq:model-lollipop} \mathbb{E}\left[ \mathbf{y}_t \rvert \mathbf{b}_t \right] = \mathbf{X} \mathbf{b}_t; \hspace{5mm} \forall t \in [T] \end{equation}\] <p>we aim to estimate $\mathbf{b}_1, \dots, \mathbf{b}_T$ such that $\mathbf{y}_t = \mathbf{X} \mathbf{b}_t$ for all $t \in [T]$. The solution is taken as:</p> \[\begin{equation} \label{eq:model-soln} \hat{\mathbf{b}}_t = \underset{\mathbf{b}_t \in [0, 1]^V}{\arg \min} \left\{ \mathcal{L}(\mathbf{y}_t - \mathbf{X} \mathbf{b}_t) \right\} \end{equation}\] <p>where $\mathcal{L}$ is some loss function.</p> <h3 id="temporal-regularization">Temporal Regularization</h3> <p>The temporal regularization is implemented with a <a href="https://web.stanford.edu/group/SOL/papers/fused-lasso-JRSSB.pdf">fused</a> ridge penalty (i.e. an $L_2$ penalty on the differences rather than the vector themselves) that enforces “temporal continuity” between the $\mathbf{b}_t$ over values of $t$. Intuitively, applying the penalty to the differences in abundance vectors enforces sparsity in the entries of the difference, which keeps the two vectors “close” from timepoint to timepoint.</p> <p>Letting \(\lambda_{i,j} &gt; 0\) denote the penalty parameter for abundance vectors, $\mathbf{b}_i$ and $\mathbf{b}_j$, at times $i$ and $j$, the loss function is:</p> \[\begin{equation} \label{eq:model-loss} \mathcal{L}(\mathbf{b}_1, \dots, \mathbf{b}_T; \lambda) = \sum_{i = 1}^T \rvert \rvert \mathbf{y}_i - \mathbf{X} \mathbf{b}_i \rvert \rvert_2^2 + \sum_{i = 1}^T \sum_{j = 1}^T \lambda_{i,j} \rvert \rvert \mathbf{X} (\mathbf{b}_i - \mathbf{b}_j) \rvert \rvert_2^2 \end{equation}\] <p>Defining $\mu_i = 1 + \sum_{j = 1}^T \lambda_{i,j}$ and the <a href="https://en.wikipedia.org/wiki/Doubly_stochastic_matrix">doubly stochastic matrix</a>:</p> \[\Lambda = \begin{bmatrix} \mu_1 - \lambda_{1, 1} &amp; \dots &amp; -\lambda_{T, 1} \\ \vdots &amp; \ddots &amp; \vdots \\ -\lambda_{1, T} &amp; \dots &amp; \mu_T - \lambda_{T, T} \end{bmatrix}\] <div class="theorem"> <strong>Claim (Solution).</strong> <ul id="lambda-soln" class="tab" data-tab="7ac9f20e-7045-4fd4-b93a-e5597161f6ba" data-name="lambda-soln"> <li class="active" id="lambda-soln-statement"> <a href="#">statement </a> </li> <li id="lambda-soln-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="7ac9f20e-7045-4fd4-b93a-e5597161f6ba" data-name="lambda-soln"> <li class="active"> <p>If $\Lambda$ is has full rank ($T$), then the solution can be found:</p> \[\begin{equation} \label{eq:model-soln-full-rank} \hat{\mathbf{B}} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{Y} \Lambda^{-1} \end{equation}\] <p>where $\mathbf{Y}$ is a matrix whose columns correspond to $\mathbf{y}_1, \dots, \mathbf{y}_T$, and $\mathbf{B}$ is a matrix whose columns correspond to $\mathbf{b}_1, \dots, \mathbf{b}_T$.</p> </li> <li> <p>Taking the gradient of Eq. \eqref{eq:model-loss} with respect to $\mathbf{b}_i$ gives:</p> \[\begin{aligned} \frac{\partial \mathcal{L}(\mathbf{b}_1, \dots, \mathbf{b}_T; \lambda)}{\partial \mathbf{b}_i} &amp;= \frac{\partial}{\partial \mathbf{b}_i} \left[ \sum_{i = 1}^T \rvert \rvert \mathbf{y}_i - \mathbf{X} \mathbf{b}_i \rvert \rvert_2^2 + \sum_{i = 1}^T \sum_{j = 1}^T \lambda_{i,j} \rvert \rvert \mathbf{X} (\mathbf{b}_i - \mathbf{b}_j) \rvert \rvert_2^2 \right] \\ &amp;= \frac{\partial}{\partial \mathbf{b}_i} \left[ \sum_{i = 1}^T (\mathbf{y}_i - \mathbf{X} \mathbf{b}_i)^\top (\mathbf{y}_i - \mathbf{X} \mathbf{b}_i) + \sum_{i = 1}^T \sum_{j = 1}^T \lambda_{i,j} (\mathbf{X} (\mathbf{b}_i - \mathbf{b}_j))^\top(\mathbf{X}(\mathbf{b}_i - \mathbf{b}_j)) \right] \\ &amp;= \frac{\partial}{\partial \mathbf{b}_i} \left[ \sum_{i = 1}^T (\mathbf{y}_i^\top \mathbf{y}_i - \mathbf{y}_i^\top \mathbf{X} \mathbf{b}_i - \mathbf{b}_i^\top \mathbf{X}^\top \mathbf{y}_i + \mathbf{b}_i^\top \mathbf{X}^\top \mathbf{X} \mathbf{b}_i) + \sum_{i = 1}^T \sum_{j = 1}^T \lambda_{i,j} (\mathbf{b}_i - \mathbf{b}_j)^\top \mathbf{X}^\top \mathbf{X}(\mathbf{b}_i - \mathbf{b}_j) \right] \\ &amp;= - 2\mathbf{X}^\top \mathbf{y}_i + 2 \mathbf{X}^\top \mathbf{X} \mathbf{b}_i + \sum_{j = 1}^T 2\lambda_{i,j} \mathbf{X}^\top \mathbf{X}(\mathbf{b}_i - \mathbf{b}_j) \\ &amp;= 2 \mathbf{X}^\top \mathbf{X} \mathbf{b}_i - 2\mathbf{X}^\top \mathbf{y}_i + 2 \mathbf{X}^\top \mathbf{X} \sum_{j = 1}^T \lambda_{i,j} (\mathbf{b}_i - \mathbf{b}_j) \\ &amp;= 2 \mathbf{X}^\top \mathbf{X} \mathbf{b}_i - 2\mathbf{X}^\top \mathbf{y}_i + 2 \mathbf{X}^\top \mathbf{X} \left( \mathbf{b}_i \sum_{j = 1}^T \lambda_{i,j} - \sum_{j = 1}^T \lambda_{i,j} \mathbf{b}_j \right) \\ &amp;= 2 \mathbf{X}^\top \mathbf{X} \mathbf{b}_i - 2\mathbf{X}^\top \mathbf{y}_i + 2 \mathbf{X}^\top \mathbf{X} \mathbf{b}_i \sum_{j = 1}^T \lambda_{i,j} - 2 \mathbf{X}^\top \mathbf{X} \sum_{j = 1}^T \lambda_{i,j} \mathbf{b}_j \end{aligned}\] <p>Setting the above equal to $0$ and solving for $\mathbf{b}_i$:</p> \[\begin{aligned} &amp;\frac{\partial \mathcal{L}(\mathbf{b}_1, \dots, \mathbf{b}_T; \lambda)}{\partial \mathbf{b}_i} = 0 \\ \implies &amp;2 \mathbf{X}^\top \mathbf{X} \mathbf{b}_i - 2\mathbf{X}^\top \mathbf{y}_i + 2 \mathbf{X}^\top \mathbf{X} \mathbf{b}_i \sum_{j = 1}^T \lambda_{i,j} - 2 \mathbf{X}^\top \mathbf{X} \sum_{j = 1}^T \lambda_{i,j} \mathbf{b}_j = 0 \\ \implies &amp; \mathbf{X}^\top \mathbf{y}_i+ \mathbf{X}^\top \mathbf{X} \sum_{j = 1}^T \lambda_{i,j} \mathbf{b}_j = \mathbf{X}^\top \mathbf{X} \mathbf{b}_i + \mathbf{X}^\top \mathbf{X} \mathbf{b}_i \sum_{j = 1}^T \lambda_{i,j} \\ \implies &amp; (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}_i + \sum_{j = 1}^T \lambda_{i,j} \mathbf{b}_j = \mathbf{b}_i \left(1 + \sum_{j = 1}^T \lambda_{i,j}\right) \\ \implies &amp; (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}_i + \sum_{j = 1}^T \lambda_{i,j} \mathbf{b}_j = \mu_i \mathbf{b}_i &amp; \left(\mu_i = 1 + \sum_{j = 1}^T \lambda_{i,j}\right)\\ \implies &amp; (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}_i = \mu_i \mathbf{b}_i - \sum_{j = 1}^T \lambda_{i,j} \mathbf{b}_j \\ \implies &amp; (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}_i = (\mu_i - \lambda_{i,i}) \mathbf{b}_i - \sum_{j = 1 \\ j \neq i}^T \lambda_{i,j} \mathbf{b}_j \\ &amp;=(\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}_i = (\mu_i - \lambda_{i,i}) \mathbf{b}_i - \sum_{j = 1 \\ j \neq i}^T \lambda_{i,j} \mathbf{b}_j \end{aligned}\] <p>where the substitution of $\mu_i$ relies upon the assumption that $\lambda_{i,j} = \lambda_{j,i}$ for all $i,j$. Organizing $\mathbf{b}_1, \dots, \mathbf{b}_T$ as the columns of a matrix, $\mathbf{B}$, and $\mathbf{y}_1, \dots, \mathbf{y}_T$ into the columns of a matrix, $\mathbf{Y}$, we can rewrite the system of equations for $i \in [T]$ as:</p> \[\begin{aligned} \begin{Bmatrix} (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}_1 = (\mu_1 - \lambda_{1, 1}) \mathbf{b}_1 - \sum_{j = 1 \\ j \neq i}^T \lambda_{1,j} \mathbf{b}_j \\ (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}_2 = (\mu_2 - \lambda_{2, 2}) \mathbf{b}_2 - \sum_{j = 1 \\ j \neq i}^T \lambda_{2,j} \mathbf{b}_j \\ \vdots \\ (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}_T = (\mu_T - \lambda_{T, T}) \mathbf{b}_T - \sum_{j = 1 \\ j \neq i}^T \lambda_{T,j} \mathbf{b}_j \end{Bmatrix} &amp;= \begin{Bmatrix} (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}_1 = (\mu_1 - \lambda_{1, 1}) \mathbf{b}_1 - \lambda_{1, 2} \mathbf{b}_2 - \dots - \lambda_{1, T} \mathbf{b}_T \\ (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}_2 = -\lambda_{2, 1} \mathbf{b}_1 + (\mu_2 - \lambda_{2, 2}) \mathbf{b}_2 - \dots - \lambda_{2, T} \mathbf{b}_T \\ \vdots \\ (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}_T = -\lambda_{T, 1} \mathbf{b}_1 - \lambda_{T,2} \mathbf{b}_{2} - \dots + (\mu_T - \lambda_{T, T}) \mathbf{b}_T \end{Bmatrix} \end{aligned}\] <p>And the righthand side of the above is equivalent to:</p> \[\begin{aligned} &amp;(\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \begin{bmatrix} \rvert &amp; &amp; \rvert \\ \mathbf{y}_1 &amp; \dots &amp; \mathbf{y}_T \\ \rvert &amp; &amp; \rvert \end{bmatrix} = \begin{bmatrix} \rvert &amp; &amp; \rvert \\ \mathbf{b}_1 &amp; \dots &amp; \mathbf{b}_T\\ \rvert &amp; &amp; \rvert \end{bmatrix} \begin{bmatrix} \mu_1 - \lambda_{1, 1} &amp; \dots &amp; -\lambda_{T, 1} \\ \vdots &amp; \ddots &amp; \vdots \\ -\lambda_{1, T} &amp; \dots &amp; \mu_T - \lambda_{T, T} \end{bmatrix} \\ \implies &amp;(\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{Y} = \mathbf{B} \Lambda \\ \implies &amp;(\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{Y} \Lambda^{-1} = \mathbf{B} \end{aligned}\] </li> </ul> </div> <p>Note that the above can easily be generalized by instead specifying a (non-negative, non-increasing) kernel smoother function of $i - j$, $k(i, j)$, instead of scalar penalty terms $\lambda_{i,j}$. Let $\bar{\mathbf{K}}$ denote the matrix where:</p> \[\bar{\mathbf{K}}_{i,j} = \frac{k(i,j)}{\sum_{l = 1}^T k(i, l)}\] <p>For example, since the rows and columns of \(\Lambda^{-1}\) sum to $1$ (because $\Lambda$ is doubly stochastic with non-negative entries), we can define \(\bar{\mathbf{K}}_{i,j} = (\Lambda^{-1})_{i,j}\) as one such kernel matrix. We can then see that Eq. \eqref{eq:model-soln-full-rank} is equivalent to:</p> \[\hat{\mathbf{B}} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{Y} \bar{\mathbf{K}}\] <p>which itself is equivalent to the least squares solution for:</p> \[\mathbb{E}\left[ \mathbf{Y} \bar{\mathbf{K}} \rvert \mathbf{B} \right] = \mathbf{X} \mathbf{B} \hspace{5mm} \iff \hspace{5mm} \frac{1}{\sum_{l = 1}^T k(i, l)} \mathbb{E}\left[\mathbf{Y} \rvert \mathbf{b}_i \right]\begin{bmatrix} k(i, 1) \\ \vdots \\ k(i, T) \end{bmatrix} = \mathbf{X} \mathbf{b}_i \hspace{2mm} \forall i \in [T]\] <p>In <i>Lollipop</i>, they choose the Gaussian kernel:</p> \[k_{G}(i, j; \kappa) = \exp\left( \frac{-(i - j)^2}{2 \kappa}\right); \hspace{8mm} \kappa &gt; 0\] <p>where $\kappa$ must be tuned.</p> <h3 id="uncertainty-quantification">Uncertainty Quantification</h3> <p>The authors provide two methods of getting confidence intervals for $\mathbf{B}$: a bootstrap method and an asymptotic method. The bootstrap method is essentially just resampling of the row indices of $\mathbf{Y}$ with replacement $B$ times. The deconvolution procedure is performed for all of the bootstrap samples, which results in $B$ estimates of $\mathbf{b}_t$ for some time point $t$. The empirical quantiles of the components of estimated variance abundance vector $\mathbf{b}_t$ can be used to make confidence intervals.</p> <p>Asymptotic confidence intervals are computed by assuming some parametric distribution for the responses. A simple version is to assume that for timepoint $t$, the proportion of reads for mutation $j$:</p> \[n \mathbf{y}_{t, j} \sim \text{Binom}(n, \pi_j)\] <p>where $n$ is the total number of reads in the sample. One can then easily derive an $\alpha$-level confidence intervals (assuming independence of reads) from the fact that:</p> \[\frac{\hat{\mathbf{b}}_i - \mathbf{b}_i}{\sqrt{\text{Var}(\hat{\mathbf{b}}_i)}} \overset{asymp.}{\sim} \mathcal{N}(\mathbf{0}_V, \mathbb{I}_{V \times V})\]]]></content><author><name></name></author><category term="virus"/><category term="deconvolution"/><category term="epidemiology"/><category term="primer"/><summary type="html"><![CDATA[A Primer]]></summary></entry><entry><title type="html">Estimation Theory</title><link href="https://aerosengart.github.io/blog/2025/estimation-theory/" rel="alternate" type="text/html" title="Estimation Theory"/><published>2025-06-17T00:00:00+00:00</published><updated>2025-06-17T00:00:00+00:00</updated><id>https://aerosengart.github.io/blog/2025/estimation-theory</id><content type="html" xml:base="https://aerosengart.github.io/blog/2025/estimation-theory/"><![CDATA[<p>Recently, I’ve run into some confusion in parameter estimation. In this post, I’ll cover the relevant sections in Tsiatis’ <i>Semiparametric Theory and Missing Data</i><d-cite key="tsiatis2006"></d-cite> and Newey and McFadden’s <i>Large Sample Estimation and Hypothesis Testing</i><d-cite key="newey1994"></d-cite> in my journey to answering some of my questions:</p> <ol> <li>What is the <i>efficient information matrix</i>?</li> <li>When exactly are the quasi-likelihood estimators asymptotically normal and consistent?</li> <li>Are any regularity conditions violated in the variance componenet estimation scheme for generalized linear mixed models?</li> </ol> <hr/> <h2 id="set-up">Set-Up</h2> <p>We assume we have independent and identically distributed random variables, $X_1, \dots, X_n$, arranged into a vector $\mathbf{X}$ defined on the probability space $(\mathcal{X}, \mathcal{A}, P)$. We assume $X_i \sim P$ where $P \in \mathcal{P}$ for some family of distributions, $\mathcal{P}$. We also assume that $P$ is defined by the values of parameters, $\theta$, where some coordinates are of interest and some are a nuisance (i.e. we can partition the parameter space). We use $\beta$ and $\eta$ to denote the parameters of interest and the nuisance parameters, respectively. We have:</p> \[\begin{equation} \label{eq:parameter-vector} \theta = (\beta^\top, \eta^\top)^\top \in \mathbb{R}^{p \times 1} \hspace{2mm} (p = q + r), \hspace{8mm} \beta \in \mathbb{R}^{q \times 1}, \hspace{8mm} \eta \in \mathbb{R}^{r \times 1} \end{equation}\] <p>As in Tsiatis’ book, we’ll restrict the parameter space, $\Omega$, to subspaces of linear vector spaces, which themselves could be finite- or infinite-dimensional (depending on whether there exist a finite number of elements in it that span it). In most applications/examples, this assumption will not be too restrictive since we will usually be working in Euclidean spaces.</p> <h3 id="notation">Notation</h3> <p>Uppercase letters denote random variables, and lowercase letters denote realizations of these variables. Boldface indicates a vector or matrix, exactly which will be clear from context.</p> <p>We will indicate parameters that are functions with parenthesis (e.g. $\gamma(\cdot)$), and we’ll use $\hat{\gamma}_n$ to denote an estimator of parameter $\gamma$. We’ll use a subscript of $0$ to denote the true parameter values (i.e. $\theta_0 = (\beta_0^\top, \eta_0^\top)^\top$).</p> <p>$\nu_X(x)$ denotes the <i>dominating measure</i> on which densities for $\mathbf{X}$ are defined. That is, the dominating measure is any measure $\mu(\cdot)$ such that the density $f_\mathbf{X}(\mathbf{x})$ exists and such that $\mu’(A) = 0$ implies $\mu(A) = 0$. For continuous $X$, we usually use the Lebesgue measure, and for discrete $X$, we use the counting measure.</p> <p>I am pretty sure all of the expectations are taking with respect to the true data-generating distribution, $P_X(x, \theta_0)$. I’ve denoted this with a subscript of $\theta_0$ on the expectation, but I may have missed a few.</p> <p>Convergence in probability and convergence in distribution when the density of the random variable $X$ is equal to $p_X(x; \beta, \eta)$ are denoted by, respectively:</p> \[\xrightarrow{P\{ \beta, \eta \}}, \hspace{10mm} \xrightarrow{D\{ \beta, \eta \}}\] <p>General convergence in probability and distribution are denoted by:</p> \[\overset{p}{\rightarrow}, \hspace{10mm} \rightsquigarrow\] <hr/> <h2 id="hilbert-spaces">Hilbert Spaces</h2> <p>Before we dive into our discussion of semiparametric inference, we need to first look at some geometric definitions.</p> <p>Consider the space of functions of the form $h(X)$ where $h: \mathcal{X} \rightarrow \mathbb{R}^q$, are measurable (w.r.t. the probability space we described earlier), and are such that:</p> \[\mathbb{E}[h(X)] = 0, \hspace{10mm} \mathbb{E}\left[ h^\top(X) h(X)\right] &lt; \infty\] <p>That is, we consider the space of all measurable, $q$-dimensional random functions with mean zero and finite variance. Notice that this space is a linear space. Let $\mathbf{0}$ (the constant function outputting a $q$-dimensional vector of zeros) denote the origin. Though these are random functions, we will sometimes drop the $(X)$ from our notation to just write $h$.</p> <p>Later we will concern ourselves with Hilbert spaces of these types of functions. We define these formally for completeness. Recall that a Hilbert space is a complete normed linear vector space with an inner product.</p> <div id="def-hilbert-space"></div> <div class="definition"> <strong>Definition (Hilbert Space of Mean-Zero Random Functions).</strong> <br/> Consider the space of $q$-dimensional random functions $h: \mathcal{X} \rightarrow \mathbb{R}^q$ that are measurable (w.r.t. probability space $(\mathcal{X}, \mathcal{A}, P)$) and satisfy: $$ \mathbb{E}[h(X)] = 0, \hspace{10mm} \mathbb{E}\left[ h^\top(X) h(X)\right] &lt; \infty $$ Define the <i>covariance inner product</i> as: $$ \langle h_1, h_2 \rangle = \mathbb{E}\left[ h_1^\top(X) h_2(X) \right], \hspace{10mm} h_1, h_2 \in \mathcal{H} $$ and denote the norm induced by this inner product with: $$ \rvert \rvert h \rvert \rvert = \langle h, h \rangle^{1/2} $$ This space, denoted by $\mathcal{H}$, is a normed linear vector space with an inner product. By the $L_2$-completeness theorem, it is also complete, so it is a Hilbert space. </div> <p>Our Hilbert spaces of interest (defined above) are dependent upon the true value $\theta_0$, and this is the value with respect to which we take the expectation in the inner product. Thus, the space will change if $\theta_0$ changes.</p> <p>We come to a theorem that will be helpful later in some definitions and results.</p> <div id="projection-theorem"></div> <div class="theorem"> <strong>Theorem (Projection Theorem).</strong> <ul id="project-theorem" class="tab" data-tab="1033d715-0baa-4c28-9085-849f73fab238" data-name="project-theorem"> <li class="active" id="project-theorem-statement"> <a href="#">statement </a> </li> <li id="project-theorem-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="1033d715-0baa-4c28-9085-849f73fab238" data-name="project-theorem"> <li class="active"> <p>Let $\mathcal{H}$ denote the Hilbert space <a href="#def-hilbert-space">defined above</a>, and let $\mathcal{U}$ denote a closed linear subspace. For any $h \in \mathcal{H}$, there exists a unique $u_0 \in \mathcal{U}$ such that:</p> \[\rvert \rvert h - u_0 \rvert \rvert \leq \rvert \rvert h - u \rvert \rvert, \hspace{10mm} \forall u \in \mathcal{U}\] <p>We call $u_0$ the <i>projection of $h$ onto $\mathcal{U}$</i>, and we usually use $\Pi (h \rvert \mathcal{U})$ to denote it. $\Pi(h \rvert \mathcal{U})$ satisfies:</p> \[\langle \Pi(h \rvert \mathcal{U}), u \rangle = 0, \hspace{10mm} \forall u \in \mathcal{U}\] </li> <li> <p>Proof to be completed.</p> </li> </ul> </div> <p>A version of the Pythagorean theorem and the Cauchy-Schwarz inequality can be derived as a result of the Projection theorem.</p> <hr/> <h2 id="extremum-estimators">Extremum Estimators</h2> <p>Before we dive into more specifics, let’s define a few different classes of estimators.</p> <div class="definition"> <strong>Definition (Extremum Estimator).</strong> <br/> Let $\hat{Q}_n(X, \theta)$ be some objective function the depends on some data, $X$, and a sample size, $n$. An <i>extremum estimator</i> is $\hat{\theta}_n$ such that: $$ \hat{\theta}_n = \underset{\theta \in \Omega}{\arg \max} \left\{ \hat{Q}_n(X, \theta) \right\} $$ </div> <p>The name comes from the fact that we are maximizing the objective (i.e. finding an extreme value). An example is the maximum likelihood estimator (MLE), which has the (normalized) log-likelihood as its objective function:</p> \[\hat{Q}_n(X, \theta) = \frac{1}{n} \sum_{i = 1}^n \log (p_X(x_i; \theta))\] <p>In certain cases, the maximization can be done by taking the derivative and setting this equal to zero. However, we should be cautious as there are cases when there are many such solutions. In this case, a solution to this first-order condition may not be the global maximum of $\hat{Q}_n(X, \theta)$ (i.e. a solution may be a local maximum). Luckily, if the extremum estimator is consistent and $\theta_0$ is on the interior of $\Omega$, then the extremum estimator will be included in the set of solutions to setting the derivative equal to zero.</p> <p>The extremum estimator class is quite broad. A subclass of extremum estimator is the $m$-estimator.</p> <div id="m-estimator"></div> <div class="definition"> <strong>Definition ($m$-Estimator).</strong> <br/> Let $m(X, \theta)$ be a $p \times 1$-dimensional function of $X$ satisfying: $$ \mathbb{E}_\theta[m(X, \theta)] = \mathbf{0}_{p \times 1}, \hspace{10mm} \mathbb{E}_\theta\left[ m^\top(X, \theta) m(X, \theta)\right] &lt; \infty, \hspace{10mm} \mathbb{E}_\theta \left[ m(X, \theta) m^\top(X, \theta) \right] \text{ positive definite } \forall \theta \in \Omega $$ The $m$-estimator of $\theta$ is the solution (if it exists) of: $$ \sum_{i = 1}^n m(X_i, \hat{\theta}_n) = 0, \hspace{10mm} X_1, \dots, X_n \overset{iid}{\sim} p_X(x; \theta), \hspace{2mm} \theta \in \Omega \subset \mathbb{R}^p $$ Note: if $m(X, \theta)$ is differentiable, then take the derivative, set equal to $\mathbf{0}$, and solve. In this case, we call $\hat{\theta}_n$ an $m$-estimator of <i>$\psi$-type</i>. Otherwise, it is of <i>$\rho$-type</i>(see <a href="https://en.wikipedia.org/wiki/M-estimator">here</a>). </div> <p>Put simply, an extremum estimator that maximizes a sample average is an $m$-estimator[^fn-newey]. Before we look at an example, let’s define the <i>score</i>.</p> <div class="definition"> <body> <strong>Definition (Score).</strong> <br/> Suppose $X \sim p_X(x; \theta)$ where $\theta = (\beta^\top, \eta^\top)^\top$. The <i>score function</i>, $U_\theta(X, \theta)$, is defined as the vector of first order partial derivatives of the log-likelihood with respect to the parameter vector: $$ U_\theta(x, \theta^*) = \frac{\partial \log(p_X(x, \theta))}{\partial \theta} \bigg\rvert_{\theta = \theta^*} $$ where $\theta^*$ is some value of $\theta$. When evaluated at the true value of $\theta$ (the value that generated the data), we call $U_\theta(x, \theta_0)$ the <i>score vector</i>. </body> </div> <p>The MLE is also an example of an $m$-estimator. Consider the function $m(X, \theta) = \log (p_X(x; \theta))$, the log-likelihood. We can maximize the log-likelihood by taking the derivative with respect to $\theta$ and setting this equal to zero. This is called the <i>score equation in $\theta$</i>:</p> \[\sum_{i = 1}^n U_\theta(X_i, \theta) = 0\] <h3 id="influence-functions">Influence Functions</h3> <p>Some estimators (asymptotically linear ones…to be defined later) can be analyzed with respect to a particular defining function, called the <i>influence function</i>.</p> <div class="definition"> <body> <strong>Definition (Influence Function).</strong> <br/> Let $\hat{\theta}_n$ be an estimator for parameter $\theta$. Define the $q$-dimensional measurable random function (i.e. a random vector) $\psi(X, \theta_0)$ such that $\mathbb{E}_{\theta_0}[\psi(X, \theta_0)] = \mathbf{0}$, $\mathbb{E}_{\theta_0}[\psi(X, \theta_0)\psi^\top(X, \theta_0)]$ exists, and: $$ \sqrt{n}(\hat{\theta}_n - \theta) = \frac{1}{\sqrt{n}}\sum_{i = 1}^n \psi(X_i, \theta_0) + o_p(1) $$ where $o_p(1)$ indicates a term that converges in probability to $0$. We call $\psi(X_i, \theta_0)$ the <i>$i$-th influence function of $\hat{\theta}_n$</i> or the <i>influence function of the $i$-th observations of $\hat{\theta}_n$</i>. </body> </div> <p>Note that the influence function is always defined with respect to the true data-generating distribution; it is evaluated at $\theta_0$, and the expectations are always taken with respect to the parameter being $\theta_0$. Thus, we don’t really need to write $\psi(X, \theta_0)$, since the influence function is not a function of $\theta$ (it is just dependent on $\theta_0$). However, I keep the full notation to avoid confusion.</p> <p>We can also define special properties of estimators to help us decribe the ones that are desirable. We say that $\hat{\theta}_n$ is <i>asymptotically linear</i> if it has an influence function(s). Thus, the maximum likelihood estimator is asymptotically linear.</p> <p>The influence function(s) provides us a clear way to analyze the behavior of our (asymptotically linear) estimator as $n \rightarrow \infty$:</p> \[\begin{equation} \label{eq:al-dist} \begin{aligned} \frac{1}{\sqrt{n}}\sum_{i = 1}^n \psi(X_i, \theta_0) &amp;\rightsquigarrow \mathcal{N}(\mathbf{0}_{q \times q}, \mathbb{E}_{\theta_0}\left[ \psi(X, \theta_0) \psi(X, \theta_0)^\top \right]) &amp; \left(\text{CLT}\right) \\ \sqrt{n}(\hat{\theta}_n - \theta_0) &amp;\rightsquigarrow \mathcal{N}(\mathbf{0}_{q \times q}, \mathbb{E}_{\theta_0}\left[ \psi(X, \theta_0) \psi(X, \theta_0)^\top \right]) &amp; \left(\text{Slutsky's theorem}\right) \end{aligned} \end{equation}\] <p>Eq. \eqref{eq:al-dist} implies that the asymptotic variance of the estimator is the variance of its influence function. Furthermore, an asymptotically linear estimator is (effectively) uniquely identified by its influence function.</p> <div class="theorem"> <strong>Theorem 3.1.<d-cite key="tsiatis2006"></d-cite></strong> <ul id="tsiatis-3-1" class="tab" data-tab="3dbdd1f1-bd75-45e8-a949-d2fc480193fc" data-name="tsiatis-3-1"> <li class="active" id="tsiatis-3-1-statement"> <a href="#">statement </a> </li> <li id="tsiatis-3-1-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="3dbdd1f1-bd75-45e8-a949-d2fc480193fc" data-name="tsiatis-3-1"> <li class="active"> <p>An asymptotically linear estimator has an almost surely unique influence function.</p> </li> <li> <p>Tsiatis proceeds by contradiction. Let our estimator be denoted by $\hat{\theta}_n$ with true value $\theta_0$, and let $n$ be our sample size. Let $\psi(X, \theta_0)$ be an influence function, and assume that there exists another influence function $\psi^*(X, \theta_0)$. Thus:</p> \[\mathbb{E}_{\theta_0}[\psi(X, \theta_0)] = \mathbb{E}_{\theta_0}[\psi^*(X, \theta_0)] = 0, \hspace{10mm} \sqrt{n}(\hat{\theta}_n - \theta_0) = \frac{1}{\sqrt{n}}\sum_{i = 1}^n \psi(X_i, \theta_0) + o_p(1) = \frac{1}{\sqrt{n}}\sum_{i = 1}^n \psi^*(X_i, \theta_0) + o_p(1)\] <p>Recall that $X_1, \dots, X_n$ are i.i.d., so, by the central limit theorem:</p> \[\frac{1}{\sqrt{n}}\sum_{i = 1}^n (\psi(X_i, \theta_0) - \psi^*(X_i, \theta_0)) \rightsquigarrow \mathcal{N}\left(\mathbf{0}, \mathbb{E}_{\theta_0}\left[ (\psi(X, \theta_0) - \psi^*(X, \theta_0))(\psi(X, \theta_0) - \psi^*(X, \theta_0))^\top\right] \right) \nonumber\] <p>However, by the <a href="https://en.wikipedia.org/wiki/Continuous_mapping_theorem">continuous mapping theorem</a>:</p> \[\begin{aligned} &amp;\frac{1}{\sqrt{n}}\sum_{i = 1}^n (\psi(X_i, \theta_0) - \psi^*(X_i, \theta_0)) = o_p(1) \\ \implies &amp;\underset{n \rightarrow \infty}{\lim} \mathbb{P}\left( \bigg\rvert \frac{1}{\sqrt{n}}\sum_{i = 1}^n (\psi(X_i, \theta_0) - \psi^*(X_i, \theta_0)) \bigg\rvert \geq \epsilon \right) = 0, \hspace{5mm} \forall \epsilon &gt; 0 \end{aligned}\] <p>For both of the above to be true, we need:</p> \[\mathbb{E}_{\theta_0}\left[ (\psi(X, \theta_0) - \psi^*(X, \theta_0))(\psi(X, \theta_0) - \psi^*(X, \theta_0))^\top\right] = \mathbf{0}_{q \times q}\] <p>which implies $\psi(X, \theta_0) = \psi^*(X, \theta_0)$ almost surely.</p> </li> </ul> </div> <p>The score vector also satisfies nice properties, which we summarize in the following theorem. First, let’s define what it means for an estimator to be “regular”.</p> <div class="definition"> <strong>Definition (Regular Estimator).</strong> <br/> Suppose we are considering a family of distributions, $\mathcal{P}$, indexed by parameter $\theta = (\beta^\top, \eta^\top)^\top$. Let $\theta^*$ be some fixed value for $\theta$. We refer to this set-up as a <i>local data generating process (LDGP)</i>. <br/> For each value of $n$, we assume we have $n$ data points: $$ X_{1, n}, X_{2, n}, \dots, X_{n, n} \overset{iid}{\sim} P(X, \theta_n) $$ where $P(X, \theta_n) \in \mathcal{P}$ is the distribution from the family where $\theta = \theta_n$ and such that: $$ \underset{n \rightarrow \infty}{\lim} \sqrt{n}(\theta_n - \theta^*) \rightarrow \mathbf{c} $$ where $\mathbf{c}$ is some constant vector. This condition implies the two values of $\theta$ are "close". <br/> An estimator $\hat{\theta}_n$ of $\theta$ is called <i>regular</i> if, for each $\theta^*$, the asymptotic distribution of $\sqrt{n}(\hat{\theta}_n - \theta)$ does not depend on the LDGP. That is, if the limiting distribution does not depend on $\theta_n$. </div> <p>In many cases, the maximum likelihood estimator is regular. When an estimator is both asymptotically linear and regular, we say it is <i>RAL</i>.</p> <div id="theorem-3-2"></div> <div class="theorem"> <strong>Theorem 3.2, Corollary 1.<d-cite key="tsiatis2006"></d-cite></strong> <ul id="t-3-2-cor-1" class="tab" data-tab="d7d03961-e227-430a-9d00-b3bdcc0e708b" data-name="t-3-2-cor-1"> <li class="active" id="t-3-2-cor-1-statement"> <a href="#">statement </a> </li> <li id="t-3-2-cor-1-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="d7d03961-e227-430a-9d00-b3bdcc0e708b" data-name="t-3-2-cor-1"> <li class="active"> <p>Let $\beta(\theta)$ be a $q$-dimensional function of $p$-dimensional parameter $\theta$ such that $q &lt; p$. Assume the following exists:</p> \[\begin{equation} \label{eq:gamma} \Gamma(\theta) = \frac{\partial \beta(\theta)}{\partial \theta^\top} \end{equation}\] <p>which is the $q \times p$ matrix of first order partial derivatives of vector $\beta(\theta)$ with respect to $\theta$, and assume it is continuous in $\theta$ in a neighborhood of $\theta_0$, the true parameter value. Let \(\hat{\beta}_n\) denote an asymptotically linear estimator with influence function $\psi(X, \theta_0)$ such that \(\mathbb{E}_\theta[\psi^\top(X, \theta_0) \psi(X, \theta_0)]\) exists and is also continuous in $\theta$ in a neighborhood of $\theta_0$. If $\hat{\beta}_n$ is regular, then:</p> \[\begin{equation} \label{eq:condition-3-2} \mathbb{E}_{\theta_0}\left[ \psi(X,\theta_0) U_\theta^\top(X, \theta_0) \right] = \Gamma(\theta_0) \end{equation}\] <p>If the parameter space can be partitioned as $\theta = (\beta^\top, \eta^\top)^\top$ where $\beta$ is $q$-dimensional and $\eta$ is $r$-dimensional, then:</p> \[\begin{equation} \label{eq:corollary-1} \mathbb{E}_{\theta_0}\left[ \psi(X, \theta_0) U_\beta^\top(X, \theta_0)\right] = \mathbb{I}_{q \times q}, \hspace{10mm} \mathbb{E}_{\theta_0}\left[ \psi(X, \theta_0) U_\eta^\top(X, \theta_0) \right] = \mathbf{0}_{q \times r} \end{equation}\] <p>where:</p> \[U_\beta(x, \theta_0) = \frac{\partial \log(p_X(x, \theta))}{\partial \beta} \bigg\rvert_{\theta = \theta_0}, \hspace{10mm} U_\eta(x, \theta_0) = \frac{\partial \log(p_X(x, \theta))}{\partial \eta} \bigg\rvert_{\theta = \theta_0},\] </li> <li> <p>See pg. 34 in <d-cite key="tsiatis2006"></d-cite>.</p> </li> </ul> </div> <p>Note that the influence functions for RAL estimators are in the subspace of elements of $\mathcal{H}$ (the Hilbert space of mean-zero measurable random functions) satisfying Eq. \eqref{eq:corollary-1}. Similarly, each element in that subspace is the influence function for some RAL estimator.</p> <p>From this geometric perspective, we can see that the asymptotic variance of an RAL estimator is basically the squared distance between the origin and its influence function in our special Hilbert space.</p> <hr/> <h2 id="consistency">Consistency</h2> <p>We can show under what conditions extremum estimators are consistent ($\hat{\theta}_n \overset{p}{\rightarrow} \theta_0$). This is called the <i>Basic Consistency Theorem</i> by Newey and McFadden. In what follows, we introduce the probability limit, $Q_0(X, \theta)$, of $\hat{Q}_n(X, \theta)$, which is the quantity described in <a href="#uniform-convergence">the appendix</a>.</p> <div id="theorem-2-1"></div> <div class="theorem"> <strong>Theorem 2.1.<d-cite key="newey1994"></d-cite></strong> <ul id="newey-2-1" class="tab" data-tab="c929c80c-71d4-4059-94e5-bb2f6c6e5697" data-name="newey-2-1"> <li class="active" id="newey-2-1-statement"> <a href="#">statement </a> </li> <li id="newey-2-1-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="c929c80c-71d4-4059-94e5-bb2f6c6e5697" data-name="newey-2-1"> <li class="active"> <p>Let $\hat{Q}_n(X,\theta)$ be the objective function for an extremum estimator, $\hat{\theta}_n$. If there exists a function $Q_0(\theta)$ satisfying:</p> <ol> <li>Identification: $Q_0(X,\theta)$ has a unique maximum at $\theta_0$, the true value of $\theta$</li> <li>Boundedness: $\Omega$, the parameter space, is compact</li> <li>Continuity: $Q_0(X,\theta)$ is continuous</li> <li>Uniform Convergence: $\hat{Q}_n(X, \theta)$ converges uniformly in probability to $Q_0(X, \theta)$</li> </ol> <p>then $\hat{\theta}_n$ is consistent; i.e.:</p> \[\hat{\theta}_n \overset{p}{\rightarrow} \theta_0\] </li> <li> <p>See pg. 2121 in <d-cite key="newey1994"></d-cite></p> </li> </ul> </div> <p>The authors note that the some of the conditions can be relaxed. Instead of assuming that $\hat{\theta}_n$ maximizes $\hat{Q}_n(X, \theta)$, we can instead assume that it “nearly” maximizes it:</p> \[\hat{Q}_n(\hat{\theta}_n) \geq \underset{\theta \in \Omega}{\sup} \hat{Q}_n(\theta) + o_p(1)\] <p>The second condition can be relaxed if the objective function, $\hat{Q}_n(X, \theta)$, is concave. Then the assumption of compactness of the parameter space, $\Omega$, can be exchanged for just convexity.</p> <p>In addition, the third condition can be relaxed to just upper semi-continuity rather than continuity proper. That is, we assume that, for any $\theta \in \Omega$ and any $\epsilon &gt; 0$, there exists an open subset $\mathcal{B} \subset \Omega$ such that $\theta \in \mathcal{B}$ and such that:</p> \[Q_0(X, \theta') &lt; Q_0(X, \theta) + \epsilon \hspace{5mm} \forall \theta' \in \mathcal{B}\] <p>The fourth condition can be changed to just require that:</p> \[\hat{Q}_n(X, \theta_0) \overset{p}{\rightarrow} Q_0(\theta_0), \hspace{5mm} \text{and} \hspace{5mm} \hat{Q}_n(X, \theta) &lt; Q_0(X, \theta) + \epsilon \hspace{5mm} \forall \epsilon &gt; 0, \hspace{1mm} \forall \theta \in \Omega\] <p>with probability approaching $1$. If we make the stronger assumption that:</p> \[\underset{\theta \in \Omega}{\sup} \big\rvert \hat{Q}_n(X, \theta) - Q_0(X, \theta) \big\rvert \overset{as}{\rightarrow} 0\] <p>instead of the fourth condition, then we have that $\hat{\theta}_n \overset{as}{\rightarrow} \theta_0$ (i.e. $\hat{\theta}_n$ is <i>strongly consistent</i>).</p> <h3 id="showing-consistency">Showing Consistency</h3> <p>In order to use <a href="#theorem-2-1">Theorem 2.1</a>, one must be able to show that the conditions (or their relaxations) hold. This can be difficult in practice, we often try to show some other property that are sufficient for the conditions. Newey and McFadden call these <i>primitive conditions</i>.</p> <div id="lemma-2-4"></div> <div class="theorem"> <strong>Lemma 2.4.<d-cite key="newey1994"></d-cite></strong> <ul id="newey-2-4" class="tab" data-tab="e08071ca-39b7-4d23-abc6-788b4d3f8dfb" data-name="newey-2-4"> <li class="active" id="newey-2-4-statement"> <a href="#">statement </a> </li> <li id="newey-2-4-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="e08071ca-39b7-4d23-abc6-788b4d3f8dfb" data-name="newey-2-4"> <li class="active"> <p>Let $A(X, \theta)$ be a matrix of functions of observation $X$ and parameter $\theta$. Let $\rvert \rvert A \rvert \rvert$ denote the Euclidean norm. Let $\Omega$ be a compact parameter space, and suppose our data are independent and identically distributed.</p> <p>Suppose that each element of $A(X, \theta)$ is continuous at each $\theta \in \Omega$ with probability one, and suppose that there exists a function $d(X)$ such that $\rvert \rvert A(X, \theta) \rvert \rvert \leq d(X)$ for all $\theta \in \Omega$. Assume $\mathbb{E}[d(X)] &lt; \infty$. Then:</p> \[\mathbb{E}[A(X, \theta)] \text{ is continuous}\] <p>and:</p> \[\underset{\theta \in \Omega}{\sup} \left\vert \left\vert\frac{1}{n} \sum_{i = 1}^n \left(A(X_i, \theta) - \mathbb{E}[A(X, \theta)] \right) \right\vert \right\vert \overset{p}{\rightarrow} 0\] </li> <li> <p>Proof to be completed.</p> </li> </ul> </div> <p>The above lemma can be used for sample averages, which is exactly what we deal with in maximum likelihood estimation (and several other estimation settings). This leads us to the next theorem that states that, under certain conditions, the MLE is consistent:</p> <div id="lemma-2-5"></div> <div class="theorem"> <strong>Lemma 2.5.<d-cite key="newey1994"></d-cite></strong> <ul id="newey-2-5" class="tab" data-tab="3befd12a-8c36-490f-8905-0efdcab5335e" data-name="newey-2-5"> <li class="active" id="newey-2-5-statement"> <a href="#">statement </a> </li> <li id="newey-2-5-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="3befd12a-8c36-490f-8905-0efdcab5335e" data-name="newey-2-5"> <li class="active"> <p>Let $X_1, X_2, \dots$ are i.i.d. data and let $p_X(x_i, \theta_0)$ denote their probability density function. If the following conditions are met, then $\hat{\theta}_n \overset{p}{\rightarrow} \theta_0$ (the MLE is consistent):</p> <ol> <li>$\theta \neq \theta_0$ implies $p_X(x_i, \theta) \neq p_X(x_i, \theta_0)$ for any $\theta \in \Omega$</li> <li>$\theta_0 \in \Omega$ where $\Omega$ is compact</li> <li>$\log(p_X(x_i, \theta))$ is continuous for all $\theta \in \Omega$ with probability one</li> <li> $\mathbb{E}_{\theta_0}\left[ \underset{\theta \in \Omega}{\sup} \left\{ \left\vert \log(p_X(x, \theta)) \right\vert \right\} \right] &lt; \infty$</li> </ol> </li> <li> <p>The result follows from ensuring the conditions of <a href="#theorem-2-1">Theorem 2.1</a> are satisfied and then applying <a href="#lemma-2-4">Lemma 2.4</a>.</p> </li> </ul> </div> <hr/> <h2 id="asymptotic-normality">Asymptotic Normality</h2> <p>To construct confidence intervals, we often rely upon asymptotical normality of an estimator.</p> <div id="theorem-3-1"></div> <div class="theorem"> <strong>Theorem 3.1.<d-cite key="newey1994"></d-cite></strong> <ul id="newey-3-1" class="tab" data-tab="1630663e-4841-40df-bd78-3782d5b33929" data-name="newey-3-1"> <li class="active" id="newey-3-1-statement"> <a href="#">statement </a> </li> <li id="newey-3-1-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="1630663e-4841-40df-bd78-3782d5b33929" data-name="newey-3-1"> <li class="active"> <p>Let $\hat{\theta}_n$ be an extremum estimator; that is, it maximizes some objective function $\hat{Q}_n(X, \theta)$ subject to $\theta \in \Omega$ give a sample size of $n$. If the following conditions are satisfied:</p> <ol> <li>$\hat{\theta}_n \overset{p}{\rightarrow} \theta_0$</li> <li>$\theta_0 \in \text{ interior of } \Omega$</li> <li>$\hat{Q}_n(X, \theta)$ is twice continuously differentiable in a neighborhood $\mathcal{B}$ of $\theta_0$</li> <li>$\sqrt{n} \frac{\partial \hat{Q}_n(X,\theta)}{\partial \theta} \bigg\rvert_{\theta = \theta_0} \rightsquigarrow \mathcal{N}(\mathbf{0}, \Sigma)$</li> <li>There exists matrix function $H(\theta)$ that is continuous at $\theta_0$ and satisfies $\underset{\theta \in \mathcal{B}}{\sup} \left\vert \left\vert \frac{\partial^2 \hat{Q}_n(X, \theta)}{\partial \theta \partial \theta^\top} - H(\theta) \right\vert\right\vert \overset{p}{\rightarrow} 0$</li> <li>$H = H(\theta_0)$ is non-singular</li> </ol> <p>Then:</p> \[\begin{equation} \label{eq:asymptotic-dist} \sqrt{n}(\hat{\theta}_n - \theta_0) \rightsquigarrow \mathcal{N}\left(\mathbf{0}, H^{-1} \Sigma H^{-1}\right) \end{equation}\] <p>where $\Sigma$ is the asymptotic variance of \(\sqrt{n} \left[ \frac{\partial \hat{Q}_n(X, \theta)}{\partial \theta} \right] \bigg\rvert_{\theta = \theta_0}\), and \(H = \underset{n \rightarrow \infty}{\text{plim}} \left[ \left. \frac{\partial^2 \hat{Q}_n(\theta)}{\partial \theta \partial \theta^\top} \right\vert_{\theta = \theta_0} \right]\).</p> </li> <li> <p>See Section 3.5 in <d-cite key="newey1994"></d-cite>.</p> </li> </ul> </div> <p>Applied to maximum likelihood estimators, <a href="#theorem-3-1">Theorem 3.1</a> gives:</p> \[\sqrt{n}(\hat{\theta}_n - \theta_0) \rightsquigarrow \mathcal{N}(\mathbf{0}, \mathcal{I}^{-1}(\theta_0))\] <details> <summary>Intuition.</summary> The more expansive explanation of the intuition is provided at the beginning of Section 3 in Newey and McFadden, but we summarize it here for convenience. <br/> For MLE, $\hat{Q}_n(\theta) = \frac{1}{n} \sum_{i = 1}^n \log(p_X(x_i, \theta))$. Suppose that the log-likelihood is differentiable and the maximum likelihood estimate, $\hat{\theta}$, is on the interior of the parameter space. This implies that the gradient of the log-likelihood will be equal to $0$ when evaluated at $\hat{\theta}_n$: $$ \left. \left[ \frac{1}{n}\sum_{i = 1}^n \frac{\partial \log(p_X(x_i, \theta))}{\partial \theta}\right] \right\vert_{\theta = \hat{\theta}_n} = 0 \nonumber $$ Suppose also that the log-likelihood is twice differentiable and the second derivative is continuous. We can apply the <a href="https://en.wikipedia.org/wiki/Mean_value_theorem">mean value theorem</a> (MVT) to each element of the gradient to rewrite the first-order condition. Let $\bar{\theta}$ be the vector of intermediate points between each coordinate of $\hat{\theta}_n$ and $\theta_0$ that are used in the MVT. The $(i,j)$-th element is: $$ \left. \left[ \frac{1}{n} \sum_{i = 1}^n \frac{\partial^2 \log(p_X(x_i, \theta))}{\partial \theta_i \partial \theta_j} \right] \right\vert_{\theta = \bar{\theta}} = \frac{1}{(\hat{\theta}_n)_j - (\theta_0)_j} \left( \left. \left[ \frac{1}{n}\sum_{i = 1}^n \frac{\partial \log(p_X(x_i, \theta))}{\partial \theta_i}\right] \right\vert_{\theta = \hat{\theta}_n} - \left. \left[ \frac{1}{n}\sum_{i = 1}^n \frac{\partial \log(p_X(x_i, \theta))}{\partial \theta_i}\right] \right\vert_{\theta = \theta_0}\right) \nonumber $$ Using the fact that the gradient of the log-likelihood evaluated at the maximum likelihood estimate is $0$: $$ ((\hat{\theta}_n)_j - (\theta_0)_j) \left. \left[ \frac{1}{n} \sum_{i = 1}^n \frac{\partial^2 \log(p_X(x_i, \theta))}{\partial \theta_i \partial \theta_j} \right] \right\vert_{\theta = \bar{\theta}} = - \left. \left[ \frac{1}{n}\sum_{i = 1}^n \frac{\partial \log(p_X(x_i, \theta))}{\partial \theta_i}\right] \right\vert_{\theta = \theta_0} \nonumber $$ And rearranging terms: $$ (\hat{\theta_j} - (\theta_0)_j) \left. \left[ \frac{1}{n} \sum_{i = 1}^n \frac{\partial^2 \log(p_X(x_i, \theta))}{\partial \theta_i \partial \theta_j} \right] \right\vert_{\theta = \bar{\theta}} + \left. \left[ \frac{1}{n}\sum_{i = 1}^n \frac{\partial \log(p_X(x_i, \theta))}{\partial \theta_i}\right] \right\vert_{\theta = \theta_0} = 0 \nonumber $$ Rewriting in matrix notation: $$ \left. \left[ \frac{1}{n} \sum_{i = 1}^n \frac{\partial^2 \log(p_X(x_i, \theta))}{\partial \theta \partial \theta^\top} \right] \right\vert_{\theta = \bar{\theta}}(\hat{\theta}_n - (\theta_0)) + \left. \left[ \frac{1}{n}\sum_{i = 1}^n \frac{\partial \log(p_X(x_i, \theta))}{\partial \theta}\right] \right\vert_{\theta = \theta_0} = \mathbf{0} \nonumber $$ Multiplying by $\sqrt{n}$ and rearranging again: $$ \sqrt{n}(\hat{\theta_j} - (\theta_0)_j) = - \left[ \left. \left[ \frac{1}{n} \sum_{i = 1}^n \frac{\partial^2 \log(p_X(x_i, \theta))}{\partial \theta \partial \theta^\top} \right] \right\vert_{\theta = \bar{\theta}} \right]^{-1} \left. \left[ \frac{\sqrt{n}}{n}\sum_{i = 1}^n \frac{\partial \log(p_X(x_i, \theta))}{\partial \theta_i}\right] \right\vert_{\theta = \theta_0} \nonumber $$ As the score, the second term has zero mean. In addition, it is the sum of (functions of) i.i.d. random variables, so we can apply the central limit theorem to see that: $$ \left. \left[ \frac{\sqrt{n}}{n} \sum_{i = 1}^n \frac{\partial \log(p_X(x_i, \theta))}{\partial \theta_i}\right] \right\vert_{\theta = \theta_0} \rightsquigarrow \mathcal{N}\left(\mathbf{0}, \mathcal{I}(\theta_0)\right) \nonumber $$ where $\mathcal{I}(\theta_0) = \text{Var}\left( \left. \left[ \frac{1}{n} \sum_{i = 1}^n \frac{\partial \log(p_X(x_i, \theta))}{\partial \theta_i}\right] \right\vert_{\theta = \theta_0} \right)$. <br/> Since $\bar{\theta}$ is just a function of $\hat{\theta}_n$ (since $\theta_0$ is fixed), if $\hat{\theta}_n$ is consistent, then $\bar{\theta}$ should also be. Supposing we can use a uniform law of large numbers (see <a href="#lemma-2-4">Lemma 2.4</a>), then: $$ \left. \left[ \frac{1}{n} \sum_{i = 1}^n \frac{\partial^2 \log(p_X(x_i, \theta))}{\partial \theta \partial \theta^\top} \right] \right\vert_{\theta = \bar{\theta}} \overset{p}{\rightarrow} \mathbb{E}_{\theta_0}\left[ \left. \left[ \frac{1}{n} \sum_{i = 1}^n \frac{\partial^2 \log(p_X(x_i, \theta))}{\partial \theta \partial \theta^\top} \right] \right\vert_{\theta = \theta_0} \right] = H \nonumber $$ where the righthand side is the expected Hessian, denoted by $H$. <br/> Since we assumed the log-likelihood is twice continuously differentiable, the <a href="https://en.wikipedia.org/wiki/Inverse_function_theorem">inverse function theorem</a> gives us: $$ -\left[\left. \left[ \frac{1}{n} \sum_{i = 1}^n \frac{\partial^2 \log(p_X(x_i, \theta))}{\partial \theta \partial \theta^\top} \right] \right\vert_{\theta = \bar{\theta}} \right]^{-1} \overset{p}{\rightarrow} -H^{-1} \nonumber $$ Since $-H^{-1}$ is a constant, by <a href="https://en.wikipedia.org/wiki/Slutsky%27s_theorem">Slutsky's theorem</a>: $$ \sqrt{n}(\hat{\theta} - \theta_0) \rightsquigarrow \mathcal{N}\left(\mathbf{0}, H^{-1}\mathcal{I}(\theta_0) H^{-1}\right) $$ Under our regularity conditions, the negative expected Hessian is the variance of the score, so: $$ -H^{-1} = \mathcal{I}^{-1}(\theta_0) \implies \mathcal{N}\left(\mathbf{0}, H^{-1}\mathcal{I}(\theta_0) H^{-1}\right) = \mathcal{N}\left(\mathbf{0}, \mathcal{I}^{-1}(\theta_0)\right) \nonumber $$ </details> <p>Newey and McFadden make some important points about the conditions stated in <a href="#theorem-3-1">Theorem 3.1</a>. Perhaps most notable is the need for the true parameter value to be located in the interior of the parameter space. When this condition is not met ($\hat{\theta}_n$ is on the boundary even as $n \rightarrow \infty$), the asymptotic normality result is no longer guaranteed (though it could still be true). This comes up in variance estimation, since we usually constrain our space to positive reals. We also need the average score over the sample to satisfy a central limit theorem (this gives us the “base” Normal distribution) and the inverse Hessian needs to converge to a constant (so we can apply Slutsky’s theorem).</p> <h3 id="estimating-asymptotic-variance">Estimating Asymptotic Variance</h3> <p>For confidence intervals, we need a consistent estimator of the asymptotic variance, $H^{-1} \Sigma H^{-1}$, of the estimator. Usually we do this by estimating the components and then plugging these in; i.e. we find $\hat{H}^{-1}$ and $\hat{\Sigma}$ and use $\hat{H}^{-1} \hat{\Sigma} \hat{H}^{-1}$</p> <div id="theorem-4-1"></div> <div class="theorem"> <strong>Theorem 4.1.<d-cite key="newey1994"></d-cite></strong> <ul id="newey-4-1" class="tab" data-tab="bef19a7d-3156-419a-a988-c86d853e401b" data-name="newey-4-1"> <li class="active" id="newey-4-1-statement"> <a href="#">statement </a> </li> <li id="newey-4-1-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="bef19a7d-3156-419a-a988-c86d853e401b" data-name="newey-4-1"> <li class="active"> <p>Under the conditions of <a href="#theorem-3-1">Theorem 3.1</a>, if \(\hat{H} = \left. \left[ \frac{\partial \hat{Q}_n(\theta)}{\partial \theta}\right]\right\vert_{\theta = \hat{\theta}}$ and $\hat{\Sigma} \overset{p}{\rightarrow} \Sigma\), then:</p> \[\hat{H}^{-1} \hat{\Sigma} \hat{H}^{-1} \overset{p}{\rightarrow} H^{-1} \Sigma H^{-1}\] </li> <li> <p>Proof to be completed.</p> </li> </ul> </div> <p>This estimator is sometimes called a <i>sandwich estimator</i> since we have the asymptotic variance “sandwiched” between the same term twice.</p> <p>For maximum likelihood estimators, one can estimate the asymptotic variance with any consistent estimator of the inverse Fisher information matrix. Some examples provided by Newey and McFadden are to use the negative Hessian (if regularity conditions are met) or the sample average of the outer product of the score. However, one should use caution because, under model misspecification, estimators of the inverse information matrix may not be consistent.</p> <hr/> <h2 id="efficiency">Efficiency</h2> <p>We can compare different asymptotically normal estimators by their <i>efficiency</i>. But before we begin, we need to define a few new quantities.</p> <div id="tangent-space"></div> <div class="definition"> <strong>Definition (Tangent Space).<d-cite key="tsiatis2006"></d-cite></strong> <br/> Let $\mathcal{H}$ be the Hilbert space of $q$-dimensional measurable functions of $X$ with mean zero and finite variance and equipped with inner product $\langle h_1, h_2 \rangle = \mathbb{E}_{\theta_0}[ h_1^\top h_2]$ as we defined <a href="#def-hilbert-space">above</a>. <br/> The <i>tangent space</i> is the linear subspace of $\mathcal{H}$ spanned by the score vector $U_\theta(X, \theta_0)$: $$ \mathcal{T} = \left\{ B U_\theta(X, \theta_0) : B \in \mathbb{R}^{q \times p} \right\} $$ </div> <p>We can define a similar space when the parameter vector can be partitioned as $\theta = (\beta^\top, \eta^\top)^\top$. The <i>nuisance tangent space</i> is the linear subspace of $\mathcal{H}$ spanned by the nuisance score vector $U_\eta(X, \theta_0)$:</p> \[\begin{equation} \label{eq:nuisance-space} \Lambda = \left\{ B U_\eta(X, \theta_0): B \in \mathbb{R}^{q \times r}\right\} \end{equation}\] <p>The tangent space generated by $U_\beta(X, \theta_0)$ is:</p> \[\begin{equation} \label{eq:interest-space} \mathcal{T}_\beta = \left\{ B U_\beta(X, \theta_0): B \in \mathbb{R}^{q \times p} \right\} \end{equation}\] <p>Notably, the direct sum of these two spaces equals the tangent space generated by the entire score vector:</p> \[\mathcal{T} = \mathcal{T}_\beta \oplus \Lambda\] <p>We can also construct the set of elements that are orthogonal to $\Lambda$ are then given by $h - \Pi(h \rvert \Lambda)$ for all $h \in \mathcal{H}$ where $\Pi(h \rvert \Lambda)$ is the <i>residual of $h$ after projecting onto $\Lambda$.</i></p> <div id="residual"></div> <div class="definition"> <strong>Definition (Residual).<d-cite key="newey1994"></d-cite></strong> <br/> Let $h \in \mathcal{H}$ be an element in the <a href="#def-hilbert-space">Hilbert space defined above</a>. By the <a href="#projection-theorem">projection theorem</a>, there exists a unique element $a_0 \in \Lambda$ in the <a href="#nuisance-tangent-space">nuisance tangent space</a> such that: $$ \rvert \rvert h - a_0 \rvert \rvert \leq \rvert \rvert h - a \rvert \rvert \hspace{3mm} \text{ and } \hspace{3mm} \langle h - a_0, a \rangle \hspace{10mm} \forall a \in \Lambda $$ The element $h - a_0$ is called the <i>residual of $h$ after projecting onto $\Lambda$</i> and is equal to: $$ h - a_0 = \Pi(h \rvert \Lambda^\perp) $$ </div> <h3 id="efficient-influence-functions">Efficient Influence Functions</h3> <p>We call the influence function with the smallest variance matrix the <i>efficient influence function</i>. In the case of RAL estimators, we say that $\psi^{(1)}(X, \theta_0)$ has <i>smaller (asymptotic) variance</i> than $\psi^{(2)}(X, \theta_0)$ if, for all $q \times 1$ constant vectors $a$:</p> \[\text{Var}(\psi^{(1)}(X, \theta_0)) \leq \text{Var}(\psi^{(2)}(X, \theta_0)) \iff \text{Var}(a^\top \psi^{(1)}(X, \theta_0)) \leq \text{Var}(a^\top \psi^{(2)}(X, \theta_0))\] <details> <summary>Implications.</summary> Since influence functions have zero mean, the righthand side of the above condition is equivalent to: $$ a^\top \mathbb{E}_{\theta_0}[\psi^{(1)}(X, \theta_0) (\psi^{(1)}(X, \theta_0))^\top]a \leq a^\top \mathbb{E}_{\theta_0}[\psi^{(2)}(X, \theta_0) (\psi^{(2)}(X, \theta_0))^\top]a $$ for all $q \times 1$ constant vectors $a$, which itself is equivalent to: $$ a^\top \left[ \mathbb{E}_{\theta_0}[\psi^{(2)}(X, \theta_0) (\psi^{(2)}(X, \theta_0))^\top] - \mathbb{E}_{\theta_0}[\psi^{(1)}(X, \theta_0) (\psi^{(1)}(X, \theta_0))^\top] \right] a \geq 0 $$ </details> <p>This brings us to a theorem that defines the subspace of influence functions.</p> <div id="theorem-3-4"></div> <div class="theorem"> <strong>Theorem 3.4.<d-cite key="tsiatis2006"></d-cite></strong> <ul id="tsiatis-3-4" class="tab" data-tab="b20ab5bb-8eea-4474-b5f1-fcecfc8fbe69" data-name="tsiatis-3-4"> <li class="active" id="tsiatis-3-4-statement"> <a href="#">statement </a> </li> <li id="tsiatis-3-4-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="b20ab5bb-8eea-4474-b5f1-fcecfc8fbe69" data-name="tsiatis-3-4"> <li class="active"> <p>The set of all influence functions is the linear variety \(\psi^*(X, \theta_0) + \mathcal{T}^\perp\) where \(\psi^*(X, \theta_0)\) is any influence function, and \(\mathcal{T}^\perp\) is the space perpendicular to the tangent space.</p> </li> <li> <p>See pg. 45-46 of <d-cite key="tsiatis2006"></d-cite>.</p> </li> </ul> </div> <p>This result states that we can construct all influence functions of RAL estimators by taking an arbitrary influence function and adding any element from the orthogonal complement of the tangent space to it. <a href="#theorem-3-4">Theorem 3.4</a> can be used to define the efficient influence function.</p> <div id="theorem-3-5"></div> <div class="theorem"> <strong>Theorem 3.5.<d-cite key="tsiatis2006"></d-cite></strong> <ul id="tsiatis-3-5" class="tab" data-tab="e48f5291-bdd1-4e56-b1c4-c09faf23ab77" data-name="tsiatis-3-5"> <li class="active" id="tsiatis-3-5-statement"> <a href="#">statement </a> </li> <li id="tsiatis-3-5-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="e48f5291-bdd1-4e56-b1c4-c09faf23ab77" data-name="tsiatis-3-5"> <li class="active"> <p>Let \(\psi^*(X, \theta_0)\) be any influence function, and let $\mathcal{T}$ be the tangent space generated by the score vector. The <i>efficient influence function</i> is given by the projection of $\psi^*(X, \theta_0)$ onto the tangent space:</p> \[\psi_{\text{eff}}(X, \theta_0) = \psi^*(X, \theta_0) - \Pi(\psi^*(X, \theta_0) \rvert \mathcal{T}^\perp) = \Pi(\psi^*(X, \theta_0) \rvert \mathcal{T})\] <p>The efficient influence function can be written as:</p> \[\psi_{\text{eff}}(X, \theta_0) = \Gamma(\theta_0) \mathcal{I}^{-1}(\theta_0) U_\theta(X, \theta_0)\] <p>where $\Gamma(\theta_0)$ is the matrix defined in Eq. \eqref{eq:gamma} and $\mathcal{I}(\theta_0)$ is the information matrix.</p> </li> <li> <p>See pg. 46-47 in <d-cite key="tsiatis2006"></d-cite>.</p> </li> </ul> </div> <h3 id="efficient-score">Efficient Score</h3> <p>We can define an efficient version of the score if we are in the setting where we can partition $\theta = (\beta^\top, \eta^\top)^\top$.</p> <div id="efficient-score"></div> <div class="definition"> <strong>Definition (Efficient Score).<d-cite key="tsiatis2006"></d-cite></strong> <br/> The <i>efficient score</i> is the residual of the score with respect to $\beta$ after projecting onto the nuisance tangent space: $$ \begin{aligned} U_{\text{eff}}(X, \theta_0) &amp;= \Pi(U_\beta(X, \theta_0) \rvert \Lambda^{\perp}) \\ &amp;= U_\beta(X, \theta_0) - \Pi(U_\beta(X, \theta_0) \rvert \Lambda) \\ &amp;= U_\beta(X, \theta_0) - \mathbb{E}_{\theta_0}\left[ U_\beta(X, \theta_0) U^\top_\eta(X, \theta_0)\right] \left[ \mathbb{E}\left[ U_\eta(X, \theta_0) U^\top_\eta(X, \theta_0) \right] \right]^{-1} U_\eta(X, \theta_0) \end{aligned} $$ </div> <p>In this setting, we can construct the efficient influence function as:</p> \[\begin{equation} \label{eq:partitioned-efficient-influence} \psi_\text{eff}(X, \theta_0) = \left[ \mathbb{E}_{\theta_0}\left[ U_\text{eff}(X, \theta_0) U^\top_\text{eff}(X, \theta_0) \right] \right]^{-1} U_\text{eff}(X, \theta_0) \end{equation}\] <p>and has variance equal to:</p> \[\text{Var}(\psi_{\text{eff}}(X, \theta_0)) = \left[ \mathbb{E}_{\theta_0} \left[ U_\text{eff}(X, \theta_0) U^\top(X, \theta_0) \right] \right]^{-1}\] <p>which is the inverse variance matrix of the efficient score. If we partition the variance of the score vector as:</p> \[\text{Var}(U_\theta(X, \theta_0)) = \mathcal{I} = \begin{bmatrix} \mathcal{I}_{\beta, \beta} = \mathbb{E}_{\theta_0}\left[ U_\beta(X, \theta_0) U^\top_\beta(X, \theta_0) \right] &amp; \mathcal{I}_{\beta, \eta} = \mathbb{E}_{\theta_0}\left[ U_\beta(X, \theta_0) U^\top_\eta(X, \theta_0) \right] \\ \mathcal{I}_{\eta, \beta} = \mathbb{E}_{\theta_0}\left[ U_\eta(X, \theta_0) U^\top_\beta(X, \theta_0) \right] &amp; \mathcal{I}_{\eta, \eta} = \mathbb{E}_{\theta_0}\left[ U_\eta(X, \theta_0) U^\top_\eta(X, \theta_0) \right] \end{bmatrix}\] <p>Then we can use the <a href="https://en.wikipedia.org/wiki/Schur_complement">Schur complement formula</a> to get the varaince of the efficient influence function is:</p> \[\begin{equation} \label{eq:var-eff-influence} \text{Var}(\psi_{\text{eff}}(X, \theta_0)) = \left[\mathcal{I}_{\beta, \beta} - \mathcal{I}_{\beta, \eta} \mathcal{I}^{-1}_{\eta, \eta} \mathcal{I}^\top_{\beta, \eta} \right]^{-1} \end{equation}\] <hr/> <h2 id="appendix">Appendix</h2> <h3 id="auxiliary-definitions">Auxiliary Definitions</h3> <div id="uniform-convergence"></div> <div class="definition"> <strong>Definition (Uniform Convergence In Probability).<d-cite key="newey1994"></d-cite></strong> <br/> Let $\hat{Q}_n(X,\theta)$ be the objective function for an extremum estimator, $\hat{\theta}_n$. We say that $\hat{Q}_n(X, \theta)$ converges <i>uniformly in probability</i> to $Q_0(X,\theta)$ if: $$ \underset{\theta \in \Omega}{\sup} \big\rvert \hat{Q}_n(X, \theta) - Q_0(X,\theta) \rvert \overset{p}{\rightarrow} 0 $$ </div> <div id="direct-sum"></div> <div class="definition"> <strong>Definition (Direct Sum).<d-cite key="tsiatis2006"></d-cite></strong> <br/> Let $M, N \subset \mathcal{H}$ be linear subspaces of Hilbert space $\mathcal{H}$. Their <i>direct sum</i>, denoted by $M \oplus N$, is the linear subspace in $\mathcal{H}$ such that every $x \in M \oplus N$ can be uniquely represented as $x = m + n$ for some $m \in M$ and $n \in N$. </div> <div id="orthogonal-complement"></div> <div class="definition"> <strong>Definition (Orthogonal Complement).<d-cite key="tsiatis2006"></d-cite></strong> <br/> Let $M \subset \mathcal{H}$ be a linear subspace of Hilbert space $\mathcal{H}$. The <i>orthogonal complement</i> of $M$, denoted by $M^\perp$, is the linear subspace of elements in $\mathcal{H}$ that are orthogonal to $M$. It also holds that: $$ \mathcal{H} = M \oplus M^\perp $$ </div> <div id="linear-variety"></div> <div class="definition"> <strong>Definition (Linear Variety).<d-cite key="tsiatis2006"></d-cite></strong> <br/> Let $M$ be a linear subspace of Hilbert space $\mathcal{H}$. A <i>linear variety</i> or <i>affine space</i> is the set: $$ V = x_0 + M $$ for all $x_0 \in \mathcal{H}$ such that $x_0 \not\in M$ and $\rvert \rvert x_0 \rvert \rvert \neq 0$. <br/> A linear variety is simply a shifting of a linear subspace. </div> <h3 id="auxiliary-results">Auxiliary Results</h3> <p>The following result is called the <i>Information inequality</i> and states that if $\theta_0$ can be identified using a MLE, then the limiting objective function will have a unique maximum at the true value.</p> <div id="information-inequality"></div> <div class="theorem"> <strong>Lemma 2.2. <d-cite key="newey1994"></d-cite></strong> <ul id="newey-2-2" class="tab" data-tab="eb91be92-28ec-49f5-bf8b-b030e3cb9bae" data-name="newey-2-2"> <li class="active" id="newey-2-2-statement"> <a href="#">statement </a> </li> <li id="newey-2-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="eb91be92-28ec-49f5-bf8b-b030e3cb9bae" data-name="newey-2-2"> <li class="active"> <p>If, for any $\theta \in \Omega$ such that $\theta \neq \theta_0$ implies that $p_X(x, \theta) \neq p_X(x, \theta_0)$ (i.e. $\theta_0$ is identified) and $\mathbb{E}[\rvert \log p_X(x, \theta) \rvert ] &lt; \infty$ for all $\theta$, then the limiting objective function $Q_0(\theta) = \mathbb{E}\left[ \log(p_X(x, \theta)) \right]$ has a unique maximum at $\theta_0$.</p> </li> <li> <p>Proof to be completed.</p> </li> </ul> </div>]]></content><author><name></name></author><category term="theory"/><category term="estimation"/><category term="primer"/><summary type="html"><![CDATA[A Primer]]></summary></entry><entry><title type="html">Generalized Linear Mixed Models</title><link href="https://aerosengart.github.io/blog/2025/glmm/" rel="alternate" type="text/html" title="Generalized Linear Mixed Models"/><published>2025-06-04T00:00:00+00:00</published><updated>2025-06-04T00:00:00+00:00</updated><id>https://aerosengart.github.io/blog/2025/glmm</id><content type="html" xml:base="https://aerosengart.github.io/blog/2025/glmm/"><![CDATA[<p>This post is a primer on generalized linear mixed models (GLMM) and their associated estimation procedures. Throughout this post, we perform derivatives of matrices, vectors, and scalars. We denote all of them with $\partial$ even though they may not <i>technically</i> be partial derivatives. The meaning should be clear from the context if you keep track of the dimensions of the variables.</p> <p>We’ll use the notation $\text{vec}(\mathbf{M})$ denote the vectorization of matrix $\mathbf{M}$, where we flatten the matrix row-by-row. Also, any function $f: \mathbb{R} \rightarrow \mathbb{R}$ will be applied element-wise to vectors and matrices.</p> <hr/> <h2 id="background">Background</h2> <p>Let’s first explain <i>why</i> generalized linear mixed models are of interest and helpful in data analysis. Suppose we have a dataset in which the observations can be grouped in some way (i.e. we have repeated observations from the same unit). One example is longitudinal studies where we obtain measurements from the same individuals over time. Another example is in education where we may observe students from the same classes or schools. In these settings, it seems reasonable to assume that observations from the same unit would be more similar that those from different ones. This can be realized in our model by including a way for the effects of covariates to differ across units.</p> <p>As in generalized linear models, we assume the mean response is some function of a linear combination of covariates. However, GLMMs are called <i>mixed</i> because the effects of the covariates are of two types: <i>fixed</i> and <i>random</i>. The fixed effects represent population-level relationships, while the random effects represent unit-specific ones. In this way, we can use GLMMs to analyze between-subject variation (via the fixed effects) and within-subject variation (via the random effects): the fixed effects determine the relationship between the covariates and the mean response for the population (i.e. overall), and the random effects describe how each group’s mean response deviates from that. Later on, we introduce measurement errors, which account for deviation of each <i>observation</i> from its group’s mean.</p> <hr/> <h2 id="set-up">Set-Up</h2> <p>We have $n$ observations coming from $k$ different clusters, each of size $n_t$ for $t \in [k]$. The full data will be denoted by $\mathbf{y}$. Though $\mathbf{y}$ is a vector, we’ll denote the $j$-th observation from cluster $i$ with $\mathbf{y}_{i,j}$. For example, \(\mathbf{y}_{i,j}\) denotes element \(\sum_{l = 1}^{i - 1} n_l + j\) of $\mathbf{y}$. We’ll denote the $n_i$-dimensional vector of responses for cluster $i$ with $\mathbf{y}_i$.</p> <p>For each observation, we will have $p$ fixed effect covariates arranged in a $p$-dimensional vector, \(\mathbf{x}_{i, j}\), and $q$ random effects covariates in a $q$-dimensional vector, \(\mathbf{z}_{i,j}\). We’ll assume that the observations within the same cluster are independent.</p> <p>We’ll let $\alpha$ denote a $p$-dimensional fixed effect coefficient vector, $\alpha$, and $\beta_t$ will denote a $q$-dimensional random effect coefficient vector corresponding to group $t$. We assume $\beta_t \overset{iid}{\sim} \mathcal{F}$ for all $t \in [k]$ for an exponential family distribution, $\mathcal{F}$, and $q \times q$ covariance matrix, $D(\tau^2)$, depending on an $m$-dimensional variance component vector, $\tau^2$. These random effects are assumed to be independent of the covariates. When they are not, we can run into issues with bias when estimating the fixed effects.</p> <details> <summary><strong>Additional Assumptions.</strong></summary> A few auxiliary assumptions must also be made for the analysis later, which we list here: <ul> <li>The third moment and higher moments of $\beta$ are of order $o(\rvert \rvert \tau^2 \rvert \rvert)$.</li> <li>The entries of $D(\tau^2)$ are linear in $\tau^2$.</li> <li>$D(\tau^2) = \text{vec}(0)$ if $\tau^2 = \text{vec}(0)$</li> </ul> </details> <p>Our model comes in the form of a specification of the conditional mean, $\mu_{i,j} = \mathbb{E}[\mathbf{y}_{i,j} \rvert \beta_i]$ (where we suppress the addition conditioning on the covariates themselves). For a monotonic and differentiable link function (e.g. $\log(\cdot)$ or $\text{logit}(\cdot)$), the conditional mean of the $j$-th observation in group $i$ is assumed to be given by:</p> \[\begin{equation} \label{eq:glmm} \mu_{i,j} = g^{-1}\left(\mathbf{x}_{i,j}^\top \alpha + \mathbf{z}_{i,j}^\top \beta_i \right) \end{equation}\] <p>We then assume that the observations themselves follow some exponential family distribution with measurement errors, $\epsilon_{i,j}$, which is the deviation of the response from its (unit-specific) conditional mean. These errors are assumed to have mean zero and be independent of each other and of the random effects. We further assume the responses, $\mathbf{y}_{i,j}$, conditional on the random effects (and the covariates), are independent with variances equal to some function of the conditional mean.</p> <p>To write Eq. \eqref{eq:glmm} in matrix notation, we assume that the observations are ordered by group, so the first $n_1$ observations are all from group $1$. We can then define the following vectors/matrices:</p> \[\mu = \begin{bmatrix} \mu_{1, 1} \\ \vdots \\ \mu_{k, n_k} \end{bmatrix}, \hspace{2mm} \mathbf{X} = \begin{bmatrix} — &amp; \mathbf{x}_{1,1} &amp; — \\ &amp; \dots &amp; \\ — &amp; \mathbf{x}_{k, n_k} &amp; — \\ \end{bmatrix}, \hspace{2mm} \tilde{\mathbf{Z}}_i = \begin{bmatrix} — &amp; \mathbf{z}_{i, 1} &amp; — \\ &amp; \dots &amp; \\ — &amp; \mathbf{z}_{i, n_i} &amp; — \\ \end{bmatrix}, \hspace{2mm} \mathbf{Z} = \begin{bmatrix} \tilde{\mathbf{Z}}_1 &amp; \dots &amp; 0 \\ \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; \dots &amp; \tilde{\mathbf{Z}}_k \end{bmatrix}, \hspace{2mm} \beta = \begin{bmatrix} \beta_1 \\ \vdots \\ \beta_k \end{bmatrix}\] <p>where $\tilde{\mathbf{Z}}_i$ is constructed by stacking the \(\mathbf{z}_{i,j}\) vectors for group $i$ into a matrix, and where $\beta$ is the vector created by stacking together all of the $\beta_i$ vectors vertically. As a reminder, \(\mu \in \mathbb{R}^{n \times 1}\), \(\mathbf{X} \in \mathbb{R}^{n \times p}\), \(\tilde{\mathbf{Z}}_i \in \mathbb{R}^{n_i \times q}\), \(\mathbf{Z} \in \mathbb{R}^{n \times (q \times k)}\), $\alpha \in \mathbb{R}^p$, and \(\beta \in \mathbb{R}^{(q \times k) \times 1}\).</p> <p>The model is then:</p> \[\begin{equation} \label{eq:glmm-matrix} \mu = g^{-1}(\mathbf{X} \alpha + \mathbf{Z} \beta) \end{equation}\] <p>We’ll use $[\cdot] \rvert_{\beta = \beta_0}$ to denote evaluation of the function in brackets when setting $\beta$ equal to $\beta_0$. Similarly, we’ll use $[\cdot] \rvert_{H_0}$ to denote evaluation under the null hypothesis. We’ll also use a superscript $0$ (e.g. $\mu^0$, $\eta^0$, etc.) to denote the quantity under the null hypothesis (i.e. $\tau^2 = \mathbf{0} \implies \beta = \mathbf{0}$).</p> <div class="example"> <strong>Example.</strong> <br/> To keep things simple, we'll assume to a fixed intercept (that is cluster-specific) and a random slope. Thus, $\alpha, \beta \in \mathbb{R}^{k}$, and $\mathbf{z}_{i, j} \in \mathbb{R}$ as well. <br/> In this simple case, $\tilde{\mathbf{Z}}_i \in \mathbb{R}^{n_t}$, so $\mathbf{Z}$ is $n \times k$. Our model is then: $$ \mathbf{y}_{i,j} \rvert \beta_i \sim \text{Poi}\left(\mu_{i,j}\right), \hspace{5mm} \mu_{i,j} = \exp\left(\alpha_i + \beta_i \mathbf{z}_{i,j} \right) $$ We'll have a scalar-valued variance component that we call $\tau^2$. The random effects will have distribution: $$ \beta_i \overset{iid}{\sim} \mathcal{N}(0, \tau^2), \hspace{5mm} \forall i \in [k] $$ If we let $\mathbf{A}$ denote the $n$-dimensional vector of intercepts where the $i$-th entry of $\alpha$ is repeated $n_i$ times, then we can write our model in vector form as: $$ \mu = \exp(\mathbf{A} + \mathbf{Z} \beta) $$ </div> <hr/> <h2 id="likelihood">Likelihood</h2> <p>We can write the conditional log-likelihood using the exponential family form (see my <a href="/posts/2025/06/03/glm.html">generalized linear models post</a>).</p> \[\begin{equation} \label{eq:condition-log-lik} \begin{aligned} \ell(\mathbf{y}; \alpha, \tau^2 \rvert \beta) &amp;= \sum_{i = 1}^{k} \sum_{j = 1}^{n_i} \left[ \frac{\zeta_{i,j} \mathbf{y}_{i,j} - A(\zeta_{i,j})}{d(\phi, \omega_{i,j})} + \log(h(\mathbf{y}_{i,j}, \phi, \omega_{i,j}))\right] \\ \mathcal{L}(\mathbf{y}; \alpha, \tau^2 \rvert \beta) &amp;= \exp \left( \sum_{i = 1}^n \left[ \frac{\zeta_i \mathbf{y}_i - A(\zeta_i)}{d(\phi, \omega_i)} + \log(h(\mathbf{y}_i, \phi, \omega_i))\right] \right) \end{aligned} \end{equation}\] <p>where $\phi &gt; 0$ is a dispersion/scale parameter; $\omega_{i,j}$ is a (prior) dispersion weights; $\zeta_{i,j}$ is a distribution parameter; and $h(\cdot)$ and $A(\cdot)$ are known functions. We assume that $d(\phi, \omega_{i,j}) = \phi \omega_i^{-1}$.</p> <p>The conditional variance of the responses is given by:</p> \[\text{Cov}(\mathbf{y} \rvert \beta) = \text{diag}(d(\phi, \omega)) \underbrace{\frac{\partial^2 A(\tau^2)}{\partial \tau^2 \partial (\tau^2)^\top}}_{=V(\mu)} = \text{diag}^{-1}\left( \frac{\omega}{\phi}\right) V(\mu)\] <hr/> <h2 id="quasi-likelihood">Quasi-Likelihood</h2> <p>We don’t actually need to specify the true likelihood, even though (in practice) we will usually be able to do so. Using quasi-likelihood methods can often be less computationally expensive than maximum likelihood for count data.</p> <p>In the quasi-likelihood scheme, we keep the conditional mean assumption that we discussed above and also make the assumption that the conditional variance can be expressed as a function of the mean in the form $\text{Cov}(\mathbf{y}) = \text{diag}\left(d(\phi, \omega)\right)V(\mu)$ for some function $V(\cdot)$. Later, we’ll denote diagonal elements of $\text{Cov}(\mathbf{y})$ with $v(\mu_{i,j})$.</p> <p>The conditional log quasi-likelihood and quasi-likelihood are given by:</p> \[\begin{equation} \label{eq:quasi-lik} \begin{aligned} \ell_q(\mathbf{y}_{i,j}; \alpha, \tau^2 \rvert \beta_i) &amp;= \int_{\mathbf{y}_{i,j}}^{\mu_{i,j}} \frac{\omega_{i,j}(\mathbf{y}_{i,j} - u)}{\phi V(u)} du \\ \mathcal{L}_q(\mathbf{y}_{i,j}; \alpha, \tau^2 \rvert \beta_i) &amp;= \exp\left(\int_{\mathbf{y}_{i,j}}^{\mu_{i,j}} \frac{\omega_{i,j}(\mathbf{y}_{i,j} - u)}{\phi V(u)} du \right) \end{aligned} \end{equation}\] <p>When $\mathcal{F}$ <i>is</i> an exponential family distribution, the log-likelihood and log quasi-likelihood are equal. See my <a href="/posts/2025/05/30/quasi-likelihood.html">post on quasi-likelihood</a> for a more in-depth discussion.</p> <p>Let $f$ denote the density associated with $\mathcal{F}$, the distribution of the random effects. The unconditional quasi-likelihood is then given by integrating out the random effects from the joint quasi-likelihood:</p> \[\begin{equation} \label{eq:uncondition-log-lik} \begin{aligned} \mathcal{L}_q(\mathbf{y}; \alpha, \tau^2) &amp;= \prod_{i = 1}^k \mathcal{L}_q(\mathbf{y}_i; \alpha, \tau^2) = \prod_{i = 1}^k \int\prod_{j = 1}^{n_i} \mathcal{L}_q(\mathbf{y}_{i,j}; \alpha, \tau^2 \rvert \beta_i) f(\beta_i) d\beta_i \end{aligned} \end{equation}\] <p>Remember that the above integral is multi-dimensional if $\beta_i$ has dimension greater than $1$!</p> <p>We usually do maximum likelihood estimation for the parameter vector $\beta$ by taking the derivative of the log of the above, setting it equal to zero, and solving for $\beta$. However, depending upon the form of $f(\cdot)$ and the number of random effects we have, Eq. \eqref{eq:uncondition-log-lik} may be a multi-dimensional integral that is difficult to evaluate, let alone differentiate. One way is to use quadrature to approximate the integral as a summation. However, there are two other, computationally more feasible, methods that provide a work-around.</p> <p>There are two common methods of approximate inference for GLMMs: penalized quasi-likelihood (PQL) and marginal quasi-likelihood (MQL). They are very similar in that they both perform a Taylor approximation of the conditional log quasi-likelihood to evaluate the integral in Eq. \eqref{eq:uncondition-log-lik}. However, PQL uses the maximum a priori estimates of the random effects as the operating point for the expansion, while MQL uses the random effects mean. See below for a discussion of the approaches.</p> <h3 id="penalized-quasi-likelihood">Penalized Quasi-Likelihood</h3> <p>Penalized quasi-likelihood (PQL) essentially uses Laplace’s method to approximate the integral above. The general idea behind <a href="https://en.wikipedia.org/wiki/Laplace%27s_method">Laplace’s method</a> is to approximate integrals of a certain form as:</p> \[\int \exp(M f(x)) dx \approx \exp(M f(x_0)) \int \exp\left( - \frac{1}{2} M \rvert f''(x_0) \rvert (x - x_0)^2 \right) dx\] <p>where $M$ is a big scalar, $f$ is a twice-differentiable function, and $x_0$ is a global maximum of $f$. If \(f''(x_0) &lt; 0\) and $M \rightarrow \infty$, then the integrand above is basically a Gaussian kernel and the approximation becomes:</p> \[\begin{equation} \label{eq:ql-gauss} \int \exp(M f(x)) dx \approx \left(\frac{2 \pi}{M \rvert f''(x_0) \rvert} \right)^{\frac{1}{2}} \exp(M f(x_0)) \end{equation}\] <div class="theorem"> <strong>Claim (Rewriting the Integral).</strong> <ul id="claim-3" class="tab" data-tab="e0b0f820-b1ec-43d0-a0d5-82c6294339d4" data-name="claim-3"> <li class="active" id="claim-3-statement"> <a href="#">statement </a> </li> <li id="claim-3-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="e0b0f820-b1ec-43d0-a0d5-82c6294339d4" data-name="claim-3"> <li class="active"> <p>Letting $c = (2 \pi)^{-\frac{m}{2}}$ and $d_{i,j}(y_{i,j}, \mu_{i,j}) = -2 \int_{y_{i,j}}^{\mu_{i,j}} \frac{y_{i,j} - z}{a_{i,j} V(z)} dz$, we can rewrite the integral as:</p> \[\begin{aligned} \mathcal{L}_q(\mathbf{y}; \alpha, \tau^2) &amp;= c \rvert D(\tau^2) \rvert^{-\frac{1}{2}} \int \exp\left( -\kappa(\beta) \right) d \beta; \\ \kappa(\beta) &amp;= \frac{1}{2\phi}\sum_{i = 1}^n \sum_{j = 1}^{n_i} d_{i,j}(y_{i,j}, \mu_{i,j}) + \frac{1}{2}\beta^\top D^{-1}(\tau^2) \beta \end{aligned}\] </li> <li> \[\begin{aligned} \mathcal{L}_q(\mathbf{y}; \alpha, \tau^2) &amp;= \int \mathcal{L}_q(\mathbf{y}; \alpha, \tau^2 \rvert \beta) \mathcal{L}(\beta) d \beta \\ &amp;= \int \left[ \prod_{i = 1}^n \prod_{j = 1}^{n_i} \exp\left( \ell_q(y_{i,j}; \mu_{i,j} \rvert \beta) \right) \right] (2 \pi)^{-\frac{m}{2}} \rvert D(\tau^2) \rvert^{-\frac{1}{2}} \exp\left(- \frac{1}{2}\beta^\top D^{-1}(\tau^2) \beta \right) d\beta \\ &amp;= (2 \pi)^{-\frac{m}{2}} \rvert D(\tau^2) \rvert^{-\frac{1}{2}} \int \exp\left(\sum_{i = 1}^n \sum_{j = 1}^{n_i} \left( \int_{y_{i,j}}^{\mu_{i,j}} \frac{y_{i,j} - z}{\phi a_{i,j} V(z)} dz \right) - \frac{1}{2}\beta^\top D^{-1}(\tau^2) \beta \right) d\beta \\ &amp;= (2 \pi)^{-\frac{m}{2}} \rvert D(\tau^2) \rvert^{-\frac{1}{2}} \int \exp\left(- \frac{1}{2 \phi} \sum_{i = 1}^n \sum_{j = 1}^{n_i} \left( -2 \int_{y_{i,j}}^{\mu_{i,j}} \frac{y_{i,j} - z}{a_{i,j} V(z)} dz \right) - \frac{1}{2}\beta^\top D^{-1}(\tau^2) \beta \right) d\beta \\ &amp;= (2 \pi)^{-\frac{m}{2}} \rvert D(\tau^2) \rvert^{-\frac{1}{2}} \int \exp\left(- \frac{1}{2 \phi} \sum_{i = 1}^n \sum_{j = 1}^{n_i} d_{i,j}(y_{i,j}, \mu_{i,j}) - \frac{1}{2}\beta^\top D^{-1}(\tau^2) \beta \right) d\beta &amp; \left( d_{i,j}(y_{i,j}, \mu_{i,j}) = -2 \int_{y_{i,j}}^{\mu_{i,j}} \frac{y_{i,j} - z}{a_{i,j} V(z)} dz \right) \end{aligned}\] </li> </ul> </div> <p>Note that the first term in $\kappa(\beta)$ <i>does</i> involve $\beta$ since \(\mu_{i,j} = g^{-1}(\mathbf{x}_{i,j}^\top \alpha + \mathbf{z}_{i,j}^\top \beta_j)\). Let $\kappa’$ and $\kappa’’$ denote the vector of first-order partial derivatives and the matrix of second-order partial derivatives, respectively, of $\kappa$ with respect to $\beta$.</p> <div class="theorem"> <strong>Claim (Partial Derivatives).</strong> <ul id="claim-4" class="tab" data-tab="86f84549-2d0b-44a9-a515-e5e2215e43d0" data-name="claim-4"> <li class="active" id="claim-4-statement"> <a href="#">statement </a> </li> <li id="claim-4-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="86f84549-2d0b-44a9-a515-e5e2215e43d0" data-name="claim-4"> <li class="active"> <p>These have the form:</p> \[\begin{aligned} \kappa'_k(\beta) &amp;= -\sum_{j = 1}^{n_k} \left(\frac{y_{k,j} - \mu_{k,j}}{\phi a_{k,j} V(\mu_{k,j})\frac{\partial g(\mu_{i,j})}{\partial \mu_{i,j}}}\right)\mathbf{z}_{i,j} + D^{-1}(\tau^2) \beta_k \\ \kappa''(\beta) &amp;\approx \tilde{\mathbf{Z}}^\top \mathbf{W} \tilde{\mathbf{Z}} + D^{-1}(\tau^2) \end{aligned}\] <p>where $\mathbf{W}$ is the $n \times n$ diagonal matrix with elements $\frac{1}{\phi a_{i,j} V(\mu_{i,j}) \frac{\partial g(\mu_{i,j})}{\partial \mu_{i,j}}}$, and $\tilde{\mathbf{Z}}$ is the $n \times q$ matrix formed by concatenating the $\tilde{\mathbf{Z}}^t$ matrices.</p> </li> <li> \[\begin{aligned} \kappa'_k(\beta) &amp;= \frac{\partial}{\partial \beta_k} \left[\frac{1}{2 \phi} \sum_{i = 1}^k \sum_{j = 1}^{n_i} d_{i,j}(y_{i,j}, \mu_{i,j}) + \frac{1}{2} \beta^\top D^{-1}(\tau^2) \beta \right] \\ &amp;= \frac{1}{2 \phi} \sum_{i = 1}^k \sum_{j = 1}^{n_i} \frac{\partial}{\partial \beta_k} \left[d_{i,j}(y_{i,j}, \mu_{i,j}) \right] + D^{-1}(\tau^2) \beta \\ &amp;= \frac{1}{2 \phi} \sum_{j = 1}^{n_k} \frac{\partial \mu_{k,j}}{\partial \beta_k} \frac{\partial}{\partial \mu_{k,j}}\left[ -2 \int_{y_{k,j}}^{\mu_{k,j}} \frac{y_{k,j} - z}{a_{k,j} V(z)}dz \right] + D^{-1}(\tau^2) \beta_k \\ &amp;= \frac{1}{2 \phi} \sum_{j = 1}^{n_k} \frac{\partial \mu_{k,j}}{\partial \beta_k} \left(-2 \frac{y_{k,j} - \mu_{k,j}}{a_{k,j} V(\mu_{k,j})}\right) + D^{-1}(\tau^2) \beta_k \\ &amp;= -\sum_{j = 1}^{n_k} \frac{\partial \mu_{k,j}}{\partial \beta_k} \left(\frac{y_{k,j} - \mu_{k,j}}{\phi a_{k,j} V(\mu_{k,j})}\right) + D^{-1}(\tau^2) \beta_k \\ \end{aligned}\] <p>Assuming $g(\cdot)$ is <i>strictly monotone and continuous</i>, then we have:</p> \[\frac{\partial g^{-1}(\eta_{i,j})}{\partial \eta_{i,j}} = \frac{\partial \mu_{i,j}}{\partial \eta_{i,j}} = \left(\frac{\partial \eta_{i,j}}{\partial \mu_{i,j}}\right)^{-1} = \left(\frac{\partial g(\mu_{i,j})}{\partial \mu_{i,j}}\right)^{-1}\] <p>This implies:</p> \[\frac{\partial \mu_{k,j}}{\partial \beta_k} = \frac{\partial \eta_{k,j}}{\partial \beta_k} \frac{\partial \mu_{k, j}}{\partial \eta_{i,j}} = \mathbf{z}_{i,j} \left(\frac{\partial \eta_{i,j}}{\partial \mu_{i,j}}\right)^{-1} = \frac{\mathbf{z}_{i,j}}{\frac{\partial g(\mu_{i,j})}{\partial \mu_{i,j}}}\] <p>Thus, $\kappa’_k(\beta)$ is:</p> \[\kappa'_k(\beta) = -\sum_{j = 1}^{n_k} \left(\frac{y_{k,j} - \mu_{k,j}}{\phi a_{k,j} V(\mu_{k,j})\frac{\partial g(\mu_{k,j})}{\partial \mu_{k,j}}}\right)\mathbf{z}_{k,j} + D^{-1}(\tau^2) \beta_k\] <p>Looking at the second-order partial derivatives:</p> \[\begin{aligned} \kappa''_{k,k}(\beta) &amp;= \frac{\partial}{\partial \beta_k^\top} \left[ -\sum_{j = 1}^{n_k} \left(\frac{y_{k,j} - \mu_{k,j}}{\phi a_{k,j} V(\mu_{k,j})\frac{\partial g(\mu_{k,j})}{\partial \mu_{k,j}}}\right)\mathbf{z}_{k,j} + D^{-1}(\tau^2) \beta_k \right] \\ &amp;= - \sum_{j = 1}^{n_k} \left( \frac{\mathbf{z}_{k,j}}{\phi a_{k,j} V(\mu_{k,j}) \frac{\partial g(\mu_{k,j})}{\partial \mu_{k,j}}} \frac{\partial}{\partial \beta_k^\top} \left[ y_{k,j} - \mu_{k,j} \right] + \mathbf{z}_{k,j}(y_{k,j} - \mu_{k,j})\frac{\partial}{\partial \beta_k^\top} \left[ \frac{1}{\phi a_{k,j} V(\mu_{k,j}) \frac{\partial g(\mu_{k,j})}{\partial \mu_{k,j}}} \right] \right) + D^{-1}(\tau^2) \\ &amp;= - \sum_{j = 1}^{n_k} \left( \frac{-\mathbf{z}_{k,j}\mathbf{z}_{k, j}^\top}{\phi a_{k,j} V(\mu_{k,j}) \left(\frac{\partial g(\mu_{k,j})}{\partial \mu_{k,j}}\right)^2} + \mathbf{z}_{k,j}(y_{k,j} - \mu_{k,j})\frac{\partial}{\partial \beta_k^\top} \left[ \frac{1}{\phi a_{k,j} V(\mu_{k,j}) \frac{\partial g(\mu_{k,j})}{\partial \mu_{k,j}}} \right] \right) + D^{-1}(\tau^2) \\ &amp;= \sum_{j = 1}^{n_k} \frac{\mathbf{z}_{k,j}\mathbf{z}_{k, j}^\top}{\phi a_{k,j} V(\mu_{k,j}) \left(\frac{\partial g(\mu_{k,j})}{\partial \mu_{k,j}}\right)^2} + D^{-1}(\tau^2) + R_k \\ &amp;=(\tilde{\mathbf{Z}}^k)^\top \mathbf{W}_k \tilde{\mathbf{Z}}^k + D^{-1}(\tau^2) + R_k \end{aligned}\] <p>where in $(i)$, we set:</p> \[R_k = -\sum_{j = 1}^{n_k} (y_{k,j} - \mu_{k,j}) \mathbf{z}_{k,j} \frac{\partial}{\partial \beta_k^\top} \left[ \frac{1}{\phi a_{k,j} V(\mu_{k,j}) \frac{\partial g(\mu_{k,j})}{\partial \mu_{k,j}}} \right]\] <p>and in $(ii)$, $\mathbf{W}_k$ is the diagonal matrix with elements:</p> \[\frac{1}{\phi a_{k,j} V(\mu_{k,j}) \frac{\partial g(\mu_{k,j})}{\partial \mu_{k,j}}}\] <p>Since \(\kappa'_k(\beta)\) only involves $\beta_k$, \(\kappa''_{k, k'}(\beta) = \mathbf{0}\). Stacking all of the $\mathbf{W}_k$ and $R_k$ together, we get:</p> \[\kappa''(\beta) = \tilde{\mathbf{Z}}^\top \mathbf{W} \tilde{\mathbf{Z}} + D^{-1}(\tau^2) + R \approx \tilde{\mathbf{Z}}^\top \mathbf{W} \tilde{\mathbf{Z}} + D^{-1}(\tau^2)\] <p>where we drop $R$ for the approximation.</p> </li> </ul> </div> <p>Letting $\hat{\beta}$ be a solution to the equation $\kappa’(\beta) = \mathbf{0}$, we can ignore constants and remainders (by assuming they are negligible in the long run…) to obtain the approximation in Eq. \eqref{eq:ql-gauss}:</p> \[\begin{aligned} \ell_q(\mathbf{y}; \alpha, \tau^2) &amp;\approx \rvert D(\tau^2) \rvert^{-\frac{1}{2}} \log\left( \left( \frac{2 \pi}{\rvert \kappa''(\beta_0) \rvert}\right)^{\frac{1}{2}} \exp(-\kappa(\beta_0)) \right) \\ &amp;= \frac{1}{2}\log(2 \pi) - \frac{1}{2} \log(\kappa''(\beta_0)) + \kappa(\beta_0) \\ &amp;\approx -\frac{1}{2} \log\left( \rvert D(\tau^2) \rvert \right) -\frac{1}{2} \log\left( \rvert \tilde{\mathbf{Z}}^\top \mathbf{W} \mathbf{\tilde{Z}} + D^{-1}(\tau^2) \rvert \right) + \frac{1}{2\phi} \sum_{i = 1}^k \sum_{j = 1}^{n_i} d_{i,j}(y_{i,j}, \mu_{i,j}) + \frac{1}{2}\beta^\top D^{-1}(\tau^2) \beta \end{aligned}\] <h3 id="marginal-quasi-likelihood">Marginal Quasi-Likelihood</h3> <p>An alternative way to think about a generalized linear mixed model is from a marginal perspective, where we instead focus on the <i>unconditional</i> expectation of the response:</p> \[\mathbb{E}[y_{i,j}] = \mu_{i,j} = g(\mathbf{x}_{i,j}^\top \alpha)\] <p>In contrast to PQL, marginal quasi-likelihood (MQL) performs a Taylor approximation of the log quasi-likelihood about the random effects (prior) mean $\beta = \mathbf{0}$ to get a closed form solution to the multi-dimensional integral. For the $i$-th cluster, this approximation is:</p> \[\begin{aligned} \ell_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i) \approx \ell_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i)\rvert_{\beta_i = \mathbf{0}} + \beta_i^\top \left[ \frac{\partial \ell_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i)}{\partial \beta_i} \right] \bigg\rvert_{\beta_i = \mathbf{0}} + \frac{1}{2} \beta_i^\top \left[ \frac{\partial^2 \ell_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i)}{\partial \beta_i \partial \beta_i^\top} \right] \bigg\rvert_{\beta_i = \mathbf{0}} \beta_i \end{aligned}\] <p>Thus:</p> \[\begin{aligned} \mathcal{L}_q(\mathbf{y}; \alpha, \tau^2) &amp;= \prod_{i = 1}^k (2 \pi)^{-\frac{q}{2}} \rvert D(\tau^2) \rvert^{-\frac{1}{2}} \int \exp\left( \ell_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i) - \frac{1}{2}\beta_i^\top D^{-1}(\tau^2) \beta_i \right) d \beta_i \\ &amp;\approx \prod_{i = 1}^k (2 \pi)^{-\frac{q}{2}} \rvert D(\tau^2) \rvert^{-\frac{1}{2}} \int \exp\left( [\ell_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i)]_{\beta_i = \mathbf{0}} + \beta_i^\top \left[ \frac{\partial \ell_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i)}{\partial \beta_i} \right] \bigg\rvert_{\beta_i = \mathbf{0}} + \frac{1}{2} \beta_i^\top \left[ \frac{\partial^2 \ell_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i)}{\partial \beta_i \partial \beta_i^\top} \right] \bigg\rvert_{\beta_i = \mathbf{0}} \beta_i - \frac{1}{2}\beta_i^\top D^{-1}(\tau^2) \beta_i \right) d \beta_i \\ &amp;= \prod_{i = 1}^k (2 \pi)^{-\frac{q}{2}} \rvert D(\tau^2) \rvert^{-\frac{1}{2}} \left[\mathcal{L}_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i) \right] \bigg\rvert_{\beta_i = \mathbf{0}} \int \exp\left( \beta_i^\top \left[ \frac{\partial \ell_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i)}{\partial \beta_i} \right] \bigg\rvert_{\beta_i = \mathbf{0}} - \frac{1}{2} \beta_i^\top \left[ D^{-1}(\tau^2) - \left[ \frac{\partial^2 \ell_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i)}{\partial \beta_i \partial \beta_i^\top} \right] \bigg\rvert_{\beta_i = \mathbf{0}} \right] \beta_i\right) d \beta_i \end{aligned}\] <p>We can rewrite the above integral in <i>canonical form</i> by defining:</p> \[\begin{aligned} \mathbf{K}_i &amp;= D^{-1}(\tau^2) - \left[ \frac{\partial^2 \ell_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i)}{\partial \beta_i \partial \beta_i^\top} \right] \bigg\rvert_{\beta_i = \mathbf{0}} \\ \mathbf{h}_i &amp;= \left[ \frac{\partial \ell_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i)}{\partial \beta_i} \right] \bigg\rvert_{\beta_i = \mathbf{0}} \end{aligned}\] <p>And:</p> \[\bar{\Sigma}_i^{-1} = \mathbf{K}_i; \hspace{5mm} \bar{\mu}_i = \bar{\Sigma}_i \mathbf{h}_i; \hspace{5mm} g_i = - \frac{1}{2} \bar{\mu}_i^\top \bar{\Sigma}_i^{-1} \bar{\mu}_i - \log\left((2 \pi)^{\frac{q}{2}} \rvert \bar{\Sigma}_i \rvert^{\frac{1}{2}} \right)\] <div class="theorem"> <strong>Claim (Likelihood Approximation).</strong> <ul id="claim-5" class="tab" data-tab="ae820cdb-dc44-4689-b481-3af6fcabfb85" data-name="claim-5"> <li class="active" id="claim-5-statement"> <a href="#">statement </a> </li> <li id="claim-5-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="ae820cdb-dc44-4689-b481-3af6fcabfb85" data-name="claim-5"> <li class="active"> <p>It follows that:</p> \[\begin{aligned} \mathcal{L}_q(\mathbf{y}; \alpha, \tau^2) &amp;\approx \prod_{i = 1}^k \left\rvert \mathbb{I}_{q \times q} - D(\tau^2) \left[ \frac{\partial^2 \ell_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i)}{\partial \beta_i \partial \beta_i^\top} \right] \bigg\rvert_{\beta_i = \mathbf{0}}\right\rvert^{-\frac{1}{2}} \left[\mathcal{L}_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i) \right]\bigg\rvert_{\beta_i = \mathbf{0}} \exp\left(\frac{1}{2} \left[\mathbf{h}_i \mathbf{K}_i^{-1} \mathbf{h}_i \right] \right) \\ \implies \ell_q(\mathbf{y}; \alpha, \tau^2) &amp;\approx \sum_{i = 1}^k \left[ -\frac{1}{2} \log\left( \left\rvert \mathbb{I}_{q \times q} - D(\tau^2) \left[ \frac{\partial^2 \ell_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i)}{\partial \beta_i \partial \beta_i^\top} \right] \bigg\rvert_{\beta_i = \mathbf{0}}\right\rvert \right) + \left[ \ell_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i) \right] \bigg\rvert_{\beta_i = \mathbf{0}} + \frac{1}{2} \mathbf{h}_i^\top \mathbf{K}_i^{-1} \mathbf{h}_i \right] \end{aligned}\] </li> <li> \[\begin{aligned} \mathcal{L}_q(\mathbf{y}; \alpha, \tau^2) &amp;\approx \prod_{i = 1}^k (2 \pi)^{-\frac{q}{2}} \rvert D(\tau^2) \rvert^{-\frac{1}{2}} \left[\mathcal{L}_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i) \right] \bigg\rvert_{\beta_i = \mathbf{0}} \underbrace{\int \exp\left( \beta_i^\top \mathbf{h}_i - \frac{1}{2} \beta_i^\top \mathbf{K}_i \beta_i\right) d \beta_i}_{= \exp(-g_i)} \\ &amp;= \prod_{i = 1}^k (2 \pi)^{-\frac{q}{2}} \rvert D(\tau^2) \rvert^{-\frac{1}{2}} \left[\mathcal{L}_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i) \right] \bigg\rvert_{\beta_i = \mathbf{0}} \exp\left(\frac{1}{2}\bar{\mu}_i^\top \bar{\Sigma}_i^{-1}\bar{\mu}_i + \log\left((2 \pi)^{\frac{q}{2}} \rvert \bar{\Sigma}_i \rvert^{\frac{1}{2}}\right) \right) \\ &amp;= \prod_{i = 1}^k (2 \pi)^{-\frac{q}{2}} \rvert D(\tau^2) \rvert^{-\frac{1}{2}} \left[\mathcal{L}_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i) \right]\bigg\rvert_{\beta_i = \mathbf{0}} \exp\left(\frac{1}{2}\bar{\mu}_i^\top \bar{\Sigma}_i^{-1}\bar{\mu}_i\right) (2 \pi)^{\frac{q}{2}} \rvert \bar{\Sigma}_i \rvert^{\frac{1}{2}} \\ &amp;= \prod_{i = 1}^k \rvert D(\tau^2)\rvert^{-\frac{1}{2}} \rvert \bar{\Sigma}_i^{-1} \rvert^{-\frac{1}{2}}\left[\mathcal{L}_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i) \right]\bigg\rvert_{\beta_i = \mathbf{0}} \exp\left(\frac{1}{2}\bar{\mu}_i^\top \bar{\Sigma}_i^{-1}\bar{\mu}_i\right) \\ &amp;= \prod_{i = 1}^k \rvert D(\tau^2) \mathbf{K}_i \rvert^{-\frac{1}{2}} \left[\mathcal{L}_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i) \right]\bigg\rvert_{\beta_i = \mathbf{0}} \exp\left(\frac{1}{2} \left[\mathbf{h}_i (\mathbf{K}_i^{-1})^\top \mathbf{K}_i \mathbf{K}_i^{-1} \mathbf{h}_i \right] \right) \\ &amp;= \prod_{i = 1}^k \left\rvert D(\tau^2) \left[ D^{-1}(\tau^2) - \left[ \frac{\partial^2 \ell_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i)}{\partial \beta_i \partial \beta_i^\top} \right] \bigg\rvert_{\beta_i = \mathbf{0}}\right] \right\rvert^{-\frac{1}{2}} \left[\mathcal{L}_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i) \right]\bigg\rvert_{\beta_i = \mathbf{0}} \exp\left(\frac{1}{2} \left[\mathbf{h}_i \mathbf{K}_i^{-1} \mathbf{h}_i \right] \right) \\ &amp;= \prod_{i = 1}^k \left\rvert \mathbb{I}_{q \times q} - D(\tau^2) \left[ \frac{\partial^2 \ell_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i)}{\partial \beta_i \partial \beta_i^\top} \right] \bigg\rvert_{\beta_i = \mathbf{0}} \right\rvert^{-\frac{1}{2}} \left[\mathcal{L}_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i) \right]\bigg\rvert_{\beta_i = \mathbf{0}} \exp\left(\frac{1}{2} \left[ \frac{\partial \ell_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i)}{\partial \beta_i^\top } \right] \bigg\rvert_{\beta_i = \mathbf{0}} \left[ D^{-1}(\tau^2) - \left[ \frac{\partial^2 \ell_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i)}{\partial \beta_i \partial \beta_i^\top} \right] \bigg\rvert_{\beta_i = \mathbf{0}} \right]^{-1} \left[ \frac{\partial \ell_q(\mathbf{y}_i; \alpha, \tau^2 \rvert \beta_i)}{\partial \beta_i} \right] \bigg\rvert_{\beta_i = \mathbf{0}}\right) \end{aligned}\] </li> </ul> </div> <h3 id="an-aside">An Aside</h3> <p>Penalized quasi-likelihood and marginal quasi-likelihood are quite similar, but they do have notable differences. For one, MQL will not result in predictions for the random effects. Thus, it is not suitable for inference at the group level. In addition, the estimates of the fixed effects are essentially for the marginal model. Fitzmaurice et al.<d-cite key="fitzmaurice2011"></d-cite> note that MQL estimates are highly biased unless the random effects variance is near zero, regardless of the number of repeated measurements.</p> <p>In contrast, though PQL can be used to estimate the fixed effects and predict the random effects, there are some complications. For small counts or low numbers of repeated measurements, PQL underestimates the fixed effects as well as the variance components. PQL and MQL can also be reframed as approximations of a GLMM with an adjusted LMM (see below).</p> <p>We can reframe penalized and marginal quasi-likelihood as arising from a Gaussian approximation of the GLMM at hand. We’ll follow Chapter 15 in Fitzmaurice et al.<d-cite key="fitzmaurice2011"></d-cite> We assume to have the following generalized linear mixed model:</p> \[\begin{equation} \label{eq:glmm-y} \begin{aligned} \mathbf{y}_{i,j} \rvert \beta_i &amp;\sim \mathcal{F}(\mu_{i,j}); \\ \mu_{i,j} &amp;= g^{-1}\left(\eta_{i,j}\right) = g^{-1}\left(\alpha^\top \mathbf{x}_{i,j} + \beta_i^\top \mathbf{z}_{i,j}\right) \end{aligned} \end{equation}\] <p>We first approximate the response by assuming it can be represented as its conditional mean plus some Gaussian error:</p> \[\mathbf{y}_{i,j} \approx \mu_{i,j} + \epsilon_{i,j}\] <p>where, for some variance function $V(\cdot)$ of the conditional mean (e.g. $V(\mu_{i,j}) = \mu_{i,j}$ in the Poisson case), the errors satisfy:</p> \[\mathbb{E}\left[ \epsilon_{i,j} \right] = 0; \hspace{5mm} \text{Var}(\epsilon_{i,j} ) = V(\mu_{i,j}); \hspace{5mm} \text{Cov}(\epsilon_{i,j}, \epsilon_{i', j'} ) = 0\] <div class="theorem"> <strong>Claim (Approximating the Response).</strong> <ul id="claim-6" class="tab" data-tab="84ae5bd1-3f27-410c-9f6b-f64c38836b1d" data-name="claim-6"> <li class="active" id="claim-6-statement"> <a href="#">statement </a> </li> <li id="claim-6-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="84ae5bd1-3f27-410c-9f6b-f64c38836b1d" data-name="claim-6"> <li class="active"> <p>Since the link function is non-linear, we will linearize the mean with a first-order Taylor approximation about $\hat{\eta}_{i,j}$, the linear predictor estimated under $H_0$:</p> \[\mathbf{y}_{i,j} \approx g^{-1}(\hat{\eta}_{i,j}) + \delta(\hat{\eta}_{i,j}) (\eta_{i,j} - \hat{\eta}_{i,j}) + \epsilon_{i,j}\] <p>where \(\delta(\hat{\eta}_{i,j}) = \frac{\partial g^{-1}(\eta_{i,j})}{\partial \eta_{i,j}}\bigg\rvert_{\eta_{i,j} = \hat{\eta}_{i,j}}\), the gradient of the inverse link function w.r.t $\eta_{i,j}$ evaluated at the estimate under $H_0$.</p> </li> <li> \[\begin{aligned} \mathbf{y}_{i,j} &amp;\approx \left[g^{-1}(\eta_{i,j})\right]\bigg\rvert_{\eta_{i,j} = \hat{\eta}_{i,j}} + \frac{\partial g^{-1}(\eta_{i,j})}{\partial \eta_{i,j}}\bigg\rvert_{\eta_{i,j} = \hat{\eta}^{(c)}_{i,j}} (\eta_{i,j} - \hat{\eta}_{i,j}) + \epsilon_{i,j} \\ &amp;= g^{-1}(\hat{\eta}_{i,j}) + \delta(\hat{\eta}_{i,j}) (\eta_{i,j} - \hat{\eta}_{i,j}) + \epsilon_{i,j} &amp; \left(\delta(\hat{\eta}_{i,j}) = \frac{\partial g^{-1}(\eta_{i,j})}{\partial \eta_{i,j}}\bigg\rvert_{\eta_{i,j} = \hat{\eta}^{(c)}_{i,j}}\right) \end{aligned}\] </li> </ul> </div> <div class="theorem"> <strong>Claim (Working Response).</strong> <ul id="claim-7" class="tab" data-tab="a76a813c-611a-4e48-879a-011b0041afe0" data-name="claim-7"> <li class="active" id="claim-7-claim"> <a href="#">claim </a> </li> <li id="claim-7-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="a76a813c-611a-4e48-879a-011b0041afe0" data-name="claim-7"> <li class="active"> <p>We can rearrange the above as:</p> \[\mathbf{y}_{i,j}^\star \approx \alpha^\top \mathbf{x}_{i,j} + \beta_i^\top \mathbf{z}_{i,j} + \epsilon^\star_{i,j}\] <p>where \(\mathbf{y}_{i,j}^\star = \frac{1}{\delta(\hat{\eta}_{i,j})}\left(\mathbf{y}_{i,j} - \hat{\mu}_{i,j}\right) + \hat{\eta}_{i,j}\) and \(\epsilon^\star_{i,j} = \frac{1}{\delta(\hat{\eta}_{i,j})}\epsilon_{i,j}\).</p> </li> <li> \[\begin{aligned} &amp;\mathbf{y}_{i,j} \approx g^{-1}(\hat{\eta}_{i,j}) + \delta(\hat{\eta}_{i,j}) (\eta_{i,j} - \hat{\eta}_{i,j}) + \epsilon_{i,j} \\ \implies &amp;\mathbf{y}_{i,j} \approx \hat{\mu}_{i,j} + \delta(\hat{\eta}_{i,j})(\alpha^\top \mathbf{x}_{i,j} + \beta_i^\top \mathbf{z}_{i,j}) - \delta(\hat{\eta}_{i,j})(\hat{\alpha}^\top \mathbf{x}_{i,j} + \mathbf{0}^\top \mathbf{z}_{i,j}) + \epsilon_{i,j} \\ \implies &amp;\mathbf{y}_{i,j} + \delta(\hat{\eta}_{i,j})(\hat{\alpha}^\top \mathbf{x}_{i,j}) \approx \hat{\mu}_{i,j} + \delta(\hat{\eta}_{i,j})(\alpha^\top \mathbf{x}_{i,j} + \beta_i^\top \mathbf{z}_{i,j}) + \epsilon_{i,j} \\ \implies &amp;\mathbf{y}_{i,j} - \hat{\mu}_{i,j} + \delta(\hat{\eta}_{i,j})(\hat{\alpha}^\top \mathbf{x}_{i,j}) \approx \delta(\hat{\eta}_{i,j})(\alpha^\top \mathbf{x}_{i,j} + \beta_i^\top \mathbf{z}_{i,j}) + \epsilon_{i,j} \\ \implies &amp;\frac{1}{\delta(\hat{\eta}_{i,j})}\left(\mathbf{y}_{i,j} - \hat{\mu}_{i,j}\right) + \hat{\alpha}^\top \mathbf{x}_{i,j} \approx \alpha^\top \mathbf{x}_{i,j} + \beta_i^\top \mathbf{z}_{i,j} + \frac{1}{\delta(\hat{\eta}_{i,j})}\epsilon_{i,j} \\ \implies &amp;\frac{1}{\delta(\hat{\eta}_{i,j})}\left(\mathbf{y}_{i,j} - \hat{\mu}_{i,j}\right) + \hat{\eta}_{i,j} \approx \alpha^\top \mathbf{x}_{i,j} + \beta_i^\top \mathbf{z}_{i,j} + \frac{1}{\delta(\hat{\eta}_{i,j})}\epsilon_{i,j} \end{aligned}\] </li> </ul> </div> <p>Notice that since \(\epsilon^*_{i,j}\) is just a scaled version of $\epsilon_{i,j}$, it is also Gaussian with:</p> \[\begin{aligned} \mathbb{E}\left[ \epsilon^\star_{i,j} \right] &amp;= \mathbb{E}\left[ \frac{1}{\delta(\hat{\eta}_{i,j})} \epsilon_{i,j} \right] = 0; \\ \text{Var}\left(\epsilon^\star_{i,j} \right) &amp;= \text{Var}\left( \frac{1}{\delta(\hat{\eta}_{i,j})} \epsilon_{i,j} \right) = \frac{1}{\delta^2(\hat{\eta}_{i,j})} \text{Var}(\epsilon_{i,j}) = \frac{V(\hat{\mu}_{i,j})}{\delta^2(\hat{\eta}_{i,j})} \end{aligned}\] <p>The above essentially specifies a linear mixed model:</p> \[\mathbf{y}^\star_{i,j} \sim \mathcal{N}\left(\alpha^\top \mathbf{x}_{i,j} + \beta_i^\top \mathbf{z}_{i,j}, \frac{V(\hat{\mu}_{i,j})}{\delta^2(\hat{\eta}_{i,j})}\right)\] <p>To use this in practice, we can first estimate the parameters with iteratively reweighted least squares (or something like that), then use the estimates to compute the working response and errors. Then we can proceed how we would with a linear mixed model.</p>]]></content><author><name></name></author><category term="regression"/><category term="likelihood"/><category term="models"/><category term="primer"/><summary type="html"><![CDATA[A Primer]]></summary></entry><entry><title type="html">Generalized Linear Models</title><link href="https://aerosengart.github.io/blog/2025/glm/" rel="alternate" type="text/html" title="Generalized Linear Models"/><published>2025-06-03T00:00:00+00:00</published><updated>2025-06-03T00:00:00+00:00</updated><id>https://aerosengart.github.io/blog/2025/glm</id><content type="html" xml:base="https://aerosengart.github.io/blog/2025/glm/"><![CDATA[<p>This post is a primer on generalized linear models and their associated estimation procedures.</p> <h2 id="set-up">Set-Up</h2> <p>Let’s assume we have covariate matrix, $\mathbf{X}$, response vector $\mathbf{y}$, and parameter (coefficient) vector, $\beta$, and error vector, $\epsilon$, given by:</p> \[\mathbf{X} = \begin{bmatrix} 1 &amp; x_{1,1} &amp; \dots &amp; x_{1, m} \\ 1 &amp; x_{2, 1} &amp; \dots &amp; x_{2, m} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 1 &amp; x_{n, 1} &amp; \dots &amp; x_{n, m} \end{bmatrix}, \hspace{8mm} \mathbf{y} = \begin{bmatrix} y_1 \\ \vdots \\ y_n \end{bmatrix}, \hspace{8mm} \beta = \begin{bmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_m \end{bmatrix}, \hspace{8mm} \epsilon = \begin{bmatrix} \epsilon_1 \\ \vdots \\ \epsilon_n \end{bmatrix}\] <p>Assume that the following holds:</p> \[\begin{equation} \label{eq:assumption-1} g(\mathbf{y}) = \mathbf{X} \beta + \epsilon; \hspace{5mm} \epsilon_i \overset{iid}{\sim} F \end{equation}\] <p>where $g(\cdot)$ is some function (called a <i>link</i> function) and $F$ is some noise distribution. Equivalently, we can assume:</p> \[\begin{equation} \label{eq:assumption-2} \mathbb{E}\left[ g(\mathbf{y}) \rvert \mathbf{X} \right] = \mathbf{X}\beta + \mathbb{E}[\epsilon] \end{equation}\] <p>For the rest of the post, we’ll use $\mathbf{x}_i$ to denote:</p> \[(1, x_{i, 1}, \dots, x_{i, m})^\top\] <hr/> <h2 id="linear-models">Linear Models</h2> <p>Let’s first cover the basics of linear regression, which will be a good base upon which to build the theory of generalized linear models. For basic linear regression, we assume that $g(\cdot)$ is thhe identity function (i.e. $g(\mathbf{y}) = \mathbf{y}$) and that, conditional on $\mathbf{X}$, $F = \mathcal{N}(0, \sigma^2)$.</p> <h3 id="least-squares">Least Squares</h3> <p>We usually estimate $\beta$ via <i>ordinary least squares (OLS)</i>, which provides a closed form solution. The least squares objective is to minimize the <i>residual sum of squares</i>:</p> \[RSS(\beta) = \sum_{i = 1}^n (y_i - \mathbf{x}_i^\top \beta )^2 = (\mathbf{y} - \mathbf{X} \beta)^\top (\mathbf{y} - \mathbf{X} \beta)\] <p>The above function is quadratic in $\beta$, so we can differentiate with respect to $\beta$, set that equal to $0$, and solve for $\beta$ to find a minimizer:</p> \[\begin{aligned} &amp;\mathbf{X}^\top(\mathbf{y} - \mathbf{X} \beta) = 0 \\ \implies &amp;\mathbf{X}^\top \mathbf{y} - \mathbf{X}^\top \mathbf{X} \beta = 0 \\ \implies &amp;\hat{\beta}_{OLS} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y} \end{aligned}\] <p>This solution is unique if $\mathbf{X}^\top \mathbf{X}$ is non-singular.</p> <h4 id="generalized-least-squares">Generalized Least Squares</h4> <p><a href="https://en.wikipedia.org/w/index.php?title=Generalized_least_squares&amp;oldid=1292245629">Generalized least squares (GLS)</a> is a method to estimate the parameters in a linear regression model when there is correlation between the errors. Suppose we are in the linear regression setting as before except we assume that we have a <i>known</i> and non-singular conditional covariance matrix, $\Omega$, for the errors. That is:</p> \[\text{Cov}(\epsilon \rvert \mathbf{X}) = \Omega\] <p>GLS estimates $\beta$ through the following optimization problem:</p> \[\begin{aligned} \hat{\beta}_{GLS} &amp;= \underset{\beta}{\arg \min} \left\{ (\mathbf{y} - \mathbf{X} \beta)^\top \Omega^{-1} (\mathbf{y} - \mathbf{X}\beta) \right\} \\ &amp;= \underset{\beta}{\arg \min} \left\{ -2 \beta^\top \mathbf{X}^\top \Omega^{-1} \mathbf{y} + \beta^\top \mathbf{X}^\top \Omega^{-1} \mathbf{X} \beta \right\} \end{aligned}\] <p>The above expression is quadratic in $\beta$, so we take the gradient, set it equal to zero, and solve for $\beta$:</p> \[\begin{aligned} 0 &amp;= -2\mathbf{X}^\top \Omega^{-1} \mathbf{y} + 2\mathbf{X}^\top \Omega^{-1} \mathbf{X} \beta \\ \implies \hat{\beta}_{GLS} &amp;= (\mathbf{X}^\top \Omega{-1} \mathbf{X})^{-1} \mathbf{X}^\top \Omega{-1} \mathbf{y} \end{aligned}\] <p>Just like in OLS, the GLS estimator of $\beta$ is unbiased, and its conditional covariance matrix also has a similiar form:</p> \[\mathbb{E}[\hat{\beta}_{GLS} \rvert \mathbf{X}] = \beta, \hspace{8mm} \text{Cov}(\hat{\beta}_{GLS} \rvert \mathbf{X}) = (\mathbf{X}^\top \Omega^{-1} \mathbf{X})^{-1}\] <h4 id="weighted-least-squares">Weighted Least Squares</h4> <p><a href="https://en.wikipedia.org/w/index.php?title=Weighted_least_squares&amp;oldid=1279139018">Weighted least squares (WLS)</a> is equivalent to generalized least squares when $\Omega$ is a diagonal matrix. However, we’ll discuss it in more detail here for completeness.</p> <p>The idea behind WLS is that we may have more or less confidence in the reliability of our observations, so we want to weight their contributions to the objective function during estimation. Let $\mathbf{W}$ be an $n \times n$ diagonal matrix of weights. The WLS goal is to find:</p> \[\hat{\beta}_{WLS} = \underset{\beta}{\arg \min} \left\{ (\mathbf{y} - \mathbf{X} \beta)^\top \mathbf{W} (\mathbf{y} - \mathbf{X} \beta) \right\}\] <p>Since this is quadratic in $\beta$, we take the derivative with respect to $\beta$. Solving the following for $\beta$ will give us our estimate:</p> \[\begin{equation} \label{eq:wls-obj} \frac{\partial}{\partial \beta} \left[ (\mathbf{y} - \mathbf{X} \beta)^\top \mathbf{W} (\mathbf{y} - \mathbf{X} \beta) \right] = -2 \mathbf{X}^\top \mathbf{W} (\mathbf{y} - \mathbf{X}\beta) = 0 \end{equation}\] <p>As we showed in the <a href="#generalized-least-squares">generalized least squares section</a>, the solution to this problem is to set:</p> \[\hat{\beta}_{WLS} = (\mathbf{X}^\top \mathbf{W} \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{W} \mathbf{y}\] <p>However, this solution isn’t finished because we don’t know what $\mathbf{W}$ is! By the same argument in the OLS case, $\hat{\beta}_{WLS}$ is unbiased for $\beta$ for any $\mathbf{W}$, so how can we determine what the best weight matrix is?</p> <p>One way is to minimize the (weighted) mean squared error. For any vector of weights $\lambda = (\lambda_0, \lambda_1, \dots, \lambda_m)^\top$, we want to find the $\mathbf{W}$ such that $\hat{\beta}_{WLS}$ minimizes the expression:</p> \[\mathbb{E}\left[ (\lambda^\top(\hat{\beta}_{WLS} - \beta))^2 \right] = \text{Var}(\lambda^\top \hat{\beta}_{WLS})\] <p>where the equality follows from the unbiasedness of the WLS estimator.</p> <h3 id="likelihood">Likelihood</h3> <p>Since we have assumed a particular distribution for the errors, we can also estimate the coefficient vector with maximum likelihood (see <a href="/posts/2025/02/03/likelihood-theory.html">my likelihood post</a> for details). Using Eq. \eqref{eq:assumption-2}, our assumption is:</p> \[\mathbb{E}\left[ \mathbf{y} \rvert \mathbf{X} \right] = \mathbf{X}\beta\] <p>Since our observations are i.i.d., the (full) log-likelihood (technically conditional on $\mathbf{X}$…) is:</p> \[\begin{aligned} \ell(\mathbf{y}; \mathbf{X}, \beta) &amp;= \log\left( \prod_{i = 1}^n \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(- \frac{(y_i - \mathbf{x}_i \beta)^2}{2 \sigma^2} \right)\right) \\ &amp;= \frac{n}{\sqrt{2\pi \sigma^2}} - \frac{1}{2 \sigma^2}\sum_{i = 1}^n (y_i - \mathbf{x}_i \beta)^2 \end{aligned}\] <p>Taking the derivative with respect to $\beta$, setting this equal to zero, and solving for $\beta$ gives us our maximum likelihood estimate:</p> \[\begin{aligned} \frac{\partial \ell(\mathbf{y}; \mathbf{X}, \beta)}{\partial \beta} &amp;= - \frac{2}{2 \sigma^2}\sum_{i = 1}^n \mathbf{x}_i^\top (y_i - \mathbf{x}_i \beta) \\ &amp;= 0 \\ \implies \mathbf{X}^\top (\mathbf{y} - \mathbf{X} \beta) &amp;= 0 \\ \implies \hat{\beta}_{MLE} &amp;= (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y} \end{aligned}\] <p>We’ve just shown that the OLS and ML estimates of the parameters are equivalent!</p> <h3 id="uncertainty-quantification">Uncertainty Quantification</h3> <p>Often a simple estimate of $\beta$ is not enough; we want to be able to say how confident we are in our estimation. This is usually done through confidence intervals, which require estimates of the mean and variance of our estimates.</p> <p>Let $\hat{\beta}$ denote the OLS (and ML since they are equivalent) estimator for $\beta$. The mean vector of $\hat{\beta}$ is given by:</p> \[\begin{aligned} \mathbb{E}\left[ \hat{\beta} \rvert \mathbf{X} \right] &amp;= (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y} \\ &amp;= (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbb{E}\left[ \mathbf{y} \rvert \mathbf{X} \right] \\ &amp;= (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{X} \beta \\ &amp;= \beta \end{aligned}\] <div class="theorem"> <strong>Claim (Estimator Covariance).</strong> <ul id="covar-claim" class="tab" data-tab="71c85216-ba69-45a1-a54c-5534c6d860c0" data-name="covar-claim"> <li class="active" id="covar-claim-statement"> <a href="#">statement </a> </li> <li id="covar-claim-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="71c85216-ba69-45a1-a54c-5534c6d860c0" data-name="covar-claim"> <li class="active"> <p>We can similarly derive the covariance matrix of $\hat{\beta}$ as: \(\text{Cov}(\hat{\beta} \rvert \mathbf{X}) = \sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1}\)</p> </li> <li> <p>Let $\mathbf{X}$ be fixed, so we can drop the conditioning. Using the fact that $\mathbb{E}[\hat{\beta}] = \beta$, we see that:</p> \[\begin{aligned} \text{Cov}(\hat{\beta}) &amp;= \mathbb{E}\left[ (\hat{\beta} - \beta) (\hat{\beta} - \beta)^\top \right] \\ &amp;= \mathbb{E}\left[(\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y} \mathbf{y}^\top \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1} \right] - 2 \mathbb{E}\left[ \hat{\beta} \right]\beta^\top + \beta \beta^\top\\ &amp;= (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbb{E}\left[ \mathbf{y} \mathbf{y}^\top \right] \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1} - \beta \beta^\top \\ &amp;= (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbb{E}\left[ (\mathbf{X} \beta + \epsilon)(\mathbf{X} \beta + \epsilon)^\top \right] \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1} - \beta \beta^\top\\ &amp;= (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{X} \beta \beta^\top \mathbf{X}^\top \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1} - 2 (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{X} \beta \mathbb{E}\left[ \epsilon \right] \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1} + (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbb{E}\left[ \epsilon \epsilon^\top \right]\mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1} - \beta \beta^\top \\ &amp;= \beta \beta^\top + (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top (\sigma^2 \mathbb{I}_{n \times n}) \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1} - \beta \beta^\top \\ &amp;= \sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1} \end{aligned}\] </li> </ul> </div> <p>Putting the above results together, we see that the estimator has a Gaussian distribution:</p> \[\hat{\beta} \sim \mathcal{N}(\beta, \sigma^2 (\mathbf{X}^\top \mathbf{X}){-1})\] <p>We can use this information for additional inference about the parameters (e.g. hypothesis testing).</p> <hr/> <h2 id="generalized-linear-models">Generalized Linear Models</h2> <p>Generalized linear models are not that different from classical linear regression models. In this case, we assume that $\mathbf{y}$ follow a distribution from an overdispersed exponential family (e.g. Poisson, Binomial, exponential, Gaussian, etc.).<d-cite key="mccullagh1989"></d-cite></p> <div id="exponential-family"></div> <div class="definition"> <strong>Definition (Overdispersed Exponential Family).</strong> <br/> Let $\theta \in \mathbb{R}^{m \times 1}$ be a parameter vector. We say that the distribution of random vector $\mathbf{y} \in \mathbb{R}^{n \times 1}$ comes from an <i>overdispersed exponential family</i> if the probability density/mass function can be written as: $$ \begin{aligned} \label{eq:exp-fam} f_Y(\mathbf{y} \rvert \theta, \tau) = \prod_{i = 1}^n h(\mathbf{y}_i, \tau_i) \exp\left(\frac{\mathbf{b}(\theta_i) \mathbf{T}(\mathbf{y}_i) - A(\theta_i)}{d(\tau_i)} \right) \end{aligned} $$ <ul> <li>$\tau$ is the <i>dispersion parameter</i>, usually related to the variance somehow.</li> <li>$d(\tau_i)$ is some function of $\tau_i$ used to introduce the overdispersion, often taken to be $\frac{\tau}{\omega_i}$.</li> <li>$h(\mathbf{y}, \tau)$ is a non-negative function. </li> <li>The vector $\mathbf{b}(\theta)$ is a function of the parameter vector. If $\mathbf{b}(\theta) = \theta$, then we say the distribution is in <i>canonical form</i>. Any distribution can be written in canonical form by letting $\theta' = \theta$ and then using $\mathbf{b}(\theta') = \theta$.</li> <li>The vector $\mathbf{T}(\mathbf{y})$ is a sufficient statistic, meaning $f_Y(\mathbf{f} \rvert \mathbf{T}(\mathbf{y}))$ is independent of $\theta$.</li> <li>The scalar $A(\theta)$ is the <i>log-partition function</i> and is the natural logarithm of the normalization constant needed so that the above function integrates to one.</li> </ul> All $h(\mathbf{y}, \tau)$, $\mathbf{b}(\theta)$, $\mathbf{T}(\mathbf{y})$, $A(\theta)$, and $d(\tau)$ are assumed to be known. However, these functions are non-unique! For example, you can easily scale $\mathbf{T}(\theta)$ one by a constant and scale $\mathbf{b}(\theta)$ by the reciprocal. </div> <h3 id="model">Model</h3> <p>We’ll assume the response, $\mathbf{y}$, comes from an exponential family distribution so that the likelihood has the form in Eq. \eqref{eq:exp-fam}. We’ll also take $\mathbf{T}(\mathbf{y}) = \mathbf{y}$, $\mathbf{b}(\theta) = \theta$, and we’ll rewrite $h(\mathbf{y}, \tau)$ as $\exp(\log(h(\mathbf{y}, \tau)))$ so we can bring it inside the exponential function. Taking the natural logarithm gives us the log-likelihood:</p> \[\begin{equation} \label{eq:log-lik-glm} \begin{aligned} \ell(\mathbf{y}; \theta, \tau) &amp;= \log(f_Y(\mathbf{y} \rvert \theta, \tau)) \\ &amp;= \sum_{i = 1}^n \left[ \frac{\theta_i \mathbf{y}_i - A(\theta_i)}{d(\tau_i)} + \log(h(\mathbf{y}_i, \tau_i))\right] \end{aligned} \end{equation}\] <p>Taking the first- and second-order derivatives of the log-likelihood with respect to the parameter $\theta$ gives us the gradient and Hessian:</p> \[\begin{equation} \label{eq:grad-hessian} \begin{aligned} \frac{\partial \ell(\mathbf{y}; \theta, \tau)}{\partial \theta} &amp;= \begin{bmatrix} \frac{\mathbf{y}_1 - \frac{d A(\theta_1)}{d \theta_1}}{d(\tau_1)} \\ \vdots \\ \frac{\mathbf{y}_n - \frac{d A(\theta_n)}{d \theta_1}}{d(\tau_n)} \\ \end{bmatrix} \\ &amp;= \text{diag}\left( \frac{1}{d(\tau)} \right) \left[ \mathbf{y} - \frac{\partial A(\theta)}{\partial \theta}\right] \\ \frac{\partial^2 \ell(\mathbf{y}; \theta, \tau)}{\partial \theta \partial \theta^\top} &amp;= \text{diag}\left( \frac{1}{d(\tau)} \right) \begin{bmatrix} - \frac{d^2(A(\theta_1))}{d \theta_1 d \theta_1} &amp; \dots &amp; - \frac{d^2(A(\theta_1))}{d \theta_1 d \theta_n} \\ \vdots &amp; \ddots &amp; \vdots \\ - \frac{d^2(A(\theta_n))}{d \theta_n d \theta_1} &amp; \dots &amp; - \frac{d^2(A(\theta_n))}{d \theta_n d \theta_n} \end{bmatrix} \\ &amp;= -\text{diag}\left( \frac{1}{d(\tau)} \right)\frac{\partial^2 A(\theta)}{\partial \theta \partial \theta^\top} \end{aligned} \end{equation}\] <p>where we let $\tau = (\tau_1, \dots, \tau_n)^\top$.</p> <div class="theorem"> <strong>Claim (Mean and Variance).</strong> <ul id="mean-var-glm" class="tab" data-tab="285006e7-27ef-4606-81bd-7a4f12a55a94" data-name="mean-var-glm"> <li class="active" id="mean-var-glm-statement"> <a href="#">statement </a> </li> <li id="mean-var-glm-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="285006e7-27ef-4606-81bd-7a4f12a55a94" data-name="mean-var-glm"> <li class="active"> <p>Using properties of the likelihood function, we can derive the mean (denoted by $\mu$) and variance of $\mathbf{y}$ as:</p> \[\begin{equation} \label{eq:mean-var-glm} \begin{aligned} \mu &amp;= \mathbb{E}[\mathbf{y}] = \frac{\partial A(\theta)}{\partial \theta} \\ \text{Cov}(\mathbf{y}) &amp;= \text{diag}\left( d(\tau) \right) \underbrace{\frac{\partial^2 A(\theta)}{\partial \theta \partial \theta^\top}}_{= V(\mu)} \end{aligned} \end{equation}\] </li> <li> <p>Under certain regularity conditions, which we’ll assume hold here, the expectation of the score function (the gradient of the log-likelihood) should equal zero (see <a href="/blog/2025/likelihood-theory">my likelihood post</a> for a discussion on this). Thus:</p> \[\begin{aligned} &amp;\mathbb{E}\left[ \frac{\partial \ell(\mathbf{y}; \theta, \tau)}{\partial \theta} \right] = \mathbf{0} \\ \implies &amp;\text{diag}\left( \frac{1}{d(\tau)} \right) \left(\mathbb{E}[\mathbf{y}] - \frac{\partial A(\theta)}{\partial \theta} \right) = \mathbf{0} \\ \implies &amp;\mathbb{E}[\mathbf{y}] = \frac{\partial A(\theta)}{\partial \theta} \end{aligned}\] <p>Under these conditions, we also have that the Fisher information (the variance of the score) is the negative expectation of the Hessian. Thus:</p> \[\begin{aligned} &amp;\mathbb{E}\left[ \frac{\partial \ell(\mathbf{y}; \theta, \tau)}{\partial \theta} \frac{\partial \ell(\mathbf{y}; \theta, \tau)}{\partial \theta^\top} \right] = -\mathbb{E}\left[\frac{\partial^2 \ell(\mathbf{y}; \theta, \tau)}{\partial \theta \partial \theta^\top} \right] \\ \implies &amp;\mathbb{E}\left[ \text{diag}\left( \frac{1}{d^2(\tau)} \right) \left(\mathbf{y} - \frac{\partial A(\theta)}{\partial \theta}\right)\left(\mathbf{y} - \frac{\partial A(\theta)}{\partial \theta}\right)^\top\right] = - \mathbb{E}\left[ -\text{diag}\left( \frac{1}{d(\tau)} \right)\frac{\partial^2 A(\theta)}{\partial \theta \partial \theta^\top} \right] \\ \implies &amp; \text{diag}\left( \frac{1}{d^2(\tau)} \right)\mathbb{E}\left[ \left(\mathbf{y} -\mathbb{E}[\mathbf{y}]\right)\left(\mathbf{y} - \mathbb{E}[\mathbf{y}]\right)^\top\right] = \text{diag}\left( \frac{1}{d(\tau)} \right)\frac{\partial^2 A(\theta)}{\partial \theta \partial \theta^\top} \\ \implies &amp;\text{Cov}(\mathbf{y}) = \text{diag}\left(d(\tau)\right) \frac{\partial^2 A(\theta)}{\partial \theta \partial \theta^\top} \end{aligned}\] </li> </ul> </div> <p>Eq. \eqref{eq:mean-var-glm} shows that the covariance matrix of $\mathbf{y}$ is the product of a function of the dispersion parameter and a function of the parameter vector $\theta$. In the literature, $\frac{\partial^2 A(\theta)}{\partial \theta \partial \theta^\top}$ is often referred to as the <i>variance function</i> and denoted by $V(\mu)$ (where $\mu$ is the mean $\mathbb{E}[\mathbf{y}]$) since a function of the parameters is just a function of the mean as $\mathbf{X}$ is fixed.</p> <p>Arguably more importantly, the righthand side of $\mu = \frac{\partial A(\theta)}{\partial \theta}$ is some function of $\theta$, which we’ll denote by $s^{-1}(\theta)$. This lets us link the mean with the canonical parameter through the expression:</p> \[\begin{equation} \label{eq:link-mean-param} \theta = s(\mu) \end{equation}\] <p>Recall that we assumed that $\mathbb{E}\left[ g(\mathbf{y}) \rvert \mathbf{X} \right] = \mathbf{X} \beta$, which is equivalent to the assumption that $\mu = g^{-1}(\mathbf{X}\beta)$. Thus, we can relate the likelihood parameter, $\theta$, to the regression parameters, $\beta$, by:</p> \[\begin{equation} \label{eq:link-param-beta} \theta = s(g^{-1}(\mathbf{X}\beta)) \end{equation}\] <p>Now we have everything we need for our generalized linear model!</p> <h3 id="estimation">Estimation</h3> <p>The parameter estimates for generalized linear models are generally found through maximum likelihood methods. Since we assume a distribution from an exponential family, we can (usually) write a likelihood function as in Eq. \eqref{eq:log-lik-glm}:</p> \[\ell(\mathbf{y}; \theta, \tau) = \sum_{i = 1}^n \left[ \frac{\theta_i^\top \mathbf{y}_i - A(\theta_i)}{d(\tau_i)} + \log(h(\mathbf{y}_i, \tau_i)) \right]\] <p>As we usually do, we want to take the gradient with respect to the parameters of interest, $\beta$. However, this expression is not in terms of $\beta$, so we use the chain rule. The maximum likelihood estimate of $\beta$ is a solution to:</p> \[\begin{equation} \label{eq:score-beta} \begin{aligned} \frac{\partial \ell(\mathbf{y}; \theta, \tau)}{\partial \beta} &amp;= \frac{\partial \theta}{\partial \beta} \frac{\partial \ell(\mathbf{y}; \theta, \tau)}{\partial \theta} = \mathbf{0} \\ \implies \frac{\partial \ell(\mathbf{y}; \theta, \tau)}{\partial \beta} &amp;= \text{diag}\left( d(\tau) \right)\frac{\partial \theta}{\partial \beta} \left(\mathbf{y} - \frac{\partial A(\theta)}{\partial \theta}\right) = \mathbf{0} &amp; \left(\text{Eq. }\eqref{eq:grad-hessian} \right) \\ \implies \frac{\partial \ell(\mathbf{y}; \theta, \tau)}{\partial \beta} &amp;= \text{diag}\left( d(\tau) \right)\frac{\partial \theta}{\partial \beta} \left(\mathbf{y} - g^{-1}(\mathbf{X} \beta) \right) = \mathbf{0} &amp; \left(\text{Eq. }\eqref{eq:mean-var-glm} \right) \end{aligned} \end{equation}\] <div class="theorem"> <strong>Claim (Derivatives).</strong> <ul id="d-theta-d-beta" class="tab" data-tab="fc2a12eb-60eb-4d66-af30-a054f6eefc75" data-name="d-theta-d-beta"> <li class="active" id="d-theta-d-beta-statement"> <a href="#">statement </a> </li> <li id="d-theta-d-beta-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="fc2a12eb-60eb-4d66-af30-a054f6eefc75" data-name="d-theta-d-beta"> <li class="active"> <p>To find $\frac{\partial \theta}{\partial \beta}$, we use the chain rule again:</p> \[\begin{equation} \label{eq:d-theta-d-beta} \frac{\partial \theta}{\partial \beta} = \frac{\partial \eta}{\partial \beta} \frac{\partial \theta}{\partial \eta} = \text{diag}\left( \frac{1}{d(\tau)} \right)\mathbf{X}^\top \text{Cov}^{-1}(\mathbf{y})\left[ \frac{\partial g(\mu)}{\partial \mu} \right]^{-1} \end{equation}\] </li> <li> <p>The first derivative is easy:</p> \[\frac{\partial \eta}{\partial \beta} = \frac{\partial}{\partial \beta} [\mathbf{X} \beta] = \mathbf{X}^\top\] <p>The second is a bit trickier. By the chain rule:</p> \[\frac{\partial \theta}{\partial \eta} = \frac{\partial \theta}{\partial \mu} \frac{\partial \mu}{\partial \eta}\] <p>Using the fact that $\theta = s(\mu)$ and $\mu = \frac{\partial A(\theta)}{\partial \theta}$, we get:</p> \[\begin{aligned} &amp;\theta = s(\mu) \\ \implies &amp;\frac{\partial \theta}{\partial \theta} = \frac{\partial s(\mu)}{\partial \theta} = \frac{\partial s(\mu)}{\partial \mu} \frac{\partial \mu}{\partial \theta} \\ \implies &amp;\mathbb{I}_{n \times n} = \frac{\partial s(\mu)}{\partial \mu} \frac{\partial}{\partial \theta} \left[ \frac{\partial A(\theta)}{\partial \theta} \right] = \frac{\partial s(\mu)}{\partial \mu} \frac{\partial^2 A(\theta)}{\partial \theta \partial \theta^\top} \end{aligned}\] <p>Using Eq. \eqref{eq:mean-var-glm}, we get:</p> \[\begin{aligned} &amp;\mathbb{I}_{n \times n} = \frac{\partial s(\mu)}{\partial \mu} \\ \implies &amp;\text{diag}\left( \frac{1}{d(\tau)} \right) \text{Cov}(\mathbf{y}) \\ \implies &amp;\text{diag}\left( d(\tau) \right) \text{Cov}^{-1}(\mathbf{y}) = \frac{\partial s(\mu)}{\partial \mu} = \frac{\partial \theta}{\partial \mu} \end{aligned}\] <p>We can do the same sort of trick for $\frac{\partial \mu}{\partial \eta}$:</p> \[\begin{aligned} &amp;\mu = g^{-1}(\eta) \\ \implies &amp;\eta = g(\mu) \\ \implies &amp;\frac{\partial \eta}{\partial \eta} = \frac{\partial g(\mu)}{\partial \eta} = \frac{\partial g(\mu)}{\partial \mu} \frac{\partial \mu}{\partial \eta} \\ \implies &amp;\mathbb{I}_{n \times n} = \frac{\partial g(\mu)}{\partial \mu} \frac{\partial \mu}{\partial \eta} \\ \implies &amp;\frac{\partial \mu}{\partial \eta} = \left[ \frac{\partial g(\mu)}{\partial \mu} \right]^{-1} \end{aligned}\] <p>Putting the two results together, we get: \(\frac{\partial \theta}{\partial \eta} = \text{diag}\left( d(\tau) \right) \text{Cov}^{-1}(\mathbf{y}) \left[ \frac{\partial g(\mu)}{\mu} \right]^{-1}\)</p> </li> </ul> </div> <p>Using the fact that each component of $g(\mu)$ uses only the corresponding component in $\mu$, we see that $\left[ \frac{\partial g(\mu)}{\partial \mu} \right]^{-1}$ is diagonal, so the inverse is the matrix where we take the reciprocal of the diagonal elements. The same goes for $\text{Cov}^{-1}(\mathbf{y})$ since we assumed independent data (i.i.d. errors).</p> <p>Plugging Eq. \eqref{eq:d-theta-d-beta} into Eq. \eqref{eq:score-beta}, our objective becomes solving:</p> \[\begin{equation} \label{eq:new-obj} \frac{\partial \ell(\mathbf{y}; \theta, \tau)}{\partial \beta} = \mathbf{X}^\top \text{Cov}^{-1}(\mathbf{y})\left[ \frac{\partial g(\mu)}{\partial \mu} \right]^{-1} \left(\mathbf{y} - g^{-1}(\mathbf{X} \beta) \right) = \mathbf{0} \end{equation}\] <p>Unfortunately, there may not be a closed form solution to the above equation since $g^{-1}(\cdot)$ may be non-linear. To get around this problem, we do a linear approximation of $g^{-1}(\mathbf{X}\beta)$ and optimize this instead.</p> <p>Let $\hat{\beta}^{(t)}$ be a guess for the optimal value of $\beta$. Also let \(\eta^{(t)} = \mathbf{X}\beta^{(t)}\) be the linear predictor evaluated at this value. We can then write the first order Taylor approximation of \(g^{-1}(\eta)\) as:</p> \[\begin{equation} \label{eq:taylor-mu} g^{-1}(\eta) \approx g^{-1}(\eta^{(t)}) + \frac{\partial g^{-1}(\eta)}{\partial \eta} \bigg\rvert_{\eta = \eta^{(t)}} (\eta - \eta^{(t)}) = g^{-1}(\eta^{(t)}) + \mathbf{W}^{(t)}(\eta - \eta^{(t)}) \end{equation}\] <p>where $\mathbf{W}^{(t)} = \frac{\partial g^{-1}(\eta)}{\partial \eta} \bigg\rvert_{\eta = \eta^{(t)}}$ is the matrix of first order partial derivatives of $\mu$ with respect to $\eta$ evaluated at our guess for $\eta$, $\eta^{(t)}$. This matrix is diagonal since any component, $\mu_i$, of the mean vector is only a function of one corresponding component of the linear predictor $\eta_i$.</p> <p>We now use this approximation into Eq. \eqref{eq:new-obj}:</p> \[\begin{aligned} \frac{\partial \ell(\mathbf{y}; \theta, \tau)}{\partial \beta} &amp;\approx \mathbf{X}^\top \text{Cov}^{-1}(\mathbf{y})\left[ \frac{\partial g(\mu)}{\partial \mu} \right]^{-1}\bigg\rvert_{\eta = \eta^{(t)}} \left(\mathbf{y} - g^{-1}(\eta^{(t)}) + \mathbf{W}^{(t)} \left( \eta^{(t)} - \eta \right) \right) \\ &amp;= \mathbf{X}^\top \text{Cov}^{-1}(\mathbf{y})\left[ \frac{\partial g(\mu)}{\partial \mu} \right]^{-1}\bigg\rvert_{\eta = \eta^{(t)}} \mathbf{W}^{(t)} \left([\mathbf{W}^{(t)}]^{-1} (\mathbf{y} - g^{-1}(\eta^{(t)})) + \eta^{(t)} - \eta \right) \\ &amp;= \mathbf{X}^\top \text{Cov}^{-1}(\mathbf{y})\left[ \frac{\partial g(\mu)}{\partial \mu} \right]^{-2} \bigg\rvert_{\eta = \eta^{(t)}} \left(\frac{\partial g(\mu)}{\partial \mu}\bigg\rvert_{\eta = \eta^{(t)}} (\mathbf{y} - g^{-1}(\eta^{(t)})) + \eta^{(t)} - \eta \right) &amp; \left(\frac{\partial \mu}{\partial \eta} \bigg\rvert_{\eta = \eta^{(t)}} = \frac{\partial g^{-1}(\eta)}{\partial \eta} \bigg\rvert_{\eta = \eta^{(t)}} = \mathbf{W}^{(t)} = \left[\frac{\partial g(\mu)}{\partial \mu} \right]^{-1} \bigg\rvert_{\eta = \eta^{(t)}} \right) \end{aligned}\] <p>Letting $\mathbf{z}^{(t)} = \frac{\partial g(\mu)}{\partial \mu} \bigg\rvert_{\eta = \eta^{(t)}} (\mathbf{y} - g^{-1}(\eta^{(t)})) + \eta^{(t)}$, and defining $\tilde{\mathbf{W}}^{(t)} = \text{Cov}^{-1}(\mathbf{y})\left[ \frac{\partial g(\mu)}{\partial \mu} \right]^{-2}\bigg\rvert_{\eta = \eta^{(t)}}$, we can rewrite this as:</p> \[\frac{\partial \ell(\mathbf{y}; \theta, \tau)}{\partial \beta} \approx \mathbf{X}^\top \tilde{\mathbf{W}}^{(t)} \left(\mathbf{z}^{(t)} - \eta \right)\] <p>Notice that setting the above equal to zero is exactly Eq. \eqref{eq:wls-obj} with $\mathbf{z}^{(t)}$ instead of $\mathbf{y}$, so we can use weighted least squares. Though we have a closed form solution, since we are using an approximation, we need to iterate the above process until convergence. So at iteration $t + 1$, we will set:</p> \[\begin{aligned} \eta^{(t+1)} &amp;= \mathbf{X} \beta^{(t)} \\ \mu^{(t+1)} &amp;= g^{-1} \eta^{(t+1)} \\ \mathbf{W}^{(t+1)} &amp;= \text{Cov}^{-1}(\mathbf{y}) \left[ \frac{\partial g(\mu)}{\partial \mu} \right]^{-2} \bigg\rvert_{\eta = \eta^{(t+1)}} \\ \mathbf{z}^{(t+1)} &amp;= \eta^{(t+1)} + \frac{ \partial g(\mu)}{\partial \mu}\bigg\rvert_{\eta = \eta^{(t+1)}}(\mathbf{y} - g^{-1}(\eta^{(t+1)})) \\ \beta^{(t+1)} &amp;= (\mathbf{X}^\top \tilde{\mathbf{W}}^{(t+1)} \mathbf{X})^{-1} \mathbf{X}^\top \tilde{\mathbf{W}}^{(t+1)} \mathbf{z}^{(t+1)} \end{aligned}\] <p>This process is called <a href="https://en.wikipedia.org/w/index.php?title=Iteratively_reweighted_least_squares&amp;oldid=1279139010">iteratively reweighted least squares</a>.</p> <h3 id="uncertainty-quantification-1">Uncertainty Quantification</h3> <p>Due to the iterative nature of the estimation procedure, it is not so clear how to quantify our uncertainty in our estimates. Since there is no closed form solution to the MLEs, <a href=" https://statisticaloddsandends.wordpress.com/2020/11/20/variance-of-coefficients-for-linear-models-and-generalized-linear-models/">there is also no closed form for the variance of the MLEs</a>.</p> <p>One can, instead, derive the asymptotic variance of the estimator using <a href="/posts/2025/02/03/likelihood-theory.html">likelihood theory</a>. As a maximum likelihood estimator, it should be asymptotically normal with mean $\beta$ and covariance matrix $-\left[ \mathbb{E}\left[ \frac{\partial^2 \ell(\mathbf{y}; \theta, \tau)}{\partial \beta \beta^\top} \right] \right]^{-1}$ <i>if the model is correct</i>.</p>]]></content><author><name></name></author><category term="regression"/><category term="likelihood"/><category term="models"/><category term="primer"/><summary type="html"><![CDATA[A Primer]]></summary></entry><entry><title type="html">Quasi-Likelihood</title><link href="https://aerosengart.github.io/blog/2025/quasi-likelihood/" rel="alternate" type="text/html" title="Quasi-Likelihood"/><published>2025-05-30T00:00:00+00:00</published><updated>2025-05-30T00:00:00+00:00</updated><id>https://aerosengart.github.io/blog/2025/quasi-likelihood</id><content type="html" xml:base="https://aerosengart.github.io/blog/2025/quasi-likelihood/"><![CDATA[<p>This post is a review of quasi-likelihood theory and mostly relies upon Wedderburn<d-cite key="wedderburn1974"></d-cite> and Breslow &amp; Clayton<d-cite key="breslow1993"></d-cite>. Quasi-likelihood functions provide an alternative and less demanding way of characterizing the distribution of observations compared to specifying a true likelihood function. In essence, we simply assume a particular relationship between the mean and the variance rather than a particular distributional family. Then a so-called <i>quasi-likelihood</i> function can be defined and used for parameter estimation.</p> <hr/> <h2 id="set-up">Set-Up</h2> <p>Let $x_1, \dots, x_n$ denote our (independent) observations. Denote the expectation and variance of an arbitrary $x_i$ with $\mathbb{E}[x_i] = \mu_i$ and $\text{var}(x_i) = \phi V(\mu_i)$ where $\phi &gt; 0$ is some scale parameter. $V(\mu_i)$ is some (known) function of the mean.</p> <p>It’s important to note that the expectations and variances need not be identical, but we enforce that the variances are proportional to some (shared) function of the expectations. We assume that $\mu_i = g(\beta_1, \dots, \beta_m)$. That is, the expectations are some (known) function of parameters $\beta_1, \dots, \beta_m$.</p> <p>The quasi-likelihood is easier to explain after defining the <i>quasi-score</i> function.</p> <div class="definition"> <strong>Definition (Quasi-Score).</strong> <br/> The <i>quasi-score</i> of $x_i$ is given by: $$ \begin{equation} \label{eq:quasi-score} U(x_i; \mu_i) = \frac{x_i - \mu_i}{\phi V(\mu_i)} \end{equation} $$ </div> <p>In likelihood theory, the score function is the gradient of the log-likelihood function with respect to the parameters. In a similar fashion, the quasi-score is the gradient of the quasi-likelihood function with respect to the mean.</p> <div class="definition"> <strong>Definition (Quasi-Likelihood).</strong> <br/> The <i>quasi-likelihood</i> (or, more precisely, the <i>quasi log-likelihood</i>) of $x_i$ is given by: $$ \begin{equation} \label{eq:quasi-likelihood} \begin{aligned} \ell_q(x_i; \mu_i) &amp;= \int_{x_i}^{\mu_i} \frac{x_i - z}{\phi V(z)} dz \\ &amp;\iff \\ \frac{\partial}{\partial \mu_i} [\ell_q(x_i; \mu_i)] &amp;= U(x_i; \mu_i) \end{aligned} \end{equation} $$ </div> <p>The quasi-score satisfies several of the properties that the score in likelihood theory satisfies, which justifies its name as a <i>quasi</i> score.</p> <div id="theorem-1"></div> <div class="theorem"> <strong>Theorem 1.<d-cite key="wedderburn1974"></d-cite></strong> <ul id="theorem-1-wedderburn" class="tab" data-tab="4e1f23af-f181-4298-b07a-ee81f94d506d" data-name="theorem-1-wedderburn"> <li class="active" id="theorem-1-wedderburn-theorem"> <a href="#">theorem </a> </li> <li id="theorem-1-wedderburn-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="4e1f23af-f181-4298-b07a-ee81f94d506d" data-name="theorem-1-wedderburn"> <li class="active"> <p>Let $x$ be some observation with expectation $\mu$ and variance $\phi V(\mu)$ for some $\phi &gt; 0$. Suppose $\mu = g(\beta_1, \dots, \beta_m)$ for some (continuous and differentiable) funciton $g(\cdot)$. The quasi-score and quasi-likelihood, as defined in Eqs. \eqref{eq:quasi-score} and \eqref{eq:quasi-likelihood}, satisfy the following properties:</p> <ol> <li>$\mathbb{E}\left[ U(x; \mu) \right] = 0$</li> <li>$\mathbb{E}\left[ \frac{\partial \ell_q(x; \mu) }{\partial \beta_i} \right] = 0$ for all $i \in [m]$</li> <li>$\text{Var}(U(x; \mu)) = - \mathbb{E}\left[ \frac{\partial^2 \ell_q(x;\mu)}{\partial \mu^2}\right] = \frac{1}{\phi V(\mu)}$</li> <li>$\mathbb{E}\left[ \frac{\partial \ell_q(x; \mu)}{\partial \beta_i} \frac{\partial \ell_q(x; \mu)}{\partial \beta_j}\right] = - \mathbb{E}\left[ \frac{\partial^2 \ell_q(x; \mu)}{\partial \beta_i \partial \beta_j} \right] = \frac{1}{\phi V(\mu)} \frac{\partial \mu}{\partial \beta_i} \frac{\partial \mu}{\partial \beta_j}$</li> </ol> </li> <li> <p>Proving (1) is simple:</p> \[\mathbb{E}\left[ U(x; \mu) \right] = \mathbb{E}\left[ \frac{x - \mu}{\phi V(\mu)} \right] = \frac{1}{\phi V(\mu)} \mathbb{E}\left[ x - \mu\right] = \frac{1}{\phi V(\mu)} (\mu - \mu) = 0\] <p>To show (2), we note that:</p> \[\frac{\partial \ell_q(x; \mu)}{\partial \beta_i} = \frac{\partial \ell_q(x; \mu)}{\mu} \frac{\partial \mu}{\beta_j} \hspace{5mm} \implies \hspace{5mm} \mathbb{E}\left[ \frac{\partial \ell_q(x; \mu) }{\partial \beta_i} \right] = \mathbb{E}\left[ \frac{\partial \ell_q(x; \mu)}{\partial \mu} \frac{\partial \mu}{\beta_i} \right] = \frac{\partial \mu}{\beta_i} \mathbb{E}\left[ U(x; \mu) \right] = 0 \nonumber\] <p>Showing (3) is also relatively easy. We first show that $\text{Var}(U(x;\mu)) = \frac{1}{\phi V(\mu)}$:</p> \[\text{Var}(U(x; \mu)) = \text{Var}\left(\frac{x - \mu}{\phi V(\mu)}\right) = \frac{1}{\phi^2 V^2(\mu)} \text{Var}(x - \mu) = \frac{1}{\phi^2 V^2(\mu)} \text{Var}(x) = \frac{\phi V(\mu)}{\phi^2 V^2(\mu)} = \frac{1}{\phi V(\mu)}\] <p>Next, we show that $- \mathbb{E}\left[ \frac{\partial^2 \ell_q(x;\mu)}{\partial \mu^2}\right] = \frac{1}{\phi V(\mu)}$:</p> \[- \mathbb{E}\left[ \frac{\partial^2 \ell_q(x;\mu)}{\partial \mu^2}\right] = - \mathbb{E}\left[ \frac{\partial U(x; \mu)}{\partial \mu} \right] = - \mathbb{E}\left[ \frac{\partial}{\partial \mu} \left[ \frac{x - \mu}{\phi V(\mu)}\right] \right] = - \mathbb{E}\left[ \frac{\phi V(\mu)(-1) - (x- \mu)(\phi V'(\mu))}{\phi^2 V^2(\mu)} \right] = \frac{1}{\phi V(\mu)} + \frac{(\mathbb{E}[x]- \mu)(\phi V'(\mu))}{\phi^2 V^2(\mu)} = \frac{1}{\phi V(\mu)}\] <p>For (4), we first show that $\mathbb{E}\left[ \frac{\partial \ell_q(x; \mu)}{\partial \beta_i} \frac{\partial \ell_q(x; \mu)}{\partial \beta_j}\right] = \frac{1}{\phi V(\mu)} \frac{\partial \mu}{\partial \beta_i} \frac{\partial \mu}{\partial \beta_j}$:</p> \[\begin{aligned} \mathbb{E}\left[ \frac{\partial \ell_q(x; \mu)}{\partial \beta_i} \frac{\partial \ell_q(x; \mu)}{\partial \beta_j}\right] &amp;= \mathbb{E}\left[ \frac{\partial \ell_q(x;\mu)}{\partial \mu} \frac{\partial \mu}{\partial \beta_i} \frac{\partial \ell_q(x;\mu)}{\partial \mu} \frac{\partial \mu}{\partial \beta_j}\right] \\ &amp;= \mathbb{E}\left[ \left( \frac{\partial \ell_q(x;\mu)}{\partial \mu} \right)^2 \right] \frac{\partial \mu}{\partial \beta_i} \frac{\partial \mu}{\partial \beta_j} \\ &amp;= \mathbb{E}\left[ U^2(x; \mu) \right] \frac{\partial \mu}{\partial \beta_i} \frac{\partial \mu}{\partial \beta_j} \\ &amp;= \mathbb{E}\left[ \frac{(x - \mu)^2}{\phi^2 V^2(\mu)} \right] \frac{\partial \mu}{\partial \beta_i} \frac{\partial \mu}{\partial \beta_j} \\ &amp;= \frac{\phi V(\mu)}{\phi^2 V(\mu)} \frac{\partial \mu}{\partial \beta_i} \frac{\partial \mu}{\partial \beta_j} \\ &amp;= \frac{1}{\phi V(\mu)} \frac{\partial \mu}{\partial \beta_i} \frac{\partial \mu}{\partial \beta_j} \end{aligned}\] <p>Then we show that $- \mathbb{E}\left[ \frac{\partial^2 \ell_q(x; \mu)}{\partial \beta_i \partial \beta_j} \right] = \frac{1}{\phi V(\mu)} \frac{\partial \mu}{\partial \beta_i} \frac{\partial \mu}{\partial \beta_j}$:</p> \[\begin{aligned} - \mathbb{E}\left[ \frac{\partial^2 \ell_q(x; \mu)}{\partial \beta_i \partial \beta_j} \right] &amp;= - \mathbb{E}\left[ \frac{\partial}{\partial \beta_j} \left[ \frac{\partial \ell_q(x; \mu)}{\partial \mu} \frac{\partial \mu}{\partial \beta_i}\right] \right] \\ &amp;= - \mathbb{E}\left[ \frac{\partial}{\partial \beta_j} \left[ U(x; \mu) \frac{\partial \mu}{\partial \beta_i}\right] \right] \\ &amp;= - \mathbb{E}\left[ \frac{\partial}{\partial \beta_j} \left[ \frac{x - \mu}{\phi V(\mu)} \frac{\partial \mu}{\partial \beta_i}\right] \right] \\ &amp;= - \mathbb{E}\left[ \frac{\partial}{\partial \beta_j} \left[\frac{x-\mu}{\phi V(\mu)} \right] \frac{\partial \mu}{\partial \beta_i} \right] + \underbrace{\mathbb{E}\left[ \frac{x - \mu}{\phi V(\mu)}\frac{\partial}{\partial \beta_j} \left[\frac{\partial \mu}{\partial \beta_i} \right] \right]}_{=0} \\ &amp;= - \mathbb{E}\left[ \frac{\phi V(\mu) \frac{\partial}{\partial \beta_j}[x - \mu] - (x-\mu) \frac{\partial}{\partial \beta_j}[\phi V(\mu)]}{\phi^2 V^(\mu)}\right] \frac{\partial \mu}{\partial \beta_i} \\ &amp;= \left(- \mathbb{E}\left[ - \frac{1}{\phi V(\mu)} \frac{\partial \mu}{\partial \beta_j} \right] + \underbrace{\mathbb{E}\left[ x- \mu\right]}_{=0} \frac{\frac{\partial}{\partial \beta_j}[\phi V(\mu)]}{\phi^2 V^2(\mu)} \right) \frac{\partial \mu}{\partial \beta_i} \\ &amp;= \frac{1}{\phi V(\mu)} \frac{\partial \mu}{\partial \beta_j} \frac{\partial \mu}{\partial \beta_i} \end{aligned}\] </li> </ul> </div> <hr/> <h2 id="connections-to-likelihood">Connections To Likelihood</h2> <p>Suppose that the distribution of $x$ is a function of $\mu$ such that a log-likelihood can be explicitly written. Let $\ell(z; \mu)$ denote this log-likelihood. The following property is due to the above theorem:</p> <div class="theorem"> <strong>Claim.</strong> <ul id="claim-lik1" class="tab" data-tab="62a39c0c-245a-435f-a895-2a0d06ce82ed" data-name="claim-lik1"> <li class="active" id="claim-lik1-statement"> <a href="#">statement </a> </li> <li id="claim-lik1-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="62a39c0c-245a-435f-a895-2a0d06ce82ed" data-name="claim-lik1"> <li class="active"> \[\begin{equation} \label{eq:corollary-1} - \mathbb{E}\left[ \frac{\partial^2 \ell_q(x; \mu)}{\partial \mu^2} \right] \leq - \mathbb{E}\left[ \frac{\partial^2 \ell(x; \mu)}{\partial \mu^2} \right] \end{equation}\] </li> <li> <p>By (4) of <a href="#theorem-1">Theorem 1</a>:</p> \[- \mathbb{E}\left[ \frac{\partial^2 \ell_q(x; \mu)}{\partial \mu^2}\right] = \frac{1}{\phi V(\mu)}\] <p>Our problem then becomes showing that:</p> \[\frac{1}{\phi V(\mu)} \leq - \mathbb{E}\left[\frac{\partial^2 \ell(x; \mu)}{\partial \mu^2} \right] \hspace{5mm} \iff \hspace{5mm} \phi V(\mu) \geq - \frac{1}{\mathbb{E}\left[\frac{\partial^2 \ell(x; \mu)}{\partial \mu^2} \right]}\] <p>Under certain regularity conditions (see <a href="/posts/2025/02/03/likelihood-theory.html">my likelihood post</a>), we have that $-\mathbb{E}\left[\frac{\partial^2 \ell(x; \mu)}{\partial \mu^2} \right]$ is the Fisher information. The result follows directly from the <a href="https://en.wikipedia.org/wiki/Cramér–Rao_bound">Cramér-Rao bound</a>.</p> </li> </ul> </div> <p>Wedderburn provides an additional connection between quasi-likelihood and likelihood functions for one-parameter distributions specified by the mean.</p> <div id="theorem-2"></div> <div class="theorem"> <strong>Theorem 2.<d-cite key="wedderburn1974"></d-cite></strong> <ul id="theorem-2-wedderburn" class="tab" data-tab="99e5675e-42c0-447b-9248-464f0ae43fff" data-name="theorem-2-wedderburn"> <li class="active" id="theorem-2-wedderburn-statement"> <a href="#">statement </a> </li> <li id="theorem-2-wedderburn-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="99e5675e-42c0-447b-9248-464f0ae43fff" data-name="theorem-2-wedderburn"> <li class="active"> <p>Let $x$ be some observation with expectation $\mu$ and variance $\phi V(\mu)$ for some $\phi &gt; 0$. Suppose $\mu = g(\beta_1, \dots, \beta_m)$ for some (continuous and differentiable) function $g(\cdot)$. The log-likelihood function, $\ell(x; \mu)$, for $x$ satisfies:</p> \[\begin{equation} \label{eq:ll-condition} \frac{\partial}{\partial \mu} \left[ \ell(x; \mu) \right] = \frac{x - \mu}{\phi V(\mu)} \end{equation}\] <p>if and only if the density function of $x$ can be written, with respect to some measure, as:</p> \[f_x = \exp\left( x \theta - h(\theta) \right)\] </li> <li> <p>We first prove the forwards direction. Assume the log-likelihood satisfies Eq. \eqref{eq:ll-condition}. We integrate with respect to $\mu$:</p> \[\begin{aligned} &amp;\int \frac{\partial}{\partial \mu} \left[ \ell(x; \mu) \right] d\mu = \int \frac{x - \mu}{\phi V(\mu)} d\mu \\ \implies &amp;\ell(x; \mu) = \frac{x}{\phi} \int \frac{1}{V(\mu)} d \mu - \frac{1}{\phi}\int \frac{\mu}{V(\mu)} d\mu \end{aligned}\] <p>Substituting in $\theta = \frac{1}{\phi} \int \frac{1}{V(\mu)} d\mu$:</p> \[\ell(x; \mu) = x \theta - ?\] </li> </ul> </div> <p>The theorem can be summarized quite nicely: the quasi-likelihood function will equal the log-likelihood function <i>if and only if</i> the distribution comes from an exponential family.</p> <p>Extending the previous corollary, we see that for a one-parameter exponential family, Eq. \eqref{eq:corollary-1} obtains equality. Under certain regularity conditions (see <a href="/blog/2025/likelihood-theory">my likelihood post</a>), $-\mathbb{E}\left[\frac{\partial^2 \ell(x; \mu)}{\partial \mu^2} \right]$ is the Fisher information, which describes the amount of information about $\mu$ that is held in $x$.</p> <p>Since equality is obtained, we can also think of $-\mathbb{E}\left[\frac{\partial^2 \ell_q(x; \mu)}{\partial \mu^2} \right]$ as describing the amount of information about $\mu$ that is held in $x$. In addition, the difference between the former and the latter can be thought of as the amount of information gained by knowing, specifically, the distribution of $z$.</p> <hr/> <h2 id="estimation">Estimation</h2> <p>Let $x_{1:n}= (x_1, \dots, x_n)$ for independent observations $x_1, \dots, x_n$, and let $\mu_{1:n} = (\mu_1, \dots, \mu_n)$. We’ll denote the gradient of the (full) quasi-likelihood with respect to the parameters $\beta_1, \dots, \beta_m$ with:</p> \[\mathbf{u} = \frac{\partial \ell_q(x_{1:n}; \mu_{1:n})}{\partial \beta} = \begin{bmatrix} \frac{\partial \ell_q(x_{1:n}; \mu_{1:n})}{\partial \beta_1} \\ \vdots \\ \frac{\partial \ell_q(x_{1:n}; \mu_{1:n})}{\partial \beta_m} \end{bmatrix} \label{eq:gradient-ql}\] <p>By <a href="#theorem-1">Theorem 1</a>, $\mathbf{u}$ has mean vector:</p> \[\begin{equation} \label{eq:u-mean} \begin{aligned} \mathbb{E}[\mathbf{u}] &amp;= \mathbb{E}\left[ \frac{\partial \ell_q(x_{1:n}; \mu_{1:n})}{\partial \beta} \right] \\ &amp;= \mathbb{E}\left[ \begin{bmatrix} \sum_{i = 1}^n \frac{\partial \ell_q(x_i; \mu_i)}{\partial \beta_1} \\ \vdots \\ \sum_{i = 1}^n \frac{\partial \ell_q(x_i; \mu_i)}{\partial \beta_m} \\ \end{bmatrix} \right] \\ &amp;= \sum_{i = 1}^n \mathbb{E}\left[ \begin{bmatrix} \frac{\partial \ell_q(x_i; \mu_i)}{\partial \beta_1} \\ \vdots \\ \frac{\partial \ell_q(x_i; \mu_i)}{\partial \beta_m} \\ \end{bmatrix} \right]\\ &amp;= \mathbf{0} \end{aligned} \end{equation}\] <p>and covariance matrix:</p> \[\begin{equation} \label{eq:u-cov} \begin{aligned} \text{Cov}(\mathbf{u}) &amp;= \mathbb{E}\left[ \frac{\partial \ell_q(x_{1:n}; \mu_{1:n})}{\partial \beta} \frac{\partial \ell_q(x_{1:n}; \mu_{1:n})}{\partial \beta^\top}\right] \\ &amp;= - \mathbb{E}\left[\frac{\partial^2 \ell_q(x_{1:n}; \mu_{1:n})}{\partial \beta \partial \beta^\top}\right] \end{aligned} \end{equation}\] <p>The <i>maximum quasi-likelihood estimates</i> of $\beta$, denoted by $\hat{\beta}$, are found by setting $\mathbf{u}$ equal to $\mathbf{0}$ and solving for $\beta$, just like we would do for maximum likelihood estimation.</p> <div id="theorem-3"></div> <div class="theorem"> <strong>Theorem 3.<d-cite key="wedderburn1974"></d-cite></strong> <ul id="theorem-3-wedderburn" class="tab" data-tab="fc45be8a-016a-404b-98b8-98270f3b0d40" data-name="theorem-3-wedderburn"> <li class="active" id="theorem-3-wedderburn-theorem"> <a href="#">theorem </a> </li> <li id="theorem-3-wedderburn-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="fc45be8a-016a-404b-98b8-98270f3b0d40" data-name="theorem-3-wedderburn"> <li class="active"> <p>Let $x$ be some observation with expectation $\mu$ and variance $\phi V(\mu)$ for some $\phi &gt; 0$. Suppose $\mu = g(\beta_1, \dots, \beta_m)$ for some (continuous and differentiable) function $g(\cdot)$.</p> <p>Denote the gradient of the (full) quasi-likelihood with respect to the parameters $\beta_1, \dots, \beta_m$ with $\mathbf{u}$, and let $\hat{\beta}$ be the maximum quasi-likelihood estimates of $\beta$. The mean of $\hat{\beta}$ is approximately $\mathbf{0}$, and the covariance of $\hat{\beta}$ is approximately:</p> \[\text{Cov}(\hat{\beta}) \approx \text{Cov}^{-1}(\mathbf{u}) = \left[ -\mathbb{E}\left[ \frac{\partial^2 \ell_q(x_{1:n}; \mu_{1:n})}{\partial \beta \partial \beta^\top} \right] \right]^{-1}\] <p>if $\phi = 1$.</p> </li> <li> <p>Let $\hat{\mathbf{u}}$ denote the gradient vector evaluated at the maximum quasi-likelihood estimate, $\hat{\beta}$. Since $\hat{\beta}$ is the value of $\beta$ such that $\mathbf{u}$ equals $\mathbf{0}$, a first-order Taylor approximation of $\mathbf{u}$ gives us:</p> \[\begin{aligned} \mathbf{u} &amp;\approx \hat{\mathbf{u}} + \frac{\partial \mathbf{u}}{\partial \beta} (\beta - \hat{\beta}) \\ &amp;= \frac{\partial^2 \ell_q(x_{1:n}; \mu_{1:n})}{\partial \beta \partial \beta^\top} (\beta - \hat{\beta}) \\ \implies \beta - \hat{\beta} &amp;\approx \left[ \frac{\partial^2 \ell_q(x_{1:n}; \mu_{1:n})}{\partial \beta \partial \beta^\top} \right]^{-1} \mathbf{u} \end{aligned}\] <p>If we approximate the inverted matrix by its expectation, whose elements are given in <a href="#theorem-1">Theorem 1</a>, we get:</p> \[\begin{aligned} &amp;\beta - \hat{\beta} \approx -\text{Cov}^{-1}(\mathbf{u})\mathbf{u} \\ \implies &amp;\hat{\beta} \approx \beta + \text{Cov}^{-1}(\mathbf{u})\mathbf{u} \end{aligned}\] <p>Since $\mathbf{u}$ has expectation zero, it is clear that $\mathbb{E}[\hat{\beta}] \approx \mathbf{0}$ as well.</p> <p>The first term on the right-hand side of the above expression is fixed (the true parameter value), so the <i>approximate</i> covariance matrix of $\hat{\beta}$ is:</p> \[\begin{aligned} \text{Cov}(\hat{\beta}) &amp;\approx \text{Cov}\left( \beta + \text{Cov}^{-1}(\mathbf{u})\mathbf{u} \right) \\ &amp;= \mathbb{E}\left[ \text{Cov}^{-1}(\mathbf{u})\mathbf{u} \left(\text{Cov}^{-1}(\mathbf{u})\mathbf{u}\right)^\top \right] \\ &amp;= \text{Cov}^{-1}(\mathbf{u})\mathbb{E}\left[ \mathbf{u} \mathbf{u}^\top\right] \text{Cov}^{-1}(\mathbf{u}) \\ &amp;= \text{Cov}^{-1}(\mathbf{u}) \end{aligned}\] </li> </ul> </div> <p>The above theorem holds only for a scale parameter $\phi = 1$. If we relax this assumption to $\phi &gt; 0$, the expectation does not change, but we need to estimate $\phi$ before we can approximate the covariance of the maximum quasi-likelihood estimates.</p>]]></content><author><name></name></author><category term="glmm"/><category term="likelihood"/><category term="theory"/><category term="primer"/><summary type="html"><![CDATA[A Primer]]></summary></entry><entry><title type="html">Betting-Based Confidence Sequences</title><link href="https://aerosengart.github.io/blog/2025/betting-bounds/" rel="alternate" type="text/html" title="Betting-Based Confidence Sequences"/><published>2025-05-21T00:00:00+00:00</published><updated>2025-05-21T00:00:00+00:00</updated><id>https://aerosengart.github.io/blog/2025/betting-bounds</id><content type="html" xml:base="https://aerosengart.github.io/blog/2025/betting-bounds/"><![CDATA[<p>A colleague introduced me to some recent work from Waudby-Smith and Ramdas here at Carnegie Mellon. Since I’ve been working on applications of concentration bounds, it certainly seems important to review their paper <i>Estimating means of bounded random variables by betting</i>, which can often achieve better bounds than the older work from Maurer and Pontil<d-cite key="maurer2009"></d-cite>. Here, I’m going to use the preprint, but their work has been published in JRSSB<d-cite key="waudbysmith2022"></d-cite>.</p> <p>This paper (among many others<d-cite key="ramdas2022"></d-cite><d-cite key="ramdas2023"></d-cite><d-cite key="shafer2019"></d-cite><d-cite key="howard2021"></d-cite>) set inference in the context of playing a betting game with two players. For each “round”, we obtain an observation (i.e. a realization of some random variable). The Forecaster assigns a probability to the observations, and the Skeptic places a bet of the form of a real-valued function of the observations. In each “round”, the Skeptic pays a cost equal to the expected value of the observation and earns a payout of the observed value.</p> <p>A lot of the discussion relies on measure and probability theory. I’ve covered most of the basic concepts in <a href="/posts/2025/04/30/measure-theory">my post on measure theory</a>, so I’ll refer the reader to that if you are looking for definitions and intuition for the relevant concepts here.</p> <p>Note: Not all of the proofs are finished/included. I am hoping to find the time to return to this post and complete them.</p> <hr/> <h2 id="background">Background</h2> <p>The paper focuses on constructing valid (and tight) confidence sets for the mean of a sequence of bounded random variables.</p> <h3 id="notation">Notation</h3> <p>Here, we’ll recap the important notation introduced in Waudby-Smith and Ramdas<d-cite key="waudbysmith2022"></d-cite>. We denote the set of all distributions on $[0, 1]$ with mean $m$ with $\mathcal{Q}^m$.</p> <p>Let \((X_t)_{t = 1}^\infty \sim P\) be a sequence of random variables taking values in $[0, 1]$ with (unknown) conditional mean $\mu \in [0, 1]$ where $P \in \mathcal{P}^\mu$ (we use the notation we defined earlier; i.e. $\mathcal{P}^\mu$ is the set of all distributions on $[0, 1]^\infty$ such that \(\mathbb{E}_P[X_t \rvert X_1, \dots, X_{t-1}] = \mu\)).</p> <p>We’ll use $(a \pm b)$ as an abbreviation of the interval $(a-b, a+b)$. We’ll also use $X_1^t$ to denote $(X_1, \dots, X_t)$. Let \(\mathcal{F}_t = \sigma(X_1^t)\) denote the $\sigma$-field generated by $X_1^t$. We’ll suppose that $\mathcal{F}_0$ is the trivial $\sigma$-field (that is, the $\sigma$-field containing the empty set and the entire set itself). We’ll use \(\mathcal{F} = \{ \mathcal{F}_t \}_{t = 0}^\infty\) to denote the canonical filtration of $X_1^t$ where \(\mathcal{F}_0 \subset \mathcal{F}_1 \subset \mathcal{F}_2 \subset \dots\).</p> <p>A stochastic process $(M_t)_{t = 0}^\infty$ adapted to canonical filtration $\mathcal{F}$ with $M_0 = 1$ is termed a <i>test supermartingale for $P$</i> if:</p> \[\mathbb{E}_P[M_t \rvert \mathcal{F}_{t - 1}] \leq M_{t-1} \hspace{5mm} \forall t \geq 1\] <p>If instead, \(\mathbb{E}_P[M_t \rvert \mathcal{F}_{t - 1}] = M_{t-1}\), then the process is called a <i>test martingale</i>. If either of these hold for all $P \in \mathcal{P}$, then we say that \((M_t)_{t = 0}^\infty\) is a <i>test (super)martingale for $\mathcal{P}$</i>. Finally, a sequence \((\lambda_t)_{t = 1}^\infty\) is called <i>predictable</i> if $\lambda_t$ is $\mathcal{F}_{t-1}$-measurable for all $t \geq 1$.</p> <h3 id="confidence-sets-and-sequences">Confidence Sets and Sequences</h3> <p>Recall that for some random sample $(X_t)_1^n \sim P$ and error tolerance $\alpha \in (0, 1)$, a $(1 - \alpha)$ confidence interval (CI) for $\mu$ is given by the random set (which is dependent upon our sample) such that the true conditional $\mu$ is contained in our interval with probability at least $1 - \alpha$ for any distribution in family $\mathcal{P}^\mu$.</p> \[C_n = C(X_1, \dots, X_n) \subseteq [0, 1] \hspace{5mm} \text{such that } \forall n \geq 1, \underset{P \in \mathcal{P}^\mu}{\inf} \left\{ P(\mu \in C_n) \right\} \geq 1 - \alpha\] <p>Hoeffding’s inequality leads to the CI:</p> \[C_n = \left( \bar{X}_n \pm \sqrt{\frac{\log(2/\alpha)}{2n}} \right) \cap [0, 1]\] <p>where the $n$ comes from the fact that $X_i \in [0, 1]$, and the intersection restricts our interval to the known possible values that $\mu$ can take on.</p> <p>An interval implies a contiguous region of $\mathbb{R}$, but we could have the union of disjoint subsets. This is why we use the term “confidence set”.</p> <p>A confidence set is constructed for some sample of fixed size $n$. An alternative scenario is that we are observing data in batches, and we want to construct a new confidence set for each batch such that the sequence of sets $(C_t)_{t=1}^\infty$ satisfies:</p> \[\underset{P \in \mathcal{P}^\mu}{\sup} \left\{ P(\exists t \geq 1: \mu \neq C_t) \right\} \leq \alpha\] <p>In words, we want the sequence of sets to be such that the probability of there being some set that does not contain $\mu$ to be at most $\alpha$ for any distribution in the family $\mathcal{P}^\mu$. We call such a $(C_t)_{t=1}^\infty$ a <i>confidence sequence</i>, and, ideally, we want $\underset{t \rightarrow \infty}{\lim} C_t = { \mu }$ (that is, we want the sequence to converge to the confidence set that is only the true value as we observe more and more data).</p> <p>Notably, the above definitions hold for arbitrary stopping times, $\tau$. Since these can, and are often desired to be, data-dependent, we need a way to decide when our data is sufficient enough to stop constructing our sequences (we have achieved the result we wanted, we have enough data, etc.).</p> <div id="sequential-test"></div> <div class="definition"> <strong>Definition (Level-$\alpha$ Sequential Test).</strong> <br/> Let $H_0$ be some hypothesis, and let $\alpha \in [0, 1]$ be some tolerance. A <i>level-$\alpha$ sequential test</i> is a sequence $(\psi_t)_{t \in T}$ (with index set $T$) taking values in $\{ 0, 1 \}$ such that $\mathbb{P}_{H_0}(\psi_\tau = 1) \leq \alpha$ for any arbitrary stopping time, $\tau$. </div> <p>A level-$\alpha$ sequential test will control the Type I error rate at $\alpha$ for <i>any stopping time</i>. This is by definition; the probability that $\psi_\tau$ outputs a $1$ is at most $\alpha$ under the null hypothesis.</p> <hr/> <h2 id="results">Results</h2> <p>The first theorem is a procedure to construct a confidence set for the mean of a sequence of bounded random variables using supermartingales.</p> <div id="theorem-1"></div> <div class="theorem"> <strong>Theorem 1.<d-cite key="waudbysmith2022"></d-cite></strong> <ul id="ws-theorem1" class="tab" data-tab="70a65369-cb49-4b01-963a-0d8ac57aaf47" data-name="ws-theorem1"> <li class="active" id="ws-theorem1-statement"> <a href="#">statement </a> </li> <li id="ws-theorem1-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="70a65369-cb49-4b01-963a-0d8ac57aaf47" data-name="ws-theorem1"> <li class="active"> <p>Let \((X_t)_{t = 1}^\infty \sim P\) be a sequence of (infinitely or finitely many) random variables taking values in $[0, 1]$ with unknown conditional mean $\mu$ and distribution \(P \sim \mathcal{P}^\mu\) where \(\mathcal{P}^\mu\) is the collection of all distributions on \([0, 1]^\infty\) such that \(\mathbb{E}_P\left[ X_t \rvert X_1 \dots X_{t-1}\right]\).</p> <ol> <li>For each $m \in [0, 1]$, we have a null hypothesis $H_0^m: P \in \mathcal{P}^m$.</li> <li>For each $m \in [0, 1]$, construct the non-negative process $M_t^m = M^m(X_1, \dots, X_t)$ (a function of $X_1, \dots, X_t$) such that $(M_t^\mu)_{t = 0}^\infty$ satisfies for each $P \in \mathcal{P}^\mu$, $(M_t^\mu)_{t = 0}^\infty$ is upperbounded by a test supermartingale for $P$. Note that this test supermartingale can change with $P$.</li> <li>For each $m \in [0, 1]$, let $(\phi_t^m)_{t = 1}^\infty$ denote the sequential test where $\phi_t^m = \mathbb{1}\left\{ M_t^m \geq \frac{1}{\alpha} \right\}$. Here, $\phi_t^m = 1$ indicates rejection of $H_0^m$ after $t$ observations.</li> <li>Define $C_t = \left\{ m \in [0, 1] \rvert \phi_t^{m} = 0 \right\}$ as the set of $m \in [0, 1]$ such that $\phi_t^m$ failes to reject $H_0^m$.</li> </ol> <p>$(C_t)_{t = 1}^\infty$ forms a $(1 - \alpha)$-confidence sequence fo $\mu$. That is, the following holds:</p> \[\underset{P \in \mathcal{P}}{\sup} \left\{ \mathbb{P}(\exists t \geq 1 \rvert \mu \not\in C_t) \right\} \leq \alpha\] </li> <li> <p>The proof is fairly simple and relies mainly on Ville’s inequality.</p> <p>For any $m \in [0, 1]$, by Step 2, if $M_t^m \geq \frac{1}{\alpha}$, then we know $\phi_t^m = 1$. Thus, under any $P \in \mathcal{P}^\mu$, the probability $\mathbb{P}_P(\exists t \geq 1 \rvert \phi_t^m = 1)$ is the same as the probability $\mathbb{P}_P\left(\exists t \geq 1 \rvert M_t^m \geq \frac{1}{\alpha}\right)$.</p> <p>By construction, $(M_t^\mu)_{t = 0}^\infty$ is upperbounded by some test supermartingale for each $P \in \mathcal{P}^\mu$, so we can apply Ville’s inequality to get:</p> \[\mathbb{P}_P(\exists t \geq 1 \rvert \phi_t^m = 1) = \mathbb{P}_P\left(\exists t \geq 1 \rvert M_t^m \geq \frac{1}{\alpha}\right) \leq \alpha \nonumber\] <p>Thus, $(\phi_t^m)_{t = 1}^\infty$ is a level $\alpha$ sequential hypothesis test.</p> <p>The construction of $(C_t)_{t=0}^\infty$ entails only choosing those $m \in [0, 1]$ such that $H_0^m$ is rejected (for a particular choice of $t$). Thus, there is some $t \geq 1$ such that $\mu \not\in C_t$ if, and only if, there is some $t \geq 1$ such that $\phi_t^\mu = 1$.</p> <p>The above holds for any $P \in \mathcal{P}^\mu$, which yields the desired result:</p> \[\underset{P \in \mathcal{P}^\mu}{\sup} \left\{ \mathbb{P}_P(\exists t \geq 1 \rvert \mu \not\in C_t) \right\} = \underset{P \in \mathcal{P}^\mu}{\sup} \left\{ \mathbb{P}_P(\exists t \geq 1 \rvert \phi_t^\mu = 1) \right\} \leq \alpha \nonumber\] </li> </ul> </div> <p>A few notes are worth mentioning. First, the process $(M_t^m)_{t = 0}^\infty$ that is constructed in Step 2 of the above theorem is an example of an <i>$e$-process</i> for $\mathcal{P}$ (see <a href="/posts/2025/martingales">my post on martingales</a>). Secondly, the power and utility of confidence sequences constructed in this way depends upon the choice of upperbounding test supermartingales. Poor choices can lead to bad performance (statistically speaking) and computational infeasibility. Lastly, because the confidence sequence is time-uniform (holds over all time points), we can pretend we have a fixed stopping point to derive a valid $(1-\alpha)$-confidence set/interval for $\mu$ for a fixed sample size (no more batches are gathered).</p> <hr/> <h2 id="predictable-plug-ins">Predictable Plug-Ins</h2> <p>The main results of this paper are based on the <i>predictable plug-in</i>, which is a sequence of predictable real-valued random variables that are chosen specially to be plugged into other sequences in order to form test (super)martingales. The choice of predictable plug-in depends upon the following Lemma.</p> <div id="lemma-1"></div> <div class="theorem"> <strong>Lemma 1: Predictable Plug-In Chernoff Supermartingales.<d-cite key="waudbysmith2022"></d-cite></strong> <ul id="ws-lemma1" class="tab" data-tab="5cdb1b75-3e16-4a1b-9153-7d9fe64cb695" data-name="ws-lemma1"> <li class="active" id="ws-lemma1-statement"> <a href="#">statement </a> </li> <li id="ws-lemma1-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="5cdb1b75-3e16-4a1b-9153-7d9fe64cb695" data-name="ws-lemma1"> <li class="active"> <p>Let \((X_t)_{t = 1}^\infty \sim P\) be a sequence of random variables. Let $\mathcal{F}$ denote the canonical filtration with respect to \((X_t)_{t =1}^\infty\).</p> <p>Suppose that, for some choice of $\mu$, $v_t$, and $\psi(\lambda)$, the following is satisfied, for each $t \geq 1$, for any $\lambda \in \Lambda \subseteq \mathbb{R}$:</p> \[\mathbb{E}_P\left[ \exp\left(\lambda(X_t - \mu) - v_t \psi(\lambda) \right) \rvert \mathcal{F}_{t-1} \right] \leq 1\] <p>For any $(\lambda_t)_{t=1}^\infty$ with $\lambda_t \in \Lambda$ for all $t$ that is predictable with respect to $\mathcal{F}$, we have that:</p> \[M_t^\psi(\mu) = \prod_{i = 1}^t \exp\left( \lambda_i(X_i - \mu) - v_i \psi(\lambda_i) \right)\] <p>is a test supermartingale with respect to $\mathcal{F}$.</p> </li> <li> <p>I am not sure how to prove it, but I think that the process defined above as \(M_t^\psi(\mu)\) is an example of Bernstein’s supermartingale<d-cite key="shafer2019"></d-cite> and is therefore a supermartingale. It suffices now to show that \(M_t^\psi(\mu)\) has expectation at most $1$ at $t = 1$ and that \(\mathbb{E}_P [ M_t^\psi(\mu) \rvert \mathcal{F}_{t-1} ] \leq M_{t-1}^\psi(\mu)\).</p> <p>Notice that, for $t = 1$, the conditional expectation of $M_t^\psi(\mu)$ is:</p> \[\mathbb{E}_P \left[M_1^\psi(\mu) \bigg\rvert \mathcal{F}_0 \right] = \mathbb{E}_P\left[ \exp\left(\lambda_1(X_1 - \mu) - v_1\psi(\lambda_1) \right) \right] \leq 1\] <p>by assumption. Because $\mathcal{F}_0$ is the trivial $\sigma$-field, which gives us no information, we drop the conditioning.</p> <p>Now consider $t \geq 2$. For any $i \leq t - 1$, \(\exp(\lambda_i(X_i - \mu) - v_i \psi(\lambda_i))\) is \(\mathcal{F}_{t-1}\)-measurable. This is because \((\lambda_t)_{t=1}^\infty\) is predictable with respect to $\mathcal{F}$, implying $\lambda_i$ is \(\mathcal{F}_{t-1}\)-measurable. To see why, consider probability space \((\Omega, \mathcal{A}, P)\) and sub-$\sigma$-field \(\mathcal{A}_1 \subset \mathcal{A}\). If a random variable $X$ is \(\mathcal{A}_1\)-measurable, then for any Borel set $B$, \(X^{-1}(B) \in \mathcal{A}_1 \subset \mathcal{A}\). Thus, $X$ is also \(\mathcal{A}\)-measurable. In addition, \(\mathcal{F}\) is the canonical filtration, so $X_i$ must be \(\mathcal{F}_{t - 1}\)-measurable since \(\mathcal{F}_{t-1} = \sigma(X_1, \dots, X_{t-1})\).</p> <p>Since \(\exp(\lambda_i(X_i - \mu) - v_i \psi(\lambda_i))\) is $\mathcal{F}_{t-1}$-measurable for all $i \leq t-1$, we know that \(\exp(\lambda_i(X_i - \mu) - v_i \psi(\lambda_i)) \in \mathcal{F}_{t-1}\). Since we are dealing with bounded random variables, we can use Theorem 4.1.14 from Durrett<d-cite key="durrett2019"></d-cite> to write the conditional expectation of $M_t^\psi(\mu)$ as:</p> \[\begin{aligned} \mathbb{E}_P \left[ M_t^\psi(\mu) \bigg\rvert \mathcal{F}_{t-1} \right] &amp;= \mathbb{E}_P \left[ \prod_{i = 1}^t \exp\left(\lambda_i(X_i - \mu) - v_i \psi(\lambda_i) \right) \bigg\rvert \mathcal{F}_{t-1}\right] \\ &amp;= \prod_{i = 1}^{t-1} \exp(\lambda_i(X_i - \mu) - v_i\psi(\lambda_i)) \underbrace{\mathbb{E}_P\left[ \exp\left( \lambda_t (X_t - \mu) - v_t \psi(\lambda_t)\right) \rvert \mathcal{F}_{t-1} \right]}_{\leq 1} \\ &amp;\leq \prod_{i = 1}^{t-1} \exp(\lambda_i(X_i - \mu) - v_i\psi(\lambda_i)) \\ &amp;= M_{t-1}^\psi(\mu) \end{aligned}\] <p>Since \(\mathbb{E}_P[M_t^\psi(\mu) \rvert \mathcal{F}_{t-1}] \leq M_{t-1}^\psi(\mu)\) for all $t \geq 1$, and \(\mathbb{E}_P[M_1^\psi(\mu) \rvert \mathcal{F}_0] \leq 1\), \((M_t^\psi(\mu))_{t = 1}^\infty\) is a test supermartingale.</p> </li> </ul> </div> <h3 id="hoeffding">Hoeffding</h3> <p>Let’s first define the <i>Hoeffding process</i>.</p> <div id="hoeffding-process"></div> <div class="definition"> <strong>Definition (Hoeffding Process).</strong> <br/> Let $(X_t)_{t = 1}^\infty \sim P$ be a sequence of random variables taking values in $[0, 1]$ with distribution $P \in \mathcal{P}^\mu$ where $\mathcal{P}$ is the set of distributions on $[0, 1]^\infty$ such that $\mathbb{E}_P[X_t \vert \mathcal{F}_{t-1}] = \mu$ for each $t$. <br/> The <i>Hoeffding process</i> for (<i>candidate mean</i>) $m \in [0, 1]$, denoted by $(M_t^H(m))_{t=0}^\infty$ is defined as: $$ M_t^{\text{PrPl-H}}(m) = \prod_{i = 1}^t \exp\left( \lambda_i(X_i - m) - \phi_H(\lambda_i)\right) $$ where $\psi_H(\lambda) = \frac{\lambda^2}{8}$ is an upper bound on the cumulant generating function for random variables in $[0, 1]$, and $(\lambda_t)_{t = 1}^\infty$ is a sequence of <i>predictable</i> $\lambda_t \in \mathbb{R}$ that is constructed. We also assume $M_0^{\text{PrPl-H}}(m) = 1$. </div> <p>This sequence is a test supermartingale for $\mathcal{P}^m$, and we call $(\lambda_t)_{t=1}^\infty$ a <i>predictable plug-in</i>.</p> <p>This result implies that if one can be clever about their choice of $(\lambda_t)_{t=1}^\infty$, one can construct a test supermartingale that can be used in Step 2 of <a href="#theorem-1">Theorem 1</a> to get a confidence sequence for $\mu$. This is formalized in the following:</p> <div id="proposition-1"></div> <div class="theorem"> <strong>Proposition 1: Predictable Plug-In Hoeffding CS.<d-cite key="waudbysmith2022"></d-cite></strong> <ul id="ws-prop1" class="tab" data-tab="2a1dbfaa-df99-4969-bec7-94ff3ea4d826" data-name="ws-prop1"> <li class="active" id="ws-prop1-statement"> <a href="#">statement </a> </li> <li id="ws-prop1-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="2a1dbfaa-df99-4969-bec7-94ff3ea4d826" data-name="ws-prop1"> <li class="active"> <p>Let \((X_t)_{t=1}^\infty \sim P\) be a stochastic process with distribution \(P \in \mathcal{P}^\mu\) where \(\mathcal{P}^\mu\) is the set of distributions on \([0, 1]^\infty\) such that \(\mathbb{E}_P[X \rvert \mathcal{F}_{t-1}] = \mu\) for each $t$. For any predictable \((\lambda_t)_{t=1}^\infty\) where \(\lambda_t \in \mathbb{R}\) for all $t$, we can construct a $(1-\alpha)$ confidence sequence for $\mu$ as:</p> \[C_t^{\text{PrPl-H}} = \left(\frac{\sum_{i =1 }^t \lambda_i X_i }{\sum_{i = 1}^t \lambda_i} \pm \frac{\log(2/\alpha) + \sum_{i = 1}^t \psi_H(\lambda_i)}{\sum_{i = 1}^t \lambda_i} \right)\] <p>The running intersection, \(\cap_{i \leq t} C_t^\text{PrPl-H}\), is also a valid $(1-\alpha)$ confidence sequence for $\mu$.</p> </li> <li> <p>Proof to be completed.</p> </li> </ul> </div> <p>The authors recommend choosing $(\lambda_t)_{t=1}^\infty$ such that:</p> \[\lambda_t^{PrPl-H} = \min \left\{ \sqrt{\frac{8 \log(2/\alpha)}{t \log(t + 1)}}, 1 \right\}\] <h3 id="empirical-bernstein">Empirical Bernstein</h3> <p>Similar to the Hoeffding process, we can define the <i>empirical Bernstein process</i>.</p> <div id="empirical-bernstein-process"></div> <div class="definition"> <strong>Definition (Empirical Bernstein Process).</strong> <br/> Let $(X_t)_{t = 1}^\infty \sim P$ be a sequence of random variables taking values in $[0, 1]$ with distribution $P \in \mathcal{P}^\mu$ where $\mathcal{P}$ is the set of distributions on $[0, 1]^\infty$ such that $\mathbb{E}_P[X_t \vert \mathcal{F}_{t-1}] = \mu$ for each $t$. The <i>empirical Bernstein process</i> for (<i>candidate mean</i>) $m \in [0, 1]$, denoted by $(M_t^{EB}(m))_{t=0}^\infty$ is defined as: $$ M_t^{EB}(m) = \prod_{i = 1}^t \exp\left( \lambda_i(X_i - m) - v_i \psi_{EB}(\lambda_i) \right) $$ where $v_i = 4(X_i - \hat{\mu}_{i - 1})^2$, $\psi_{EB}(\lambda) = (-\log(1-\lambda) - \lambda)/4$ for $\lambda \in [0, 1)$, and $(\lambda_t)_{t = 1}^\infty$ is a sequence of <i>predictable</i> $\lambda_t \in \mathbb{R}$ that is constructed. </div> <p>One can perform a similar procedure as before, using $(M_t^\text{PrPl-EB}(m))$ in Step 2 of <a href="#theorem-1">Theorem 1</a> to get a confidence sequence.</p> <div id="theorem-2"></div> <div class="theorem"> <strong>Theorem 2: Predictable Plug-In Empirical Bernstein CS.<d-cite key="waudbysmith2022"></d-cite></strong> <ul id="ws-theorem2" class="tab" data-tab="a4691818-0c20-4eb6-b3df-c0d9a0c5981c" data-name="ws-theorem2"> <li class="active" id="ws-theorem2-statement"> <a href="#">statement </a> </li> <li id="ws-theorem2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="a4691818-0c20-4eb6-b3df-c0d9a0c5981c" data-name="ws-theorem2"> <li class="active"> <p>Let \((X_t)_{t=1}^\infty \sim P\) be a stochastic process with distribution \(P \in \mathcal{P}^\mu\) where \(\mathcal{P}^\mu\) is the set of distributions on \([0, 1]^\infty\) such that \(\mathbb{E}_P[X \rvert \mathcal{F}_{t-1}] = \mu\) for each $t$. For any predictable \((\lambda_t)_{t=1}^\infty\) where $\lambda_t \in (0, 1)$ for all $t$, we can construct a $(1-\alpha)$ confidence sequence for $\mu$ as:</p> \[C_t^{\text{PrPl-EB}} = \left(\frac{\sum_{i =1 }^t \lambda_i X_i }{\sum_{i = 1}^t \lambda_i} \pm \frac{\log(2/\alpha) + \sum_{i = 1}^t v_i \psi_{EB}(\lambda_i)}{\sum_{i = 1}^t \lambda_i} \right)\] <p>The running intersection, \(\cap_{i \leq t} C_t^\text{PrPl-EB}\), is also a valid $(1-\alpha)$ confidence sequence for $\mu$.</p> </li> <li> <p>Proof to be completed.</p> </li> </ul> </div> <p>The authors recommend choosing $(\lambda_t)_{t=1}^\infty$ such that:</p> \[\begin{equation} \label{eq:empirical-bernstein-choices} \lambda_t^{PrPl-EB} = \min \left\{ \sqrt{\frac{2\log(2/\alpha)}{\hat{\sigma}_{t-1}^2 t \log(1 + t)}}, c \right\} \end{equation}\] <p>where:</p> \[\begin{aligned} \hat{\sigma}_t^2 &amp;= \frac{\frac{1}{4} + \sum_{i = 1}^t (X_i - \hat{\mu}_i)^2}{t+1} \\ \hat{\mu}_t &amp;= \frac{\frac{1}{2} + \sum_{i = 1}^t X_i}{t + 1} \\ c &amp;= \frac{1}{2} \text{ or } \frac{3}{4} \end{aligned}\] <p>This procedure yields two sequences, \((\hat{\mu}_t)_{t=1}^\infty\) and \((\hat{\sigma}_t^2)_{t=1}^\infty\), which can be thought of as predictable, regularized sample means and variances.</p> <p>For fixed time, we can simply use the running intersection to form a $(1-\alpha)$ confidence interval for $\mu$. Suppose we have $n$ samples. Then we can form a confidence interval as $C_n^{\text{PrPl-EB_CI}} = \cap_{i \leq n} C_i^{\text{PrPl-EB}}$ with any predictable sequence $(\lambda_t)_{t = 1}^\infty$. The authors recommend choosing:</p> \[\lambda_t^{\text{PrPl-EB}(n)} = \min \left\{ \sqrt{\frac{2 \log(2/\alpha)}{n \hat{\sigma}_{t-1}^2}} \right\}\] <p>with $\hat{\sigma}_i^2$ and $c$ as in Eq. \eqref{eq:empirical-bernstein-choices}.</p> <p>Something very interesting is that the width of the above confidence interval goes to $\sigma \sqrt{2 \log(2/\alpha)}$ for i.i.d. data (where $\sigma$ is the true standard deviation). In contrast, the original empriical Bernstein confidence intervals introduced by Maurer and Pontil (2009) only go to $\sigma \sqrt{2 \log(4 / \alpha)}$.</p> <hr/> <h2 id="a-betting-perspective">A Betting Perspective</h2> <p>In order to improve the confidence sequences derived above, the authors dive deeper into the gambling analogy. Suppose we are playing a game in which we can accumulate wealth by making bets against some hypothesis, $H_0^m$, being true. We start off with one dollar, and, in each “round”, we will make a bet that is determined by our predictable sequence $(\lambda_t^m)_{t=1}^\infty$. More specifically, we make bet $b_t = s_t\rvert \lambda_t^m \rvert$ where $s_t = -1$ if we think $\mu &lt; m$ and $s_t = 1$ if we think $\mu &gt; m$. The second term, $\rvert \lambda_t^m \rvert$, is the amount we are willing to lose/win in our bet (e.g. $\rvert \lambda_t^m \rvert = 0$ implies we are risking nothing).</p> <p>We can conceptualize the total wealth we have at any given time with the <i>capital process</i>.</p> <div id="capital-process"></div> <div class="definition"> <strong>Definition (Capital Process).</strong> <br/> Let $(X_t)_{t = 1}^\infty \sim P$ be a sequence of random variables taking values in $[0, 1]$ with distribution $P \in \mathcal{P}^\mu$ where $\mathcal{P}$ is the set of distributions on $[0, 1]^\infty$ such that $\mathbb{E}_P[X_t \vert \mathcal{F}_{t-1}] = \mu$ for each $t$. <br/> The <i>capital process</i> for any $m \in [0, 1]$ is defined as: $$ \mathcal{K}_t(m) = \prod_{i = 1}^t ( 1 + \lambda_i(m) \cdot (X_i - m)) $$ where $\mathcal{K}_0(m) = 1$ and $(\lambda_t(m))_{t = 1}^\infty$ is a predictable sequence taking values in $\left(-\frac{1}{1-m}, \frac{1}{m}\right)$ (where $\frac{1}{m} = \infty$ if $m = 0$ and $\frac{1}{1-m} = \infty$ if $m = 1$). </div> <p>There is a deep connection between test (super)martingales, bounded random variables, and the capital process that the authors emphasize. This is formalized below.</p> <div id="proposition-2"></div> <div class="theorem"> <strong>Proposition 2.<d-cite key="waudbysmith2022"></d-cite></strong> <ul id="ws-prop2" class="tab" data-tab="247ea126-460e-47ed-84b2-7bc75b34628c" data-name="ws-prop2"> <li class="active" id="ws-prop2-statement"> <a href="#">statement </a> </li> <li id="ws-prop2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="247ea126-460e-47ed-84b2-7bc75b34628c" data-name="ws-prop2"> <li class="active"> <p>Let $X_1, X_2, \dots \sim P$ be a sequence of random variables taking values in $[0, 1]$, and let $\mu \in [0, 1]$. The following are equivalent:</p> <ol> <li>$\mathbb{E}_P[X_t \rvert \mathcal{F}_{t-1}] = \mu$ for all $t \in \mathbb{N}$ where $\mathcal{F}_{t-1} = \sigma(X_1, \dots, X_{t-1})$</li> <li>There exists $\lambda \in \mathbb{R} \setminus \{ 0 \}$ such that the capital process $(\mathcal{K}_t(\mu))_{t = 0}^\infty$ is a strictly positive test martingale for $P$</li> <li>For any $\lambda \in \left(-\frac{1}{1-\mu}, \frac{1}{\mu} \right)$, the capital process $(\mathcal{K}_t(\mu))_{t = 0}^\infty$ is a test martingale for $P$</li> <li>For all $\left(-\frac{1}{1-\mu}, \frac{1}{\mu} \right)$-valued predictable sequence $(\lambda_t)_{t = 1}^\infty$, the capital process $(\mathcal{K}_t(\mu))_{t = 0}^\infty$ is a test martingale for $P$</li> </ol> </li> <li> <p>Proof to be completed.</p> </li> </ul> </div> <p>The main takeaway is that \((\mathcal{K}_t(\mu))_{t=0}^\infty\) is a test martingale (<i>not</i> supermartingale) for $P$ if the random variables have conditional expectation equal to $\mu$. If the hypothesis $H_0^m$ is true (i.e. $m = \mu$), then, by <a href="#proposition-2">Proposition 2</a>, we expect to not make any money since the conditional expectation is constant at every round (i.e. \(\mathbb{E}_P[X_t \rvert \mathcal{F}_{t-1}] = \mu\) for all $t \in \mathbb{N}$). However, if nature is wrong and $m \neq \mu$, then, by Ville’s inequality, the probability of ever earning at least $\frac{1}{\alpha}$ capital is at most $\alpha$.</p> <p>This naturally extends to hypothesis testing: under the null hypothesis, the probability of earning a lot of money is very low, so we should reject $H_0^m$ if our capital becomes too large because it goes against the expectation of constant wealth. The authors then explain that one could imagine playing a game for every value of $m \in [0, 1]$. For a time $t$, one could the construct a confidence sequence as the collection of games with small capital at $t$.</p> <p>The reason the authors introduce this concept is because the capital process is a test <i>martingale</i> when $m = \mu$. Unlike the Hoeffding and empirical Bernstein processes, which are test <i>supermartingales</i>. Since test supermartingales expect the capital to decrease, test constructed from them can be more conservative (this is because we expect to be losing wealth over time, so $\mathbb{E}[X_0]$ will be the maximal value, and Ville’s inequality will yield a relatively larger bound).</p> <h3 id="hedging-our-bets">Hedging Our Bets</h3> <p>Since the capital process describes the wealth we accumulate during a better game, we can ask the question: Is there a way to mitigate our risk? In general betting games, one can <i>hedge</i> one’s bets. This means that, after placing an initial bet, the player bets again on a different outcome (or multiple different outcomes) as they see the game play out. This can reduce their net loss because they could possibly win back some money with their later bets that are based on more information about the game.</p> <p>Now, consider making two simultaneous bets: one with some part $(\theta)$ of one’s current wealth and the other with the rest. One bet will be made for the case that $\mu \geq m$, and the other will be made for the case that $\mu &lt; m$. We can describe separate capital processes for these two bets and use these to describe the wealth accumulated across both betting strategies. This is called the <i>hedged capital process</i>.</p> <div id="hedged-capital-process"></div> <div class="definition"> <strong>Definition (Hedged Capital Process).</strong> <br/> Let $(X_t)_{t = 1}^\infty \sim P$ be a sequence of random variables taking values in $[0, 1]$ with distribution $P \in \mathcal{P}^\mu$ where $\mathcal{P}$ is the set of distributions on $[0, 1]^\infty$ such that $\mathbb{E}_P[X_t \vert \mathcal{F}_{t-1}] = \mu$ for each $t$. <br/> Let $(\mathcal{K}_t^+(m))_{t = 1}^\infty$ and $(\mathcal{K}_t^-(m))_{t=1}^\infty$ be sequences where: $$ \mathcal{K}_t^+ = \prod_{i = 1}^t (1 + \lambda_i^+(m) \cdot (X_i - m)) \hspace{5mm} \text{ and } \hspace{5mm} \mathcal{K}_t^- = \prod_{i = 1}^t (1 + \lambda_i^-(m) \cdot (X_i - m)) \nonumber $$ for predictable sequences $(\lambda_t^+(m))_{t=1}^\infty$ and $(\lambda_t^-(m))_{t=1}^\infty$ with $\lambda_t^+(m) \in \left[0, \frac{1}{m}\right)$ and $\lambda_t^-(m) \in \left[0, \frac{1}{1-m}\right)$. <br/> The <i>hedged capital process</i> for any $m \in [0, 1]$ and any $\theta \in [0, 1]$ is defined as: $$ \mathcal{K}_t^\pm(m) = \max \left\{ \theta \mathcal{K}_t^+(m), (1-\theta) \mathcal{K}_t^-(m) \right\} $$ </div> <p>In the event that $m = \mu$, we would expect our hedged bets to both yield no earnings. However, if $m \neq \mu$, then one bet should lose and the other should win money. With this capital process, we can construct a confidence sequence.</p> <div id="theorem-3"></div> <div class="theorem"> <strong>Theorem 3: Hedged Capital CS.<d-cite key="waudbysmith2022"></d-cite></strong> <ul id="ws-theorem3" class="tab" data-tab="8164e31f-6a27-439f-8a31-2c2cbb8b3f40" data-name="ws-theorem3"> <li class="active" id="ws-theorem3-statement"> <a href="#">statement </a> </li> <li id="ws-theorem3-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="8164e31f-6a27-439f-8a31-2c2cbb8b3f40" data-name="ws-theorem3"> <li class="active"> <p>Let \((X_t)_{t = 1}^\infty \sim P\) be a sequence of random variables taking values in \([0, 1]\) with distribution \(P \in \mathcal{P}^\mu\) where \(\mathcal{P}\) is the set of distributions on \([0, 1]^\infty\) such that \(\mathbb{E}_P[X_t \vert \mathcal{F}_{t-1}] = \mu\) for each $t$.</p> <p>Let \((\tilde{\lambda}_t^+)_{t=1}^\infty\) and \((\tilde{\lambda}_t^-)_{t=1}^\infty\) be predictable, real-valued sequences, independent of $m$, and define, for each $t \geq 1$:</p> \[\lambda_t^+(m) = \min\left\{ \rvert \tilde{\lambda}_t^+\rvert, \frac{c}{m} \right\} \hspace{5mm} \text{ and } \hspace{5mm} \lambda_t^-(m) = \min\left\{ \rvert \tilde{\lambda}_t^-\rvert, \frac{c}{1-m} \right\}\] <p>and for some chosen \($c \in [0, 1)\). The sequence \((\mathcal{B}_t^{\pm})_{t = 1}^\infty\) forms a $(1-\alpha)$ confidence sequence for $\mu$ where:</p> \[\mathcal{B}_t^{\pm} = \left\{ m \in [0, 1] \bigg\rvert \mathcal{K}_t^\pm(m) &lt; \frac{1}{\alpha} \right\}\] <p>The running intersection, \(\cap_{i \leq t} \mathcal{B}_i^\pm\) is also a valid $(1-\alpha)$ confidence sequence for $\mu$. Furthermore, \(\mathcal{B}_t^{\pm}\) is a valid $(1-\alpha)$ confidence interval for each $t \geq 1$.</p> </li> <li> <p>Proof to be completed.</p> </li> </ul> </div> <p>The authors recommend using $\tilde{\lambda}_t^+ = \tilde{\lambda}_t^- = \lambda_t^{\text{PrPl}\pm}$ for the predictable plug-ins, where:</p> \[\begin{equation} \label{eq:hedged-capital-choices} \lambda_t^{\text{PrPl}\pm} = \sqrt{\frac{2 \log(2/\alpha)}{\hat{\sigma}_{t-1}^2 t \log(t+1)}} \end{equation}\] <p>where:</p> \[\begin{aligned} \hat{\sigma}_t^2 &amp;= \frac{\frac{1}{4} + \sum_{i = 1}^t (X_i - \hat{\mu}_i)^2}{t+1} \\ \hat{\mu}_t &amp;= \frac{\frac{1}{2} + \sum_{i = 1}^t X_i}{t + 1} \\ c &amp;= \frac{1}{2} \text{ or } \frac{3}{4} \end{aligned}\] <p>Similar to the empirical Bernstein case above, we can use the running intersection, $\cap_{i \leq n} \mathcal{B}_i^\pm$, to form a $(1- \alpha)$ confidence interval for a fixed sample size $n$, where we choose:</p> \[\tilde{\lambda}_t^+ = \tilde{\lambda}_t^- = \tilde{\lambda}_t^\pm = \sqrt{\frac{2 \log(2/\alpha)}{n \hat{\sigma}_{t - 1}^2}}\] <p>for the same $\hat{\sigma}^2$ as defined in Eq. \eqref{eq:hedged-capital-choices}.</p> <p>A key finding of Waudby-Smith and Ramdas is that, for large sample sizes, the hedged capital confidence intervals are <i>almost surely better</i> than the confidence intervals one can create from Hoeffding’s inequality. Even more, the hedged capital confidence intervals converge at a rate of $O(1/\sqrt{n})$, which is optimal.</p>]]></content><author><name></name></author><category term="theory"/><category term="probability"/><category term="stochastic-processes"/><category term="paper-review"/><summary type="html"><![CDATA[A colleague introduced me to some recent work from Waudby-Smith and Ramdas here at Carnegie Mellon. Since I’ve been working on applications of concentration bounds, it certainly seems important to review their paper Estimating means of bounded random variables by betting, which can often achieve better bounds than the older work from Maurer and Pontil. Here, I’m going to use the preprint, but their work has been published in JRSSB.]]></summary></entry><entry><title type="html">Martingales</title><link href="https://aerosengart.github.io/blog/2025/martingales/" rel="alternate" type="text/html" title="Martingales"/><published>2025-05-20T00:00:00+00:00</published><updated>2025-05-20T00:00:00+00:00</updated><id>https://aerosengart.github.io/blog/2025/martingales</id><content type="html" xml:base="https://aerosengart.github.io/blog/2025/martingales/"><![CDATA[<p>This post works through highlights of Aaditya Ramdas’ 2018 minicourse<d-cite key="ramdas2018"></d-cite> on martingales at Carnegie Mellon University with some supplemental information taken Durrett<d-cite key="durrett2019"></d-cite> and some definitions from Wikipedia. Additional definitions are from Shafer et al.<d-cite key="shafer2011"></d-cite> and Ramdas et al.<d-cite key="ramdas2022"></d-cite>.</p> <p>This is mostly for my own benefit because I have trouble conceptualizing (and then remembering) many topics in stochastic processes. The main topic of this post will be the <i>martingale</i> (to be more rigously defined later). We can think of a martingale as the amount of wealth a player has at a given round (time point) in a betting game.</p> <hr/> <h2 id="definitions">Definitions</h2> <p>We now continue with several “building block” definitions covered in Ramdas et al.<d-cite key="ramdas2022"></d-cite>. We start with the $p$-process, which is sometimes called the <i>anytime-valid $p$-value</i> in other works.</p> <div id="p-process"></div> <div class="definition"> <strong>Definition ($p$-Process).</strong> <br/> Let $H_0$ be some hypothesis. A <i>$p$-process</i> is any sequence of $p$-values $(p_t)_{t \in T}$ (with index set $T$) such that $\mathbb{P}_{H_0}(p_\tau \leq a) \leq a$ for all $a \in [0, 1]$ for any arbitrary stopping time, $\tau$. </div> <p>Intuitively, a $p$-process is simply a sequence of $p$-values satisfying the condition that, if we were to stop our sequence at any time (and the stopping time can be whatever we want — even data-dependent!), then the probability that the $p$-value is, at most, any value in $[0, 1]$ cannot exceed that value (kinda like calibration).</p> <div id="e-process"></div> <div class="definition"> <strong>Definition ($e$-Process).</strong> <br/> Let $H_0$ be some hypothesis. An <i>e-process</i> is any non-negative sequence $(e_t)_{t \in T}$ (with index set $T$) such that $\mathbb{E}_{H_0}[e_\tau] \leq 1$ for any arbitrary stopping time, $\tau$. </div> <p>Examples of $e$-processes include test martingales and test supermartingales. Related to the $e$-process is the $e$-value.</p> <div id="e-value"></div> <div class="definition"> <strong>Definition ($e$-Value).</strong> <br/> Let $H_0$ denote a null hypothesis about the distribution of our data $X = (X_1, \dots, X_\tau)$, which is a sequence of observations with stopping time/sample size $\tau$. An <i>$e$-variable</i> or <i>$e$-statistic</i> is a non-negative random variable $E = E(X)$ such that, for all $P \in H_0$, $\mathbb{E}_P[E] \leq 1$. An <i>$e$-value</i> is a value taken on by $E$, but is commonly used to refer to the $e$-statistic itself. </div> <hr/> <p>We now turn to ideas in stochastic processes.</p> <div id="random-walk"></div> <div class="definition"> <strong>Definition (Random Walk).</strong> <br/> Let $X_1, X_2, \dots$ denote independent and identically distributed random variables in $\mathbb{R}$. The sum of the first $n$, $S_n = X_1 + X_2 + \dots + X_n$, is a <i>random walk</i>. </div> <p>A random walk is a very basic stochastic process. If the walk satisfies the condition that $\mathbb{P}(X_i = 1) = \mathbb{P}(X_i = -1) = \frac{1}{2}$ (i.e. the random variables take values in ${ -1, 1}$ equiprobably), then $S_n$ is a <i>simple random walk</i>.</p> <p>Suppose we want to conceptualize the information we have at time $n$. One way to do so would be to consider the $\sigma$-field generated by $X_1, \dots, X_n$. This leads to the <i>filtration</i>.</p> <div id="filtration"></div> <div class="definition"> <strong>Definition (Filtration).</strong> <br/> Let $(\Omega, \mathcal{F}, P)$ be a probability space, and let $I$ be an index set equipped with a total order denoted by $\leq$. For every $i \in I$, let $\mathcal{F}_i$ be a sub-$\sigma$-field of $\mathcal{F}$. A <i>filtration</i>, denoted by $\mathbb{F} = (\mathcal{F}_i)_{i \in I}$ is the set of all such $\mathcal{F}_i$ such that $\mathcal{F}_k \subseteq \mathcal{F}_l$ for all $k \leq l$. </div> <p>Intuitively, a filtration is a collection of sub-$\sigma$-fields of $\mathcal{F}$ that has a non-decreasing order that captures the information had up to a given point (usually in time). We can kind of think about filtrations as keeping track of all of the questions we can answer about our process.</p> <p>At timepoint $t=0$, we know very little about our stochastic process. We know that $(X_t)_{t\in T}$ each take on <i>some</i> value in $\mathcal{F}$, and we know that they <i>must</i> take on values in $\mathcal{F}$. However, that’s all we know. Thus, \(\mathcal{F}_0 = \{ F_0, \emptyset \}\) where $F_0 = \mathcal{F}^{\rvert T \rvert}$, the $\rvert T \rvert$-ary Cartesian power of $\mathcal{F}$.</p> <p>At timepoint $t = 1$, we gain the knowledge of the outcome of $X_1$, and we can therefore answer any question that is only about $X_1$. Some simple examples include “Is $X_1 = 0, 1$ or $2$?” or “Is $X_1$ odd?”. Thus, \(\mathcal{F}_1 = \{ A \times F^{\rvert T \rvert - 1} \rvert A \subseteq \mathcal{P}(\mathcal{F}) \}\) where $\mathcal{P}(S)$ denotes the power set of set $S$. In words, $\mathcal{F}_1$ allows us to answer any question about $X_1$ <i>but nothing about the rest of the process</i>.</p> <p>This sort of relationship continues for all of the sub-$\sigma$-fields in our filtration. Each successive sub-$\sigma$-field, \(\mathcal{F}_{t + 1}\) contains all of the information about our stochastic process’s past development. From our explanation for $t=1$, it is clear that \(\mathcal{F}_t \subseteq \mathcal{F}_{t+1}\). However, $\mathcal{F}_{t+1}$ contains finer grained subsets since we gain knowledge about an additional timepoint in our process.</p> <div class="example"> <strong>Example (Filtration).</strong> <br/> Suppose we are flipping a coin two times. Let the tuple $(\Omega, \mathcal{F}, P)$ denote the probability space upon which the random variables representing these flips are defined. The sample space, $\Omega$, is the set $\{ HH, HT, TH, TT \}$. <br/> At $t = 0$, $\mathcal{F}_0 = \{ \emptyset, \Omega \}$, since we don't know anything about what the outcomes will be. <br/> At $t = 1$, we observe one coin flip, which has to either come out as heads or tails. Thus, $\mathcal{F}_1 = \{ \emptyset, \Omega, \{ HH, HT \}, \{ TH, TT \} \}$. We can interpret the additional two sets as showing us the possible outcomes once we have observed the first flip. $\{ HH, HT \}$ states that, once we see a $H$, that the only possible final results are $HH$ or $HT$. <br/> At $t = 2$, we observe the second coin flip, so we know everything about this process. $\mathcal{F}_2 = \{ \emptyset, \Omega, \{ HH, HT \}, \{ TH, TT \}, \{ HH \}, \{ HT \}, \{ TT \}, \{ TH \} \}$. Similar to the case with $\mathcal{F}_1$, after we see both flips, we only have one possible outcome (the one we observed). Thus, we add all singleton sets. Notice that this is $\mathcal{P}(\Omega)$, the power set of the sample space. </div> <p>The natural filtration is the $\sigma$-field generated by $(X_s)_{s \leq t}$.</p> <div id="natural-filtration"></div> <div class="definition"> <strong>Definition (Natural Filtration).</strong> <br/> Let $X: T \times \Omega \rightarrow S$ be a stochastic process on probability space $(\Omega, \mathcal{F}, P)$ with (measurable) state space $(S, \Sigma)$. The <i>natural filtration of $\mathcal{F}$ with respect to $X$</i> is the filtration $\mathbb{F}^X = (\mathcal{F}_t^X)_{t \in T}$ where: $$ \mathcal{F}_t^X = \sigma\left(X^{-1}_j(A) \big\rvert j \in T, j \leq t \text{ and } A \in \Sigma\right) \nonumber $$ </div> <p>A stochastic process \((X_t)_{t \in T}\) is said to be <i>adapted to the filtration $\mathbb{F}$</i> if $X_t: \Omega \rightarrow S$ is a $(\mathcal{F}_t, \Sigma)$-measurable function for all $t \in T$.</p> <p>For a filtration $\mathcal{F}_n$ with $n \geq 0$, we call a sequence $H_n$ for $n \geq 1$ <i>predictable</i> if $H_n$ is \(\mathcal{F}_{n-1}\)-measurable for all $n \geq 1$.</p> <h3 id="properties">Properties</h3> <p>Sometimes we need additional assumptions to hold on a given filtration. One is <i>continuity</i>, and another is <i>completeness</i>.</p> <div id="continuous"></div> <div class="definition"> <strong>Definition (Right-Continuity).</strong> <br/> Let $\mathbb{F} = (\mathcal{F}_t)_{t \in T}$ be a filtration. If, for all $t \in T$, $\mathcal{F}_t = \bigcap_{s &gt; t} \mathcal{F}_s$, then we call $\mathbb{F}$ <i>right-continuous</i>. </div> <div id="complete"></div> <div class="definition"> <strong>Definition (Complete).</strong> <br/> Let $(\Omega, \mathcal{F}, P)$ be a probability space, and let $N_P = \{ A \subseteq \Omega \rvert A \subseteq B, B \in \mathcal{F} \text{ s.t. } P(B) = 0 \}$ be the set of all sets contained in the null set (with respect to $P$). We call a filtration $(\mathcal{F}_t)_{t \in T}$ <i>complete</i> if $N_P \subseteq \mathcal{F}_t$ for all $t$. <br/> Equivalently, $(\mathcal{F}_t)_{t \in T}$ is complete if $(\Omega, \mathcal{F}_i, P)$ is a complete measure space for all $t$. </div> <p>We can define the <i>$P$-completion of a $\sigma$-field</i>, $\mathcal{F}$, as the union of $\mathcal{F}$ with all sets $E \in \Omega$ such that $P(E) = 0$.</p> <p>A probability space $(\Omega, \mathcal{F}, P)$ equipped with filtration \(\mathbb{F} = (\mathcal{F}_t)_{t \geq 0}\) where $\mathcal{F}_t$ is a sub-$\sigma$-field of $\mathcal{F}$ is called a <i>filtered probability space</i>.</p> <p>Stochastic processes can also be described by a <i>stopping time</i>, which is another random variable the describes when a stochastic process will display some phenomenon or behavior.</p> <div id="stopping-time"></div> <div class="definition"> <strong>Definition (Stopping Time).</strong> <br/> Let $(\Omega, \mathcal{F}, \mathbb{F}, P)$ be a filtered probability space. Let $\tau$ be a random variable defined on this space and taking values in index set $T$. $\tau$ is a <i>stopping time</i> (with respect to $\mathbb{F} = (\mathcal{F}_t)_{t \in T}$) if $\{ \tau \leq t \} \in \mathcal{F}_t$ for all $t \in T$. </div> <p>In simpler terms, $\tau \in { 0, 1, 2, \dots } \cup { \infty }$ is a <i>stopping time</i> if, for any $n \in \mathbb{N}$, the event ${ \tau = n }$ is entirely known just from the information up until time $n$ (i.e. determined by ${X_1, \dots, X_n}$). This can be written as ${ \tau = n } \in \mathcal{F}_n$.</p> <div id="stopping-time-ex"></div> <div class="example"> <strong>Example (Stopping Time).</strong> <br/> Let $X_1, X_2, \dots$ be random variables in $\mathbb{R}$. Let $S_n = X_1 + \dots + X_n$. The random variable defined by: $$ \tau = \inf\{t \rvert S_t \geq c \} \nonumber $$ for $c \in \mathbb{R}$ is a stopping time. </div> <p>Constant times (e.g. $\tau = c$ for some $c \in { 0, 1, 2, \dots } \cup { \infty }$) are stopping times, and the minimum and maximum of two stopping times is also a valid stopping time.</p> <hr/> <h2 id="martingales">Martingales</h2> <p>Related to stochastic processes and filtrations is the <i>martingale</i>. Here I’ll cover some basic definitions, but interested readers can see my post on martingales for more details.</p> <div id="martingale"></div> <div class="definition"> <strong>Definition (Martingale).</strong> <br/> Let $S$ be a Banach space with norm $\rvert \rvert \cdot \rvert \rvert_S$. A stochastic process $X: T \times \Omega \rightarrow S$ on $(\Omega, \mathcal{F}, P)$ with state space $(S, \Sigma)$ is called a <i>martingale with respect to the filtration $\mathbb{F} = \{ \mathcal{F}_t: t \in T \}$ and probability measure $P$</i> if: <ol> <li>$\mathbb{F}$ is a filtration of $(\Omega, \mathcal{F}, P)$</li> <li>$X$ is adapted to $\mathbb{F}$ (i.e. $X_t$ is $\mathcal{F}_t$-measurable for all $t \in T$)</li> <li>For all $t \in T$, $\mathbb{E}_P[ \rvert \rvert X_t \rvert \rvert_S ] &lt; +\infty$</li> <li>For all $s, t \in T$ with $s &lt; t$, and for all subsets $F \in \mathcal{F}_s$, $\mathbb{E}_P[\mathbf{1}\{(X_t - X_s) \in F \}] = 0$</li> </ol> This last condition can be rewritten in terms of the conditional expectation with respect to a sub-$\sigma$-field: $\mathbb{E}_P[ X_t \rvert \mathcal{F}_s ] = 0$. This notation emphasizes that this expectation is an $\mathcal{F}_s$-measurable function. Put more intuitively, albeit with some simplification, a discrete-time martingale is a stochastic process $\{ X(t, \omega): t \in T, \omega \in \Omega\}$ such that, for any $t \in T$: <ol> <li>$\mathbb{E}[\rvert X_t \rvert] &lt; \infty$</li> <li>$\mathbb{E}[X_{t+1} \rvert X_1, \dots, X_t] = X_t$</li> </ol> </div> <p>A relaxation of the condition of a martingale leads to the following object used frequently in game theoretic statistics.</p> <div id="supermartingale"></div> <div class="definition"> <strong>Definition (Supermartingale).</strong> <br/> Let $(\Omega, \mathcal{F}, P)$ be a probability space. Let $( X_t )_{t \in T}$ be a stochastic process (with index set $T = \{ 0, 1, \dots \}$ or $T = [0, \infty)$) defined on this space, and let $\mathbb{F} = ( \mathcal{F}_t )_{t \in T}$ be a filtration of sub-$\sigma$-fields of $\mathcal{F}$ (i.e. $\mathcal{F}_t \subseteq \mathcal{F}$ for all $t \in T$). $(X_t, \mathcal{F}_t)$ is a <i>supermartingale</i> if: <ul> <li>$X_t$ is $\mathcal{F}_t$-measurable for all $t \in T$ (i.e. $(X_t)_{t \in T}$ is adapted to $\mathbb{F}$)</li> <li>$X_t$ is integrable for all $t \in T$</li> <li>$\mathbb{E}[X_t \rvert \mathcal{F}_s] \leq X_s$ for all $t \in T$ and all $s &lt; t$ almost surely</li> </ul> We also call the stochastic process $(X_t)_{t \in T}$ a supermartingale if $(X_t, \mathcal{F}_t)$ is a supermartingale and $\mathcal{F}_t$ is the $\sigma$-field generated by $X_t$. </div> <p>More specifically, game theoretic statistics makes use of the <i>test supermartingale</i>.</p> <div id="test-supermartingale"></div> <div class="definition"> <strong>Definition (Test Supermartingale).</strong> <br/> A supermartingale $(X_t, \mathcal{F}_t)_{t \in T}$ is called a <i>test supermartingale</i> if $X_t \geq 0$ for all $t$ and $\mathbb{E}[X_0] \leq 1$ (or $X_0 = 1$, which is basically the same as the previous statement if we set $X_t = 1$ for $t &lt; 0$). <br/> Similarly, a <i>test martingale</i> is a non-negative martingale such that $\mathbb{E}[X_0] = 1$. </div> <p>Intuitively, a test supermartingale describes the cumulative gain or loss of a player in game where they bet against some hypothesis described by $P$. The initial condition basically specifies the amount of capital the player begins with (1 dollar), and the non-negatively condition ensures the player never loses (cumulatively) more than the initial dollar they they started with.</p> <p>If the player has a lot of money at round $t$ (i.e. large $X_t$), then they have evidence against $P$ being true. This interpretation makes sense when you think about real betting. Winning big is rare at a casino, so having a large payout is associated with low probability events in a game.</p> <p>One property of test supermartingales is the <i>maximal inequality</i>:</p> \[P\left( \underset{ t \leq \infty}{\sup} X_t \geq c \right)\leq \frac{1}{c} \hspace{5mm} \forall c \geq 1\] <p>This inequality states thats the probability (with respect to the probability measure, $P$, of our probability space) that our test supermartingale exceeds some value, $c$, that is at least $1$ is inversely related to the magnitude of $c$. This implies that the probability that a player earns an infinite amount of money in the game is zero.</p> <p>A <i>very</i> important result in probability theory is <a href="https://en.wikipedia.org/w/index.php?title=Ville%27s_inequality&amp;oldid=1213448789">Ville’s inequality</a>, which upperbounds the probability a supermartingale gets at least as big as some chosen value.</p> <div id="ville-inequality"></div> <div class="theorem"> <strong>Theorem (Ville's Inequality).</strong> <ul id="ville" class="tab" data-tab="0459a816-d860-41da-90b9-2b306488a7b6" data-name="ville"> <li class="active" id="ville-statement"> <a href="#">statement </a> </li> <li id="ville-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="0459a816-d860-41da-90b9-2b306488a7b6" data-name="ville"> <li class="active"> <p>Let $(X_t)_{t = 0}^\infty$ be a non-negative supermartingale. For any positive $a \in \mathbb{R}$:</p> \[\mathbb{P}\left( \underset{n \geq 0}{\sup} X_n \geq a \right) \leq \frac{\mathbb{E}[X_0]}{a}\] </li> <li> <p>Proof to be completed.</p> </li> </ul> </div>]]></content><author><name></name></author><category term="theory"/><category term="probability"/><category term="stochastic-processes"/><category term="primer"/><summary type="html"><![CDATA[A Primer]]></summary></entry><entry><title type="html">Measure Theory</title><link href="https://aerosengart.github.io/blog/2025/measure-theory/" rel="alternate" type="text/html" title="Measure Theory"/><published>2025-04-30T00:00:00+00:00</published><updated>2025-04-30T00:00:00+00:00</updated><id>https://aerosengart.github.io/blog/2025/measure-theory</id><content type="html" xml:base="https://aerosengart.github.io/blog/2025/measure-theory/"><![CDATA[<p>My work has become much more technical that I am used to, so I thought it would be good to take some notes on basic measure and probability theory in anticipation of working through several theoretical papers. A lot of the definitions below come from Wikipedia, Durrett<d-cite key="durrett2019"></d-cite>, and Axler<d-cite key="axler2025"></d-cite>, but I’ve added some more intuitive ways of thinking about these concepts that I’ve come up with or collected from some really helpful sites I’ve found.</p> <p>Note: Not all of the proofs are finished/included. I am hoping to find the time to return to this post and complete them.</p> <hr/> <h2 id="building-blocks">Building Blocks</h2> <p>Measure theory, in my mind, is just about sets, mappings, and ways to describe them. To formalize these ideas, however, we need to define some basic building blocks.</p> <h3 id="sets">Sets</h3> <p>We’ll begin with a $\sigma$-field. This is simply a collection of subsets of some other set.</p> <div id="sigma-field"></div> <div class="definition"> <strong>Definition ($\sigma$-Field).</strong> <br/> Let $X$ be a set, and let $\mathcal{P}(X)$ denote its power set (the set of all possible subsets of $X$). A $\sigma$<i>-field</i> is any subset $\mathcal{S} \subseteq \mathcal{P}(X)$ satisfying the following: <ol> <li> $X \in \mathcal{S}$ </li> <li> Closed Under Complementation: $A \in \mathcal{S} \implies A^c \in \mathcal{S}$ </li> <li> Closed Under Countable Unions: $A_1, A_2, \dots \in \mathcal{S} \implies A = \bigcup_{i = 1}^\infty A_i \in \mathcal{S}$ </li> </ol> </div> <aside><p>For the technical reader, a $\sigma$-field, also called a $\sigma$-<i>algebra</i>, is a generalization of the <i>algebra</i>, which has the same definition except must be closed under <i>finite</i> unions.</p></aside> <p>An example may make it a bit more concrete in one’s mind…</p> <div class="example"> <strong>Example ($\sigma$-Field).</strong> <br/> Let $X = \{ 1, 2, 3\}$. One $\sigma$-field on $X$ is $\mathcal{S} = \{ \emptyset, \{ 1 \}, \{2, 3 \}, \{1, 2, 3 \} \}$. <br/> It is easy to see that the first two properties are satisfied. The entirety of $X$ is in $\mathcal{S}$ by construction. Any element in $\mathcal{S}$, its complement is also in $\mathcal{S}$ (the complement of $\emptyset$ is $X$, and the complement of $\{ 1\}$ is $\{ 2, 3 \}$). <br/> Taking any countable union of elements of $\mathcal{S}$ also yields an element of $\mathcal{S}$ (the union of any $A \in \mathcal{S}$ and the empty set is the set itself; the complement of the entirety of $X$ and any $A \in \mathcal{S}$ will just be $X$; and the union of $\{ 1\}$ and $\{ 2, 3\}$ is $X$). This satisfies the third property, which completes the proof that $\mathcal{S}$ is a $\sigma$-field on $X$. </div> <p>Notice that the first and second properties in the above definition imply that $\emptyset \in \mathcal{S}$ as well. The properties also imply that a $\sigma$-field must be closed under countable intersection. That is, $\cap_{i = 1}^\infty A_i \in \mathcal{S}$ for some sequence of $A_1, A_2, \dots \in \mathcal{S}$.</p> <p>A $\sigma$-field is a generalization of the concept of an <i>algebra</i> (also called a <i>field</i>).</p> <div id="algebra"></div> <div class="definition"> <strong>Definition (Algebra).</strong> <br/> Let $X$ be a set. A collection, $\mathcal{A}$, of subsets of $X$ is an <i>algebra</i> (or <i>field</i>) if the following are satisfied: <ol> <li>$X \in \mathcal{A}$</li> <li>Closed Under Complementation: $A \in \mathcal{A} \implies A^c \in \mathcal{A}$</li> <li>Closed Under Finite Unions: $A, B \in \mathcal{A} \implies A \cup B \in \mathcal{A}$</li> </ol> </div> <p>Now we can define <i>measurable spaces</i>!</p> <div id="measurable-space"></div> <div class="definition"> <strong>Definition (Measurable Space).</strong> <br/> Let $X$ be some set, and let $\mathcal{S}$ be a $\sigma$-field on $X$. The tuple $(X, \mathcal{S})$ is called a <i>measurable space</i>, and any element of $\mathcal{S}$ is called an <i>$\mathcal{S}$-measurable set</i>. </div> <p>We’ll come back to this definition later when we discuss measures, but a measurable space is just a space that <i>could</i> be assigned a measure.</p> <p>Let’s finish up this sub-section by introducing <i>topological spaces</i> and <i>Borel sets</i>.</p> <div id="topology"></div> <div class="definition"> <strong>Definition (Topology).</strong> <br/> Let $X$ be a non-empty space. A <i>topology</i>, $\tau$, on $X$ is any collection of subsets of $X$ that satisfy: <ol> <li>The empty set, $\emptyset$, and the entirety of $X$ are in $\tau$.</li> <li>The union (finite or infinite) of any subset in $\tau$ is also in $\tau$.</li> <li>The intersection of a finite number of subsets in $\tau$ is also in $\tau$.</li> </ol> We call the tuple $(X, \tau)$ a <i>topological space</i>. </div> <div id="borel-set"></div> <div class="definition"> <strong>Definition (Borel Set).</strong> <br/> A <i>Borel set</i> on a topological space, $X$, denoted by $\mathcal{B}(X)$, is any subset of $X$ that can be constructed from open sets on that space in $X$ via countable unions, countable intersections, and set differences. <br/> The <i>Borel $\sigma$-field</i> (or <i>Borel algebra</i>) is the collection of all Borel sets on a space. </div> <p>This definition is a bit tricky to develop intuition for. The Borel $\sigma$-field is just the collection of <i>all possible open sets</i> in a given space, $X$.</p> <p>An important Borel $\sigma$-field that will come up again when we discuss measures and probability is the Borel $\sigma$-field on the real line. Several examples follow from our definition:</p> <ol> <li>Any <i>closed</i> subset of $\mathbb{R}$ is a Borel set because $\sigma$-fields are closed under complementation.</li> <li>Any <i>countable</i> subset of $\mathbb{R}$ is a Borel set because $\sigma$-fields are closed under countable unions, and a single point is a closed subset of $\mathbb{R}$.</li> <li>Any half-open interval is a Borel set because $\sigma$-fields are closed under countable intersections.</li> </ol> <p>Borel sets on $\mathbb{R}$ can also be extended to $[-\infty, \infty]$.</p> <p>Along with the Borel set and the $\sigma$-field is the <i>semialgebra</i>.</p> <div id="semialgebra"></div> <div class="definition"> <strong>Definition (Semialgebra).</strong> <br/> Let $\mathcal{S}$ be a collection of sets. $\mathcal{S}$ is called a <i>semialgebra</i> if it satisfies the following properties: <ol> <li>Closed Under Intersection: $S,T \in \mathcal{S} \implies S \cap T \in \mathcal{S}$</li> <li>(Sort Of) Closed Under Complementation: $S \in \mathcal{S} \implies S^c$ is a finite disjoint union of $T \in \mathcal{S}$</li> </ol> </div> <p>This concept will not be as useful in later discussions, but we include it for completeness. An example of a semialgebra is the union of ${ \emptyset }$ and the collection of sets that can be written as:</p> \[(a_1, b_1] \times \dots \times (a_d, b_d] \subset \mathbb{R}^d \hspace{5mm} \text{for } -\infty \leq a_i &lt; b_i \leq \infty\] <p>Given a semialgebra, $\mathcal{S}$, the collection of finite disjoint unions of sets in $\mathcal{S}$ forms an algebra called the <i>algebra generated by $\mathcal{S}$</i>.</p> <h3 id="functions">Functions</h3> <p>We now need to define a concept that is at the crux of our discussions of mappings: the <i>inverse image</i>.</p> <div id="inverse-image"></div> <div class="definition"> <strong>Definition (Inverse Image/Pre-Image).</strong> <br/> Let $f: X \rightarrow Y$ be some function, and let $A \subseteq Y$. The <i>inverse image</i>, also called the <i>pre-image</i>, of subset $A$ is defined as the set: $$ f^{-1}(A) = \left\{x \in X \rvert f(x) \in A \right\} \nonumber $$ The inverse image satisfies nice properties: <ol> <li>For any $A \subseteq Y$, $f^{-1}(Y \setminus A) = X \setminus f^{-1}(A)$</li> <li>For any set $\mathcal{A}$ of subsets of $Y$: $f^{-1}(\cup_{A \in \mathcal{A}} A) = \cup_{A \in \mathcal{A}}f^{-1}(A)$</li> <li>For any set $\mathcal{A}$ of subsets of $Y$: $f^{-1}(\cap_{A \in \mathcal{A}} A) = \cap_{A \in \mathcal{A}}f^{-1}(A)$</li> <li>For function $g: Y \rightarrow W$: $(g \circ f)^{-1}(A) = f^{-1}(g^{-1}(A))$s for any $A \subseteq W$</li> </ol> </div> <p>In words, the inverse image of a subset $A$ of $Y$ under function $f$ is the subset of elements in the domain $X$ that map to elements in $A$. It’s important to note that the inverse image of the whole of $Y$ does not necessarily have to be the whole of $X$!</p> <p>We now introduce a definition that describes what it means for functions of a certain type to be “nice” with respect to a $\sigma$-field.</p> <div id="measurable-function"></div> <div class="definition"> <strong>Definition (Measurable Function).</strong> <br/> Let $(X, \mathcal{S})$ be a measurable space, and let $f: X \rightarrow [-\infty, \infty]$ be a function mapping to the extended real line. We say $f$ is <i>$\mathcal{S}$-measurable</i> if $f^{-1}(B) \in \mathcal{S}$ for every Borel set $B \subseteq [-\infty, \infty]$. Any function from $X$ to $\mathbb{R}$ is $\mathcal{S}$-measurable if $\mathcal{S} = \mathcal{P}(X)$, the power set of $X$. <br/> Furthermore, for $\mathcal{S}$-measurable functions $f,g: X \rightarrow \mathbb{R}$: <ol> <li>$f+g$, $f-g$, and $fg$ are $\mathcal{S}$-measurable.</li> <li>$f/g$ is $\mathcal{S}$-measurable if $g(x) \neq 0$ for all $x \in X$.</li> </ol> More generally, for measurable spaces $(X, \mathcal{S})$ and $(Y, \mathcal{S}')$, $f: X \rightarrow Y$ is $(\mathcal{S}, \mathcal{S}')$-measurable if, for all $E \in \mathcal{S}'$, we have $f^{-1}(E) \in \mathcal{S}$. </div> <p>The basic idea behind an $\mathcal{S}$-measurable function is that we should be able to achieve any Borel set as output for <i>some</i> part of $\mathcal{S}$, which is in its domain (since $\text{dom}(f) = X$). It is important to remember that measurability is with respect to the $\sigma$-fields of the two measure spaces of interest.</p> <p>To put it intuitively, a measurable function $f$ needs to take on values that “make sense” with respect to the $\sigma$-field of interest. For example, only constant functions are measurable with respect to the trivial $\sigma$-field ${ \emptyset, \Omega }$ for some $\Omega$. In addition, we have the following claim:</p> <div class="theorem"> <strong>Claim.</strong> <ul id="const-funcs" class="tab" data-tab="7df440f3-b0bf-4e8e-8dc1-cdf4fdffaf23" data-name="const-funcs"> <li class="active" id="const-funcs-statement"> <a href="#">statement </a> </li> <li id="const-funcs-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="7df440f3-b0bf-4e8e-8dc1-cdf4fdffaf23" data-name="const-funcs"> <li class="active"> <p>Constant functions are measurable with respect to <i>any</i> $\sigma$-field.</p> </li> <li> <p>Suppose we have measurable spaces $(X, \mathcal{S})$ and $(Y, \mathcal{S}’)$. Let $\mathcal{S} = { \emptyset, \Omega }$, and suppose we have non-constant function $f: X \rightarrow Y$. That is, there exist $a, b \in \Omega$ such that $f(a), f(b) \in \mathcal{S}’$ and $f(a) \neq f(b)$.</p> <p>Consider the pre-image of one of these points. We know that $f^{-1}(f(a)) = a \notin \mathcal{S}$ since $a$ is neither the null set nor the entirety of $\Omega$ (since we also have $b$ and, necessarily, $a \neq b$).</p> <p>To prove the second claim, consider $\mathcal{S} = { \emptyset, \Omega }$ and arbitrary $\mathcal{S}’$ in the previous set-up. Since $f$ is constant, it must be the case that $f(x) = a$ for all $x \in X$ and some $a$. Pick any $s \in \mathcal{S}’$. If $a \in s$, then $f^{-1}(s) = \Omega$, since any input value maps to $a$ ($f$ is constant). If $a \notin s$, then $f^{-1}(s) = \Omega^c = \emptyset$ by the same argument.</p> <p>Thus, for any $s \in \mathcal{S}’$, $f^{-1}(s) \in \mathcal{S}$, implying that $f$ is $(\mathcal{S}, \mathcal{S}’)$-measurable for <i>any</i> $\mathcal{S}’$.</p> </li> </ul> </div> <p>To check whether a function is $\mathcal{S}$-measurable, it is sufficient to check whether \(f^{-1}((a, \infty]) = \{ x \in X \rvert f(x) &gt; a \} \in \mathcal{S}\) for all $a \in \mathbb{R}$.</p> <p>Furthermore, in the special case that $X \subseteq \mathbb{R}$ and $\mathcal{S}$ is the set of Borel subsets of $\mathbb{R}$ that are contained in $X$, then a function $f: X \rightarrow \mathbb{R}$ is called <i>Borel measurable</i> if $f^{-1}(B)$ is a Borel set for all Borel sets $B \subseteq \mathbb{R}$. It can be shown that any <i>continuous</i> or <i>increasing</i> function $f: X \rightarrow \mathbb{R}$ where $X$ is a Borel subset of $\mathbb{R}$ is Borel measurable.</p> <hr/> <h2 id="measures">Measures</h2> <p>We have finally come to the star of our discussion: the measure. A measure is a function that assigns a “size” to sets (it is similar to the idea of length for intervals or area for two dimensional regions).</p> <div id="measure"></div> <div class="definition"> <strong>Definition (Measure).</strong> <br/> Let $(X, \mathcal{S})$ be a measure space. A function $\mu: \mathcal{S} \rightarrow [0, \infty]$ is called a <i>measure</i> on $(X, \mathcal{S})$ if: <ol> <li>$\mu(\emptyset) = 0$</li> <li>$\mu\left(\bigcup_{i = 1}^\infty A_i \right) = \sum_{i = 1}^\infty \mu(A_i)$ for every disjoint (i.e. $A_i \cap A_j = \emptyset$ for all $i \neq j$) sequence $A_1, A_2, \dots$ of sets in $\mathcal{S}$</li> </ol> </div> <p>With this definition, we define a <i>measure space</i>, which is the tuple $(X, \mathcal{S}, \mu)$. For measure space $(X, \mathcal{S}, \mu)$ and $A, B \in \mathcal{S}$ such that $A \subseteq B$, we have that $\mu(A) \leq \mu(B)$ and $\mu(B \setminus A) = \mu(B) - \mu(A)$ (assuming that $\mu(A)$ is finite). We also have the additional property of <i>countable subadditivity</i>, which is basically a generalization of Boole’s inequality:</p> \[\mu\left(\bigcup_{i = 1}^\infty A_i \right) \leq \sum_{i = 1}^\infty \mu(A_i)\] <p>for any sequence of sets $A_1, A_2, \dots \in \mathcal{S}$. Measures also satisfy $\mu(A \cup B) = \mu(A) + \mu(B) - \mu(A \cap B)$ (assuming that $\mu(A \cap B)$ is finite).</p> <p>If we have two $\sigma$-finite (see <a href="#sigma-finite">below</a>) measure spaces, $(X, \mathcal{S}, \mu_1)$ and $(Y, \mathcal{T}, \mu_2)$, we can define two addition sets:</p> \[\begin{aligned} \Omega &amp;= X \times Y = \{ (x, y): x \in X, y \in Y\} \\ \mathcal{U} &amp;= \{ S \times T: S \in \mathcal{S}, T \in \mathcal{T}\} \end{aligned}\] <p>Sets $U \in \mathcal{U}$ are <i>rectangles</i>. Let $\mathcal{F} = \mathcal{S} \times \mathcal{T}$ be the $\sigma$-filed generated by $\mathcal{U}$. The unique measure $\mu = \mu_1 \times \mu_2$ on $\mathcal{F}$ defined as $\mu(S \times T) = \mu_1(S) \mu_2(T)$ is called a <i>product measure</i>. This result can be extended to finitely many $\sigma$-finite measurable spaces.</p> <h3 id="characteristics">Characteristics</h3> <p>Measures can be characterized in a variety of ways. First, consider the $\sigma$-finite measure.</p> <div id="sigma-finite"></div> <div class="definition"> <strong>Definition ($\sigma$-Finite).</strong> <br/> Let $(X, \mathcal{S})$ be a measure space, and let $\mu$ be a measure defined on it. We call $\mu$ a <i>$\sigma$-finite measure</i> if any of the following are satisfied: <ul> <li>There exist countably many $A_1, A_2, \dots \in \mathcal{S}$ with $\mu(A_n) &lt; \infty$ for all $n \in \mathbb{N}$ such that $\bigcap_{n \in \mathbb{N}} = X$. That is, $X$ can be covered with the intersection of countably many measurable sets in $\mathcal{S}$.</li> <li>There exist disjoint and countably many $B_1, B_2, \dots \in \mathcal{S}$ with $\mu(B_n) &lt; \infty$ for all $n \in \mathbb{N}$ such that $\bigcup_{n \in \mathbb{N}} = X$. That is, $X$ can be covered by the union of countably many disjoint measurable sets in $\mathcal{S}$.</li> <li>There exist countably many $C_1, C_2, \dots \in \mathcal{S}$ with $C_1 \subset C_2 \subset \dots$ with $\mu(C_n) &lt; \infty$ for all $n \in \mathbb{N}$ such that $\bigcup_{n \in \mathbb{N}} C_n = X$. That is, $X$ can be covered with the union of a countable monotone sequence of measurable sets in $\mathcal{S}$.</li> <li>There exists a function $f$ such that $f(x) &gt; 0$ for all $x \in X$ and $\int f(x) \mu(dx) &lt; \infty$. That is, there exists a strictly positive function with finite integral that is measurable with respect to $\mu$.</li> </ul> </div> <p>We can also define a sense of continuity to measures.</p> <div id="absolute-continuity"></div> <div class="definition"> <strong>Definition (Absolute Continuity).</strong> <br/> Let $\mu$ be a measure on the Borel subsets of $\mathbb{R}$. We call $\mu$ <i>absolutely continuous</i> with respect to the Lebesgue measure, $\lambda$, (see below for definition), if, for every $\lambda$-measurable set $A$, $\lambda(A) = 0$ implies $\mu(A) = 0$. This condition is denoted by $\mu &lt;&lt; \lambda$, and we say that $\mu$ is <i>dominated</i> by $\lambda$. </div> <p>Measures can also be “coarsened” by restricting the $\sigma$-field on which they operate.</p> <div id="restricted-measure"></div> <div class="definition"> <strong>Definition (Restricted Measure).</strong> <br/> Let $(\Omega, \mathcal{F}, \mu)$ be a measure space. Let $\mathcal{F}'$ be a sub-$\sigma$-field of $\mathcal{F}$. The <i>restricted measure</i> of $\mu$ to $\mathcal{F}'$ is the mapping $\nu: \mathcal{F}' \rightarrow \mathbb{R} \cup \{ -\infty, +\infty\}$ such that $\nu(E') = \mu(E')$ for all $E' \in \mathcal{F}'$. </div> <p>A restricted measure is basically the original measure but its domain is shrunken to whatever sub-$\sigma$-field it is restricted to. Measures also satisfy several properties.</p> <div id="theorem-1-1-1"></div> <div class="theorem"> <strong>Theorem 1.1.1.<d-cite key="durett2019"></d-cite></strong> <ul id="theorem-1-1-1" class="tab" data-tab="bc72e5b8-4cdb-44b1-9b07-dcc9a5fd0d7a" data-name="theorem-1-1-1"> <li class="active" id="theorem-1-1-1-statement"> <a href="#">statement </a> </li> <li id="theorem-1-1-1-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="bc72e5b8-4cdb-44b1-9b07-dcc9a5fd0d7a" data-name="theorem-1-1-1"> <li class="active"> <p>Let $\mu$ be a measure on $(\Omega, \mathcal{F})$, and let $A_i \uparrow A$ denote $A_1 \subset A_2 \subset \dots$ with $\cup_i A_i = A$. The measure $\mu$ satisfies the following:</p> <ul> <li>Monotonicity: $A \subset B \implies \mu(A) \leq \mu(B)$</li> <li>Subadditivity: $A \subset \cup_{m = 1}^\infty A_m \implies \mu(A) \leq \sum_{m= 1}^\infty \mu(A_m)$</li> <li>Continuity From Below: $A_i \uparrow A \implies \mu(A_i) \uparrow \mu(A)$</li> <li>Continuity From Above: $A_i \downarrow A \implies \mu(A_i) \downarrow \mu(A)$</li> </ul> </li> <li> <p>Proof to be completed.</p> </li> </ul> </div> <p>A sense of “convergence” with respect to a measure can be defined for measurable functions.</p> <div id="in-measure"></div> <div class="definition"> <strong>Definition (In Measure).</strong> <br/> Let $\mu$ be a $\sigma$-finite probability measure, and let $f$ be a measurable function and let $\{ f_n \}_{n = 1}^\infty$ be a sequence of measurable functions. We say that $f_n \rightarrow f$ <i>in measure</i> if, for any $e &gt; 0$, we have that: $$ \mu\left(\{ x : \rvert f_n(x) - f(x) \rvert &gt; e \right) \rightarrow 0 \hspace{5mm} \text{ as } n \rightarrow \infty $$ </div> <p>Before we can move on to some of the core concepts in probability theory, we need one more definition.</p> <div id="measurable-map"></div> <div class="definition"> <strong>Definition (Measurable Map).</strong> <br/> Let $(\Omega, \mathcal{F})$ and $(S, \mathcal{S})$ be two measurable spaces. The function $X: \Omega \rightarrow S$ is a <i>measurable map</i> (from $(\Omega, \mathcal{F})$ to $(S, \mathcal{S})$) if, for all $B \in \mathcal{S}$: $$ X^{-1}(B) = \{ \omega: X(\omega) \in B \} \in \mathcal{F} \nonumber $$ </div> <hr/> <h2 id="probability">Probability</h2> <p>With our building blocks in place, we can move on to probability theory. We’ll start with a fundamental definition: the <i>probability space</i>, which is just a special measure space!</p> <div id="probability-space"></div> <div class="definition"> <strong>Definition (Probability Space).</strong> <br/> A <i>probability space</i> is a measure space $(\Omega, \mathcal{F}, P)$ such that $P(\Omega) = 1$. We have the following conventions: <ol> <li>The set $\Omega$ is called the <i>sample space</i>. It is the set of all possible outcomes.</li> <li>The $\sigma$-field $\mathcal{F}$ over $\Omega$ is called the <i>event space</i>. It is a measurable set of subsets of the sample space.</li> <li>The measure $P$ is called the <i>probability measure</i>. It assigns a value (in $[0, 1]$) to give an event to give a sense of that event's likelihood of occurring.</li> </ol> </div> <h3 id="random-variables">Random Variables</h3> <p>Using the above, we can define random variables and vectors in a rigorous way. Note that the following can be generalized to the extended real line (i.e. $\mathbb{R} \cup {-\infty, \infty }$).</p> <div id="random-variable"></div> <div class="definition"> <strong>Definition (Random Variable/Vector).</strong> <br/> Let $(\Omega, \mathcal{F}, P)$ be a probability space, and let $(\mathbb{R}^d, \mathcal{B}^d)$ be our measurable space, where $\mathcal{B}^d(\mathbb{R}^d)$ is the $\sigma$-field on $\mathbb{R}^d$. A measurable map $X: \Omega \rightarrow \mathbb{R}^d$ is called a <i>random vector</i> if $d &gt; 1$ and a <i>random variable</i> otherwise. </div> <p>Random variables map each element in the sample space to an element in $H$, which is the set of all possible values the variable can take on. Naturally, we need the pre-image of all elements in $\mathcal{H}$ to be in $\mathcal{F}$. When we refer to a random variable being measurable with respect to some $\mathcal{F}’$ (a sub-$\sigma$-field of $\mathcal{F}$), we mean that it is $(\mathcal{F}’, \mathcal{B}(\mathbb{R}))$-measurable.</p> <div id="theorem-1-3-567"></div> <div class="theorem"> <strong>Theorems 1.3.5, 1.3.6, 1.3.7.<d-cite key="durett2019"></d-cite></strong> <ul id="theorems-5-6-7" class="tab" data-tab="5bd65d80-09b2-49d7-9b5b-3a8351f409fe" data-name="theorems-5-6-7"> <li class="active" id="theorems-5-6-7-statement"> <a href="#">statement </a> </li> <li id="theorems-5-6-7-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="5bd65d80-09b2-49d7-9b5b-3a8351f409fe" data-name="theorems-5-6-7"> <li class="active"> <p>Let $X_1, X_2, \dots$ be random variables, and let $f: (\mathbb{R}^n, \mathcal{B}^n) \rightarrow (\mathbb{R}, \mathcal{B})$ be a measurable function. Then the following are also random variables:</p> <ul> <li>$f(X_1, \dots, X_n)$</li> <li>$X_1 + \dots + X_n$</li> <li>$\underset{n}{\inf} X_n$</li> <li>$\underset{n}{\sup} X_n$</li> <li>$\underset{n}{\lim \inf} X_n$</li> <li>$\underset{n}{\lim \sup} X_n$</li> </ul> </li> <li> <p>Proof to be completed.</p> </li> </ul> </div> <p>The <i>distribution</i> of a random variable can also be defined from a measure theoretic perspective.</p> <div id="distribution"></div> <div class="definition"> <strong>Definition (Distribution).</strong> <br/> Let $(\Omega, \mathcal{F}, P)$ be a probability space. The probability measure induced by a random variable, $X$, defined on this space is called its <i>distribution</i> and is defined as: $\mu(A) = \mathbb{P}_P(X \in A)$ for all Borel sets $A \in \mathcal{B}$. <br/> The <i>distribution function</i>, $F(x) = \mathbb{P}_P(X \leq x)$, describes the distribution of $X$ and satisfies the following properties: <ol> <li>$F$ is non-decreasing</li> <li>$\underset{x \rightarrow \infty}{\lim} F(x) = 1$ and $\underset{x \rightarrow -\infty}{\lim} F(x) = 0$</li> <li>$\underset{y \downarrow x}{\lim} F(x) = 0$ (right continuous)</li> <li>$F(x-) = \mathbb{P}_P(X &lt; x)$ </li> <li>$\mathbb{P}_P(X = x) = F(x) - F(x-)$</li> </ol> where $F(x-) = \underset{y \uparrow x}{\lim} F(y)$. </div> <p>Durrett provides the best intuition for the distribution of a random variable: “In words, we pull $A \in \mathbb{B}$ back to $X^{-1}(A) \in \mathcal{F}$ and then take $P$ of that set”[^fn-durrett].</p> <p>It’s important to remember that two different random variables can induce the same distribution. In this case, we say that the random variables (denote them by $X$ and $Y$) are <i>equal in distribution</i>, which we denote with $X \overset{d}{=} Y$.</p> <p>A distribution function with the form $F(x) = \int_{-\infty}^x f(y) dy$ can also be described by its <i>density function</i>, $f$, satisfying:</p> \[\mathbb{P}(X = x) = \underset{e \rightarrow 0}{\lim} \int_{x - e}^{x + e} f(y) dy = 0\] <p>In this case, we say that $F$ is <i>absolutely continuous</i>. Integrating the density function over the entire sample space/support will equal $1$, and the density function will always be non-negative.</p> <p>Similarly, we can define a <i>discrete</i> distribution function (i.e. an induced probability measure) as one in which there exists a countable set $S$ such that $P(S^c) = 0$.</p> <div class="example"> <strong>Example (Discrete Distribution).</strong> <br/> Suppose a random variable on $(\mathbb{R}, \mathcal{B})$ induces distribution $F$ such that $F(x) = 1$ for $x \geq 0$ and $F(x) = 0$ for $x &lt; 0$. This measure is discrete, and we call it a <i>point mass</i> at $0$. </div> <p>Now, since random variables are just measurable functions, we can use its mapping to define special $\sigma$-fields.</p> <div id="generated-sigma-field"></div> <div class="definition"> <strong>Definition (Generated $\sigma$-Field - Function).</strong> <br/> Let $(\Omega, \mathcal{A})$ and $(S, \mathcal{B})$ be measurable spaces and let $f: \Omega \rightarrow S$ be a measurable function, then the $\sigma$-field $f^{-1}(\mathcal{A})$ is called the <i>$\sigma$-field generated by $f$</i>. We denote it by $\sigma(f)$. <br/> More intuitively, $\sigma(f)$ is the smallest $\sigma$-field on which $f$ is measurable. </div> <p>Put intuitively, the $\sigma$-field generated by random variable $X$ is the collection of all possible subsets of the set of values $X$ can take on such that the probability of the event that $X$ takes on that value can be determined (i.e. is measurable).</p> <p>We can also define $\sigma$-fields generated by arbitary subsets. This is the smallest $\sigma$-field containing a given collection of subsets.</p> <div id="generated-sigma-field2"></div> <div class="definition"> <strong>Definition (Generated $\sigma$-Field - Family).</strong> <br/> Let $X$ be a set, and let $A$ be a collection of subsets of $X$. The $\sigma$-field generated by $A$, notated as $\sigma(A)$, is the collection of all subsets of $X$ that can be constructed from elements of $A$ under countable unions, intersections, and complementations. <br/> If $A = \emptyset$, then $\sigma(A) = \{ \emptyset, X\}$. If $A$ is a single event, then $\sigma(A) = \{ \emptyset, A, A^c, X \}$. </div> <div class="example"> <strong>Example (Generated $\sigma$-Field).</strong> <br/> Consider the Borel $\sigma$-field. This $\sigma$-field can be generated by any of the following sets: <ul> <li>$\{ (a, b) \rvert a, b \in \mathbb{R}, a &lt; b \}$ or $\{ (a, \infty) \rvert a \in \mathbb{R} \}$</li> <li>$\{ (a, b] \rvert a, b \in \mathbb{R}, a &lt; b \}$ or $\{ (-\infty, a) \rvert a \in \mathbb{R} \}$</li> <li>$\{ [a, b) \rvert a, b \in \mathbb{R}, a &lt; b \}$ or $\{ [a, \infty) \rvert a \in \mathbb{R} \}$</li> <li>$\{ [a, b] \rvert a, b \in \mathbb{R}, a &lt; b \}$ or $\{ (-\infty, a] \rvert a \in \mathbb{R} \}$</li> </ul> </div> <h3 id="stochastic-processes">Stochastic Processes</h3> <p>We can also think of having many random variables, each associated with some step in a sequence (perhaps time or space). We call this a <i>stochastic process</i>.</p> <div id="stochastic-process"></div> <div class="definition"> <strong>Definition (Stochastic Process).</strong> <br/> Let $(\Omega, \mathcal{F}, P)$ be a probability space, and let $(S, \Sigma)$ be a measurable space. Let $T$ be an index set. We call a collection of random variables, $\{ X(t, \omega): t \in T, \omega \in \Omega \}$, taking on values in $S$ a <i>stochastic process</i> on $(\Omega, \mathcal{F}, P)$ with state space $(S, \Sigma)$. In other words, a stochastic process is a random function $X: T \times \Omega \rightarrow S$. <br/> We will sometimes alternatively denote a stochastic process with $\{ X(t): t \in T\}$ and $(X_t)_{t \in T}$. </div> <p>Stochastic processes can be characterized by their <i>continuity</i> (or lack thereof).</p> <div id="sp-continuity"></div> <div class="definition"> <strong>Definition (Right-Continuous).</strong> <br/> Let $X: T \times \Omega \rightarrow S$ be a stochastic process on probability space $(\Omega, \mathcal{F}, P)$ with (measurable) state space $(S, \Sigma)$. If, for all $\omega \in \Omega$, there exists $\epsilon &gt; 0$ such that $X_s(\omega) = X_t(\omega)$ for all $s, t$ such that $t \leq s \leq t + \epsilon$, then we call $(X_t)_{t \in T}$ a <i>right-continuous</i> stochastic process. <br/> Note that this definition requires the index set to be over the non-negative reals. </div> <hr/> <h2 id="integration">Integration</h2> <p>Before we can look at random variables any further, we need to discuss a <i>very</i> important concept in mathematics. In the following, we will restrict our discussion to $\mathbb{R}$, but the definitions can easily be generalized to higher dimensions by exchanging lengths for volumes via Cartesian products.</p> <p>First, we define a special indicator function that got a fancy name (not sure why).</p> <div id="characteristic-function"></div> <div class="definition"> <strong>Definition (Characteristic Function).</strong> <br/> Let $E \subseteq X$. We define the <i>characteristic function</i> of $E$ as the function $\chi_E: X \rightarrow \mathbb{R}$ defined by: $$ \chi_E(x) = \begin{cases} 1 &amp; \text{if } x \in E \\ 0 &amp; \text{if } x \notin E \end{cases} $$ </div> <p>Though not very useful for our discussion, we’ll define the <i>outer measure</i> of a set $A \subseteq \mathbb{R}$. The outer measure formalizes the size of a set by using the lengths of open intervals.</p> <div id="outer-measure"></div> <div class="definition"> <strong>Definition (Outer Measure).</strong> <br/> Let $I$ be some open interval on the real line, and let $\ell(I)$ denote the <i>length</i> of $I$ defined as: $$ \ell(I) = \begin{cases} b - a &amp; \text{if } I = (a, b) \text{ for some } a, b \in \mathbb{R} \text{ such that } a &lt; b \\ 0 &amp; \text{if } I = \emptyset \\ \infty &amp; \text{otherwise } \end{cases} \nonumber $$ We define the <i>outer measure</i> of $A \subseteq \mathbb{R}$ as: $$ \rvert A \rvert = \inf\left\{ \sum_{i = 1}^\infty \ell(I_i) \bigg\rvert I_1, I_2, \dots \text{ are open intervals such that } A \subseteq \bigcup_{i = 1}^\infty I_i \right\} \nonumber $$ The outer measure satisfies: <ol> <li>$\rvert A \rvert \leq \rvert B \rvert$ for $A \subseteq B \subseteq \mathbb{R}$</li> <li>$\rvert \{ t + a \rvert a \in A \}\rvert = \rvert A \rvert$ for $t \in \mathbb{R}$ and $A \subseteq \mathbb{R}$</li> <li>$\rvert \cup_{i = 1]^\infty} A_i \rvert \leq \sum_{i = 1}^\infty \rvert A_i \rvert$ for $A_1, A_2, \dots \subseteq \mathbb{R}$</li> </ol> </div> <p>In words, the outer measure of a set is the smallest total length of some sequence of open intervals of $\mathbb{R}$ that, together, contain $A$. Finite sets have outer measure $0$ because we can make our open intervals arbitrarily “short” (i.e. force them to have length approaching $0$). By similar reasoning, any countable subset of $\mathbb{R}$ also has outer measure $0$.</p> <p>It’s important to remember that the outer measure is not a true measure in the sense that we defined. However, the outer measure allows us to define a special (and true) measure called the <i>Lebesgue measure</i>.</p> <div id="lebesgue-measure"></div> <div class="definition"> <strong>Definition (Lebesgue Measure).</strong> <br/> Let $\mathcal{B}$ be the $\sigma$-field of Borel subsets of $\mathbb{R}$, and let $(\mathbb{R}, \mathcal{B})$ be our measurable space. The <i>Lebesgue measure</i> on $(\mathbb{R}, \mathcal{B})$ is the measure such that $\mu(B) = \rvert B \rvert$ for any Borel set $B \in \mathcal{B}$. </div> <p>In words, the outer measure becomes a true measure if we restrict ourselves to only Borel sets. The Lebesgue measure leads to a refinement of the idea of a measurable set. A set $A \subseteq \mathbb{R}$ is called <i>Lebesgue measurable</i> if it is really “close” to being a Borel set. Put formally, $A$ is Lebesgue measurable if there exists a Borel set $B \subseteq A$ such that $\rvert A \setminus B \rvert = 0$. There are also many equivalent definitions (see pg. 52 of Axler (2025)).</p> <p>Note that sometimes the definition of the Lebesgue measure is <span class="popup" onclick="PopupFunc('pop2')">altered<span class="popuptext" id="pop2">The change is limited to the function’s domain (Borel vs. Lebesgue measurable sets).</span></span> to mean the measure on $(\mathbb{R}, \mathcal{L})$ where $\mathcal{L}$ is the $\sigma$-field of Lebesgue measurable subsets of $\mathbb{R}$.</p> <p>A function $f: A \rightarrow \mathbb{R}$ for $A \subseteq \mathbb{R}$ is <i>Lebesgue measurable</i> if $f^{-1}(B)$ is a Lebesgue measurable set for every Borel set $B \subseteq \mathbb{R}$.</p> <p>A lot of things in probability depend upon integration. For example, expectation, variance, cumulative probability, and many more things can all be stated as some type of integral. Thus, it’s important we have a solid understanding of the integral.</p> <h3 id="non-negative-functions">Non-Negative Functions</h3> <p>We start with the integral of the characteristic function:</p> \[\int \chi_E d\mu = \mu(E) \hspace{5mm} \forall E \in \mathcal{S}\] <p>Recall that a simple function is any function that takes on finitely many values. Any piecewise function with finitely many pieces is simple. We can use the integral of the characteristic function to that of simple functions by taking a linear combination.</p> <p>Let $(X, \mathcal{S}, \mu)$ be a measure space, let $A_1, \dots, A_n$ be disjoint set in $\mathcal{S}$, and let $c_1, \dots, c_n \in [0, \infty]$. Then:</p> \[\int \left(\sum_{i = 1}^n c_i \chi_{A_i} \right) d\mu = \sum_{i = 1}^n c_i \mu(A_i)\] <p>With these definitions in mind, we can define the integral of any non-negative function.</p> <div id="integral-nonnegative"></div> <div class="definition"> <strong>Definition (Integral of a Non-Negative Function).</strong> <br/> Let $(X, \mathcal{S}, \mu)$ be a measure space, and let $f: X \rightarrow [0, \infty]$ be an $\mathcal{S}$-measurable function. Its integral with respect to $\mu$ is defined as: $$ \int f d\mu = \sup \left\{ \sum_{i = 1}^n c_i \mu(A_i) \bigg\rvert \text{ disjoint } A_1, \dots, A_n \in \mathcal{S}; \hspace{2mm} c_1, \dots, c_n \in [0, \infty); \hspace{2mm} f(x) \geq \sum_{i = 1}^n c_i \chi_{A_{i}}(x) \text{ for all } x \in X \right\} $$ </div> <h3 id="real-valued-functions">Real-Valued Functions</h3> <p>We begin with a definition.</p> <div id="f-plus-minus"></div> <div class="definition"> <strong>Definition ($f^+$ and $f^-$).</strong> <br/> Let $f: X \rightarrow [-\infty, \infty]$ be a function. We have that $f = f^+ - f^-$ and $\rvert f \rvert = f^+ + f^-$ for piecewise functions: $$ f^+ = \begin{cases} f(x) &amp; \text{ if } f(x) \geq 0 \\ 0 &amp; \text{ if } f(x) &lt; 0 \end{cases} \hspace{10mm} \text{and} \hspace{10mm} f^- = \begin{cases} 0 &amp; \text{ if } f(x) \geq 0 \\ f(x) &amp; \text{ if } -f(x) &lt; 0 \end{cases} \nonumber $$ </div> <p>Notice that if $f(x) \geq 0$, then $f^+(x) \geq 0$ and $f^-(x) = 0$. Alternatively, if $f(x) &lt; 0$, then $f^+(x) = 0$ and $f^-(x) = -f(x) &gt; 0$. Thus, $f^+$ and $f^-$ are both non-negative functions. This allows us extend the definition of the integral to real-valued functions.</p> <div id="integral"></div> <div class="definition"> <strong>Definition (Integral of a Real-Valued Function).</strong> <br/> Let $(X, \mathcal{S}, \mu)$ be a measure space, and let $f: X \rightarrow [-\infty, \infty]$ be an $\mathcal{S}$-measurable function such that $\int f^+ d\mu &lt; \infty$, $\int f^- d\mu &lt; \infty$, or both. The <i>integral</i> of $f$ with respect to $\mu$ is defined as: $$ \int f d\mu = \int f^+ d\mu - \int f^- d\mu \nonumber $$ The integral is homogeneous (i.e. $\int c f d\mu = c \int f d\mu$ for any $c \in \mathbb{R}$) and additive (i.e. $\int (f + g) d\mu = \int f d\mu + \int g d\mu$ for $\mathcal{S}$-measurable $f$ and $g$ satisfying $\int \rvert f \rvert d\mu &lt; \infty$ and $\int \rvert g \rvert d \mu &lt; \infty$). </div> <p>If we have $(\Omega, \mathcal{F}, \mu) = (\mathbb{R}^d, \mathcal{B}^d, \lambda)$, then we denote $\int f d\lambda$ with $\int f(x) dx$, and if $(\Omega, \mathcal{F}, \mu) = (\mathbb{R}, \mathcal{B}, \lambda)$ and we have some interval $E = [a, b]$, we write $\int_a^b f(x) dx$ instead of $\int_E f d\lambda$.</p> <p>Integration can be restricted to a subset of the domain of a function. That is, for $E \in \mathcal{S}$:</p> \[\int_E f d\mu = \int f \chi_E d \mu\] <p>It can also be restricted to an interval of the extended real line. First, we call a bounded function $f: [a, b] \rightarrow \mathbb{R}$ <i>Riemann integrable</i> if the set of points in $[a, b]$ at which $f$ is not continuous has length $0$. If we have Lebesgue measure on $\mathbb{R}$, $\lambda$, and $f: (a, b) \rightarrow \mathbb{R}$ is a Lebesgue measurable function, then for $-\infty \leq a &lt; b \leq \infty$ we let $\int_a^b f(x) dx = \int_{(a,b)} f d\lambda$.</p> <h3 id="radon-nikodym">Radon-Nikodym</h3> <p>Two different measures can be related via the <a href="https://en.wikipedia.org/w/index.php?title=Radon%E2%80%93Nikodym_theorem&amp;oldid=1288156682">Radon-Nikodym Theorem</a>, which states that (under certain conditions), there exists a function such that one measure is equivalent to the integral of the function with respect to a second measure.</p> <div id="rn-theorem"></div> <div class="theorem"> <strong>Radon-Nikodym Theorem.</strong> <ul id="radon-nikodym" class="tab" data-tab="3c9219a3-901a-4851-b90a-96a370b62c79" data-name="radon-nikodym"> <li class="active" id="radon-nikodym-statement"> <a href="#">statement </a> </li> <li id="radon-nikodym-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="3c9219a3-901a-4851-b90a-96a370b62c79" data-name="radon-nikodym"> <li class="active"> <p>Let $(X, \mathcal{S})$ be a measurable space, and let $\mu$ and $\nu$ denote two $\sigma$-finite measures on this space such that $\nu « \mu$ ($\nu$ is absolutely continuous with respect to $\mu$). Then there existgs a $\mathcal{S}$-measurable function, $f: X \rightarrow [0, \infty)$ such that, for any measurable $A \subset \mathcal{S}$:</p> \[\nu(A) = \int_A f d\mu\] </li> <li> <p>Proof to be completed.</p> </li> </ul> </div> <p>A fun fact is that $f$ is unique up to some set of measure $0$ with respect to $0$. That is, for any other $g$ that satisfies the definition, $f(x) = g(x)$ for all $x \in X$ except some $x \in X’ \subset X$ such that $\mu(X’) = 0$. Such a function, $f$, is called the <i>Radon-Nikodym derivative</i> and can be denoted by $\frac{d \nu}{d \mu}$.</p> <h3 id="integral-properties">Integral Properties</h3> <p>Here we list and prove several properties of integrals that are ubiquitous in theoretical statistics.</p> <div id="jensen"></div> <div class="theorem"> <strong>Jensen's Inequality.</strong> <ul id="jensen" class="tab" data-tab="d05556bc-b4b9-4104-bd29-2f31f1fbbdc3" data-name="jensen"> <li class="active" id="jensen-statement"> <a href="#">statement </a> </li> <li id="jensen-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="d05556bc-b4b9-4104-bd29-2f31f1fbbdc3" data-name="jensen"> <li class="active"> <p>Let $\phi$ be a convex function (i.e. $\lambda \phi(x) + (1- \lambda)\phi(y) \geq \phi(\lambda x + (1-\lambda)y)$ for all $\lambda \in (0, 1)$, $x,y \in \mathbb{R}$). Let $\mu$ be a probability measure, and let $f$ and $\phi(f)$ be integrable. Jensen’s inequality states:</p> \[\phi\left(\int f d\mu \right) \leq \int \phi(f)d\mu\] </li> <li> <p>Proof to be completed.</p> </li> </ul> </div> <div id="holder"></div> <div class="theorem"> <strong>Hölder's Inequality.</strong> <ul id="holder" class="tab" data-tab="04ce7fa7-7931-48f7-bece-62dcfceab0c8" data-name="holder"> <li class="active" id="holder-statement"> <a href="#">statement </a> </li> <li id="holder-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="04ce7fa7-7931-48f7-bece-62dcfceab0c8" data-name="holder"> <li class="active"> <p>Let $\mu$ be a probability measure, and let $p, q \in (1, \infty)$ such that $\frac{1}{p} + \frac{1}{q} = 1$. Hölder’s inequality states:</p> \[\int \rvert fg \rvert d\mu \leq \rvert \rvert f \rvert \rvert_p \rvert \rvert g \rvert \rvert_1\] <p>where $\rvert \rvert f \rvert \rvert_p = (\int \rvert f \rvert^p d\mu)^{\frac{1}{p}}$ for $1 \leq p &lt; \infty$.</p> </li> <li> <p>Proof to be completed.</p> </li> </ul> </div> <p>If $p = q = 2$, the above is called the <i>Cauchy-Schwarz inequality</i>.</p> <div id="bounded-convergence"></div> <div class="theorem"> <strong>Bounded Convergence Theorem.</strong> <ul id="bounded-conv" class="tab" data-tab="c5bdeb8b-1e15-4a05-816f-5fb2b3ba68e1" data-name="bounded-conv"> <li class="active" id="bounded-conv-statement"> <a href="#">statement </a> </li> <li id="bounded-conv-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="c5bdeb8b-1e15-4a05-816f-5fb2b3ba68e1" data-name="bounded-conv"> <li class="active"> <p>Let $E$ be a set of finite measure (i.e. $\mu(E) &lt; \infty$), and let ${ f_n }$ be a sequence of functions that vanish on $E^c$, are uniformly pointwise bounded (i.e. $\rvert f_n(x) \rvert \leq M$), and $f_n \rightarrow f$ in measure. Then:</p> \[\int f d\mu = \underset{n \rightarrow \infty}{\lim} \int f_n d\mu\] </li> <li> <p>Proof to be completed.</p> </li> </ul> </div> <div id="monotone-convergence"></div> <div class="theorem"> <strong>Monotone Convergence Theorem.</strong> <ul id="monotone-conv" class="tab" data-tab="c7e67b25-40ca-4622-8ebf-00005c6e35da" data-name="monotone-conv"> <li class="active" id="monotone-conv-statement"> <a href="#">statement </a> </li> <li id="monotone-conv-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="c7e67b25-40ca-4622-8ebf-00005c6e35da" data-name="monotone-conv"> <li class="active"> <p>Let $(\Omega, \mathcal{F}, \mu)$ be a measure space, and let $X \in \mathcal{F}$ be a measurable set. Let \(\{ f_k \}_{k = 0}^\infty\) be a pointwise non-decreasing sequence of \((\mathcal{F}, \mathbb{B}(\bar{\mathbb{R}}_{\geq 0})\)-measurable, non-negative functions (i.e. \(0 \leq \dots \leq f_k(x) \leq f_{k+1}(x) \leq \dots \leq \infty\) for every $k \geq 1$ and $x \in X$). Then the pointwise supremum, defined as the function:</p> \[\underset{k}{\sup} f_k: x \rightarrow \underset{k}{\sup} f_k(x)\] <p>is $(\mathcal{F}, \mathbb{B}(\bar{\mathbb{R}}_{\geq 0}))$-measurable and satisfies:</p> \[\underset{k}{\sup} \int_X f_k d\mu = \int_X \underset{k}{\sup} f_k d\mu\] </li> <li> <p>Proof to be completed.</p> </li> </ul> </div> <div id="dominated-convergence"></div> <div class="theorem"> <strong>Dominated Convergence Theorem.</strong> <ul id="domin-conv" class="tab" data-tab="2f3868ca-3a25-47bb-aaba-b6ce738adfbd" data-name="domin-conv"> <li class="active" id="domin-conv-statement"> <a href="#">statement </a> </li> <li id="domin-conv-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="2f3868ca-3a25-47bb-aaba-b6ce738adfbd" data-name="domin-conv"> <li class="active"> <p>Let $(\Omega, \mathcal{F}, \mu)$ be a measure space, and let \(\{ f_k \}_{k \in T}\) be a sequence of measurable functions (with index set $T$) on this space such that \(\underset{n \rightarrow \infty}{\lim} f_n(x) = f(x)\) for some function $f$ for all $x \in \Omega$ (i.e. \(\{ f_k \}_{k \in T}\) converges pointwise to $f$). Suppose that our sequence is <i>dominated</i> by some other integrable function, $g$; that is:</p> \[\rvert f_n(x) \rvert \leq g(x) \hspace{5mm} \forall x \in \Omega, \hspace{2mm} \forall n \in T\] <p>The Dominated Convergence Theorem states that $f_n$ and $f$ are both (Lebesgue) integrable and:</p> \[\underset{n \rightarrow \infty}{\lim} \int_\Omega f_n d\mu = \int_\Omega \underset{n \rightarrow \infty}{\lim} f_n d\mu = \int_\Omega f d\mu\] </li> <li> <p>Proof to be completed.</p> </li> </ul> </div> <div id="fatou"></div> <div class="theorem"> <strong>Fatou's Lemma.</strong> <ul id="fatou" class="tab" data-tab="c8376671-f685-4c73-b141-36f1fd086dfd" data-name="fatou"> <li class="active" id="fatou-statement"> <a href="#">statement </a> </li> <li id="fatou-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="c8376671-f685-4c73-b141-36f1fd086dfd" data-name="fatou"> <li class="active"> <p>Let $(\Omega, \mathcal{F}, \mu)$ be a measure space, and let \(\{ f_n: \Omega \rightarrow [0, \infty]\}\) be a sequence of non-negative measurable functions. Then:</p> \[\int_X \underset{n \rightarrow \infty}{\lim} \underset{m \geq n}{\inf} f_n d\mu \leq \underset{n \rightarrow \infty}{\lim} \underset{m \geq n}{\inf} \int_X f_n d\mu\] </li> <li> <p>Proof to be completed.</p> </li> </ul> </div> <div id="fubini"></div> <div class="theorem"> <strong>Fubini's Theorem.</strong> <ul id="fubini" class="tab" data-tab="e160283c-948c-4fe1-b384-34627ef649b3" data-name="fubini"> <li class="active" id="fubini-statement"> <a href="#">statement </a> </li> <li id="fubini-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="e160283c-948c-4fe1-b384-34627ef649b3" data-name="fubini"> <li class="active"> <p>Let $(X, \mathcal{S}, \mu_1)$ and $(Y, \mathcal{T}, \mu_2)$ be $\sigma$-finite measurable spaces, and let $\mu = \mu_1 \times \mu_2$ (the product meeasure). If we have a function $f$ such that $f \geq 0$ or $\int \rvert f \rvert d \mu$, then:</p> \[\int_X \int_Y f(x,y) \mu_2(dy) \mu_1(dx) = \int_{X \times Y} f d \mu = \int_Y \int_X f(x,y) \mu_1(dx) \mu_2(dy)\] </li> <li> <p>Proof to be completed.</p> </li> </ul> </div> <p>Fubini’s Theorem tells us when it is okay to exchange the order of a double integral and to compute a double integral as an interated integral.</p> <hr/> <h2 id="expectation">Expectation</h2> <p>For a random variable, $X$ on probability space $(\Omega, \mathcal{F}, P)$, how can we describe its central tendency (i.e. what values $X$ usually takes on)? We answer this question with the following definitions, which use ideas from integration (see <a href="#integration">later in this post</a>).</p> <div id="expectation"></div> <div class="definition"> <strong>Definition (Expectation).</strong> <br/> Let $X$ be a real-valued random variable on probability space $(\Omega, \mathcal{F}, P)$. Its <i>expectation</i> or <i>expected value</i> is defined as the following Lebesgue integral: $$ \mathbb{E}[X] = \int_\Omega X dP $$ </div> <p>The expected value or expectation of a random variable is basically just integration with respect to the probability measure of the space the variable is defined on. It can be any real number or even $\infty$. Since it is just an integral, we can extend all of the results in the previous section to the expectation. The results are the same, just rewritten with $\mathbb{E}[X]$ instead of $\int_\Omega X dP$.</p> <p>We can also define the <i>conditional expectation</i> of a random variable with respect to a particular sub-$\sigma$-field.</p> <div id="conditional-expectation"></div> <div class="definition"> <strong>Definition (Conditional Expectation).</strong> <br/> Let $(\Omega, \mathcal{F}, P)$ be a probability space, let $X: \Omega \rightarrow \mathbb{R}^n$ be a real-valued random variable with finite expectation, and let $\mathcal{H} \subseteq \mathcal{F}$ be a sub-$\sigma$-field of $\mathcal{F}$. A <i>conditional expectation of $X$ given $\mathcal{H}$</i> is any $\mathcal{H}$-measurable function, $\mathbb{E}(X \rvert \mathcal{H}): \Omega \rightarrow \mathbb{R}^n$, satisfying: $$ \int_H \mathbb{E}[X \rvert \mathcal{H}] dP = \int_H X dP \hspace{5mm} \forall H \in \mathcal{H} $$ This function exists and is unique. </div> <div id="conditional-expectation-ex"></div> <div class="example"> <strong>Definition (Conditional Expectation).</strong> <br/> Suppose $X \in \mathcal{F}$. Then $\mathbb{E}_P[X \rvert \mathcal{F}] = X$ itself. In this scenario, we have perfect information. Since $X \in \mathcal{F}$, we have complete knowledge of whether it occurred or not when we are given all of $\mathcal{F}$. </div> <p>These definitions are a bit confusing, so let’s parse them by coming at the topic from a different angle (see <a href="https://math.stackexchange.com/questions/375994/intuition-behind-conditional-expectation-when-sigma-algebra-isnt-generated-by-a">this post</a>).</p> <p>Let’s say we have a random variable $X$ on some probability space $(\Omega, \mathcal{F}, P)$. We don’t know anything about it, so our best guess at its value would be some sort of weighted average over all of the possible values it could take on. These weights are determined by the probability measure, $P$, since a good guess should be closer to the more likely outcomes.</p> <p>Now, suppose we know some information about $X$’s outcome (i.e. we can answer some set of questions about $X$). We could formulate this as a collection of subsets of $\Omega$. For example, if we were rolling dice, the question “Is $X$ odd?” could be contained in the set \(\{1, 3, 5\}\) or \(\{2, 4, 6\}\). We could imagine outputting a different best guess depending upon what set of information we are given, which is basically what the conditional expectation does.</p> <p>In one way of thinking, $\mathbb{E}[X \rvert \mathcal{H}]$ is a random variable mapping from the possible values of $X$ to the best guesses. The condition $\int_H \mathbb{E}[X \rvert \mathcal{H}] dP = \int_H X dP$ for all $H \in \mathcal{H}$ can be thought of as enforcing the idea that, if we only are guessing values that are consistent with $H$, then our best guess using <i>only</i> the information in $H$ should be the same as the weighted average of $X$ itself over $H$. More concretely, if \(H = \{ 1, 3, 5\}\) in our dice rolling example, having $\int_H \mathbb{E}[X \rvert \mathcal{H} dP = \int_H X dP$ implies that, given $H$, we can guess the average of $X$ perfectly.</p> <p>We can relate this measure theoretic definition with the more common ones learned in statistics courses. First, partition the sample space, $\Omega$, into disjoint sets $\Omega_1, \Omega_2, \dots$ such that $\mu(\Omega_i) &gt; 0$ for all $i$. Let $\mathcal{F} = \sigma(\Omega_1, \Omega_2, \dots)$ be the $\sigma$-field generated by this collection of sets. For random variable $X$ defined on $(\Omega, \mathcal{F}, \mu)$, we have:</p> \[\mathbb{E}_\mu[X \rvert \mathcal{F}] = \frac{\mathbb{E}_\mu[X \rvert \Omega_i]}{\mu(\Omega_i)} \hspace{5mm} \text{on } \Omega_i\] <p>When we are given some information, $\Omega_i$, about which set in our partition $X$ can be found in, our best guess at $X$ becomes the average of $X$ over that set.</p> <p>In most probability courses, we also learn about conditional expectations with respect to some other random variable. In this case, we write $\mathbb{E}[X \rvert Y]$ to mean $\mathbb{E}[X \rvert \sigma(Y)]$.</p> <hr/> <h2 id="miscellaneous">Miscellaneous</h2> <h3 id="vector-spaces">Vector Spaces</h3> <p>We now introduce the idea of <i>vector spaces</i>. We begin with the definition of a <i>field</i>.</p> <div id="field"></div> <div class="definition"> <strong>Definition (Field).</strong> <br/> A <i>field</i> is a set $F$ with the operations of addition ($+$) and multiplication ($\cdot$), which satisfy for any $a, b, c \in F$: <ol> <li>Associativity: $a + (b + c) = (a + b) + c$ and $a \cdot (b \cdot c) = (a \cdot b) \cdot c$</li> <li>Communtatitvity: $a + b = b + a$ and $a \cdot b = b \cdot a$</li> <li>Identity: $\exists 0, 1 \in F$ such taht $a + 0 = a$ and $a \cdot 1 = a$</li> <li>Additive Inverses: $\forall a \in F$, $\exists -a \in F$ such taht $a + (-a) = 0$</li> <li>Multiplicative Inverses: $\forall a \in F$ such that $a \neq 0$, $\exists a^{-1} \in F$ such that $a \cdot a^{-1} = 1$</li> <li>Distributivity: $a \cdot (b + c) = (a \cdot b) + (a \cdot c)$</li> </ol> </div> <p>A vector space is defined with respect to a field. In generality, it is a set of elements that satisfy some special properties in relation to some field.</p> <div id="vector-space"></div> <div class="definition"> <strong>Definition (Vector Space).</strong> <br/> Let $F$ be a field. A <i>vector space</i>, $V$, is some (non-empty) set with the operation of <i>vector addition</i> ($+$) and the function of <i>scalar multiplication</i>. Vector addition takes two vectors in $V$ and assigns them a sum, which is just a third vector in $V$. Scalar multiplication takes any a vector in $V$ and any scalar $a$ in $F$ and assigns it a product, which is another vector in $V$. This operation and function satisfy the following for any $\mathbf{u}, \mathbf{v}, \mathbf{w} \in V$ and any $a, b \in F$: <ol> <li>Associativity: $\mathbf{u} + (\mathbf{v} + \mathbf{w}) = (\mathbf{u} + \mathbf{v}) + \mathbf{w}$</li> <li>Commutativity: $\mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u}$</li> <li>Vector Addition Identity: $\exists \mathbf{0} \in V$ such that $\mathbf{v} + \mathbf{0} = \mathbf{v}$ for all $\mathbf{v} \in V$</li> <li>Scalar Multiplication Identity: $1 \mathbf{v} = \mathbf{v}$ where $1$ is the multiplicative identity in $F$</li> <li>Inverses: $\exists -\mathbf{v} \in V$ for every $ \mathbf{v} \in V$ such that $\mathbf{v} + (-\mathbf{v}) = \mathbf{0}$</li> <li>Compatibility: $a(b \mathbf{v}) = (ab) \mathbf{v}$</li> <li>Distributivity: $a(\mathbf{u} + \mathbf{v}) = a \mathbf{u} + a \mathbf{v}$ and $(a + b) \mathbf{v} = a \mathbf{v} + b \mathbf{v}$</li> </ol> </div> <p>Many concepts in linear algebra and general mathematics are derived from the vector space, including linear combinations, subspaces, and bases. It’s important to note that, though we usually think of vectors as tuples, they don’t need to be. You could define a vector to be different cheeses, and as long as the definition is satisfied, it will be a valid vector space.</p> <p>If we equip a vector space with a special type of map, then we get an <i>inner product space</i>.</p> <div id="inner-product-space"></div> <div class="definition"> <strong>Definition (Inner Product Space).</strong> <br/> An <i>inner product space</i> is a vector space, $V$, over the field, $F$, of real numbers or complex numbers with the map $\langle, \cdot, \cdot, \rangle: V \times V \rightarrow F$, called an <i>inner product</i>, which satisfies the following for all $\mathbf{u}, \mathbf{v}, \mathbf{w} \in V$ and all $a, b \in F$: <ol> <li>Conjugate Symmetry: $\langle \mathbf{u}, \mathbf{v} \rangle = \overline{\langle \mathbf{v}, \mathbf{u} \rangle}$</li> <li>Linearity (in the first argument): $\langle a \mathbf{u} + b \mathbf{v}, \mathbf{w} \rangle = a \langle \mathbf{u}, \mathbf{w} \rangle + b \langle \mathbf{v}, \mathbf{w} \rangle$</li> <li>Positive-Definiteness: $\langle \mathbf{u}, \mathbf{u} \rangle &gt; 0$ for any $\mathbf{u} \neq \mathbf{0}$</li> </ol> </div> <p>Something that will be very useful is a map from a vector space to the real numbers that can be thought of as assigning a “size” to vectors in the space. We call this a <i>norm</i>, and if we equip a vector space with a norm, then we have a <i>normed vector space</i>.</p> <div id="norm"></div> <div class="definition"> <strong>Definition (Norm).</strong> <br/> Let $V$ be a vector space over a scalar field $K$. A <i>norm</i>, $\rvert \rvert \cdot \rvert \rvert: V \rightarrow \mathbb{R}$, is a map satisfying: <ol> <li>Non-negativity: $\rvert \rvert x \rvert \rvert \geq 0$ for all $x \in V$ </li> <li>Positive definiteness: $\rvert \rvert x \rvert \rvert = 0$ if and only if $x$ is the zero vector, for all $x \in V$ </li> <li>Absolute homogeneity: $\rvert \rvert \lambda x \rvert \rvert = \rvert \lambda \rvert \rvert \rvert x \rvert \rvert$ for all $\lambda \in K$ and $x \in V$</li> <li>Triangle inequality: $\rvert \rvert x + y \rvert \rvert \leq \rvert \rvert x \rvert \rvert + \rvert \rvert y \rvert \rvert$ for all $x, y \in V$</li> </ol> </div> <p>We can define the <i>canonical norm</i> of an inner product space as $\rvert \rvert x \rvert \rvert \sqrt{\langle x, x \rangle}$. Thus, any inner product space is a normed vector space. A special type of normed vector space is the <i>Banach space</i>.</p> <div id="banach-space"></div> <div class="definition"> <strong>Definition (Banach Space).</strong> <br/> Let $X$ be a vector space over a scalar field (perhaps $\mathbb{R}$ or $\mathbb{C}$), and let $\rvert \rvert \cdot \rvert \rvert: X \rightarrow \mathcal{R}$ be a norm. Together, $(X, \rvert \rvert \cdot \rvert \rvert)$ form a <i>normed space</i>. If this space is also complete, then $(X, \rvert \rvert \cdot \rvert \rvert)$ is a <i>Banach space</i>. <br/> <i>Note: Any finite-dimensional normed vector space is a Banach space. This includes finite-dimensional Euclidean spaces.</i> </div> <p>By “complete”, we mean that the space does not have any “holes” in it. Formally put, any Cauchy sequence taking values in $X$ converges to a point in $X$ as well.</p> <p>Norms can also induce what we call a <i>distance metric</i> or <i>function</i> (or just <i>metric</i> for short) which assigns a value to represent how “far apart” two vectors in our space are.</p> <div id="distance-metric"></div> <div class="definition"> <strong>Definition (Distance Metric).</strong> <br/> Let $X$ be a set. A <i>distance metric</i> is any function $d: X \times X \rightarrow \mathbb{R}$ satisfying the following for all $x, y, z, \in X$: <ol> <li>$d(x, x) = 0$</li> <li>Positivity: $x \neq y \implies d(x,y) &gt; 0$ </li> <li>Symmetry: $d(x,y) = d(y,x)$</li> <li>Triangle inequality: $d(x,z) \leq d(x,y) + d(y, z)$</li> </ol> </div> <p>The <i>induced metric</i> (i.e. the distance metric induced by the norm of a vector space) is the function $d: V \times V \rightarrow \mathbb{R}$ satisfying $d(x, y) = \rvert \rvert x - y \rvert \rvert$ for all $x,y \in V$. If we combine a metric with a set, then we get a <i>metric space</i>, which is just a set on which we have a particular sense of distance between its elements.</p> <div id="metric-space"></div> <div class="definition"> <strong>Definition (Metric Space).</strong> <br/> Let $X$ be a set, and let $d$ be a distance metric. A <i>metric space</i> is the ordered pair $(X, d)$. A metric space is called <i>complete</i> if every Cauchy sequence in $X$ converges to a point in $X$. </div> <p>Using our definitions of inner product and complete metric spaces, we can define what is known as a <i>Hilbert space</i>.</p> <div id="hilbert-space"></div> <div class="definition"> <strong>Definition (Hilbert Space).</strong> <br/> A <i>Hilbert space</i> is a real (or complex) inner product space that is also a complete metric space where the distance metric is that induced by its inner product. </div> <h3 id="helpful-definitions">Helpful Definitions</h3> <div id="open-cover"></div> <div class="definition"> <strong>Definition (Open Cover).</strong> <br/> An <i>open cover</i> of subset $A \subseteq \mathbb{R}$ is any collection $\mathcal{C}$ of open subsets of $\mathbb{R}$ such that $A \subseteq \bigcup_{C \in \mathcal{C}} C$. <br/> A <i>finite subcover</i> of an open cover $\mathcal{C}$ of $A$ is any finite subset of sets in $\mathcal{C}$. </div> <div id="almost-every"></div> <div class="definition"> <strong>Definition (Almost Every).</strong> <br/> Let $(X, \mathcal{S}, \mu)$ be a measure space, and let $A \in \mathcal{S}$. We say that $A$ contains $\mu$-<i>almost every</i> element of $X$ if $\mu(X \setminus A) = 0$ (in words, if $A$ contains all of $X$ except a subset of measure $0$). </div> <div id="almost-everywhere"></div> <div class="definition"> <strong>Definition (Almost Everywhere).</strong> <br/> Let $\mu$ be a $\sigma$-finite measure on $(\Omega, \mathcal{F})$, and let $\phi$ and $\psi$ be functions on $(\Omega, \mathcal{F}, \mu)$. We say that $\phi \geq \psi$ <i>$\mu$-almost everywhere</i> if $\mu(\{ \omega: \phi(\omega) &lt; \psi(\omega) \}) = 0$. </div> <div id="almost-surely"></div> <div class="definition"> <strong>Definition (Almost Surely).</strong> <br/> Let $(\Omega, \mathcal{F}, P)$ be a probability space. An event $A$ happens <i>almost surely</i> if $P(A) = 1$. </div> <div id="partial-order"></div> <div class="definition"> <strong>Definition (Partial Order).</strong> <br/> A <i>partial order</i> is a binary relation, $\leq$, between a set, $X$, and itself satisfying the following for any $a, b, c \in X$: <ol> <li> Reflexivity: $a \leq a$ </li> <li> Antisymmetry: $a \leq b$ and $b \leq a \implies a = b$ </li> <li> Transitivity: $a \leq b$ and $b \leq c \implies a \leq c$ </li> </ol> Partial orders as defined above are sometimes called <i>reflexive</i>, <i>weak</i>, or <i>non-strict</i>. A <i>strict partial order</i> is a binary relation, $&lt;$, between a set, $X$, and itself satisfying the following for all $a, b, c \in X$: <ol> <li> Irreflexivity: $\neg(a &lt; a)$ </li> <li> Asymmetry: $a &lt; b \implies \neg (b &lt; a)$ </li> <li> Transitivity: $a &lt; b$ and $b &lt; c \implies a &lt; c$ </li> </ol> </div> <div id="total-order"></div> <div class="definition"> <strong>Definition (Total Order).</strong> <br/> A <i>total order</i>, also called a <i>linear order</i>, is a partial order satisfying the additional property for all $a, b, c \in X$: <ul> <li> Totality: $a \leq b$ or $b \leq a$ </li> </ul> Total orders as defined above are sometimes called <i>non-strict</i>. A <i>strict total order</i> is a strict partial order that satisfies the following additional proerpty for all $a, b \in X$: <ul> <li> Connectivity: $a \neq b \implies a &lt; b$ or $b &lt; a$ </li> </ul> </div> <h3 id="assorted-results">Assorted Results</h3> <div id="doob"></div> <div class="theorem"> <strong>Doob's (First) Convergence Theorem.</strong> <ul id="doob" class="tab" data-tab="f7196b52-2822-4499-8bda-8a3a314d0a75" data-name="doob"> <li class="active" id="doob-statement"> <a href="#">statement </a> </li> <li id="doob-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="f7196b52-2822-4499-8bda-8a3a314d0a75" data-name="doob"> <li class="active"> <p>Let $(\Omega, \mathcal{F}, P)$ be a probability space, and let \(\mathbb{F} = (\mathcal{F}_t)_{t \geq 0}\) be a filtration such that $\mathcal{F}_t$ is a sub-$\sigma$-field of $\mathcal{F}$ for all $t$. (That is, $(\Omega, \mathcal{F}, \mathbb{F}, P)$ is a <i>filtered probability space</i>). Suppose we also have \(X: [0, \infty) \times \Omega \rightarrow \mathbb{R}\), a right-continuous supermartingale with respect to $\mathbb{F}$. <br/> For $t \geq 0$, define \(X^-_t = \max\{-X_t, 0 \}\). Assume \(\underset{t &gt; 0}{\sup} \mathbb{E}[X_t^-] &lt; +\infty\). Then (the point-wise limit) \(X(\omega) = \underset{t \rightarrow + \infty}{\lim} X_t(\omega)\) exists and is finite for all $\omega \in \Omega$ except a $P$-null set.</p> </li> <li> <p>Proof to be completed.</p> </li> </ul> </div>]]></content><author><name></name></author><category term="theory"/><category term="likelihood"/><category term="primer"/><summary type="html"><![CDATA[A Primer]]></summary></entry></feed>