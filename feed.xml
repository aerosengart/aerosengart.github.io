<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://aerosengart.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://aerosengart.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-12-09T21:21:32+00:00</updated><id>https://aerosengart.github.io/feed.xml</id><title type="html">Anna Rosengart</title><entry><title type="html">Score and Information</title><link href="https://aerosengart.github.io/blog/2025/score-info/" rel="alternate" type="text/html" title="Score and Information"/><published>2025-10-29T00:00:00+00:00</published><updated>2025-10-29T00:00:00+00:00</updated><id>https://aerosengart.github.io/blog/2025/score-info</id><content type="html" xml:base="https://aerosengart.github.io/blog/2025/score-info/"><![CDATA[<p>This post is just a catch-all for my derivations for my score test project. Our set-up is as follows. We have $n$ observations coming from $k$ different clusters, each of size $n_t$ for $t \in [k]$. The full data will be denoted by $\mathbf{y}$. Though $\mathbf{y}$ is a vector, we’ll denote the $j$-th observation from cluster $i$ with $\mathbf{y}_{i,j}$. For example, \(\mathbf{y}_{i,j}\) denotes element \(\sum_{l = 1}^{i - 1} n_l + j\) of $\mathbf{y}$. We’ll also denote the $n_i$-dimensional vector of responses for cluster $i$ with $\mathbf{y}_i$.</p> <p>For each observation, we will have $p$ fixed effect covariates arranged in a $p$-dimensional vector, \(\mathbf{x}_{i, j}\), and $q$ random effects covariates in a $q$-dimensional vector, \(\mathbf{z}_{i,j}\). We’ll assume that the observations within the same cluster are independent.</p> <p>Our model comes in the form of a specification of the conditional mean, $\mu_{i,j} = \mathbb{E}[\mathbf{y}_{i,j} \rvert \beta_i]$ (where we suppress the addition conditioning on the covariates themselves). For a monotonic and differentiable link function (e.g. $\log(\cdot)$ or $\text{logit}(\cdot)$), the conditional mean of the $j$-th observation in group $i$ is assumed to be given by:</p> \[\mu_{i,j} = g^{-1}\left(\alpha^\top \mathbf{x}_{i,j} + \beta_i^\top \mathbf{z}_{i,j} \right) \label{eq:glmm}\] <p>We then assume that the observations themselves follow some exponential family distribution with measurement errors, $\epsilon_{i,j}$, which is the deviation of the response from its (unit-specific) conditional mean. These errors are assumed to have mean zero and be independent of each other and of the random effects. We further assume the responses, $\mathbf{y}_{i,j}$, conditional on the random effects (and the covariates), are independent with variances equal to some function of the conditional mean.</p> <p>In general, we will assume that:</p> \[\beta_i \overset{iid}{\sim} \mathcal{N}\left(\mathbf{0}_q, D(\tau^2) \right)\] <p>for some variance component, $\tau^2$. We’ll use $[\cdot] \rvert_{H_0}$ to denote evaluation of the function in brackets when setting $\beta$ equal to $\beta_0$. We’ll also use a superscript $0$ (e.g. $\mu^0$, $\eta^0$, etc.) to denote the quantity under the null hypothesis (i.e. $\tau^2 = \mathbf{0} \implies \beta = \mathbf{0}$).</p> <hr/> <h2 id="gaussian-case">Gaussian Case</h2> <p>In this example, we’ll have the simple setting of a Gaussian response, which means $g(\cdot)$ is the identity function. We will have a fixed (but cluster-specific) intercept and a single random slope. We will have $k$ clusters and $n$ observations per cluster. We assume:</p> \[\mathbf{y}_{i, j} = \alpha_i + \beta_i \mathbf{z}_{i,j} + \epsilon_{i,j}, \hspace{8mm} \epsilon_{i,j} \overset{iid}{\sim} \mathcal{N}(0, \sigma^2), \hspace{5mm} \beta_i \overset{iid}{\sim} \mathcal{N}(0, \tau^2)\] <p>where we also assume the random effects and errors are independent. \(\mathbf{z}_i \in \mathbb{R}^n\) is the vector of covariate values for the $n$ samples in cluster $i$. We’ll denote the vector of responses for cluster $i$ with $\mathbf{y}_i$ so that \(\mathbf{y}_{i,j}\) denotes the $j$-th component of said vector. Marginally, the response vector $\mathbf{y}_i$ has mean \(\alpha_i \mathbb{1}_n\) and variance-covariance matrix:</p> <ul id="covar" class="tab" data-tab="e913091f-2fbb-4bd4-8ef0-768456016ab5" data-name="covar"> <li class="active" id="covar-equation"> <a href="#">equation </a> </li> <li id="covar-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="e913091f-2fbb-4bd4-8ef0-768456016ab5" data-name="covar"> <li class="active"> \[\Sigma_{y_i} = \sigma^2 \mathbb{I}_{n \times n} + \tau^2 \mathbf{z}_i \mathbf{z}_i^\top\] </li> <li> <p>For a single cluster:</p> \[\begin{aligned} \mathbb{E}\left[ (\mathbf{y}_{i,j} - \alpha_i)^2\right] &amp;= \mathbb{E}\left[ (\beta_i \mathbf{z}_{i,j} + \epsilon_{i,j})^2 \right] \\ &amp;= \mathbb{E}\left[\beta_i^2 \mathbf{z}_{i,j}^2 \right] + 2 \mathbb{E}\left[ \beta_i \mathbf{z}_{i,j} \epsilon_{i,j} \right] + \mathbb{E}\left[ \epsilon_{i,j}^2 \right] \\ &amp;= \tau^2 \mathbf{z}_{i,j}^2 + \sigma^2 \\ \mathbb{E}\left[ (\mathbf{y}_{i,j} - \alpha_i)(\mathbf{y}_{i,j'} - \alpha_i) \right] &amp;= \mathbb{E}\left[ (\beta_i \mathbf{z}_{i,j} + \epsilon_{i,j})(\beta_i \mathbf{z}_{i,j'} + \epsilon_{i,j'})\right] \\ &amp;= \mathbb{E}\left[ \beta_i^2 \mathbf{z}_{i,j} \mathbf{z}_{i,j'} \right] + \mathbb{E}\left[ \beta_i \mathbf{z}_{i,j} \epsilon_{i,j'}\right] + \mathbb{E}\left[ \beta_i \mathbf{z}_{i,j'} \epsilon_{i,j}\right] + \mathbb{E}\left[ \epsilon_{i,j} \epsilon_{i,j'}\right] \\ &amp;= \tau^2 \mathbf{z}_{i,j} \mathbf{z}_{i,j'} \nonumber \end{aligned}\] <p>Thus, the variance-covariance matrix for $\mathbf{y}_i$:</p> \[\Sigma_{y_i} = \begin{bmatrix} \sigma^2 + \tau^2 \mathbf{z}_{i,1}^2 &amp; \dots &amp; \tau^2 \mathbf{z}_{i,1} \mathbf{z}_{i,n} \\ \vdots &amp; \ddots &amp; \vdots \\ \tau^2 \mathbf{z}_{i,n} \mathbf{z}_{i, 1} &amp; \dots &amp; \sigma^2 + \tau^2 \mathbf{z}_{i, n}^2 \end{bmatrix} = \sigma^2 \mathbb{I}_{n \times n} + \tau^2 \mathbf{z}_i \mathbf{z}_i^\top \nonumber\] </li> </ul> <p>Since the $\beta_i$ are independent, observations from different clusters have covariance zero. Let $\mathbf{y} = (\mathbf{y}_1, \dots, \mathbf{y}_k)$ denote the full data, $\alpha = \begin{bmatrix} \alpha_1 &amp; \dots &amp; \alpha_k\end{bmatrix}^\top$, $\beta = \begin{bmatrix} \beta_1 &amp; \dots &amp; \beta_k\end{bmatrix}^\top$, and $\theta = (\alpha, \beta)$. The complete, marginal likelihood and log-likelihood are:</p> \[\begin{aligned} \mathcal{L}(\theta; \mathbf{y}) &amp;= \prod_{i = 1}^k (2 \pi)^{-\frac{n}{2}} \rvert \Sigma_{y_i} \rvert^{-\frac{1}{2}} \exp\left(- \frac{1}{2} (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \Sigma_{y_i}^{-1} (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right) \\ \ell(\theta; \mathbf{y}) &amp;= \sum_{i = 1}^k \left[ -\frac{n}{2} \log(2 \pi) - \frac{1}{2}\log(\rvert \Sigma_{y_i} \rvert) - \frac{1}{2} (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \Sigma_{y_i}^{-1} (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \end{aligned}\] <h3 id="score">Score</h3> <p>We first find the gradient of the log-likelihood with respect to $\theta$ parameter-wise. Using the Sherman-Morrison formula, we can find $\Sigma_{y_i}^{-1}$ to be:</p> <ul id="sigma-inv" class="tab" data-tab="ee5f036c-6463-4131-aef7-18412d071da5" data-name="sigma-inv"> <li class="active" id="sigma-inv-equation"> <a href="#">equation </a> </li> <li id="sigma-inv-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="ee5f036c-6463-4131-aef7-18412d071da5" data-name="sigma-inv"> <li class="active"> \[\Sigma_{y_i}^{-1} = \frac{1}{\sigma^2} \mathbb{I}_{n \times n} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top\] </li> <li> \[\begin{aligned} \Sigma_{y_i}^{-1} &amp;= \left[ \sigma^2 \mathbb{I}_{n \times n} + \mathbf{z}_i [\tau^2 \mathbb{I}_{n \times n}] \mathbf{z}_i^\top \right]^{-1} \\ &amp;= \frac{1}{\sigma^2} \mathbb{I}_{n \times n} - \left(1 + \tau^2 \mathbf{z}_i^\top \left[ \frac{1}{\sigma^2} \mathbb{I}_{n \times n} \right]\mathbf{z}_i \right)^{-1} \left( \left(\frac{1}{\sigma^2} \mathbb{I}_{n \times n}\right) \left(\tau^2 \mathbf{z}_{i} \mathbf{z}_i^\top \right) \left(\frac{1}{\sigma^2} \mathbb{I}_{n \times n}\right) \right) \\ &amp;= \frac{1}{\sigma^2} \mathbb{I}_{n \times n} - \left(1 + \frac{\tau^2}{\sigma^2} \mathbf{z}_i^\top \mathbf{z}_i \right)^{-1}\left(\frac{\tau^2}{(\sigma^2)^2} \mathbf{z}_i \mathbf{z}_i^\top \right) \\ &amp;= \frac{1}{\sigma^2} \mathbb{I}_{n \times n} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \end{aligned} \nonumber\] </li> </ul> <h4 id="derivatives">Derivatives</h4> <p>Let’s find the derivative with respect to $\sigma^2$:</p> <ul id="ell-deriv-1" class="tab" data-tab="4a531319-d429-4d03-a49b-959774fd7669" data-name="ell-deriv-1"> <li class="active" id="ell-deriv-1-equation"> <a href="#">equation </a> </li> <li id="ell-deriv-1-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="4a531319-d429-4d03-a49b-959774fd7669" data-name="ell-deriv-1"> <li class="active"> \[\frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} = -\frac{1}{2}\sum_{i = 1}^k \left[ \frac{n}{\sigma^2} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \text{tr}\left[\mathbf{z}_i \mathbf{z}_i^\top\right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2)^2} \mathbb{I}_{n \times n} + \frac{2\tau^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top - \frac{(\tau^2)^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top\right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right]\] </li> <li> \[\begin{aligned} \frac{\partial}{\partial \sigma^2} \left[ \log(\rvert \Sigma_{y_i} \rvert) \right] &amp;= \text{tr}\left[ \Sigma_{y_i}^{-1} \frac{\partial}{\partial \sigma^2} \left[\Sigma_{y_i}\right] \right] \\ &amp;= \text{tr}\left[ \Sigma_{y_i}^{-1} \mathbb{I}_{n \times n} \right] \\ &amp;= \text{tr}\left[ \Sigma^{-1} \right] \\ &amp;= \text{tr}\left[\frac{1}{\sigma^2} \mathbb{I}_{n \times n} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \right] \\ &amp;=\text{tr}\left[ \frac{1}{\sigma^2}\mathbb{I}_{n \times n} \right] \text{tr}\left[- \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top\right] \\ &amp;=\frac{n}{\sigma^2} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \text{tr}\left[\mathbf{z}_i \mathbf{z}_i^\top\right] \end{aligned} \nonumber\] \[\begin{aligned} \frac{\partial}{\partial \sigma^2} \left[ \Sigma_{y_i}^{-1} \right] &amp;= - \Sigma_{y_i}^{-1} \frac{\partial}{\partial \sigma^2} \left[ \Sigma_{y_i}\right] \Sigma_{y_i}^{-1} \\ &amp;= -\left[ \frac{1}{\sigma^2} \mathbb{I}_{n \times n} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \right] \mathbb{I}_{n \times n} \left[\frac{1}{\sigma^2} \mathbb{I}_{n \times n} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \right] \\ &amp;= - \left[ \frac{1}{(\sigma^2)^2} \mathbb{I}_{n \times n} - \frac{2\tau^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top + \frac{(\tau^2)^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \\ &amp;= - \frac{1}{(\sigma^2)^2} \mathbb{I}_{n \times n} + \frac{2\tau^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top - \frac{(\tau^2)^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \end{aligned}\] <p>The above imply:</p> \[\begin{aligned} \frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} &amp;= \frac{\partial}{\partial \sigma^2} \left[ \sum_{i = 1}^k - \frac{1}{2} \log(\rvert \Sigma_{y_i} \rvert) - \frac{1}{2}(\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \Sigma_{y_i}^{-1} (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \\ &amp;= -\frac{1}{2}\sum_{i = 1}^k \left[ \frac{n}{\sigma^2} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \text{tr}\left[\mathbf{z}_i \mathbf{z}_i^\top\right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2)^2} \mathbb{I}_{n \times n} + \frac{2\tau^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top - \frac{(\tau^2)^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top\right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \end{aligned}\] </li> </ul> <p>We do the same with $\tau^2$:</p> <ul id="ell-tau-1" class="tab" data-tab="da25deb9-eba4-4f7b-bedf-6d9f0dc9cef4" data-name="ell-tau-1"> <li class="active" id="ell-tau-1-equation"> <a href="#">equation </a> </li> <li id="ell-tau-1-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="da25deb9-eba4-4f7b-bedf-6d9f0dc9cef4" data-name="ell-tau-1"> <li class="active"> \[\frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} = - \frac{1}{2} \sum_{i = 1}^k \left[ \frac{1}{\sigma^2} \text{tr}[\mathbf{z}_i \mathbf{z}_i^\top] - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top\right] + (\mathbf{y}_i - \alpha_i\mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2)^2} \mathbf{z}_i \mathbf{z}_i^\top + \frac{2\tau^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{(\tau^2)^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top\mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}^\top \mathbf{z}_i \mathbf{z}_i^\top \right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right]\] </li> <li> \[\begin{aligned} \frac{\partial}{\partial \tau^2} \left[ \log(\rvert \Sigma_{y_i} \rvert) \right] &amp;= \text{tr}\left[ \Sigma_{y_i}^{-1} \frac{\partial}{\partial \tau^2} \left[\Sigma_{y_i}\right] \right] \\ &amp;= \text{tr}\left[ \Sigma_{y_i}^{-1} \mathbf{z}_i \mathbf{z}_i^\top \right] \\ &amp;= \text{tr} \left[ \left( \frac{1}{\sigma^2} \mathbb{I}_{n \times n} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top\right) \mathbf{z}_i \mathbf{z}_i^\top\right] \\ &amp;= \frac{1}{\sigma^2} \text{tr}[\mathbf{z}_i \mathbf{z}_i^\top] - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top\right] \end{aligned} \nonumber\] \[\begin{aligned} \frac{\partial}{\partial \tau^2} \left[ \Sigma_{y_i}^{-1} \right] &amp;= - \Sigma_{y_i}^{-1} \frac{\partial}{\partial \tau^2} \left[ \Sigma_{y_i}\right] \Sigma_{y_i}^{-1} \\ &amp;= -\left[ \frac{1}{\sigma^2} \mathbb{I}_{n \times n} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \right] \left[ \mathbf{z}_i \mathbf{z}_i^\top \right] \left[\frac{1}{\sigma^2} \mathbb{I}_{n \times n} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \right] \\ &amp;= - \frac{1}{(\sigma^2)^2} \mathbf{z}_i \mathbf{z}_i^\top + \frac{2\tau^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{(\tau^2)^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top\mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}^\top \mathbf{z}_i \mathbf{z}_i^\top \end{aligned} \nonumber\] \[\begin{aligned} \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} &amp;= \frac{\partial}{\partial \tau^2} \left[ \sum_{i = 1}^k - \frac{1}{2} \log(\rvert \Sigma_{y_i} \rvert) - \frac{1}{2}(\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \Sigma_{y_i}^{-1} (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \\ &amp;= - \frac{1}{2} \sum_{i = 1}^k \left[ \frac{1}{\sigma^2} \text{tr}[\mathbf{z}_i \mathbf{z}_i^\top] - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top\right] + (\mathbf{y}_i - \alpha_i\mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2)^2} \mathbf{z}_i \mathbf{z}_i^\top + \frac{2\tau^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{(\tau^2)^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top\mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}^\top \mathbf{z}_i \mathbf{z}_i^\top \right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \end{aligned}\] </li> </ul> <p>And then take the gradient with respect to $\alpha$:</p> <ul id="ell-alpha-1" class="tab" data-tab="9278cfc3-7af3-44c9-ba0a-d67cb9b93454" data-name="ell-alpha-1"> <li class="active" id="ell-alpha-1-equation"> <a href="#">equation </a> </li> <li id="ell-alpha-1-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="9278cfc3-7af3-44c9-ba0a-d67cb9b93454" data-name="ell-alpha-1"> <li class="active"> \[\frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha} = \begin{bmatrix} (\mathbf{y}_1 - \alpha_1 \mathbf{1}_n)^\top \Sigma_{y_i}^{-1} \mathbf{1}_n \\ \vdots \\ (\mathbf{y}_k - \alpha_k \mathbf{1}_n)^\top \Sigma_{y_i}^{-1} \mathbf{1}_n \end{bmatrix}\] </li> <li> <p>We do the computations component-wise: \(\begin{aligned} \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} &amp;= \sum_{i = 1}^k - \frac{1}{2} \frac{\partial}{\partial \alpha_j} \left[ (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \Sigma^{-1}_{y_i} (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \\ &amp;= - \frac{1}{2} \left(2 (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \Sigma_{y_i}^{-1}(- \mathbf{1}_n) \right) \\ &amp;= (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \Sigma_{y_i}^{-1} \mathbf{1}_n \end{aligned}\) So then: \(\begin{aligned} \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha} &amp;= \begin{bmatrix} (\mathbf{y}_1 - \alpha_1 \mathbf{1}_n)^\top \Sigma_{y_i}^{-1} \mathbf{1}_n \\ \vdots \\ (\mathbf{y}_k - \alpha_k \mathbf{1}_n)^\top \Sigma_{y_i}^{-1} \mathbf{1}_n \end{bmatrix} \end{aligned}\)</p> </li> </ul> <h4 id="mles">MLEs</h4> <p>We can then find the MLE vector, $\hat{\theta}$, by setting the above equations equal to zero, substituting $\tau^2 = 0$, and solving. The MLE vector, $\hat{\theta}$ is:</p> <ul id="mle-1" class="tab" data-tab="013a5aac-f6cd-4e41-a381-4ae42661b0a2" data-name="mle-1"> <li class="active" id="mle-1-equation"> <a href="#">equation </a> </li> <li id="mle-1-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="013a5aac-f6cd-4e41-a381-4ae42661b0a2" data-name="mle-1"> <li class="active"> \[\hat{\theta} = \begin{bmatrix} \frac{1}{n} \sum_{j = 1}^n \mathbf{y}_{1,j} \\ \vdots \\ \frac{1}{n} \sum_{j = 1}^n \mathbf{y}_{k,j} \\ \frac{1}{nk} \sum_{i = 1}^k (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ 0 \end{bmatrix}\] </li> <li> <p>We set the derivative with respect to $\sigma^2$ equal to zero, substitute $\tau^2 = 0$ (under $H_0$), and solve for $\sigma^2$:</p> \[\begin{aligned} 0 &amp;= \frac{\partial}{\partial \sigma^2} \left[ \ell(\theta; \mathbf{y}) \right] \bigg\rvert_{\theta = \theta_0}\\ 0 &amp;= -\frac{1}{2}\sum_{i = 1}^k \left[ \frac{n}{\sigma^2} - \frac{0}{\sigma^2(\sigma^2 + 0 \cdot\mathbf{z}_i^\top \mathbf{z}_i)} \text{tr}\left[\mathbf{z}_i \mathbf{z}_i^\top\right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2)^2} \mathbb{I}_{n \times n} + \frac{2\cdot 0}{(\sigma^2)^2(\sigma^2 + 0 \cdot \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top + \frac{(0)^2}{(\sigma^2)^2(\sigma^2 - 0 \cdot \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top\right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \\ 0 &amp;= -\frac{1}{2}\sum_{i = 1}^k \left[ \frac{n}{\sigma^2} + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2)^2} \mathbb{I}_{n \times n} \right](\mathbf{y}_i - \alpha_i \mathbf{1}_n)\right] \\ 0 &amp;= - \frac{nk}{2 \sigma^2} - \frac{1}{2} \sum_{i = 1}^k - \frac{1}{(\sigma^2)^2} (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ 0 &amp;= - \frac{nk}{2\sigma^2} + \frac{1}{2 (\sigma^2)^2} \sum_{i = 1}^k (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ \frac{nk}{2 \sigma^2} &amp;= \frac{1}{2(\sigma^2)^2} \sum_{i = 1}^k (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ \sigma^2 n k &amp;= \sum_{i = 1}^k (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ \sigma^2 &amp;= \frac{1}{nk} \sum_{i = 1}^k (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \end{aligned}\] <p>We set the gradient w.r.t $\alpha$ equal to zero, substitute $\tau^2 = 0$ (under $H_0$), and solve for $\alpha$.</p> \[\begin{aligned} \mathbf{0} &amp;= \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha} \bigg\rvert_{\theta = \theta_0} \\ \mathbf{0} &amp;= \begin{bmatrix} (\mathbf{y}_1 - \alpha_1 \mathbf{1}_n)^\top \left[\frac{1}{\sigma^2}\mathbb{I}_{n \times n} \right] \mathbf{1}_n \\ \vdots \\ (\mathbf{y}_k - \alpha_k \mathbf{1}_n)^\top \left[\frac{1}{\sigma^2}\mathbb{I}_{n \times n} \right] \mathbf{1}_n \end{bmatrix} \end{aligned}\] <p>Since each entry of the gradient only has one component of $\alpha$, we can solve then all separately:</p> \[\begin{aligned} 0 &amp;= (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ \frac{1}{\sigma^2} \mathbb{I}_{n \times n} \right] \mathbf{1}_n \\ 0 &amp;= \frac{1}{\sigma^2} (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \mathbf{1}_n \\ 0 &amp;= \frac{1}{\sigma^2} \sum_{j = 1}^n (\mathbf{y}_{i,j} - \alpha_i) \\ 0 &amp;= \frac{1}{\sigma^2} \left( \sum_{j =1 }^n \mathbf{y}_{i,j} - n \alpha_i \right) \\ n \alpha_i &amp;= \sum_{j =1 }^n \mathbf{y}_{i,j} \\ \alpha_i &amp;= \frac{1}{n} \sum_{j = 1}^n \mathbf{y}_{i,j} \end{aligned}\] </li> </ul> <p>It is easiest to write the score after evaluating it at the MLE of the parameter vector under $H_0$, which we denote with $\hat{\theta}$ (uncollapse the proof to see all of the details). The MLE is given by:</p> \[\hat{\theta} = \begin{bmatrix} \frac{1}{n} \sum_{j = 1}^n \mathbf{y}_{1,j} \\ \vdots \\ \frac{1}{n} \sum_{j = 1}^n \mathbf{y}_{k,j} \\ \frac{1}{nk} \sum_{i = 1}^k (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ 0 \end{bmatrix}\] <p>Thus, the score evaluated at $\theta = \hat{\theta}$ is:</p> \[\begin{aligned} U_\theta (\hat{\theta}) &amp;= \begin{bmatrix} \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha} \bigg\rvert_{\theta = \hat{\theta}} \\ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} \bigg\rvert_{\theta = \hat{\theta}} \\ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \bigg\rvert_{\theta = \hat{\theta}} \end{bmatrix} = \begin{bmatrix} \frac{1}{\hat{\sigma}^2} (\mathbf{y}_1 - \hat{\alpha}_1 \mathbf{1}_n)^\top \mathbf{1}_n \\ \vdots \\ \frac{1}{\hat{\sigma}^2} (\mathbf{y}_k - \hat{\alpha}_k \mathbf{1}_n)^\top \mathbf{1}_n \\ - \frac{1}{2} \sum_{i = 1}^k \left[ \frac{n}{\hat{\sigma}^2} - \frac{1}{(\hat{\sigma}^2)^2} (\mathbf{y}_i - \hat{\alpha}_i \mathbf{1}_n)^\top (\mathbf{y}_i - \hat{\alpha}_i \mathbf{1}_n) \right] \\ -\frac{1}{2} \sum_{i = 1}^k \left[ \frac{\text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right]}{\hat{\sigma}^2} + \frac{1}{(\hat{\sigma}^2)^2}(\mathbf{y}_i - \hat{\alpha}_i \mathbf{1}_n)^\top \mathbf{z}_i \mathbf{z}_i^\top (\mathbf{y}_i - \hat{\alpha}_i \mathbf{1}_n) \right] \end{bmatrix} \end{aligned}\] <h3 id="information">Information</h3> <p>To find the information, we need to compute the second-order derivatives of the log-likelihood, take the expectation under $H_0$ of minus those quantities, and evaluate them by plugging in $\hat{\theta}$.</p> <h4 id="derivatives-1">Derivatives</h4> <p>We start by taking the derivative with respect to $\theta$ (component-wise) of the first derivative with respect to $\sigma^2$:</p> <ul id="deriv-theta-sigma" class="tab" data-tab="0ec76723-aae9-41c0-b218-a26ec84696a3" data-name="deriv-theta-sigma"> <li class="active" id="deriv-theta-sigma-equation"> <a href="#">equation </a> </li> <li id="deriv-theta-sigma-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="0ec76723-aae9-41c0-b218-a26ec84696a3" data-name="deriv-theta-sigma"> <li class="active"> \[\begin{aligned} \frac{\partial}{\partial \sigma^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} \right] &amp;= - \frac{1}{2}\sum_{i = 1}^k -\frac{n}{(\sigma^2)^2} + \frac{\tau^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top\right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ \frac{2}{(\sigma^2)^3} \mathbb{I}_{n \times n} - \frac{2\tau^2(3\sigma^2 + 2\tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2}\mathbf{z}_i \mathbf{z}_i^\top + \frac{2(\tau^2)^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3 (\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z})^3}\mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right](\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ \frac{\partial}{\partial \sigma^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] &amp;= -\frac{1}{2}\sum_{i = 1}^k -\frac{1}{(\sigma^2)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] - \frac{-\tau^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ \frac{2}{(\sigma^2)^3} \mathbf{z}_i \mathbf{z}_i^\top + \frac{-2\tau^2(2\sigma^2 + 3\tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{-2(\tau^2)^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3 (\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^3} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ \frac{\partial}{\partial \sigma^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] &amp;= (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left[ -\frac{1}{(\sigma^2)^2} \mathbb{I}_{n \times n} + \frac{\tau^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^2 (\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \right] \mathbf{1}_n \end{aligned}\] </li> <li> \[\begin{aligned} \frac{\partial}{\partial \sigma^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} \right] &amp;= -\frac{1}{2}\sum_{i = 1}^k \frac{\partial}{\partial \sigma^2}\left[ \frac{n}{\sigma^2} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \text{tr}\left[\mathbf{z}_i \mathbf{z}_i^\top\right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2)^2} \mathbb{I}_{n \times n} + \frac{2\tau^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top - \frac{(\tau^2)^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top\right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \\ &amp;= - \frac{1}{2}\sum_{i = 1}^k -\frac{n}{(\sigma^2)^2} - \frac{-\tau^2(2\sigma^2+\tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top\right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ \frac{2}{(\sigma^2)^3} \mathbb{I}_{n \times n} + \frac{-2\tau^2(3\sigma^2 + 2\tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2}\mathbf{z}_i \mathbf{z}_i^\top - \frac{-2(\tau^2)^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3 (\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z})^3}\mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right](\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ &amp;= - \frac{1}{2}\sum_{i = 1}^k -\frac{n}{(\sigma^2)^2} + \frac{\tau^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top\right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ \frac{2}{(\sigma^2)^3} \mathbb{I}_{n \times n} - \frac{2\tau^2(3\sigma^2 + 2\tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2}\mathbf{z}_i \mathbf{z}_i^\top + \frac{2(\tau^2)^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3 (\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z})^3}\mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right](\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ \frac{\partial}{\partial \sigma^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] &amp;= - \frac{1}{2} \sum_{i = 1}^k \frac{\partial}{\partial \sigma^2} \left[ \frac{1}{\sigma^2} \text{tr}[\mathbf{z}_i \mathbf{z}_i^\top] - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top\right] + (\mathbf{y}_i - \alpha_i\mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2)^2} \mathbf{z}_i \mathbf{z}_i^\top + \frac{2\tau^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{(\tau^2)^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top\mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}^\top \mathbf{z}_i \mathbf{z}_i^\top \right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \\ &amp;= -\frac{1}{2}\sum_{i = 1}^k -\frac{1}{(\sigma^2)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] - \frac{-\tau^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ \frac{2}{(\sigma^2)^3} \mathbf{z}_i \mathbf{z}_i^\top + \frac{-2\tau^2(2\sigma^2 + 3\tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{-2(\tau^2)^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3 (\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^3} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ \frac{\partial}{\partial \sigma^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] &amp;= \frac{\partial}{\partial \sigma^2} \left[ (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left[ \frac{1}{\sigma^2} \mathbb{I}_{n \times n} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \right] \mathbf{1}_n \right] \\ &amp;= (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left[ -\frac{1}{(\sigma^2)^2} \mathbb{I}_{n \times n} + \frac{\tau^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^2 (\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \right] \mathbf{1}_n \end{aligned}\] </li> </ul> <p>Next, we do the same with the derivative with respect to $\tau^2$:</p> <ul id="deriv-theta-tau" class="tab" data-tab="e8290f55-6c13-40ee-9605-9943741fa5cc" data-name="deriv-theta-tau"> <li class="active" id="deriv-theta-tau-equation"> <a href="#">equation </a> </li> <li id="deriv-theta-tau-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="e8290f55-6c13-40ee-9605-9943741fa5cc" data-name="deriv-theta-tau"> <li class="active"> \[\begin{aligned} \frac{\partial}{\partial \tau^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} \right] &amp;= -\frac{1}{2}\sum_{i = 1}^k - \frac{1}{(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[\frac{2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top - \frac{2\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^3} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ \frac{\partial}{\partial \tau^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] &amp;= -\frac{1}{2}\sum_{i = 1}^k - \frac{1}{(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ \frac{2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2}\mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{2\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^3} \mathbf{z}_i \mathbf{z}_i^\top\mathbf{z}_i \mathbf{z}_i^\top\mathbf{z}_i \mathbf{z}_i^\top\right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ \frac{\partial}{\partial \tau^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] &amp;= (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \right] \mathbf{1}_n \\ \end{aligned}\] </li> <li> \[\begin{aligned} \frac{\partial}{\partial \tau^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} \right] &amp;= -\frac{1}{2}\sum_{i = 1}^k \frac{\partial}{\partial \tau^2}\left[ \frac{n}{\sigma^2} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \text{tr}\left[\mathbf{z}_i \mathbf{z}_i^\top\right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2)^2} \mathbb{I}_{n \times n} + \frac{2\tau^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top - \frac{(\tau^2)^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top\right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \\ &amp;= -\frac{1}{2}\sum_{i = 1}^k 0 - \frac{1}{(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[0 + \frac{2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top - \frac{2\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^3} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ &amp;= -\frac{1}{2}\sum_{i = 1}^k - \frac{1}{(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[\frac{2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top - \frac{2\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^3} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ \frac{\partial}{\partial \tau^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] &amp;= - \frac{1}{2} \sum_{i = 1}^k \frac{\partial}{\partial \tau^2} \left[ \frac{1}{\sigma^2} \text{tr}[\mathbf{z}_i \mathbf{z}_i^\top] - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top\right] + (\mathbf{y}_i - \alpha_i\mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2)^2} \mathbf{z}_i \mathbf{z}_i^\top + \frac{2\tau^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{(\tau^2)^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top\mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}^\top \mathbf{z}_i \mathbf{z}_i^\top \right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \\ &amp;= -\frac{1}{2}\sum_{i = 1}^k 0 - \frac{1}{(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ 0 + \frac{2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2}\mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{2\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^3} \mathbf{z}_i \mathbf{z}_i^\top\mathbf{z}_i \mathbf{z}_i^\top\mathbf{z}_i \mathbf{z}_i^\top\right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ &amp;= -\frac{1}{2}\sum_{i = 1}^k - \frac{1}{(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ \frac{2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2}\mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{2\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^3} \mathbf{z}_i \mathbf{z}_i^\top\mathbf{z}_i \mathbf{z}_i^\top\mathbf{z}_i \mathbf{z}_i^\top\right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ \frac{\partial}{\partial \tau^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] &amp;= \frac{\partial}{\partial \tau^2} \left[ (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left[ \frac{1}{\sigma^2} \mathbb{I}_{n \times n} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \right] \mathbf{1}_n \right] \\ &amp;= (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left[ 0 - \frac{1}{(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \right] \mathbf{1}_n \\ &amp;= (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \right] \mathbf{1}_n \end{aligned}\] </li> </ul> <p>And finally with $\alpha_j$:</p> <ul id="deriv-theta-alpha" class="tab" data-tab="5771ea29-2e68-44f9-87ab-632b858b021d" data-name="deriv-theta-alpha"> <li class="active" id="deriv-theta-alpha-equation"> <a href="#">equation </a> </li> </ul> <ul class="tab-content" id="5771ea29-2e68-44f9-87ab-632b858b021d" data-name="deriv-theta-alpha"> <li class="active"> \[\begin{aligned} \frac{\partial}{\partial \alpha_j} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} \right] &amp;= (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2)^2} \mathbb{I}_{n \times n} + \frac{2\tau^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_j^\top \mathbf{z}_j)} \mathbf{z}_j \mathbf{z}_j^\top - \frac{(\tau^2)^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_j^\top \mathbf{z}_j)^2} \mathbf{z}_j \mathbf{z}_j^\top \mathbf{z}_j \mathbf{z}_j^\top \right]\mathbf{1}_n \\ \frac{\partial}{\partial \alpha} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] &amp;= (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2)^2} \mathbf{z}_i \mathbf{z}_i^\top + \frac{2\tau^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{(\tau^2)^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top\mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \mathbf{1}_n \\ \frac{\partial}{\partial \alpha_j} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] &amp;= - \mathbf{1}_n^\top \left[ \frac{1}{\sigma^2} \mathbb{I}_{n \times n} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \right] \mathbf{1}_n \\ \frac{\partial}{\partial \alpha_{j'}} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] &amp;= 0 \end{aligned}\] </li> </ul> <h4 id="expectations">Expectations</h4> <p>We take the expectation under the null of all of the terms we found in the previous section. First we do the $\sigma^2$ terms:</p> <ul id="expectation-sigma-1" class="tab" data-tab="168518a5-28f8-46c0-9dc0-ba3645ccaed1" data-name="expectation-sigma-1"> <li class="active" id="expectation-sigma-1-equation"> <a href="#">equation </a> </li> <li id="expectation-sigma-1-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="168518a5-28f8-46c0-9dc0-ba3645ccaed1" data-name="expectation-sigma-1"> <li class="active"> \[\begin{aligned} \mathbb{E}\left[ \frac{\partial}{\partial \sigma^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} \right] \right] &amp;= - \frac{1}{2}\sum_{i = 1}^k -\frac{n}{(\sigma^2)^2} + \frac{\tau^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top\right] + \text{tr}\left[ \left[ \frac{2}{(\sigma^2)^3} \mathbb{I}_{n \times n} - \frac{2\tau^2(3\sigma^2 + 2\tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2}\mathbf{z}_i \mathbf{z}_i^\top + \frac{2(\tau^2)^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3 (\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z})^3}\mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \Sigma_{y_i} \right] \\ \mathbb{E}\left[ \frac{\partial}{\partial \sigma^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] \right] &amp;= -\frac{1}{2}\sum_{i = 1}^k -\frac{1}{(\sigma^2)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] - \frac{-\tau^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] + \text{tr}\left[ \left[ \frac{2}{(\sigma^2)^3} \mathbf{z}_i \mathbf{z}_i^\top + \frac{-2\tau^2(2\sigma^2 + 3\tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{-2(\tau^2)^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3 (\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^3} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \Sigma_{y_i} \right] \\ \mathbb{E}\left[ \frac{\partial}{\partial \sigma^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] \right] &amp;= 0 \end{aligned}\] </li> <li> \[\begin{aligned} \mathbb{E}\left[ \frac{\partial}{\partial \sigma^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} \right] \right] &amp;= \mathbb{E}\left[ - \frac{1}{2}\sum_{i = 1}^k -\frac{n}{(\sigma^2)^2} + \frac{\tau^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top\right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ \frac{2}{(\sigma^2)^3} \mathbb{I}_{n \times n} - \frac{2\tau^2(3\sigma^2 + 2\tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2}\mathbf{z}_i \mathbf{z}_i^\top + \frac{2(\tau^2)^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3 (\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z})^3}\mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right](\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \\ &amp;= - \frac{1}{2}\sum_{i = 1}^k -\frac{n}{(\sigma^2)^2} + \frac{\tau^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top\right] + \mathbb{E}\left[ (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ \frac{2}{(\sigma^2)^3} \mathbb{I}_{n \times n} - \frac{2\tau^2(3\sigma^2 + 2\tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2}\mathbf{z}_i \mathbf{z}_i^\top + \frac{2(\tau^2)^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3 (\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z})^3}\mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right](\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \\ &amp;= - \frac{1}{2}\sum_{i = 1}^k -\frac{n}{(\sigma^2)^2} + \frac{\tau^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top\right] + \text{tr}\left[ \left[ \frac{2}{(\sigma^2)^3} \mathbb{I}_{n \times n} - \frac{2\tau^2(3\sigma^2 + 2\tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2}\mathbf{z}_i \mathbf{z}_i^\top + \frac{2(\tau^2)^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3 (\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z})^3}\mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \mathbb{E}\left[ (\mathbf{y}_i - \alpha_i \mathbf{1}_n) (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \right] \right] \\ &amp;= - \frac{1}{2}\sum_{i = 1}^k -\frac{n}{(\sigma^2)^2} + \frac{\tau^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top\right] + \text{tr}\left[ \left[ \frac{2}{(\sigma^2)^3} \mathbb{I}_{n \times n} - \frac{2\tau^2(3\sigma^2 + 2\tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2}\mathbf{z}_i \mathbf{z}_i^\top + \frac{2(\tau^2)^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3 (\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z})^3}\mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \Sigma_{y_i} \right] \\ \mathbb{E}\left[ \frac{\partial}{\partial \sigma^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] \right] &amp;= -\frac{1}{2}\sum_{i = 1}^k -\frac{1}{(\sigma^2)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] - \frac{-\tau^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] + \text{tr}\left[ \left[ \frac{2}{(\sigma^2)^3} \mathbf{z}_i \mathbf{z}_i^\top + \frac{-2\tau^2(2\sigma^2 + 3\tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{-2(\tau^2)^2(2\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)}{(\sigma^2)^3 (\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^3} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \Sigma_{y_i} \right] \\ \mathbb{E}\left[ \frac{\partial}{\partial \sigma^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] \right] &amp;= 0 \end{aligned}\] </li> </ul> <p>And the $\tau^2$ terms:</p> \[\begin{aligned} \mathbb{E}\left[ \frac{\partial}{\partial \tau^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} \right] \right] &amp;= -\frac{1}{2}\sum_{i = 1}^k - \frac{1}{(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] + \text{tr} \left[ \left[ \frac{2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top - \frac{2\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^3} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \Sigma_{y_i} \right] \\ \mathbb{E}\left[ \frac{\partial}{\partial \tau^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] \right] &amp;= -\frac{1}{2}\sum_{i = 1}^k - \frac{1}{(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] +\text{tr} \left[\left[ \frac{2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^2}\mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{2\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)^3} \mathbf{z}_i \mathbf{z}_i^\top\mathbf{z}_i \mathbf{z}_i^\top\mathbf{z}_i \mathbf{z}_i^\top\right] \Sigma_{y_i} \right] \\ \mathbb{E}\left[ \frac{\partial}{\partial \tau^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] \right] &amp;= 0 \end{aligned}\] <p>And finally the $\alpha_j$ ones:</p> <ul id="expectation-alpha-1" class="tab" data-tab="9fd11053-cca7-4350-acbd-394978708c9b" data-name="expectation-alpha-1"> <li class="active" id="expectation-alpha-1-equation"> <a href="#">equation </a> </li> <li id="expectation-alpha-1-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="9fd11053-cca7-4350-acbd-394978708c9b" data-name="expectation-alpha-1"> <li class="active"> \[\begin{aligned} \mathbb{E}\left[ \frac{\partial}{\partial \alpha_j} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} \right] \right] &amp;= 0 \\ \mathbb{E}\left[ \frac{\partial}{\partial \alpha} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] \right] &amp;= 0 \\ \mathbb{E}\left[ \frac{\partial}{\partial \alpha_j} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] \right] &amp;= - \mathbf{1}_n^\top \left[ \frac{1}{\sigma^2} \mathbb{I}_{n \times n} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \right] \mathbf{1}_n \\ \mathbb{E}\left[\frac{\partial}{\partial \alpha_{j'}} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] \right] &amp;= 0 \end{aligned}\] </li> <li> \[\begin{aligned} \mathbb{E}\left[ \frac{\partial}{\partial \alpha_j} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} \right] \right] &amp;= \mathbb{E}\left[(\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2)^2} \mathbb{I}_{n \times n} + \frac{2\tau^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_j^\top \mathbf{z}_j)} \mathbf{z}_j \mathbf{z}_j^\top - \frac{(\tau^2)^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_j^\top \mathbf{z}_j)^2} \mathbf{z}_j \mathbf{z}_j^\top \mathbf{z}_j \mathbf{z}_j^\top \right]\mathbf{1}_n \right] \\ &amp;= 0 \\ \mathbb{E}\left[ \frac{\partial}{\partial \alpha} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] \right] &amp;= \mathbb{E}\left[(\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left[ - \frac{1}{(\sigma^2)^2} \mathbf{z}_i \mathbf{z}_i^\top + \frac{2\tau^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{(\tau^2)^2}{(\sigma^2)^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top\mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \mathbf{1}_n \right] \\ &amp;= 0 \\ \mathbb{E}\left[ \frac{\partial}{\partial \alpha_j} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] \right] &amp;= - \mathbf{1}_n^\top \left[ \frac{1}{\sigma^2} \mathbb{I}_{n \times n} - \frac{\tau^2}{\sigma^2(\sigma^2 + \tau^2 \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \right] \mathbf{1}_n \\ \mathbb{E}\left[\frac{\partial}{\partial \alpha_{j'}} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] \right] &amp;= 0 \end{aligned}\] </li> </ul> <p>We then evaluate the Fisher information at the MLEs:</p> <ul id="fisher-1" class="tab" data-tab="dd5aa92c-5525-414f-923e-9397e8d622ad" data-name="fisher-1"> <li class="active" id="fisher-1-equation"> <a href="#">equation </a> </li> <li id="fisher-1-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="dd5aa92c-5525-414f-923e-9397e8d622ad" data-name="fisher-1"> <li class="active"> \[\begin{aligned} \mathcal{I}_{\theta, \theta} (\hat{\theta}) &amp;= -\mathbb{E}\left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \theta \partial \theta^\top}\right]\bigg\rvert_{\theta = \hat{\theta}} \\ &amp;= \begin{bmatrix} \frac{n}{\hat{\sigma}^2} &amp; \dots &amp; 0 &amp; 0 &amp; 0\\ \vdots &amp; \ddots &amp; \vdots &amp; \vdots &amp; \vdots \\ 0 &amp; \dots &amp; \frac{n}{\hat{\sigma}^2} &amp; 0 &amp; 0 \\ 0 &amp; \dots &amp; 0 &amp; \frac{nk}{2\hat{\sigma}^2} &amp; \frac{1}{2(\hat{\sigma}^2)^2} \sum_{i = 1}^k \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right]\\ 0 &amp; \dots &amp; 0 &amp; \frac{1}{2(\hat{\sigma}^2)^2} \sum_{i = 1}^k \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] &amp; \frac{1}{2(\hat{\sigma}^2)^2} \sum_{i = 1}^k \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \end{bmatrix} \end{aligned}\] </li> <li> <p>Note that:</p> \[\begin{aligned} \Sigma_{y_i} \bigg\rvert_{\theta = \hat{\theta}} &amp;= \hat{\sigma}^2 \mathbb{I}_{n \times n} \end{aligned}\] <p>Thus:</p> \[\begin{aligned} -\mathbb{E}\left[ \frac{\partial}{\partial \sigma^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} \right] \right] \bigg\rvert_{\theta = \hat{\theta}} &amp;= \frac{1}{2}\sum_{i = 1}^k -\frac{n}{(\hat{\sigma}^2)^2} + \frac{0 \cdot (2\hat{\sigma}^2 + 0)}{(\hat{\sigma}^2)^2(\hat{\sigma}^2 + 0)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top\right] + \text{tr}\left[ \left[ \frac{2}{(\hat{\sigma}^2)^3} \mathbb{I}_{n \times n} - 0 \cdot \mathbf{z}_i \mathbf{z}_i^\top + 0 \cdot \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \Sigma_{y_i}\rvert_{\theta = \hat{\theta}} \right] \\ &amp;= \frac{1}{2}\sum_{i = 1}^k - \frac{n}{(\hat{\sigma}^2)^2} + \text{tr}\left[ \left[ \frac{2}{(\hat{\sigma}^2)^3} \mathbb{I}_{n \times n} \right] \left[ \hat{\sigma}^2 \mathbb{I}_{n \times n} \right] \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[ -\frac{n}{(\hat{\sigma}^2)^2} + \frac{2}{\hat{\sigma}^2} \text{tr}\left[ \mathbb{I}_{n \times n} \right]\right] \\ &amp;= \frac{1}{2}\sum_{i = 1}^k \left[ -\frac{n}{(\hat{\sigma}^2)^2} + \frac{2n}{\hat{\sigma}^2} \right] \\ &amp;= \sum_{i = 1}^k \frac{n}{2 \hat{\sigma}^2} \\ &amp;= \frac{nk}{2\hat{\sigma}^2} \\ -\mathbb{E}\left[ \frac{\partial}{\partial \sigma^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] \right] \bigg\rvert_{\theta = \hat{\theta}} &amp;=\frac{1}{2}\sum_{i = 1}^k -\frac{1}{(\hat{\sigma}^2)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] - \frac{-0 \cdot (2\hat{\sigma}^2 + 0 \cdot \mathbf{z}_i^\top \mathbf{z}_i)}{(\hat{\sigma}^2)^2(\hat{\sigma}^2 + 0 \cdot \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] + \text{tr}\left[ \left[ \frac{2}{(\hat{\sigma}^2)^3} \mathbf{z}_i \mathbf{z}_i^\top + \frac{-0 \cdot (2\hat{\sigma}^2 + 0 \cdot \mathbf{z}_i^\top \mathbf{z}_i)}{(\hat{\sigma}^2)^3(\hat{\sigma}^2 + 0 \cdot \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{-2(0)^2(2\hat{\sigma}^2 + 0 \cdot \mathbf{z}_i^\top \mathbf{z}_i)}{(\hat{\sigma}^2)^3 (\hat{\sigma}^2 + 0 \cdot \mathbf{z}_i^\top \mathbf{z}_i)^3} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \Sigma_{y_i} \rvert_{\theta = \hat{\theta}} \right] \\ &amp;= \frac{1}{2}\sum_{i = 1}^k -\frac{1}{(\hat{\sigma}^2)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] + \text{tr}\left[ \left[ \frac{2}{(\hat{\sigma}^2)^3} \mathbf{z}_i \mathbf{z}_i^\top \right] \left[ \hat{\sigma}^2 \mathbb{I}_{n \times n} \right] \right] \\ &amp;= \frac{1}{2}\sum_{i = 1}^k \left( - \frac{1}{(\hat{\sigma}^2)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top\right] + \frac{2}{(\hat{\sigma}^2)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] \right) \\ &amp;= \sum_{i = 1}^k \frac{1}{2(\hat{\sigma}^2)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] \\ &amp;= \frac{1}{2(\hat{\sigma}^2)^2} \sum_{i = 1}^k \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] \\ -\mathbb{E}\left[ \frac{\partial}{\partial \sigma^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha} \right] \right] \bigg\rvert_{\theta = \hat{\theta}} &amp;= 0 \\ - \mathbb{E}\left[ \frac{\partial}{\partial \tau^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} \right] \right]\bigg\rvert_{\theta = \hat{\theta}} &amp;= \frac{1}{2}\sum_{i = 1}^k - \frac{1}{(\hat{\sigma}^2 + 0 \cdot \mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] + \text{tr} \left[ \left[\frac{2}{\hat{\sigma}^2(\hat{\sigma}^2 + 0 \cdot \mathbf{z}_i^\top \mathbf{z}_i)^2} \mathbf{z}_i \mathbf{z}_i^\top - \frac{2 \cdot 0 }{\hat{\sigma}^2(\hat{\sigma}^2 + 0 \cdot \mathbf{z}_i^\top \mathbf{z}_i)^3} \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \Sigma_{y_i}\rvert_{\theta = \hat{\theta}} \right] \\ &amp;= \frac{1}{2}\sum_{i = 1}^k -\frac{1}{(\hat{\sigma}^2)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top\right] + \text{tr}\left[ \left[\frac{2}{(\hat{\sigma}^2)^3} \mathbf{z}_i \mathbf{z}_i^\top \right] \left[ \hat{\sigma}^2 \mathbb{I}_{n \times n} \right]\right] \\ &amp;= \frac{1}{2}\sum_{i =1 }^k \left( -\frac{1}{(\hat{\sigma}^2)^2} + \frac{2}{(\hat{\sigma}^2)^2} \right) \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \right] \\ &amp;= \frac{1}{2(\hat{\sigma}^2)^2} \sum_{i = 1}^k \text{tr} \left[ \mathbf{z}_i \mathbf{z}_i^\top \right] \\ -\mathbb{E}\left[ \frac{\partial}{\partial \tau^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] \right] &amp;= \frac{1}{2}\sum_{i = 1}^k - \frac{1}{(\hat{\sigma}^2 + 0 \cdot\mathbf{z}_i^\top \mathbf{z}_i)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] +\text{tr} \left[\left[ \frac{2}{\hat{\sigma}^2(\hat{\sigma}^2 + 0 \cdot \mathbf{z}_i^\top \mathbf{z}_i)^2}\mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top - \frac{2\cdot 0}{\hat{\sigma}^2(\hat{\sigma}^2 + 0 \cdot \mathbf{z}_i^\top \mathbf{z}_i)^3} \mathbf{z}_i \mathbf{z}_i^\top\mathbf{z}_i \mathbf{z}_i^\top\mathbf{z}_i \mathbf{z}_i^\top\right] \Sigma_{y_i} \rvert_{\theta = \hat{\theta}} \right] \\ &amp;= \frac{1}{2}\sum_{i = 1}^k - \frac{1}{(\hat{\sigma}^2)^2} \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] +\text{tr} \left[\left[ \frac{2}{(\hat{\sigma}^2)^3}\mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \left[\hat{\sigma}^2 \mathbb{I}_{n \times n} \right] \right] \\ &amp;= \frac{1}{2}\sum_{i = 1}^k \left( -\frac{1}{(\hat{\sigma}^2)^2} + \frac{2}{(\hat{\sigma}^2)^2} \right)\text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \\ &amp;= \frac{1}{2(\hat{\sigma}^2)^2} \sum_{i = 1}^k \text{tr}\left[ \mathbf{z}_i \mathbf{z}_i^\top \mathbf{z}_i \mathbf{z}_i^\top \right] \\ -\mathbb{E}\left[ \frac{\partial}{\partial \tau^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha} \right] \right] \bigg\rvert_{\theta = \hat{\theta}} &amp;= 0 \\ -\mathbb{E}\left[ \frac{\partial}{\partial \alpha_j} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \sigma^2} \right] \right] \bigg\rvert_{\theta = \hat{\theta}} &amp;= 0 \\ -\mathbb{E}\left[ \frac{\partial}{\partial \alpha} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] \right] \bigg\rvert_{\theta = \hat{\theta}} &amp;= 0 \\ - \mathbb{E}\left[ \frac{\partial}{\partial \alpha_j} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] \right] &amp;= \mathbf{1}_n^\top \left[ \frac{1}{\hat{\sigma}^2} \mathbb{I}_{n \times n} - \frac{0}{\hat{\sigma}^2(\hat{\sigma}^2 + 0 \cdot \mathbf{z}_i^\top \mathbf{z}_i)} \mathbf{z}_i \mathbf{z}_i^\top \right] \mathbf{1}_n \\ &amp;= \frac{n}{\hat{\sigma}^2} \\ - \mathbb{E}\left[\frac{\partial}{\partial \alpha_{j'}} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] \right] \bigg\rvert_{\theta = \hat{\theta}} &amp;= 0 \end{aligned}\] </li> </ul> <hr/> <h2 id="negative-binomial-case">Negative Binomial Case</h2> <p>In this example, we’ll let the responses be negative binomial. To keep things simple, we’ll say we only have a single fixed intercept and a single random effect. We let $\phi &gt; 0$, denote the <i>known</i> dispersion parameter and assume the conditional mean to be given by:</p> \[\mu_{i,j} = \exp\left( \alpha_i + \beta_i \mathbf{z}_{i,j} \right) \label{eq:neg-bin-mean}\] <p>The likelihood based on a single observation, $\mathbf{y}_{i,j}$, is given by:</p> \[\mathcal{L}(\mathbf{y}_{i,j}; \alpha_i, \tau^2 \rvert \beta_i) = \frac{\Gamma\left(\mathbf{y}_{i,j} + \frac{1}{\phi}\right)}{\Gamma(\mathbf{y}_{i,j} + 1) \Gamma\left(\frac{1}{\phi} \right)}\left(\frac{1}{1 + \phi \mathbf{y}_{i,j}}\right)^{\frac{1}{\phi}} \left( \frac{\phi \mu_{i,j}}{1 + \phi \mu_{i,j}} \right)^{\mathbf{y}_{i,j}} \label{eq:neg-bin-single-lik}\] <p>where $\Gamma(\cdot)$ is the gamma function:</p> \[\Gamma(x) = \int_0^\infty t^{x - 1} \exp(-t) dt\] <p>The above parametrization of the likelihood implies that the conditional variance of the responses is given by:</p> \[V(\mu_{i,j}) = \mu_{i,j} + \frac{1}{\phi} \mu_{i,j}^2\] <p>The conditional log-likelihood based on cluster $i$ is:</p> \[\ell(\mathbf{y}_i; \alpha_i, \tau^2 \rvert \beta_i) = \sum_{j = 1}^{n_i} \left[ \log \Gamma \left( \mathbf{y}_{i,j} + \frac{1}{\phi} \right) - \log \Gamma\left(\mathbf{y}_{i,j} + 1\right) - \log\Gamma\left(\frac{1}{\phi} \right) - \frac{1}{\phi} \log\left(1 + \phi \mathbf{y}_{i,j} \right) + \mathbf{y}_{i,j} \left( \log(\phi \mu_{i,j}) - \log(1 + \phi \mu_{i,j}) \right) \right] \label{eq:neg-bin-full-cond-ll}\] <p>We follow a pseudo-likelihood approach (see <a href="/posts/2025/06/04/glmm.html">here</a>). We assume to have the following generalized linear mixed model:</p> \[\mathbf{y}_{i,j} \rvert \beta_i \sim \text{NegBin}(\mu_{i,j}, \phi); \hspace{10mm} \mu_{i,j} = \exp\left(\eta_{i,j}\right) = \exp\left(\alpha_i + \beta_i \mathbf{z}_{i,j}\right) \label{eq:glmm-y}\] <p>We’ll use a superscript $\star$ to denote a quantity evaluated at the parameter estimates made under $H_0$ (i.e. $\tau^2 = \mathbf{0}$). Our <i>working</i> responses and errors are:</p> \[\mathbf{y}^\star_{i,j} = \alpha_i + \beta_i \mathbf{z}_{i,j} + \epsilon^\star_{i,j}; \hspace{10mm} \epsilon^\star_{i,j} \sim \mathcal{N}\left(0, \frac{V(\hat{\mu}_{i,j})}{\delta^2(\hat{\eta}_{i,j})}\right)\] <p>where \(\delta(\hat{\eta}_{i,j}) = \frac{\partial g^{-1}(\eta_{i,j})}{\partial \eta_{i,j}}\bigg\rvert_{\eta_{i,j} = \hat{\eta}_{i,j}}\). We can then just apply all of the results we found in the previous section to this case but make \(\hat{\sigma}^2\) different for each observation, where \(\hat{\sigma}^2_{i,j} = \text{Var}(\epsilon_{i,j})\).</p> <p>To do so, we need an estimate of $\alpha_i$ under $H_0$, which we can do with <a href="/posts/2025/06/03/glm.html#weighted-least-squares">iteratively reweighted least squares</a> or some comparable algorithm. With these estimates, we can compute \(\hat{\sigma}_{i,j}^2\) and \(\mathbf{y}^\star_{i,j}\) and proceed as in the Gaussian case but without $\sigma^2$ in the parameter vector since we assume them to be fixed at \(\hat{\sigma}^2_{i,j}\).</p> <p>Since $g(\cdot) = \log(\cdot)$, we have that \(\delta(\hat{\eta}_{i,j}) = \exp(\hat{\eta}_{i,j})\), implying that the working error variances are:</p> \[\frac{V(\hat{\mu}_{i,j})}{\delta^2(\hat{\eta}_{i,j})} = \frac{\exp(\hat{\eta}_{i,j}) + \frac{1}{\phi}\exp(\hat{\eta}_{i,j})}{\exp^2(\hat{\eta}_{i,j})} = \frac{1}{\exp(\hat{\eta}_{i,j})}\left(1 + \frac{1}{\phi}\right)\] <p>where we recall that we assume $\phi$ is know and $\hat{\eta}_{i,j} = \hat{\alpha}_i$ under $H_0$. Dropping the star superscript, the likelihood and log-likelihood functions we will work with are giving by:</p> \[\begin{aligned} \mathcal{L}(\theta; \mathbf{y}) &amp;= \prod_{i = 1}^k (2 \pi)^{-\frac{n}{2}} \rvert \Sigma_{y_i} \rvert^{-\frac{1}{2}} \exp\left(- \frac{1}{2} (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \Sigma_{y_i}^{-1} (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right) \\ \ell(\theta; \mathbf{y}) &amp;= \sum_{i = 1}^k \left[ -\frac{n}{2} \log(2 \pi) - \frac{1}{2}\log(\rvert \Sigma_{y_i} \rvert) - \frac{1}{2} (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \Sigma_{y_i}^{-1} (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \end{aligned}\] <p>but using the working responses and their respective covariances and whatnot.</p> <h3 id="score-1">Score</h3> <p>The marginal covariance matrix is very similar to the Gaussian outcome model above. The only thing that has changed is that each error has its own variance:</p> \[\Sigma_{y_i} = \text{diag}([\hat{\sigma}^2_{i,1}, \dots, \hat{\sigma}^2_{i, n}]) + \tau^2 \mathbf{z}_i \mathbf{z}_i^\top\] <p>Its inverse, $\Sigma^{-1}_{y_i}$, is:</p> <ul id="sigma-inv-2" class="tab" data-tab="5d040b00-b352-45a8-aec1-bdfbec8e740d" data-name="sigma-inv-2"> <li class="active" id="sigma-inv-2-equation"> <a href="#">equation </a> </li> <li id="sigma-inv-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="5d040b00-b352-45a8-aec1-bdfbec8e740d" data-name="sigma-inv-2"> <li class="active"> \[\Sigma^{-1}_{y_i} = \text{diag}\left(\left[ \frac{1}{\hat{\sigma}^2_{i, 1}}, \dots, \frac{1}{\hat{\sigma}^2_{i,n}} \right]\right) - \frac{\text{diag}\left( \left[ \frac{\tau^2}{(\hat{\sigma}_{i,1}^2)^2}, \dots, \frac{\tau^2}{(\hat{\sigma}_{i,n}^2)^2} \right] \right) \mathbf{z}_i \mathbf{z}_i^\top}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}}\] </li> <li> <p>First, let’s let \(\mathbf{w}_i = \tau^2 \mathbf{z}_i\), which is the $n$-vector $\mathbf{z}_i$ where each coordinate has been multiplied by $\tau^2$. We’ll also let \(\hat{\sigma}^2 = (\hat{\sigma}^2_{i,1}, \dots, \hat{\sigma}^2_{i, n})^\top\), the vector of the error variances for cluster $i$, and $\frac{1}{\hat{\sigma}^2}$ will be the vector of the reciprocals of the coordinates of $\hat{\sigma}^2$. Using the <a href="https://en.wikipedia.org/wiki/Sherman–Morrison_formula">Sherman-Morrison formula</a>, we have:</p> \[\begin{aligned} \Sigma^{-1}_{y_i} &amp;= \left(\text{diag}(\hat{\sigma}^2) - \mathbf{w}_i \mathbf{v}_i^\top\right)^{-1} \\ &amp;= \text{diag}^{-1}(\hat{\sigma}^2) - \frac{\text{diag}^{-1}(\hat{\sigma}^2) \mathbf{w}_i \mathbf{z}_i^\top \text{diag}^{-1}(\hat{\sigma}^2)}{1 + \mathbf{z}_i^\top \text{diag}^{-1}(\hat{\sigma}^2) \mathbf{w}_i} \\ &amp;= \text{diag}\left(\frac{1}{\hat{\sigma}^2}\right) - \frac{\text{diag}\left(\frac{\tau^2}{(\hat{\sigma}^2)^2}\right) \mathbf{z}_i \mathbf{z}_i^\top}{1 + \mathbf{z}_i^\top \text{diag}\left(\frac{\tau^2}{\hat{\sigma}^2}\right) \mathbf{z}_i} \\ &amp;= \text{diag}\left(\left[ \frac{1}{\hat{\sigma}^2_{i, 1}}, \dots, \frac{1}{\hat{\sigma}^2_{i,n}} \right]\right) - \frac{\text{diag}\left( \left[ \frac{\tau^2}{(\sigma_{i,1}^2)^2}, \dots, \frac{\tau^2}{(\sigma_{i,n}^2)^2} \right] \right) \mathbf{z}_i \mathbf{z}_i^\top}{1 + \mathbf{z}_i^\top \text{diag}\left( \left[ \frac{\tau^2}{\sigma_{i,1}^2}, \dots, \frac{\tau^2}{\sigma_{i,n}^2} \right] \right) \mathbf{z}_i} \\ &amp;= \text{diag}\left(\left[ \frac{1}{\hat{\sigma}^2_{i, 1}}, \dots, \frac{1}{\hat{\sigma}^2_{i,n}} \right]\right) - \frac{\text{diag}\left( \left[ \frac{\tau^2}{(\sigma_{i,1}^2)^2}, \dots, \frac{\tau^2}{(\sigma_{i,n}^2)^2} \right] \right) \mathbf{z}_i \mathbf{z}_i^\top}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}} \end{aligned}\] <p>We also have:</p> \[[\Sigma^{-1}_{y_i}]_{j,j'} = - \frac{\tau^2 \mathbf{z}_{i,j} \mathbf{z}_{i,j'}}{(\hat{\sigma}_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)} \hspace{10mm} \text{and} \hspace{10mm} [\Sigma^{-1}_{y_i}]_{j,j} = \frac{1}{\hat{\sigma}^2_{i,j}} - \frac{\tau^2 \mathbf{z}_{i,j}^2}{(\hat{\sigma}_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)}\] </li> </ul> <h4 id="derivatives-2">Derivatives</h4> <p>We first find the derivative with respect to $\tau^2$:</p> <ul id="deriv-tau-2" class="tab" data-tab="ce5e0bce-89d4-4e49-a31a-8e57fd105db5" data-name="deriv-tau-2"> <li class="active" id="deriv-tau-2-equation"> <a href="#">equation </a> </li> <li id="deriv-tau-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="ce5e0bce-89d4-4e49-a31a-8e57fd105db5" data-name="deriv-tau-2"> <li class="active"> \[\frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} = - \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} - \sum_{l = 1}^n \sum_{j = 1}^n \frac{\tau^2 \mathbf{z}_{i,l}^2 \mathbf{z}_{i,j}^2}{(\hat{\sigma}_{i,l}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}\right)} - \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \frac{\mathbf{z}_{i,a} \mathbf{z}_{i,b}}{(\hat{\sigma}^2_{i,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \right]\] </li> <li> <p>First, the log determinant term:</p> \[\begin{aligned} \frac{\partial}{\partial \tau^2} \left[ \log (\rvert \Sigma_{y_i} \rvert) \right] &amp;= \text{tr}\left[ \Sigma^{-1}_{y_i} \frac{\partial}{\partial \tau^2} \left[ \Sigma_{y_i} \right] \right] \\ &amp;= \text{tr} \left[ \left( \text{diag}\left(\left[ \frac{1}{\hat{\sigma}^2_{i, 1}}, \dots, \frac{1}{\hat{\sigma}^2_{i,n}} \right]\right) - \frac{\text{diag}\left( \left[ \frac{\tau^2}{(\hat{\sigma}_{i,1}^2)^2}, \dots, \frac{\tau^2}{(\hat{\sigma}_{i,n}^2)^2} \right] \right) \mathbf{z}_i \mathbf{z}_i^\top}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}} \right) \mathbf{z}_i \mathbf{z}_i^\top \right] \\ &amp;= \sum_{l = 1}^n \sum_{j = 1}^n \left( \text{diag}\left(\left[ \frac{1}{\hat{\sigma}^2_{i, 1}}, \dots, \frac{1}{\hat{\sigma}^2_{i,n}} \right]\right) - \frac{\text{diag}\left( \left[ \frac{\tau^2}{(\hat{\sigma}_{i,1}^2)^2}, \dots, \frac{\tau^2}{(\hat{\sigma}_{i,n}^2)^2} \right] \right) \mathbf{z}_i \mathbf{z}_i^\top}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}} \right)_{l,j} \left(\mathbf{z}_i \mathbf{z}_i^\top \right)_{l,j} \\ &amp;= \sum_{l = 1}^n \left( \text{diag}\left(\left[ \frac{1}{\hat{\sigma}^2_{i, 1}}, \dots, \frac{1}{\hat{\sigma}^2_{i,n}} \right]\right) - \frac{\text{diag}\left( \left[ \frac{\tau^2}{(\hat{\sigma}_{i,1}^2)^2}, \dots, \frac{\tau^2}{(\hat{\sigma}_{i,n}^2)^2} \right] \right) \mathbf{z}_i \mathbf{z}_i^\top}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}} \right)_{l,l} \left(\mathbf{z}_i \mathbf{z}_i^\top \right)_{l,l} + \sum_{l = 1}^n \sum_{j \neq l} \left( \text{diag}\left(\left[ \frac{1}{\hat{\sigma}^2_{i, 1}}, \dots, \frac{1}{\hat{\sigma}^2_{i,n}} \right]\right) - \frac{\text{diag}\left( \left[ \frac{\tau^2}{(\hat{\sigma}_{i,1}^2)^2}, \dots, \frac{\tau^2}{(\hat{\sigma}_{i,n}^2)^2} \right] \right) \mathbf{z}_i \mathbf{z}_i^\top}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}} \right)_{l,j} \left(\mathbf{z}_i \mathbf{z}_i^\top \right)_{l,j} \\ &amp;= \sum_{l = 1}^n \left( \frac{1}{\hat{\sigma}^2_{i,l}} - \frac{\frac{\tau^2}{(\hat{\sigma}_{i,l}^2)^2}\mathbf{z}_{i,l}^2}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}} \right) \mathbf{z}_{i,l}^2 - \sum_{l = 1}^n \sum_{j \neq l} \left( \frac{\frac{\tau^2}{(\hat{\sigma}^2_{i,l})^2} \mathbf{z}_{i,l} \mathbf{z}_{i,j}}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}} \right) \mathbf{z}_{i,l} \mathbf{z}_{i,j} \\ &amp;= \sum_{l = 1}^n \left( \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} - \frac{\frac{\tau^2}{(\hat{\sigma}_{i,l}^2)^2}\mathbf{z}_{i,l}^4}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}} \right) - \sum_{l = 1}^n \sum_{j \neq l} \left( \frac{\frac{\tau^2}{(\hat{\sigma}^2_{i,l})^2} \mathbf{z}_{i,l}^2 \mathbf{z}_{i,j}^2}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}} \right) \\ &amp;= \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} - \sum_{l = 1}^n \sum_{j = 1}^n \frac{\frac{\tau^2}{(\hat{\sigma}_{i,l}^2)^2}\mathbf{z}_{i,l}^2 \mathbf{z}_{i,j}^2}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}} \end{aligned}\] <p>Next, the quadratic term. We will compute the derivative of $\Sigma^{-1}_{y_i}$ with respect to $\tau^2$ element-wise:</p> \[\begin{aligned} \frac{\partial}{\partial \tau^2}\left[ [\Sigma^{-1}_{y_i}]_{j,j'}\right] &amp;= \frac{\partial}{\partial \tau^2} \left[ - \frac{\tau^2 \mathbf{z}_{i,j} \mathbf{z}_{i,j'}}{(\hat{\sigma}_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)} \right] \\ &amp;= -\frac{(\hat{\sigma}_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)(\mathbf{z}_{i,j}\mathbf{z}_{i,j'}) - (\tau^2 \mathbf{z}_{i,j}\mathbf{z}_{i,j'})\left( (\hat{\sigma}_{i,j}^2)^2\sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)}{(\hat{\sigma}_{i,j}^2)^4 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \\ &amp;= -\frac{\mathbf{z}_{i,j}\mathbf{z}_{i,j'} \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}- \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)}{(\hat{\sigma}_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \\ &amp;= -\frac{\mathbf{z}_{i,j}\mathbf{z}_{i,j'}}{(\hat{\sigma}_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \\ \frac{\partial}{\partial \tau^2}\left[ [\Sigma^{-1}_{y_i}]_{j,j}\right] &amp;= \frac{\partial}{\partial \tau^2} \left[ \frac{1}{\hat{\sigma}^2_{i,j}} - \frac{\tau^2 \mathbf{z}_{i,j}^2}{(\hat{\sigma}_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)} \right] \\ &amp;= - \frac{(\hat{\sigma}_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)\mathbf{z}_{i,j}^2 - \tau^2 \mathbf{z}_{i,j}^2 (\hat{\sigma}_{i,j}^2)^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}}{(\hat{\sigma}_{i,j}^2)^4 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \\ &amp;= - \frac{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)\mathbf{z}_{i,j}^2 - \tau^2 \mathbf{z}_{i,j}^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}}{(\hat{\sigma}_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \\ &amp;= - \frac{\mathbf{z}_{i,j}^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}- \tau^2\sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)}{(\hat{\sigma}_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \\ &amp;= - \frac{\mathbf{z}_{i,j}^2}{(\hat{\sigma}_{i,j}^2)^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \\ \end{aligned}\] <p>In matrix notation, we have:</p> \[\frac{\partial}{\partial \tau^2}\left[ \Sigma^{-1}_{y_i} \right] = \text{diag}\left(\left[ -\frac{1}{(\hat{\sigma}^2_{i,1})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2}, \dots, -\frac{1}{(\hat{\sigma}^2_{i,n})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \right] \right) \mathbf{z}_i \mathbf{z}_i^\top\] <p>Then:</p> \[\begin{aligned} (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \frac{\partial}{\partial \tau^2} \left[ \Sigma^{-1}_{y_i} \right](\mathbf{y}_i - \alpha_i \mathbf{1}_n) &amp;= (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \left[ \text{diag}\left(\left[ -\frac{1}{(\hat{\sigma}^2_{i,1})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2}, \dots, -\frac{1}{(\hat{\sigma}^2_{i,n})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \right] \right) \mathbf{z}_i \mathbf{z}_i^\top \right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \\ &amp;= \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \left[ \text{diag}\left(\left[ -\frac{1}{(\hat{\sigma}^2_{i,1})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2}, \dots, -\frac{1}{(\hat{\sigma}^2_{i,n})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \right] \right) \mathbf{z}_i \mathbf{z}_i^\top \right]_{a,b} \\ &amp;= - \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \frac{\mathbf{z}_{i,a} \mathbf{z}_{i,b}}{(\hat{\sigma}^2_{i,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \end{aligned}\] <p>And thus:</p> \[\begin{aligned} \frac{\partial}{\partial \tau^2} \left[ \ell(\theta; \mathbf{y}) \right] &amp;= \frac{\partial}{\partial \tau^2} \left[ \sum_{i = 1}^k \left[ -\frac{n}{2} \log(2 \pi) - \frac{1}{2}\log(\rvert \Sigma_{y_i} \rvert) - \frac{1}{2} (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \Sigma_{y_i}^{-1} (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \right] \\ &amp;= - \frac{1}{2} \sum_{i = 1}^k \left[ \frac{\partial}{\partial \tau^2} \left[ \log(\rvert \Sigma_{y_i} \rvert) \right] + (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \frac{\partial}{\partial \tau^2} \left[ \Sigma_{y_i}^{-1} \right] (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right]\\ &amp;= - \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} - \sum_{l = 1}^n \sum_{j = 1}^n \frac{\frac{\tau^2}{(\hat{\sigma}_{i,l}^2)^2}\mathbf{z}_{i,l}^2 \mathbf{z}_{i,j}^2}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}} - \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \frac{\mathbf{z}_{i,a} \mathbf{z}_{i,b}}{(\hat{\sigma}^2_{i,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \right] \end{aligned}\] </li> </ul> <p>And now we find the gradient with respect to $\alpha$:</p> <ul id="deriv-alpha-2" class="tab" data-tab="4641ccdf-d46a-4e58-a226-229d819f663b" data-name="deriv-alpha-2"> <li class="active" id="deriv-alpha-2-equation"> <a href="#">equation </a> </li> <li id="deriv-alpha-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="4641ccdf-d46a-4e58-a226-229d819f663b" data-name="deriv-alpha-2"> <li class="active"> \[\frac{\partial}{\partial \alpha}[ \ell(\theta; \mathbf{y})] = \begin{bmatrix} (\mathbf{y}_1 - \alpha_1 \mathbf{1}_n)^\top \Sigma_{y_1}^{-1} \mathbf{1}_n \\ \vdots \\ (\mathbf{y}_k - \alpha_k \mathbf{1}_n)^\top \Sigma_{y_k}^{-1} \mathbf{1}_n \end{bmatrix}\] </li> <li> <p>We do this component-wise:</p> \[\begin{aligned} \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} &amp;= \frac{\partial}{\partial \alpha_j} \left[ \sum_{i = 1}^k \left[ -\frac{n}{2} \log(2 \pi) - \frac{1}{2}\log(\rvert \Sigma_{y_i} \rvert) - \frac{1}{2} (\mathbf{y}_i - \alpha_i \mathbf{1}_n)^\top \Sigma_{y_i}^{-1} (\mathbf{y}_i - \alpha_i \mathbf{1}_n) \right] \right] \\ &amp;= - \frac{1}{2} \frac{\partial}{\partial \alpha_j} \left[(\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \Sigma_{y_j}^{-1} (\mathbf{y}_j - \alpha_j \mathbf{1}_n) \right] \\ &amp;= - \frac{1}{2}\left(2 (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \Sigma_{y_j}^{-1} (-\mathbf{1}_n) \right) \\ &amp;= (\mathbf{y}_i - \alpha_j \mathbf{1}_n)^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ \implies \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha} &amp;= \begin{bmatrix} (\mathbf{y}_1 - \alpha_1 \mathbf{1}_n)^\top \Sigma_{y_1}^{-1} \mathbf{1}_n \\ \vdots \\ (\mathbf{y}_k - \alpha_k \mathbf{1}_n)^\top \Sigma_{y_k}^{-1} \mathbf{1}_n \end{bmatrix} \end{aligned}\] </li> </ul> <h4 id="mles-1">MLEs</h4> <p>We can find $\hat{\theta}$ by setting the above equal to zero and substitute $\tau^2 = 0$. We get:</p> <ul id="mle-2" class="tab" data-tab="1add3298-8d22-4f70-b230-efc34a115336" data-name="mle-2"> <li class="active" id="mle-2-equation"> <a href="#">equation </a> </li> <li id="mle-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="1add3298-8d22-4f70-b230-efc34a115336" data-name="mle-2"> <li class="active"> \[\hat{\theta} = \begin{bmatrix} \left(\sum_{l' = 1}^n \frac{1}{\hat{\sigma}^2_{1,l'}} \right)^{-1} \sum_{l' = 1}^n \frac{\mathbf{y}_{1,l'}}{\hat{\sigma}^2_{1,l'}} \\ \vdots \\ \left(\sum_{l' = 1}^n \frac{1}{\hat{\sigma}^2_{k,l'}} \right)^{-1} \sum_{l' = 1}^n \frac{\mathbf{y}_{k,l'}}{\hat{\sigma}^2_{k,l'}} \\ 0 \end{bmatrix}\] </li> <li> <p>We only need to deal with $\alpha$, which we can again do component-wise. First notice that:</p> \[\begin{aligned} \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} &amp;= (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ &amp;= (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left( \text{diag}\left(\left[ \frac{1}{\hat{\sigma}^2_{j, 1}}, \dots, \frac{1}{\hat{\sigma}^2_{j,n}} \right]\right) - \frac{\text{diag}\left( \left[ \frac{\tau^2}{(\hat{\sigma}_{j,1}^2)^2}, \dots, \frac{\tau^2}{(\hat{\sigma}_{j,n}^2)^2} \right] \right) \mathbf{z}_j \mathbf{z}_j^\top}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\hat{\sigma}^2_{j,l}}} \right) \mathbf{1}_n \\ &amp;= (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left(\begin{bmatrix} \frac{1}{\hat{\sigma}^2_{j, 1}} \\ \vdots \\ \frac{1}{\hat{\sigma}^2_{j,n}} \end{bmatrix} - \left( \frac{1}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\hat{\sigma}_{j,l}^2}} \right)\text{diag}\left( \left[ \frac{\tau^2}{(\hat{\sigma}_{j,1}^2)^2}, \dots, \frac{\tau^2}{(\hat{\sigma}_{j,n}^2)^2} \right] \right) \begin{bmatrix} \sum_{l = 1}^n \mathbf{z}_{j,1} \mathbf{z}_{j,l} \\ \vdots \\ \sum_{l = 1}^n \mathbf{z}_{j,n} \mathbf{z}_{j,l} \end{bmatrix} \right) \\ &amp;= \left( \sum_{l = 1}^n \frac{\mathbf{y}_{j,l} - \alpha_j}{\hat{\sigma}^2_{j,l}} \right)- \left( \frac{1}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\hat{\sigma}_{j,l}^2}} \right) (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \begin{bmatrix} \frac{\tau^2}{(\hat{\sigma}_{j,1}^2)^2} \sum_{l = 1}^n \mathbf{z}_{j,1} \mathbf{z}_{j,l} \\ \vdots \\ \frac{\tau^2}{(\hat{\sigma}_{j,n}^2)^2} \sum_{l = 1}^n \mathbf{z}_{j,n} \mathbf{z}_{j,l} \end{bmatrix} \\ &amp;= \left( \sum_{l = 1}^n \frac{\mathbf{y}_{j,l} - \alpha_j}{\hat{\sigma}^2_{j,l}} \right)- \left( \frac{1}{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\hat{\sigma}_{j,l}^2}} \right) \sum_{l' = 1}^n (\mathbf{y}_{j,l'} - \alpha_j) \frac{\tau^2}{(\hat{\sigma}_{j,l'}^2)^2} \sum_{l = 1}^n \mathbf{z}_{j,l'} \mathbf{z}_{j,l} \\ &amp;= \left( \sum_{l = 1}^n \frac{\mathbf{y}_{j,l} - \alpha_j}{\hat{\sigma}^2_{j,l}} \right)- \left( \frac{\sum_{l = 1}^n \mathbf{z}_{j,l} }{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\hat{\sigma}_{j,l}^2}} \right) \left( \sum_{l' = 1}^n \frac{\tau^2 (\mathbf{y}_{j,l'} - \alpha_j) \mathbf{z}_{j,l'} }{(\hat{\sigma}_{j,l'}^2)^2} \right) \end{aligned}\] <p>So then we solve for $\alpha_j$ in:</p> \[\begin{aligned} 0 &amp;= (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ \implies 0 &amp;= \left( \sum_{l = 1}^n \frac{\mathbf{y}_{j,l} - \alpha_j}{\hat{\sigma}^2_{j,l}} \right)- \left( \frac{\sum_{l = 1}^n \mathbf{z}_{j,l} }{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\hat{\sigma}_{j,l}^2}} \right) \left( \sum_{l' = 1}^n \frac{\tau^2 (\mathbf{y}_{j,l'} - \alpha_j) \mathbf{z}_{j,l'} }{(\hat{\sigma}_{j,l'}^2)^2} \right) \\ \implies 0 &amp;= \sum_{l = 1}^n \frac{\mathbf{y}_{j,l}}{\hat{\sigma}^2_{j,l}} - \alpha_j \sum_{l = 1}^n \frac{1}{\hat{\sigma}^2_{j,l}} - \left( \frac{\sum_{l = 1}^n \mathbf{z}_{j,l} }{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\hat{\sigma}_{j,l}^2}} \right) \left[ \sum_{l' = 1}^n \frac{\tau^2 \mathbf{y}_{j,l'} \mathbf{z}_{j,l'}}{(\hat{\sigma}^2_{j,l'})^2} - \alpha_j \sum_{l' =1}^n \frac{\tau^2 \mathbf{z}_{j,l'}}{(\hat{\sigma}^2_{j,l'})} \right] \\ \implies \alpha_j \left[ \sum_{l = 1}^n \frac{1}{\hat{\sigma}^2_{j,l}} - \sum_{l = 1}^n \frac{\tau^2 \mathbf{z}_{j,l'}}{(\hat{\sigma}^2_{j,l'})^2}\right] &amp;= \sum_{l = 1}^n \frac{\mathbf{y}_{j,l}}{\hat{\sigma}^2_{j,l}} - \left( \frac{\sum_{l = 1}^n \mathbf{z}_{j,l} }{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\hat{\sigma}_{j,l}^2}} \right) \sum_{l' = 1}^n\frac{\tau^2 \mathbf{y}_{j,l'} \mathbf{z}_{j,l'}}{(\hat{\sigma}^2_{j,l'})^2} \\ \implies \alpha_j \left(\sum_{l' = 1}^n \frac{1}{\hat{\sigma}^2_{j,l'}} \left[1 - \frac{\tau^2 \mathbf{z}_{j,l'}}{\hat{\sigma}^2_{j,l'}} \right]\right) &amp;= \sum_{l' = 1}^n \frac{\mathbf{y}_{j,l'}}{\hat{\sigma}^2_{j,l'}} \left[ 1 - \left( \frac{\sum_{l = 1}^n \mathbf{z}_{j,l} }{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\hat{\sigma}_{j,l}^2}} \right) \frac{\tau^2 \mathbf{z}_{j,l'}}{\hat{\sigma}^2_{j,l'}} \right] \\ \implies \alpha_j &amp;= \left(\sum_{l' = 1}^n \frac{1}{\hat{\sigma}^2_{j,l'}} \left[1 - \frac{\tau^2 \mathbf{z}_{j,l'}}{\hat{\sigma}^2_{j,l'}} \right]\right)^{-1} \sum_{l' = 1}^n \frac{\mathbf{y}_{j,l'}}{\hat{\sigma}^2_{j,l'}} \left[ 1 - \left( \frac{\sum_{l = 1}^n \mathbf{z}_{j,l} }{1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}^2}{\hat{\sigma}_{j,l}^2}} \right) \frac{\tau^2 \mathbf{z}_{j,l'}}{\hat{\sigma}^2_{j,l'}} \right] \end{aligned}\] <p>Under $H_0$, $\tau^2 = 0$, so we get:</p> \[\hat{\alpha}_j = \left(\sum_{l' = 1}^n \frac{1}{\hat{\sigma}^2_{j,l'}} \right)^{-1} \sum_{l' = 1}^n \frac{\mathbf{y}_{j,l'}}{\hat{\sigma}^2_{j,l'}}\] </li> </ul> <p>Thus, the score evaluated at $\theta = \hat{\theta}$ is then:</p> \[U_{\theta}(\hat{\theta}) = \begin{bmatrix} \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha} \bigg\rvert_{\theta = \hat{\theta}} \\ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \bigg\rvert_{\theta = \hat{\theta}} \end{bmatrix} = \begin{bmatrix} \left(\sum_{l' = 1}^n \frac{1}{\hat{\sigma}^2_{1,l'}} \right)^{-1} \sum_{l' = 1}^n \frac{\mathbf{y}_{1,l'}}{\hat{\sigma}^2_{1,l'}} \\ \vdots \\ \left(\sum_{l' = 1}^n \frac{1}{\hat{\sigma}^2_{k,l'}} \right)^{-1} \sum_{l' = 1}^n \frac{\mathbf{y}_{k,l'}}{\hat{\sigma}^2_{k,l'}} \\ - \frac{1}{2}\sum_{i = 1}^k \left[ \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}_{i,l}^2} + \sum_{l = 1}^n \sum_{l' = 1}^n \frac{\mathbf{z}_{i,l}\mathbf{z}_{i,l'}(\mathbf{y}_{i,l} - \hat{\alpha}_i)(\mathbf{y}_{i,l'} - \hat{\alpha}_i)}{\hat{\sigma}^2_{i,l} \hat{\sigma}^2_{i,l'}}\right] \end{bmatrix}\] <h3 id="information-1">Information</h3> <p>As before, to find the information, we need to compute the second-order derivatives of the log-likelihood, take the expectation under $H_0$ of minus those quantities, and evaluate them by plugging in $\hat{\theta}$.</p> <h4 id="derivatives-3">Derivatives</h4> <p>We’ll take all of the derivatives component-wise. We’ll start with those with respect to $\tau^2$.</p> <ul id="deriv-theta-tau-2" class="tab" data-tab="098e835e-5652-42ff-af3c-47a28ff777de" data-name="deriv-theta-tau-2"> <li class="active" id="deriv-theta-tau-2-equation"> <a href="#">equation </a> </li> <li id="deriv-theta-tau-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="098e835e-5652-42ff-af3c-47a28ff777de" data-name="deriv-theta-tau-2"> <li class="active"> \[\begin{aligned} \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \tau^2 \partial \theta} &amp;= \begin{bmatrix} - \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{1,l}}{\hat{\sigma}^2_{1,l}}\right)^2} \left(\sum_{l' = 1}^n \frac{\mathbf{z}_{1,l'}(\mathbf{y}_{1,l'} - \alpha_1)}{(\hat{\sigma}^2_{1,l'})^2}\right)\left(\sum_{l'' = 1}^n \mathbf{z}_{1,l''}\right) \\ \vdots \\ - \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{k,l}}{\hat{\sigma}^2_{k,l}}\right)^2} \left(\sum_{l' = 1}^n \frac{\mathbf{z}_{k,l'}(\mathbf{y}_{k,l'} - \alpha_k)}{(\hat{\sigma}^2_{k,l'})^2}\right)\left(\sum_{l'' = 1}^n \mathbf{z}_{k,l''}\right) \\ \frac{1}{2} \sum_{i = 1}^k \left[ \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} \right)^2} \left( \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} \right)^2 - \frac{2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^3} \left( \sum_{a = 1}^n (\mathbf{y}_{i,a} - \alpha_i) \frac{\mathbf{z}_{i,a}}{\hat{\sigma}^2_{i,a}} \right)^2 \right] \end{bmatrix} \end{aligned} \label{eq:info-tau-tau}\] </li> <li> \[\begin{aligned} \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial (\tau^2)^2} &amp;= \frac{\partial}{\partial \tau^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] \\ &amp;= \frac{\partial}{\partial \tau^2} \left[ - \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} - \sum_{l = 1}^n \sum_{j = 1}^n \frac{\tau^2 \mathbf{z}_{i,l}^2 \mathbf{z}_{i,j}^2}{(\hat{\sigma}_{i,l}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}\right)} - \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \frac{\mathbf{z}_{i,a} \mathbf{z}_{i,b}}{(\hat{\sigma}^2_{i,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \right] \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[\sum_{l = 1}^n \sum_{j = 1}^n \frac{\partial}{\partial \tau^2} \left[ \frac{\tau^2 \mathbf{z}_{i,l}^2 \mathbf{z}_{i,j}^2}{(\hat{\sigma}_{i,l}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}\right)} \right] + \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \frac{\partial}{\partial \tau^2} \left[ \frac{\mathbf{z}_{i,a} \mathbf{z}_{i,b}}{(\hat{\sigma}^2_{i,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \right] \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \left(\frac{\mathbf{z}_{i,l}^2 \mathbf{z}_{i,j}^2 (\hat{\sigma}^2_{i,l})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}\right) - \tau^2 \mathbf{z}_{i,l}^2 \mathbf{z}_{i,j}^2 (\hat{\sigma}^2_{i,l})^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} }{\left((\hat{\sigma}^2_{i,l})^2 \left( 1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}\right)\right)^2}\right) + \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \left(\frac{-2\mathbf{z}_{i,a}\mathbf{z}_{i,b}(\hat{\sigma}^2_{i,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}_{i,l}}\right) \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}_{i,l}^2}}{\left((\hat{\sigma}^2_{i,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}} \right)^2\right)^2}\right) \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\hat{\sigma}^2_{i,l})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} \right)^2} - 2 \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \left(\frac{\mathbf{z}_{i,a}\mathbf{z}_{i,b}(\hat{\sigma}^2_{i,a})^2 \left(\sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}} \right)}{(\hat{\sigma}^2_{i,a})^4 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}} \right)^3}\right) \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[ \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} \right)^2} \left( \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{(\hat{\sigma}^2_{i,l})^2} \right) \left( \sum_{j = 1}^n \mathbf{z}_{i,j}^2 \right) - \frac{2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^3} \left( \sum_{a = 1}^n (\mathbf{y}_{i,a} - \alpha_i) \frac{\mathbf{z}_{i,a}}{(\hat{\sigma}^2_{i,a})^2} \right) \left( \sum_{b = 1}^n (\mathbf{y}_{i,b} - \alpha_i) \mathbf{z}_{i,b} \right) \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[ \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} \right)^2} \left( \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{(\hat{\sigma}^2_{i,l})^2} \right) \left( \sum_{l = 1}^n \mathbf{z}_{i,l}^2 \right) - \frac{2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^3} \left( \sum_{a = 1}^n (\mathbf{y}_{i,a} - \alpha_i) \frac{\mathbf{z}_{i,a}}{\hat{\sigma}^2_{i,a}} \right) \left( \sum_{a = 1}^n (\mathbf{y}_{i,a} - \alpha_i) \frac{\mathbf{z}_{i,a}}{\hat{\sigma}^2_{i,a}} \right) \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[ \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} \right)^2} \left( \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} \right)^2 - \frac{2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^3} \left( \sum_{a = 1}^n (\mathbf{y}_{i,a} - \alpha_i) \frac{\mathbf{z}_{i,a}}{\hat{\sigma}^2_{i,a}} \right)^2 \right] \end{aligned}\] <p>Next, we take the derivative (with respect to $\tau^2$) of the derivative with respect to $\alpha_j$:</p> \[\begin{aligned} \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \tau^2 \partial \alpha_j } &amp;= \frac{\partial}{\partial \tau^2} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha_j} \right] \\ &amp;= \frac{\partial}{\partial \tau^2} \left[ (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \right] \\ &amp;= \begin{bmatrix} (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \left[ \text{diag}\left( \begin{bmatrix} -\frac{1}{(\hat{\sigma}^2_{j,1})^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2} &amp; \dots &amp; -\frac{1}{(\hat{\sigma}^2_{j,n})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2} \end{bmatrix} \right) \mathbf{z}_{j} \mathbf{z}_j^\top \right] \mathbf{1}_n \end{bmatrix} \\ &amp;= \begin{bmatrix} -\frac{\mathbf{y}_{j, 1} - \alpha_j}{(\hat{\sigma}^2_{j,1})^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2} &amp; \dots &amp; -\frac{\mathbf{y}_{j,n} - \alpha_j}{(\hat{\sigma}^2_{j,n})^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2} \end{bmatrix} \mathbf{z}_j \mathbf{z}_j^\top \mathbf{1}_n \\ &amp;= \left(- \sum_{l' = 1}^n \frac{\mathbf{z}_{j,l'}(\mathbf{y}_{j,l'} - \alpha_j)}{(\hat{\sigma}^2_{j,l'})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2 }\right)\left(\sum_{l'' = 1}^n \mathbf{z}_{j,l''}\right) \\ &amp;= - \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2} \left(\sum_{l' = 1}^n \frac{\mathbf{z}_{j,l'}(\mathbf{y}_{j,l'} - \alpha_j)}{(\hat{\sigma}^2_{j,l'})^2}\right)\left(\sum_{l'' = 1}^n \mathbf{z}_{j,l''}\right) \end{aligned}\] <p>Putting the two together into a matrix yield:</p> \[\begin{aligned} \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \tau^2 \partial \theta } &amp;= \frac{\partial}{\partial \tau^2} \left[ \begin{bmatrix} \frac{\partial \ell(\theta; \mathbf{y})}{\partial \alpha} \\ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \end{bmatrix} \right] \\ &amp;= \begin{bmatrix} - \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{1,l}}{\hat{\sigma}^2_{1,l}}\right)^2} \left(\sum_{l' = 1}^n \frac{\mathbf{z}_{1,l'}(\mathbf{y}_{1,l'} - \alpha_1)}{(\hat{\sigma}^2_{1,l'})^2}\right)\left(\sum_{l'' = 1}^n \mathbf{z}_{1,l''}\right) \\ \vdots \\ - \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{k,l}}{\hat{\sigma}^2_{k,l}}\right)^2} \left(\sum_{l' = 1}^n \frac{\mathbf{z}_{k,l'}(\mathbf{y}_{k,l'} - \alpha_k)}{(\hat{\sigma}^2_{k,l'})^2}\right)\left(\sum_{l'' = 1}^n \mathbf{z}_{k,l''}\right) \\ \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\hat{\sigma}^2_{i,l})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} \right)^2} - 2 \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \left(\frac{\mathbf{z}_{i,a}\mathbf{z}_{i,b}(\hat{\sigma}^2_{i,a})^2 \left(\sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}} \right)}{(\hat{\sigma}^2_{i,l})^4 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}} \right)^3}\right) \right] \end{bmatrix} \end{aligned}\] </li> </ul> <p>We then do the same but with respect to $\alpha$.</p> <ul id="deriv-theta-alpha-2" class="tab" data-tab="343168af-4899-4066-96cc-8b5d15dc72d7" data-name="deriv-theta-alpha-2"> <li class="active" id="deriv-theta-alpha-2-equation"> <a href="#">equation </a> </li> <li id="deriv-theta-alpha-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="343168af-4899-4066-96cc-8b5d15dc72d7" data-name="deriv-theta-alpha-2"> <li class="active"> \[\frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j \partial \theta} = \begin{bmatrix} 0 \\ \vdots \\ - \mathbf{1}_n^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ \vdots \\ 0 \\ -\frac{1}{\left( 1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2 } \sum_{a = 1}^n \frac{\mathbf{z}_{j,a}(\mathbf{y}_{j, a} - \alpha_j)}{(\hat{\sigma}_{j,a}^2)^2} \left(\sum_{b = 1}^n \mathbf{z}_{j,b} \right) \end{bmatrix}\] </li> <li> <p>First, we find the gradient (with respect to $\alpha$) of the derivative with respect to $\tau^2$. This should be equivalent to the corresponding components of Eq. \eqref{eq:info-tau-tau}.</p> \[\begin{aligned} \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j \partial \tau^2 } &amp;= \frac{\partial}{\partial \alpha_j} \left[ \frac{\partial \ell(\theta; \mathbf{y})}{\partial \tau^2} \right] \\ &amp;= \frac{\partial}{\partial \alpha_j} \left[ - \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} - \sum_{l = 1}^n \sum_{j = 1}^n \frac{\tau^2 \mathbf{z}_{i,l}^2 \mathbf{z}_{i,j}^2}{(\hat{\sigma}_{i,l}^2)^2\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}}\right)} - \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \frac{\mathbf{z}_{i,a} \mathbf{z}_{i,b}}{(\hat{\sigma}^2_{i,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \right] \right] \\ &amp;= \frac{\partial}{\partial \alpha_j} \left[ \frac{1}{2} \sum_{i = 1}^k \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \frac{\mathbf{z}_{i,a} \mathbf{z}_{i,b}}{(\hat{\sigma}^2_{i,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}}\right)^2} \right] \\ &amp;= \frac{1}{2} \sum_{a = 1}^n \sum_{b = 1}^n \frac{\partial}{\partial \alpha_j} \left[ (\mathbf{y}_{j,a} - \alpha_j)(\mathbf{y}_{j,b} - \alpha_j) \frac{\mathbf{z}_{j,a} \mathbf{z}_{j,b}}{(\hat{\sigma}^2_{j,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2} \right] \\ &amp;= \frac{1}{2} \sum_{a = 1}^n \sum_{b = 1}^n \frac{\mathbf{z}_{j,a} \mathbf{z}_{j,b}}{(\hat{\sigma}^2_{j,a})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2} \frac{\partial}{\partial \alpha_j} \left[ \mathbf{y}_{j,a} \mathbf{y}_{j,b} - \alpha_j \mathbf{y}_{j,a} - \alpha \mathbf{y}_{j,b} + \alpha_j^2 \right] \\ &amp;= -\frac{1}{2 \left( 1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2 } \sum_{a = 1}^n \sum_{b = 1}^n \left[ \frac{\mathbf{z}_{j,a}\mathbf{z}_{j,b}(\mathbf{y}_{j, a} - \alpha_j)}{(\hat{\sigma}_{j,a}^2)^2} + \frac{\mathbf{z}_{j,a}\mathbf{z}_{j,b}(\mathbf{y}_{j,b} - \alpha_j)}{(\hat{\sigma}_{j,a}^2)^2} \right] \\ &amp;= -\frac{1}{2 \left( 1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2 } \left[ \sum_{a = 1}^n \frac{\mathbf{z}_{j,a}(\mathbf{y}_{j, a} - \alpha_j)}{(\hat{\sigma}_{j,a}^2)^2} \sum_{b = 1}^n \mathbf{z}_{j,b} + \sum_{b = 1}^n \frac{\mathbf{z}_{j,b}(\mathbf{y}_{j,b} - \alpha_j)}{(\hat{\sigma}_{j,b}^2)^2} \sum_{a = 1}^n \mathbf{z}_{j,a}\right] \\ &amp;= -\frac{1}{\left( 1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2 } \sum_{a = 1}^n \frac{\mathbf{z}_{j,a}(\mathbf{y}_{j, a} - \alpha_j)}{(\hat{\sigma}_{j,a}^2)^2} \left(\sum_{b = 1}^n \mathbf{z}_{j,b} \right) \end{aligned}\] <p>Now, we find the vector of second derivatives of the log-likelihood with respect to the components of $\alpha$:</p> \[\begin{aligned} \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j^2} &amp;= \frac{\partial}{\partial \alpha_j} \left[ (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \Sigma_{y_j}^{-1} \mathbf{1}_n\right] \\ &amp;= - \mathbf{1}_n^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ \frac{\partial^2 \ell(\theta; \mathbf{y})}{ \partial \alpha_{j'} \partial \alpha_j} &amp;= \frac{\partial}{\partial \alpha_{j'}} \left[ (\mathbf{y}_j - \alpha_j \mathbf{1}_n)^\top \Sigma_{y_j}^{-1} \mathbf{1}_n\right] \\ &amp;= 0 \\ \implies \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j \partial \alpha} &amp;= \begin{bmatrix} 0 \\ \vdots \\ - \mathbf{1}_n^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ \vdots \\ 0 \end{bmatrix} \end{aligned}\] <p>Putting the two above results together gives us:</p> \[\frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j \partial \theta} = \begin{bmatrix} 0 \\ \vdots \\ - \mathbf{1}_n^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ \vdots \\ 0 \\ -\frac{1}{\left( 1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2 } \sum_{a = 1}^n \frac{\mathbf{z}_{j,a}(\mathbf{y}_{j, a} - \alpha_j)}{(\hat{\sigma}_{j,a}^2)^2} \left(\sum_{b = 1}^n \mathbf{z}_{j,b} \right) \end{bmatrix}\] </li> </ul> <h4 id="expectations-1">Expectations</h4> <p>We now take the expectation of the above vectors. We’ll evaluate the second order partial derivatives with respect to $\tau^2$ first.</p> <ul id="info-tau-tau-2" class="tab" data-tab="55247241-10d4-45f6-a0c9-8afebaf95492" data-name="info-tau-tau-2"> <li class="active" id="info-tau-tau-2-equation"> <a href="#">equation </a> </li> <li id="info-tau-tau-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="55247241-10d4-45f6-a0c9-8afebaf95492" data-name="info-tau-tau-2"> <li class="active"> \[\mathbb{E}\left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \tau^2 \partial \theta } \right] = \begin{bmatrix} 0 \\ \vdots \\ 0 \\ \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\hat{\sigma}^2_{i,l})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} \right)^2} \right] \end{bmatrix}\] </li> <li> \[\begin{aligned} \mathbb{E}\left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial (\tau^2)^2 }\right] &amp;= \mathbb{E}\left[ \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\hat{\sigma}^2_{i,l})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} \right)^2} - 2 \sum_{a = 1}^n \sum_{b = 1}^n (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i) \left(\frac{\mathbf{z}_{i,a}\mathbf{z}_{i,b}(\hat{\sigma}^2_{i,a})^2 \left(\sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}} \right)}{(\hat{\sigma}^2_{i,l})^4 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}} \right)^3}\right) \right] \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\hat{\sigma}^2_{i,l})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} \right)^2} - 2 \sum_{a = 1}^n \sum_{b = 1}^n \mathbb{E}\left[ (\mathbf{y}_{i,a} - \alpha_i)(\mathbf{y}_{i,b} - \alpha_i)\right] \left(\frac{\mathbf{z}_{i,a}\mathbf{z}_{i,b}(\hat{\sigma}^2_{i,a})^2 \left(\sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}} \right)}{(\hat{\sigma}^2_{i,l})^4 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}} \right)^3}\right) \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\hat{\sigma}^2_{i,l})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} \right)^2} - 2 \sum_{a = 1}^n \sum_{b = 1}^n \text{Cov}\left(\mathbf{y}_{i,a}, \mathbf{y}_{i,b} \right) \left(\frac{\mathbf{z}_{i,a}\mathbf{z}_{i,b}(\hat{\sigma}^2_{i,a})^2 \left(\sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}} \right)}{(\hat{\sigma}^2_{i,l})^4 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}}{\hat{\sigma}^2_{i,l}} \right)^3}\right) \right] \\ &amp;= \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\hat{\sigma}^2_{i,l})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} \right)^2} \right] &amp; \left(\text{obs. ind. under } H_0 \right) \\ \mathbb{E}\left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \tau^2 \partial \alpha_j}\right] &amp;= \mathbb{E}\left[ - \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2} \left(\sum_{l' = 1}^n \frac{\mathbf{z}_{j,l'}(\mathbf{y}_{j,l'} - \alpha_j)}{(\hat{\sigma}^2_{j,l'})^2}\right)\left(\sum_{l'' = 1}^n \mathbf{z}_{j,l''}\right) \right] \\ &amp;= - \frac{1}{\left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2} \left(\sum_{l' = 1}^n \frac{\mathbf{z}_{j,l'} \mathbb{E}\left[ (\mathbf{y}_{j,l'} - \alpha_j)\right] }{(\hat{\sigma}^2_{j,l'})^2}\right)\left(\sum_{l'' = 1}^n \mathbf{z}_{j,l''}\right) \\ &amp;= 0 \end{aligned}\] </li> </ul> <p>And then we do the same for $\alpha_j$:</p> <ul id="info-alpha-alpha-2" class="tab" data-tab="26233ff0-8f61-4a8d-a943-f5d2e4d26165" data-name="info-alpha-alpha-2"> <li class="active" id="info-alpha-alpha-2-equation"> <a href="#">equation </a> </li> <li id="info-alpha-alpha-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="26233ff0-8f61-4a8d-a943-f5d2e4d26165" data-name="info-alpha-alpha-2"> <li class="active"> \[\mathbb{E}\left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j \partial \theta } \right] = \begin{bmatrix} 0 \\ \vdots \\ - \mathbf{1}_n^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ \vdots \\ 0 \\ 0 \end{bmatrix}\] </li> <li> \[\begin{aligned} \mathbb{E}\left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j \partial \tau^2} \right] &amp;= \mathbb{E}\left[ -\frac{1}{\left( 1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2 } \sum_{a = 1}^n \frac{\mathbf{z}_{j,a}(\mathbf{y}_{j, a} - \alpha_j)}{(\hat{\sigma}_{j,a}^2)^2} \left(\sum_{b = 1}^n \mathbf{z}_{j,b} \right) \right] \\ &amp;= -\frac{1}{\left( 1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{j,l}}{\hat{\sigma}^2_{j,l}}\right)^2 } \sum_{a = 1}^n \frac{\mathbf{z}_{j,a} \mathbb{E}\left[\mathbf{y}_{j, a} - \alpha_j\right] }{(\hat{\sigma}_{j,a}^2)^2} \left(\sum_{b = 1}^n \mathbf{z}_{j,b} \right) \\ &amp;= 0 \\ \mathbb{E}\left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j^2} \right] &amp;= \mathbb{E}\left[ - \mathbf{1}_n^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \right] \\ &amp;= - \mathbf{1}_n^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ \mathbb{E}\left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j \partial \alpha_{j'}} \right] &amp;= 0 \end{aligned}\] </li> </ul> <p>We then evaluate the Fisher information at the MLE:</p> <ul id="info-2" class="tab" data-tab="6242d5e4-89e5-4e3c-b5e0-a18f31583038" data-name="info-2"> <li class="active" id="info-2-equation"> <a href="#">equation </a> </li> <li id="info-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="6242d5e4-89e5-4e3c-b5e0-a18f31583038" data-name="info-2"> <li class="active"> \[\mathcal{I}_{\theta, \theta}(\hat{\theta}) = \begin{bmatrix} - \mathbf{1}_n^\top\text{diag}\left(\left[ \frac{1}{\hat{\sigma}^2_{1,1}}, \dots, \frac{1}{\hat{\sigma}^2_{1,n}}\right]\right) \mathbf{1}_n &amp; 0 &amp; \dots &amp; 0 \\ 0 &amp; - \mathbf{1}_n^\top \text{diag}\left(\left[ \frac{1}{\hat{\sigma}^2_{2,1}}, \dots, \frac{1}{\hat{\sigma}^2_{2,n}}\right]\right) \mathbf{1}_n &amp; \dots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \dots &amp; \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\hat{\sigma}^2_{i,l})^2} \right] \end{bmatrix}\] </li> <li> <p>Note that:</p> \[\Sigma_{y_i}^{-1} \bigg\rvert_{\theta = \hat{\theta}} = \text{diag}\left(\left[ \frac{1}{\hat{\sigma}^2_{i,1}}, \dots, \frac{1}{\hat{\sigma}^2_{i,n}}\right]\right)\] <p>Then we have:</p> \[\begin{aligned} - \mathbb{E}\left. \left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \tau^2 \partial \theta} \right] \right\rvert_{\theta = \hat{\theta}} &amp;= - \mathbb{E} \left.\left[ \begin{bmatrix} 0 \\ \vdots \\ 0 \\ \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\hat{\sigma}^2_{i,l})^2 \left(1 + \tau^2 \sum_{l = 1}^n \frac{\mathbf{z}_{i,l}^2}{\hat{\sigma}^2_{i,l}} \right)^2} \right] \end{bmatrix} \right] \right\rvert_{\theta = \hat{\theta}} \\ &amp;= \begin{bmatrix} 0 \\ \vdots \\ 0 \\ \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\hat{\sigma}^2_{i,l})^2} \right] \end{bmatrix} \\ - \mathbb{E} \left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \alpha_j \partial \theta} \right] \bigg\rvert_{\theta = \hat{\theta}} &amp;= - \mathbb{E} \left. \left[ \begin{bmatrix} 0 \\ \vdots \\ - \mathbf{1}_n^\top \Sigma_{y_j}^{-1} \mathbf{1}_n \\ \vdots \\ 0 \\ 0 \end{bmatrix} \right] \right\rvert_{\theta = \hat{\theta}} \\ &amp;= \begin{bmatrix} 0 \\ \vdots \\ - \mathbf{1}_n^\top \text{diag}\left(\left[ \frac{1}{\hat{\sigma}^2_{j,1}}, \dots, \frac{1}{\hat{\sigma}^2_{j,n}}\right]\right) \mathbf{1}_n \\ \vdots \\ 0 \\ 0 \end{bmatrix} \end{aligned}\] <p>Putting these together into a big matrix:</p> \[\begin{aligned} \mathcal{I}_{\theta, \theta}(\hat{\theta}) &amp;= - \mathbb{E}\left[ \frac{\partial^2 \ell(\theta; \mathbf{y})}{\partial \theta \partial \theta^\top} \right] \bigg\rvert_{\theta = \hat{\theta}} \\ &amp;= \begin{bmatrix} - \mathbf{1}_n^\top\text{diag}\left(\left[ \frac{1}{\hat{\sigma}^2_{1,1}}, \dots, \frac{1}{\hat{\sigma}^2_{1,n}}\right]\right) \mathbf{1}_n &amp; 0 &amp; \dots &amp; 0 \\ 0 &amp; - \mathbf{1}_n^\top \text{diag}\left(\left[ \frac{1}{\hat{\sigma}^2_{2,1}}, \dots, \frac{1}{\hat{\sigma}^2_{2,n}}\right]\right) \mathbf{1}_n &amp; \dots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \dots &amp; \frac{1}{2} \sum_{i = 1}^k \left[ \sum_{l = 1}^n \sum_{j = 1}^n \frac{\mathbf{z}_{i,l}^2\mathbf{z}_{i,j}^2}{(\hat{\sigma}^2_{i,l})^2} \right] \end{bmatrix} \end{aligned}\] </li> </ul> <hr/> ]]></content><author><name></name></author><category term="glmm"/><category term="glmm"/><category term="information"/><category term="score"/><summary type="html"><![CDATA[Calculations for GLMMs]]></summary></entry><entry><title type="html">One-Sided Score Test</title><link href="https://aerosengart.github.io/blog/2025/score-test-one-sided/" rel="alternate" type="text/html" title="One-Sided Score Test"/><published>2025-02-18T00:00:00+00:00</published><updated>2025-02-18T00:00:00+00:00</updated><id>https://aerosengart.github.io/blog/2025/score-test-one-sided</id><content type="html" xml:base="https://aerosengart.github.io/blog/2025/score-test-one-sided/"><![CDATA[<p>In many cases, we may want to test the null hypothesis that a parameter is zero against a one-sided alternative (e.g. the parameter is non-negative). In this setting, we are constraining the alternative parameter space, and for some parameters (such as variance components), the value of the parameter under the null may be on the boundary.</p> <p>In this post, I’ll cover some of the literature on tests against one-sided alternatives. We’ll mostly be in the mindset of tests of homogeneity in mixed models (i.e. testing whether the random effects variance is zero), but a lot of these results are generally applicable.</p> <hr/> <h2 id="some-intuition">Some Intuition</h2> <p>First, let’s discuss some of the intuition behind the score test (also called Rao’s test (after C. R. Rao) and the Lagrange multiplier test). Suppose we have some model or data-generating process parametrized by $\theta$. Our goal will be to test:</p> \[H_0: \theta = \theta_0\] <p>Let $\ell(\theta; \mathbf{X})$ denote the log-likelihood function for parameter $\theta$ given data $\mathbf{X}$. We can imagine that if our maximum likelihood estimate, $\hat{\theta}$, is far from $\theta_0$, then our data provide evidence against $H_0$.</p> <p>The score test uses the slope of the log-likelihood (i.e. the derivative), called the <i>score</i> to determine what it means for $\hat{\theta}$ to be far from $\theta_0$. If the derivative is quite large (in absolute value) at $\theta_0$, then that implies that we have moved quite far away from the root of the log-likelihood, $\hat{\theta}$.</p> <p>Under the assumption that the log-likelihood is partially differentiable w.r.t. each component of $\theta$ and the Fisher information exists and is invertible at $\theta_0$, the standard score test statistic for i.i.d. sample $\mathbf{X} = (\mathbf{X}_1, \dots, \mathbf{X}_n)$ is computed as:</p> \[t = \frac{1}{n} U^\top(\theta_0) \mathcal{I}^{-1} (\theta_0) U(\theta_0)\] <p>where</p> \[U(\theta^*) = \sum_{i = 1}^n \frac{\partial}{\partial \theta} \ell(\theta; \mathbf{X}_i) \rvert_{\theta = \theta^*}; \hspace{8mm} \mathcal{I}(\theta^*) = \mathbb{E}\left[ U(\theta) U^\top(\theta) \right] \bigg\rvert_{\theta = \theta^*}\] <ul id="claim-1" class="tab" data-tab="aff674b3-620c-4d19-a215-51f6e47dcd01" data-name="claim-1"> <li class="active" id="claim-1-claim"> <a href="#">claim </a> </li> <li id="claim-1-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="aff674b3-620c-4d19-a215-51f6e47dcd01" data-name="claim-1"> <li class="active"> <p>Assuming $U(\theta)$ has finite variance and mean zero (which it will under certain conditions — see <a href="/posts/2025/02/02/likelihood-theory.html">this post on likelihood theory</a>), it is asymptotically multivariate Gaussian when suitably centered and scaled (by the central limit theorem).<d-cite key="dasgupta2008"></d-cite></p> </li> <li> <p>Notice that $U(\theta_0)$ is the sum of (functions of) i.i.d. random variables. As we noted above, $U(\theta_0)$ has mean zero under certain regularity conditions. The CLT states that:</p> \[\frac{\sqrt{n}\left(\frac{1}{n} \sum_{i = 1}^n X_i - \mu \right) }{\sigma} \rightsquigarrow \mathcal{N}(0, 1) \hspace{4mm} \iff \hspace{4mm} (\frac{1}{n} \sum_{i = 1}^n X_i - \mu_X) \rightsquigarrow \mathcal{N}\left(0, \frac{\sigma^2}{n}\right) \nonumber\] <p>If $\mathcal{I}(\theta)$ is the covariance of $U(\theta)$, then $n^2 \mathcal{I}(\theta)$ is the covariance of $\frac{1}{n}U(\theta)$. Thus:</p> \[\sqrt{n} \left(\frac{1}{n} U^\top(\theta)\right) \rightsquigarrow \mathcal{N}\left(\mathbf{0}, \mathcal{I}(\theta) \right) \hspace{4mm} \iff \hspace{4mm} \sqrt{n} \left(\frac{1}{n} U^\top(\theta)\right)\mathcal{I}^{-1/2}(\theta) \rightsquigarrow \mathcal{N}\left(\mathbf{0}, \mathbb{I}\right) \nonumber\] <p>This can then be used to derive the asymptotic null distribution of the score test statistic, since the distribution of a squared Gaussian random variable is $\chi^2$:</p> \[\left( \sqrt{n} \left(\frac{1}{n} U^\top(\theta)\right)\mathcal{I}^{-1/2}(\theta)\right)^2 \rightsquigarrow \chi^2_k \hspace{4mm} \iff \hspace{4mm} \frac{1}{n} U^\top(\theta) \mathcal{I}^{-1}(\theta) U(\theta) \rightsquigarrow \chi^2_k\] <p>where $k$ is the dimension of $\theta$.</p> </li> </ul> <p>The alternative for the above test is implicitly two-sided: $H_1: \theta \neq \theta_0$.</p> <hr/> <h2 id="introduction">Introduction</h2> <p>In our setting, it is important to consider a one-sided alternative because variances are non-negative. As pointed out by Hall and Praestgaard<d-cite key="hall2001"></d-cite> , omnibus tests like that of Lin (1997)<d-cite key="lin1997"></d-cite> , which implicitly test against a two-sided alternative, have power against these impossible cases. They also explain that the claim that Lin’s global test for homogeneity is locally asymptotically most stringest does not hold in particular boundary cases. Thus, it seems worthwhile to pursue explicitly one-sided tests, most of which are related to cones.</p> <h3 id="all-about-cones">All About Cones</h3> <p>As an introduction into the theoretical/geometric setting we are interested in, I will cover some of the background and results in Shapiro<d-cite key="shapiro1988"></d-cite> This will give us a better foundation for what’s to come.</p> <p>Shapiro restricts his attention to closed and convex cones, so I’ll do the same for the rest of this section.</p> <aside><p>A set is <i>closed</i> if it contains all of its boundary points.</p></aside> <p>A <i>cone</i>, $C$, in $\mathbb{R}^m$ is defined as the set \(C := \\{ t\mathbf{x} \in C \rvert \mathbf{x} \in C \\}\) for any $t &gt; 0$. $C$ is called a <i>pointed cone</i> if $\mathbf{x} \in C$ and $-\mathbf{x} \in C$ implies $\mathbf{x} = \mathbf{0}$, the zero vector.</p> <aside><p>A set, $S$, is <i>convex</i> if $tx + (1-t)y \in S$ for $x,y \in S$ and $t \in [0, 1]$.</p></aside> <p>Shapiro denotes the orthogonal projection of a point onto $C$ with</p> \[P(\mathbf{x}, C) = \underset{\eta \in C}{\arg\min} \left \{ (\mathbf{x} - \eta)^\top \mathbf{U} (\mathbf{x} - \eta) \right \}\] <p>where $\mathbf{U}$ is any positive-definite matrix. This orthogonal projection maps the input point, $\mathbf{x}$, to the closest point on $C$, $\eta$, where closeness is defined by the norm associated with the matrix $\mathbf{U}$. We’ll denote this norm with $\rvert \rvert \mathbf{x} \rvert \rvert = \sqrt{\mathbf{x}^\top \mathbf{U} \mathbf{x}}$, and we’ll use $\langle\mathbf{x}, \mathbf{y}\rangle = \mathbf{x}^\top \mathbf{U}\mathbf{y}$ to denote the inner product of $\mathbf{x}$ and $\mathbf{y}$ associated with $\mathbf{U}$.</p> <p>This brings us to the <i>dual</i> cone, which I have very little intuition…</p> <div id="dual-cone"></div> <blockquote> <p><strong>Definition (Dual Cone).</strong> <br/> The <i>dual cone</i> is the set $C^0= \{ \mathbf{y} \rvert \langle \mathbf{x}, \mathbf{y} \rangle \leq 0 \hspace{2mm} \forall \mathbf{x} \in C \}$. If $C$ is a vector subspace, then $C^0$ is the orthogonal complement of $C$, and if $C$ is closed and convex, then $(C^0)^0 = C$.</p> </blockquote> <p>If we have another convex cone $K$, then:</p> \[\rvert \rvert \mathbf{x} - P(\mathbf{x}, C) \rvert \rvert^2 = \rvert \rvert \mathbf{x} - P(\mathbf{x}, K) \rvert \rvert^2 + \rvert \rvert P(\mathbf{x}, K) - P(\mathbf{x}, C) \rvert \rvert^2 \nonumber\] <p>if $C$ or $K$ is a lienar space and $C \subset K$.</p> <p>Furthermore, if $C$ is a vector subspace, then $\mathbf{x} - P(\mathbf{x}, C) = P(\mathbf{x}, C^0)$. That is, the difference between $\mathbf{x}$ and the orthogonal projection of $\mathbf{x}$ onto $C$ is equivalent to its orthogonal projection onto the dual cone $C^0$.</p> <div id="chi-bar-squared"></div> <blockquote> <p><strong>Definition ($\bar{\chi}^2$-Statistic).</strong> <br/> Let $\mathbf{y} \sim \mathcal{N}(\mathbf{0}, \mathbf{V})$ be a Gaussian random vector of $m$ dimensions with some covariance matrix, $\mathbf{V}$, and let $C$ be a convex cone. A <i>$\bar{\chi}^2$-statistic</i> is given by the following: \(\begin{aligned} \bar{\chi}^2 &amp;= \mathbf{y}^\top \mathbf{V}^{-1} \mathbf{y} - \underset{\eta \in C}{\min} \left\{ (\mathbf{y} - \eta)^\top \mathbf{V}^{-1}(\mathbf{y} - \eta) \right\} \\ &amp;= \mathbf{y}^\top \mathbf{V}^{-1} \mathbf{y} - (\mathbf{y} - P(\mathbf{y}, C))^\top \mathbf{V}^{-1}(\mathbf{y} - P(\mathbf{y}, C)) \\ &amp;= (\mathbf{y} - \mathbf{y} + P(\mathbf{y}, C))^\top \mathbf{V}^{-1}(\mathbf{y} - \mathbf{y} + P(\mathbf{y}, C)) \\ &amp;= \rvert \rvert P(\mathbf{y}, C) \rvert \rvert^2 \end{aligned} \nonumber\) where in the above, the inner products/norms are taken using the matrix $\mathbf{V}^{-1}$.</p> </blockquote> <p>The chi-bar-squared statistic follows a mixture of $\chi^2$ distributions:</p> \[\mathbb{P}(\bar{\chi}^2 \geq c) = \sum_{i = 1}^m w_i \mathbb{P}(\chi_i^2 \geq c) \nonumber\] <p>where $\chi_i^2$ is a $\chi^2$ random variable with $i$ degrees of freedom, and $w_i$ are individuals weights that sum to $1$. As is standard, we let $\chi^2_0$ be a point mass at $0$. We’ll denote this mixture distribution as $\mathcal{\bar{X}}^2(\mathbf{V}, C)$, since it depends on both $\mathbf{V}$ and the cone, $C$.</p> <p>Note that, if we have the dual cone to $C$, then we can ignore the first term in the test statistic equation to get:</p> \[\bar{\chi}^2 = \underset{\eta \in D}{\min} \left\{ (\mathbf{y} - \eta)^\top \mathbf{V}^{-1} (\mathbf{y} - \eta) \right\} \nonumber\] <p>which follows a $\mathcal{\bar{X}}^2(\mathbf{V}, C^0)$ distribution. We can also note that the mixture’s weights satisfy:</p> \[w_i(m, \mathbf{V}, C^0) = w_{m - i}(m, \mathbf{V}, C) \hspace{15mm} i = 0, \dots, m\] <hr/> <h2 id="a-multivariate-one-sided-test">A Multivariate One-Sided Test</h2> <p>Some of the later literature is based upon mid-century work by Kudô (1963)<d-cite key="kudo1963"></d-cite> for testing the mean vector of a multivariate Gaussian distribution. Kudô’s work has some very nice geometric interpretations that permit the derivation of the limiting distribution of his test statistic, and all of it begins with the likelihood ratio.</p> <h3 id="set-up">Set-Up</h3> <p>We have a multivariate Gaussian population with mean vector $\theta^\top = (\theta_1, \dots, \theta_k)$ and known, non-singular, positive definite variance-covariance matrix $\Sigma$. Given a sample of $n$ i.i.d. observations $\mathbf{X} = { \mathbf{X}^{(1)}, \dots, \mathbf{X}^{(n)} }$, we wish to test:</p> \[H_0: \theta_i = 0 \hspace{2mm} i = 1, 2, \dots, k \hspace{8mm} \text{vs.} \hspace{8mm} H_1: \theta_i \geq 0 \hspace{2mm} i = 1, 2, \dots, k\] <p>where at least one inequality in the alternative hypothesis setting is strict.</p> <p>Letting $\bar{\mathbf{X}} = \frac{1}{n} \sum_{i = 1}^n \mathbf{X}^{(i)}$, the sample mean vector, we can rewrite the likelihood of the sample under $H_1$ as:</p> \[\begin{aligned} \mathcal{L}(\theta, \Sigma; \mathbf{X}) &amp;= \frac{1}{(\sqrt{2 \pi})^{kn} \rvert \Sigma \rvert^n} \exp\left(- \frac{1}{2} \sum_{i = 1}^n (\mathbf{X}^{(i)} - \theta)^\top \Sigma^{-1} (\mathbf{X}^{(i)} - \theta) \right) \\ &amp;= \frac{1}{(\sqrt{2 \pi})^{kn} \rvert \Sigma \rvert^n} \exp\left(- \frac{1}{2} \sum_{i = 1}^n \left[ (\mathbf{X}^{(i)} - \bar{\mathbf{X}})^\top \Sigma^{-1} (\mathbf{X}^{(i)} - \bar{\mathbf{X}}) + n(\bar{\mathbf{X}} - \theta)^\top \Sigma^{-1}(\bar{\mathbf{X}} - \theta) \right] \right) \\ \end{aligned}\] <p>The standard likelihood ratio test statistic is given by:</p> \[\begin{aligned} t_{LRT} &amp;= \frac{\underset{\theta_i = 0 \\ i = 1, \dots, k}{\max} \{ \mathcal{L}(\theta, \Sigma; \mathbf{X}) \}}{\underset{\theta_i \geq 0 \\ i = 1, \dots, k}{\max} \{ \mathcal{L}(\theta, \Sigma; \mathbf{X}) \}} = \frac{\exp\left(-\frac{1}{2} n \bar{\mathbf{X}}^\top \Sigma^{-1}\bar{\mathbf{X}}\right)}{\underset{\theta_i \geq 0 \\ i = 1, \dots, k}{\max} \{ \exp\left(-\frac{1}{2}n(\bar{\mathbf{X}} - \theta)^\top \Sigma^{-1}(\bar{\mathbf{X}} - \theta)\right) \}} \end{aligned}\] <ul id="claim-2" class="tab" data-tab="8cdec507-e9cc-49bf-baad-a2c177c30360" data-name="claim-2"> <li class="active" id="claim-2-claim"> <a href="#">claim </a> </li> <li id="claim-2-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="8cdec507-e9cc-49bf-baad-a2c177c30360" data-name="claim-2"> <li class="active"> <p>Notice that the argument maximum of the above is equivalent to:</p> \[\bar{\chi}^2 = n \left[ \bar{\mathbf{X}}^\top \Sigma^{-1}\bar{\mathbf{X}} - \underset{\theta_i \geq 0 \\ i = 1, \dots, k}{\arg\min} \left\{ (\bar{\mathbf{X}} - \theta)^\top \Sigma^{-1}(\bar{\mathbf{X}} - \theta) \right\} \right]\] </li> <li> \[\begin{aligned} \theta^* &amp;= \frac{\exp\left(-\frac{1}{2} n \bar{\mathbf{X}}^\top \Sigma^{-1}\bar{\mathbf{X}}\right)}{\underset{\theta_i \geq 0 \\ i = 1, \dots, k}{\arg\max} \{ \exp\left(-\frac{1}{2}n(\bar{\mathbf{X}} - \theta)^\top \Sigma^{-1}(\bar{\mathbf{X}} - \theta)\right) \}} \\ &amp;= \underset{\theta_i \geq 0 \\ i = 1, \dots, k}{\arg\min} \left\{ \frac{\exp\left(-\frac{1}{2} n \bar{\mathbf{X}}^\top \Sigma^{-1}\bar{\mathbf{X}}\right)}{\exp\left(-\frac{1}{2}n(\bar{\mathbf{X}} - \theta)^\top \Sigma^{-1}(\bar{\mathbf{X}} - \theta)\right)} \right\} &amp; \left(\text{maximize denom. = minimize quotient} \right) \\ &amp;= \underset{\theta_i \geq 0 \\ i = 1, \dots, k}{\arg\min} \left\{ \exp\left( -\frac{1}{2} n \bar{\mathbf{X}}^\top \Sigma^{-1}\bar{\mathbf{X}} + \frac{1}{2}n(\bar{\mathbf{X}} - \theta)^\top \Sigma^{-1}(\bar{\mathbf{X}} - \theta) \right) \right\} \\ &amp;= \underset{\theta_i \geq 0 \\ i = 1, \dots, k}{\arg\min} \left\{ -\frac{1}{2} n \bar{\mathbf{X}}^\top \Sigma^{-1}\bar{\mathbf{X}} + \frac{1}{2}n(\bar{\mathbf{X}} - \theta)^\top \Sigma^{-1}(\bar{\mathbf{X}} - \theta) \right\} &amp; \left(\exp \text{ monotonic} \right)\\ &amp;= n \underset{\theta_i \geq 0 \\ i = 1, \dots, k}{\arg\min} \left\{ -\bar{\mathbf{X}}^\top \Sigma^{-1}\bar{\mathbf{X}} + (\bar{\mathbf{X}} - \theta)^\top \Sigma^{-1}(\bar{\mathbf{X}} - \theta) \right\} \\ &amp;= n \underset{\theta_i \geq 0 \\ i = 1, \dots, k}{\arg\max} \left\{ \bar{\mathbf{X}}^\top \Sigma^{-1}\bar{\mathbf{X}} - (\bar{\mathbf{X}} - \theta)^\top \Sigma^{-1}(\bar{\mathbf{X}} - \theta) \right\} &amp; \left( \text{minimize = maximize negative} \right)\\ &amp;= n \left[ \bar{\mathbf{X}}^\top \Sigma^{-1}\bar{\mathbf{X}} - \underset{\theta_i \geq 0 \\ i = 1, \dots, k}{\arg\min} \left\{ (\bar{\mathbf{X}} - \theta)^\top \Sigma^{-1}(\bar{\mathbf{X}} - \theta) \right\} \right] &amp; \left(\text{ maximize quantity = minimize positive subtraction} \right) \end{aligned}\] </li> </ul> <hr/> <h2 id="a-one-sided-score-test">A One-Sided Score Test</h2> <p>I’ll next dip into the work of Silvapulle and Silvapulle[^fn-silvapulle], who present a score-type test statistic for one-side alternative hypotheses based upon estimating functions instead of the true score function.</p> <h3 id="set-up-1">Set-Up</h3> <p>The authors present a fairly general setting where we do not assume we know the exact form of the distribution of the observations, only that they depend on some $k \times 1$-dimensional vector-valued parameter, $\theta$, that is partitioned into the nuisance parameters, $\lambda$, and the components of interest, $\psi$. We’ll write the partitioned parameter vector as $(\lambda : \psi) = (\lambda^\top, \psi^\top)^\top$ where $\lambda$ is $(k - q) \times 1$ and $\psi$ is $q \times 1$.</p> <p>We’re interested in testing hypotheses of the form:</p> \[H_0: \psi = \mathbf{0}_q \hspace{10mm} H_A: \psi \in \mathcal{C} \nonumber\] <p>where $\mathbf{0}_q$ is a $q$-dimensional vector of zeros and $\mathcal{C}$ is the $q$-dimensional Euclidean space, or a closed, convex cone in $q$-dimensional Euclidean space with its vertex at the origin. The latter case encompasses alternatives of the form $\psi \geq \mathbf{0}_q$, which is the alternative hypothesis I am interested in for variance component testing.</p> <p>Define $\mathbf{D}(\lambda)$ as some (fixed) matrix-valued function of the nuisance parameter, $\lambda$, that is independent of some (potentially vector-valued) random variable, $\delta$. Also define the $q \times 1$ vector, $\mathbf{U}_0$, as some function of the data.</p> <p>Suppose under the sequence of alternatives $K_n: \psi = n^{-1/2}\delta$, $\mathbf{U}_0$ satisfies:</p> <p>\begin{equation} \label{eq:U-condition} \mathbf{U}_0 \rightsquigarrow \mathcal{N}(\delta, \mathbf{D}(\lambda)) \end{equation}</p> <p>as our sample size $n \rightarrow \infty$. Notice that testing the alternative hypothesis that $\psi \geq 0$ is equivalent to testing $\delta \geq \mathbf{0}_q$, which implies that the null hypothesis is equivalent to $\delta = \mathbf{0}_q$.</p> <h3 id="a-general-test-statistic">A General Test Statistic</h3> <p>The authors define a very general test statistic as the following.</p> <blockquote> <p><strong>Definition (Test Statistic).</strong> <br/> Let $\tilde{\mathbf{D}}(\lambda)$ be any consistent estimator under the null hypothesis of $\mathbf{D}(\lambda)$, the asymptotic covariance matrix of $\mathbf{U}_0$. The test statistic for $H_0: \psi = \mathbf{0}$ against $H_A: \psi \in \mathcal{C}$ has the form: \(T = \mathbf{U}_0^\top \tilde{\mathbf{D}}(\lambda)^{-1}\mathbf{U}_0 - \underset{\mathbf{b} \in \mathcal{C}}{\inf} \left\{ (\mathbf{U}_0 - \mathbf{b})^\top \tilde{\mathbf{D}}(\lambda)^{-1}(\mathbf{U}_0 - \mathbf{b})\right\}\)</p> </blockquote> <p>A $p$-value for large sample sizes can be found by defining $\mathbf{Z} \sim \mathcal{N}(\mathbf{0}, \mathbf{D}(\lambda))$ and:</p> <p>\begin{equation} \label{eq:xi-defn} \xi(t, \mathbf{D}(\lambda), \mathcal{C}) = \mathbb{P}\left( \left[ \mathbf{Z}^\top \mathbf{D}(\lambda)^{-1} \mathbf{Z} - \underset{\mathbf{b} \in \mathcal{C}}{\inf} \left\{ (\mathbf{Z} - \mathbf{b})^\top \mathbf{D}(\lambda)^{-1}(\mathbf{Z} - \mathbf{b}) \right\} \right] \geq t \right) \end{equation}</p> <p>The quantity $1 - \xi(t, \mathbf{D}(\lambda), \mathcal{C})$ follows a chi-bar-squared distribution; that is, a mixture of chi-squared distributions as we introduced in the previous section.[^fn-shapiro] The weights for the mixture can be hard to find, but we can get around this using the fact that, for large enough $n$ and under $H_0$ (i.e. $\delta = \mathbf{0}$), $\mathbf{U}_0$ is approximately $\mathcal{N}(\mathbf{0}, \mathbf{D}(\lambda))$. Thus, $\mathbb{P}(T \geq t; \lambda) \approx \xi(t, \mathbf{D}(\lambda), \mathcal{C})$.</p> <p>Suppose we observe a value of $T$, $t^*$. Define \(\mathbf{D}^*(\lambda)\) as a consistent estimator of $\mathbf{D}(\lambda)$ for any $\lambda$. Then it follows that:</p> \[p \approx \underset{\lambda}{\sup} \left\{ \xi(t^*, \mathbf{D}^*(\lambda), \mathcal{C}) \right\} \nonumber\] <p>for large enough $n$ because $\lambda$ is a nuisance parameter, so we can take the “best” probability over all of its values.</p> <h3 id="use">Use</h3> <p>How do we use this test statistic in practice? This is pretty much just a question of what function, $\mathbf{U}_0$, of our data we want to pick. Silvapulle and Silvapulle explain how to construct a score-type test statistic using their general statistic.</p> <p>Let’s define $\mathbf{S}_n(\theta)$ as any $k \times 1$ vector estimating equation (so it should have expectation zero) for $\theta$ (e.g. the score function or something else). We need this vector to satisfy a couple of conditions.</p> <h4 id="conditions">Conditions</h4> <p>Suppose $\mathbf{S}_n(\theta)$ is such that there exist non-singular $\mathbf{G}(\theta)$ and $\mathbf{V}(\theta)$ satisfying for any $a &gt; 0$:</p> <p>\begin{equation} \label{eq:condition-a1} \frac{1}{\sqrt{n}}\mathbf{S}_n(\theta) \rightsquigarrow \mathcal{N}(\mathbf{0}, \mathbf{V}(\theta)) \end{equation}</p> <p>and</p> <p>\begin{equation} \label{eq:condition-a2} \underset{\rvert \rvert \mathbf{h} \rvert \rvert \leq a}{\sup} \left\{ \frac{1}{\sqrt{n}} \left( \mathbf{S}_n\left(\theta + \frac{1}{\sqrt{n}} \mathbf{h}\right) - \mathbf{S}_n(\theta) \right) + \mathbf{G}(\theta) \mathbf{h} \right\} = o_p(1) \end{equation}</p> <p>where $o_p(1)$ is stochastic order notation for convergence in probability to $0$.</p> <p>The first condition basically states that $\mathbf{S}_n(\theta)$ is asymptotically Gaussian when suitably scaled. Let’s take a closer look at the second condition (this will be kind of hand-wavy).</p> <p>Suppose we fix $\mathbf{h}$. The directional derivative of $\mathbf{S}_n(\theta)$ at $\theta$ along $\mathbf{h}$ is given by the limit:</p> \[\nabla_{\mathbf{h}} \mathbf{S}_n(\theta) = \underset{s \rightarrow 0}{\lim} \left[ \frac{\mathbf{S}_n(\theta + s \mathbf{h}) - \mathbf{S}_n(\theta)}{s \rvert \rvert \mathbf{h} \rvert \rvert} \right]\] <p>Technically, this is the definition for a scalar function, but we can just use the above notation to mean the limits are taken element-wise to get the result for a vector-valued function.</p> <p>If we let $s = \frac{1}{\sqrt{n}}$, then we can rewrite the above as the limit as $n \rightarrow \infty$:</p> \[\nabla_{\mathbf{h}} \mathbf{S}_n(\theta) = \underset{n \rightarrow \infty}{\lim} \left[ \frac{\sqrt{n}}{\rvert \rvert \mathbf{h} \rvert \rvert} \left( \mathbf{S}_n \left(\theta + \frac{1}{\sqrt{n}} \mathbf{h} \right) - \mathbf{S}_n(\theta) \right) \right]\] <p>Scaling by $- \frac{1}{n}$, we get:</p> \[\nabla_{\mathbf{h}} \left[ -\frac{1}{n} \mathbf{S}_n(\theta)\right] = -\frac{1}{\rvert \rvert \mathbf{h} \rvert \rvert} \underset{n \rightarrow \infty}{\lim} \left[ \frac{1}{\sqrt{n} } \left( \mathbf{S}_n \left(\theta + \frac{1}{\sqrt{n}} \mathbf{h} \right) - \mathbf{S}_n(\theta) \right) \right]\] <p>Recall that for differentiable functions, the directional derivative is equal to the dot product between the gradient and the normalized direction vector. That is:</p> \[\begin{aligned} &amp;\nabla_{\mathbf{h}} \left[ -\frac{1}{n} \mathbf{S}_n(\theta)\right] = \frac{\partial}{\partial \theta} \left[ - \frac{1}{n} \mathbf{S}_n(\theta) \right] \cdot \frac{\mathbf{h}}{\rvert \rvert \mathbf{h}\rvert \rvert} \\ \implies &amp;-\underset{n \rightarrow \infty}{\lim} \left[ \frac{1}{\sqrt{n} } \left( \mathbf{S}_n \left(\theta + \frac{1}{\sqrt{n}} \mathbf{h} \right) - \mathbf{S}_n(\theta) \right) \right] = \frac{\partial}{\partial \theta} \left[ - \frac{1}{n} \mathbf{S}_n(\theta) \right] \cdot \mathbf{h} \end{aligned}\] <p>Let’s subtract $\mathbf{G}(\theta) \mathbf{h}$ from both sides.</p> \[\begin{aligned} &amp;- \mathbf{G}(\theta) \mathbf{h} - \underset{n \rightarrow \infty}{\lim} \left[ \frac{1}{\sqrt{n} } \left( \mathbf{S}_n \left(\theta + \frac{1}{\sqrt{n}} \mathbf{h} \right) - \mathbf{S}_n(\theta) \right) \right] = \left( \frac{\partial}{\partial \theta} \left[ - \frac{1}{n} \mathbf{S}_n(\theta) \right] - \mathbf{G}(\theta) \right) \mathbf{h} \\ \implies &amp; - \left(\underset{n \rightarrow \infty}{\lim} \left[ \frac{1}{\sqrt{n} } \left( \mathbf{S}_n \left(\theta + \frac{1}{\sqrt{n}} \mathbf{h} \right) - \mathbf{S}_n(\theta) \right) \right] + \mathbf{G}(\theta) \mathbf{h} \right) = \left( \frac{\partial}{\partial \theta} \left[ - \frac{1}{n} \mathbf{S}_n(\theta) \right] - \mathbf{G}(\theta) \right) \mathbf{h} \end{aligned}\] <p>Notice that both $\mathbf{G}(\theta)$ and $\mathbf{h}$ are independent of $n$, so we can take them inside the limit:</p> \[- \left(\underset{n \rightarrow \infty}{\lim} \left[ \frac{1}{\sqrt{n} } \left( \mathbf{S}_n \left(\theta + \frac{1}{\sqrt{n}} \mathbf{h} \right) - \mathbf{S}_n(\theta) \right) + \mathbf{G}(\theta) \mathbf{h} \right] \right) = \left( \frac{\partial}{\partial \theta} \left[ - \frac{1}{n} \mathbf{S}_n(\theta) \right] - \mathbf{G}(\theta) \right) \mathbf{h}\] <p>Condition \eqref{eq:condition-a2} basically implies that the lefthand side will approach $0$, which itself implies that $\mathbf{G}(\theta) = \frac{\partial}{\partial \theta} \left[ - \frac{1}{n} \mathbf{S}_n(\theta) \right]$ in the limit.</p> <p>Let’s partition our vectors and matrices in the following ways:</p> \[\mathbf{S}_n(\theta) = \begin{bmatrix} \mathbf{S}^\top_{n, \lambda}(\theta) &amp; \mathbf{S}^\top_{n, \psi}(\theta) \end{bmatrix}^\top \hspace{2mm} \text{ and } \hspace{2mm} \mathbf{G}(\theta) = \begin{bmatrix} \mathbf{G}_{\lambda, \lambda}(\theta) &amp; \mathbf{G}_{\lambda, \psi}(\theta) \\ \mathbf{G}_{\psi, \lambda}(\theta) &amp; \mathbf{G}_{\psi, \psi}(\theta) \end{bmatrix} \hspace{2mm} \text{ and } \hspace{2mm} \mathbf{V}(\theta) = \begin{bmatrix} \mathbf{V}_{\lambda, \lambda}(\theta) &amp; \mathbf{V}_{\lambda, \psi}(\theta) \\ \mathbf{V}_{\psi, \lambda}(\theta) &amp; \mathbf{V}_{\psi, \psi}(\theta) \end{bmatrix}\] <p>Let \(\theta_0 = (\lambda : \mathbf{0})\) denote the value of $\theta$ under the null hypothesis, and suppose the null is true. Define the quantities:</p> \[\begin{aligned} \mathbf{Z}_n(\theta_0) &amp;= n^{-1/2} \left( \mathbf{S}_{n, \psi}(\theta_0) - \mathbf{G}_{\psi, \lambda}(\theta_0) \mathbf{G}_{\lambda, \lambda}^{-1}(\theta_0) \mathbf{S}_{n, \lambda}(\theta_0) \right) \\ \mathbf{C}(\theta_0) &amp;= \mathbf{V}_{\psi, \psi}(\theta_0) - \mathbf{G}_{\psi, \lambda}(\theta_0) \mathbf{G}^{-1}_{\lambda, \lambda}(\theta_0) \mathbf{V}_{\lambda, \psi}(\theta_0) - \left( \mathbf{V}_{\psi, \lambda}(\theta_0) - \mathbf{G}_{\psi, \lambda}(\theta_0) \mathbf{G}_{\lambda, \lambda}^{-1}(\theta_0) \mathbf{V}_{\lambda, \lambda}(\theta_0) \right)\left(\mathbf{G}^{-1}_{\lambda, \lambda}(\theta_0)\right)^\top \mathbf{G}^\top_{\psi, \lambda}(\theta_0) \end{aligned}\] <p>Since the above condition (Eq. \eqref{eq:condition-a1}) is assumed to be satisfied, and $\mathbf{Z}_n$ is just a function of $\mathbf{S}_n$:</p> \[n^{-1/2} \mathbf{S}_n(\theta_0) \rightsquigarrow \mathcal{N}(\mathbf{0}, \mathbf{V}(\theta_0)) \hspace{2mm} \implies \hspace{2mm} \mathbf{Z}_n(\theta_0) \rightsquigarrow \mathcal{N}(\mathbf{0}, \mathbf{C}(\theta_0))\] <p>Denote consistent estimators for $\mathbf{G}(\theta_0)$ and $\mathbf{V}(\theta_0)$ with $\tilde{\mathbf{G}}(\theta_0)$ and $\tilde{\mathbf{V}}(\theta_0)$, respectively. Furthermore, let $\tilde{\lambda}$ denote a “suitable” estimator for $\lambda$ (where suitable is not really specific, but examples are given in the text), and let $\tilde{\theta}_0 = (\tilde{\lambda} : \mathbf{0})$. Define:</p> \[\begin{aligned} \mathbf{G}^{\psi, \psi}(\theta) &amp;= \left( \mathbf{G}_{\psi, \psi}(\theta) - \mathbf{G}_{\psi, \lambda}(\theta) \mathbf{G}^{-1}_{\lambda, \lambda}(\theta) \mathbf{G}_{\lambda, \psi}(\theta) \right)^{-1} \\ \mathbf{A}(\theta) &amp;= \left( \mathbf{G}^\top(\theta) \mathbf{V}^{-1}(\theta)\mathbf{G}(\theta) \right)^{-1} \\ \tilde{\mathbf{Z}}_n(\tilde{\theta}_0) &amp;= n^{-1/2} \left( \mathbf{S}_{n, \psi}(\tilde{\theta}_0) - \tilde{\mathbf{G}}_{\psi, \lambda}(\tilde{\theta}_0) \tilde{\mathbf{G}}_{\lambda, \lambda}^{-1}(\tilde{\theta}_0) \mathbf{S}_{n, \lambda}(\tilde{\theta}_0) \right) \end{aligned}\] <p>Using these, define:</p> \[\mathbf{U}(\tilde{\theta}_0) = \tilde{\mathbf{G}}^{\psi, \psi}(\theta_0) \tilde{\mathbf{Z}}_n(\tilde{\theta}_0)\] <p>where $\tilde{\mathbf{G}}^{\psi, \psi}$ is a consistent estimator for $\mathbf{G}^{\psi, \psi}(\theta_0)$.</p> <p>Let’s partition $\mathbf{A}(\theta)$ in the same way that we did with $\mathbf{V}(\theta)$ and $\mathbf{G}(\theta)$. With some work, we can see that for a fixed \(\delta \in \mathcal{C}\), \(\mathbf{U}(\tilde{\theta}_0) \rightsquigarrow \mathcal{N}(\delta, \mathbf{A}_{\psi, \psi}(\theta_0))\) under the sequence of alternatives $H_n: \psi = n^{-1/2} \delta$ as we take $n \rightarrow \infty$. Thus, $\mathbf{U}$ is a function of the data satisfying the condition in Eq. \eqref{eq:U-condition} and can be used in our test statistic construction.</p> <div id="test-statistic"></div> <div class="definition"> <body> <strong>Definition (One-Sided Test Statistic).</strong> <br/> The test statistic for $H_0: \psi = \mathbf{0}$ against $H_A: \psi \in \mathcal{C}$ is given by: $$ T_s = \mathbf{U}^\top(\tilde{\theta}_0) \tilde{\mathbf{A}}_{\psi, \psi}^{-1}(\tilde{\theta}_0) \mathbf{U}(\tilde{\theta}_0) - \underset{\mathbf{b} \in \mathcal{C}}{\inf} \left\{ (\mathbf{U}(\tilde{\theta}_0) - \mathbf{b})^\top \tilde{\mathbf{A}}_{\psi, \psi}^{-1}(\tilde{\theta}_0) (\mathbf{U}(\tilde{\theta}_0) - \mathbf{b}) \right\} \label{eq:test-stat-2} $$ where $\tilde{\mathbf{A}}_{\psi, \psi}^{-1}(\tilde{\theta}_0)$ is the partition of $\mathbf{A}_{\psi, \psi}^{-1}(\tilde{\theta}_0)$ corresponding to $(\psi, \psi)$ and constructed using $\tilde{\mathbf{G}}(\tilde{\theta}_0)$ and $\tilde{\mathbf{V}}(\tilde{\theta}_0)$ (I think...the authors never define $\tilde{\mathbf{A}}$). </body> </div> <p>A large sample $p$-value can be obtained as:</p> \[p \approx \underset{\lambda}{\sup} \left\{ \xi(t^*, \mathbf{A}_{\psi, \psi}(\theta_0), \mathcal{C}) \right\}\] <p>where $\xi(\cdot, \cdot, \cdot)$ is defined as in Eq. \eqref{eq:xi-defn} and $t^*$ is the observed value of $T_s$.</p> <hr/> <h2 id="results">Results</h2> <h4 id="lemma-1-silvapulle-and-silvapulle">Lemma 1 (Silvapulle and Silvapulle)</h4> <ul id="lemma-1" class="tab" data-tab="e26499b7-c6d1-4d22-b086-a77247a55ea2" data-name="lemma-1"> <li class="active" id="lemma-1-statement"> <a href="#">statement </a> </li> <li id="lemma-1-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="e26499b7-c6d1-4d22-b086-a77247a55ea2" data-name="lemma-1"> <li class="active"> <p>Let $\hat{\theta}$ be an estimator of $\theta$ using the entire parameter space (no restrictions imposed). Let $\mathcal{P}$ denote a closed and convex cone with its vertex at the origin. Let $\mathbf{B}$ be a positive definite matrix independent of $\theta$, and let \(\mathbf{B}_{\psi, \psi \cdot \lambda} = \mathbf{B}_{\psi, \psi} - \mathbf{B}_{\psi, \lambda} \mathbf{B}_{\lambda, \lambda}^{-1} \mathbf{B}_{\lambda, \psi}\).</p> <p>Note that:</p> \[\begin{aligned} (\hat{\theta} - \theta)^\top \mathbf{B} (\hat{\theta} - \theta) &amp;= \left( \begin{bmatrix} \hat{\psi} \\ \hat{\lambda} \end{bmatrix} - \begin{bmatrix} \psi \\ \lambda \end{bmatrix} \right)^\top \begin{bmatrix} \mathbf{B}_{\psi, \psi} &amp; \mathbf{B}_{\psi, \lambda} \\ \mathbf{B}_{\lambda, \psi} &amp; \mathbf{B}_{\lambda, \lambda} \end{bmatrix} \left( \begin{bmatrix} \hat{\psi} \\ \hat{\lambda} \end{bmatrix} - \begin{bmatrix} \psi \\ \lambda \end{bmatrix} \right) \end{aligned} \nonumber\] <p>The minimum of the above expression over just $\psi \in \mathcal{P}$ is equivalent to \(\underset{\psi \in \mathcal{P}}{\min} \left \{ (\hat{\psi} - \psi)^\top \mathbf{B}_{\psi, \psi \cdot \lambda}(\hat{\psi} - \psi) \right \}\) where we get \(\mathbf{B}_{\psi, \psi \cdot \lambda}\) by adjusting $\mathbf{B}_{\psi, \psi}$ for the uncertainty in $\hat{\lambda}$.</p> <p>Let:</p> \[(\bar{\psi} : \bar{\lambda}) := \underset{\psi \in \mathcal{P}}{\arg \min} \left \{ (\hat{\theta} - \theta)^\top \mathbf{B}(\hat{\theta} - \theta) \right \}\] <p>Then:</p> \[\bar{\lambda} = \hat{\lambda} + \mathbf{B}_{\lambda, \lambda}^{-1} \mathbf{B}_{\lambda, \psi}(\hat{\psi} - \bar{\psi})\] </li> <li> <p>There are two steps that go into the proof. We need to show that:</p> \[\underset{\psi \in \mathcal{P}}{\min} \left\{ (\hat{\theta} - \theta)^\top \mathbf{B} (\hat{\theta} - \theta) \right\} = \underset{\psi \in \mathcal{P}}{\min} \left\{ (\hat{\psi} - \psi)^\top \mathbf{B}_{\psi, \psi \cdot \lambda}(\hat{\psi} - \psi) \right\} \nonumber\] <p>We also need to show that \(\bar{\lambda} = \hat{\lambda} + \mathbf{B}_{\lambda, \lambda}^{-1} \mathbf{B}_{\lambda, \psi}(\hat{\psi} - \bar{\psi})\).</p> <p>The second claim can be shown by supposing we know $\bar{\psi}$ and finding $\bar{\lambda}$ by optimizing over $\lambda$. Plugging in our expression for $\bar{\lambda}$ into the objective function, we can decompose it into two pieces, one of which is the objective function in the RHS of the first claim. Then all we need to do is show that the minimizer of that piece is the same as $\bar{\psi}$.</p> <p>Since the minimization is over $\psi \in \mathcal{P}$, we’ll assume $\psi \in \mathcal{P}$. To save space, define $J(\theta) = (\hat{\theta} - \theta)^\top \mathbf{B}(\hat{\theta} - \theta)$.</p> <p>Notice that $\underset{\theta}{\min} \left{ J(\theta) \right}$ is the same as $\underset{\psi}{\min} \left{ J(\psi : \bar{\lambda}) \right}$ and also $\underset{\lambda}{\min} \left{ J(\bar{\psi} : \lambda) \right}$. That is, minimizing $J(\theta)$ over all values of $\theta$ is the same as if we plugged in the minimizing value of $\psi$ ($\bar{\psi}$) and then just minimized over values of $\lambda$ (and vice versa).</p> <p>Since $J(\bar{\psi} : \lambda)$ has a quadratic form, we can set the derivative equal to $0$ and solve to get a minimizing value for $\lambda$:</p> \[\begin{aligned} \frac{\partial}{\partial \lambda} \left[ J(\bar{\psi} : \lambda) \right] &amp;= \frac{\partial}{\partial \lambda} \left[ \left( \begin{bmatrix} \hat{\psi} \\ \hat{\lambda} \end{bmatrix} - \begin{bmatrix} \psi \\ \lambda \end{bmatrix} \right)^\top \begin{bmatrix} \mathbf{B}_{\psi, \psi} &amp; \mathbf{B}_{\psi, \lambda} \\ \mathbf{B}_{\lambda, \psi} &amp; \mathbf{B}_{\lambda, \lambda} \end{bmatrix} \left( \begin{bmatrix} \hat{\psi} \\ \hat{\lambda} \end{bmatrix} - \begin{bmatrix} \psi \\ \lambda \end{bmatrix} \right) \right] \\ &amp;= \frac{\partial}{\partial \lambda} \left[ \begin{bmatrix} (\hat{\psi} - \psi)^\top \mathbf{B}_{\psi, \psi} + (\hat{\lambda} - \lambda)^\top \mathbf{B}_{\lambda, \psi} \\ (\hat{\psi} - \psi)^\top \mathbf{B}_{\psi, \lambda} + (\hat{\lambda} - \lambda)^\top \mathbf{B}_{\lambda, \lambda} \end{bmatrix}^\top \left( \begin{bmatrix} \hat{\psi} \\ \hat{\lambda} \end{bmatrix} - \begin{bmatrix} \psi \\ \lambda \end{bmatrix} \right) \right] \\ &amp;= \frac{\partial}{\partial \lambda} \left[ (\hat{\psi} - \psi)^\top \mathbf{B}_{\psi, \psi}(\hat{\psi} - \psi) + (\hat{\lambda} - \lambda)^\top \mathbf{B}_{\lambda, \psi} (\hat{\psi} - \psi) + (\hat{\psi} - \psi)^\top \mathbf{B}_{\psi, \lambda}(\hat{\lambda} - \lambda) + (\hat{\lambda} - \lambda)^\top \mathbf{B}_{\lambda, \lambda} (\hat{\lambda} - \lambda) \right] \\ &amp;= 2(\hat{\lambda} - \lambda)^\top \mathbf{B}_{\lambda, \lambda} - (\hat{\psi} - \psi)^\top \mathbf{B}_{\psi, \lambda} - \left( \mathbf{B}_{\lambda, \psi} (\hat{\psi} - \psi) \right)^\top\\ &amp;= -2(\hat{\lambda} - \lambda)^\top \mathbf{B}_{\lambda, \lambda} - 2(\hat{\psi} - \psi)^\top \mathbf{B}_{\psi, \lambda} \end{aligned} \nonumber\] <p>where in the last line we assume the authors mean <i>symmetric</i> positive definite when they say positive definite. <br/></p> <p>Setting this equal to $0$ yields:</p> \[\begin{aligned} &amp;0 = \frac{\partial}{\partial \lambda} \left[ J(\bar{\psi} : \lambda) \right] \\ \implies &amp;0 = -2(\hat{\lambda} - \lambda)^\top \mathbf{B}_{\lambda, \lambda} - 2(\hat{\psi} - \bar{\psi})^\top \mathbf{B}_{\psi, \lambda} \\ \implies &amp;-(\hat{\psi} - \bar{\psi})^\top\mathbf{B}_{\psi, \lambda} = (\hat{\lambda} - \lambda)^\top \mathbf{B}_{\lambda, \lambda} \\ \implies &amp;\lambda^\top \mathbf{B}_{\lambda, \lambda} = (\hat{\psi} - \bar{\psi})^\top \mathbf{B}_{\psi, \lambda} + \hat{\lambda}^\top \mathbf{B}_{\lambda, \lambda} \\ \implies &amp;\lambda^\top = (\hat{\psi} - \bar{\psi})^\top\mathbf{B}_{\psi, \lambda} \mathbf{B}_{\lambda, \lambda}^{-1} + \hat{\lambda}^\top \\ \implies &amp;\bar{\lambda} = \hat{\lambda} + \mathbf{B}_{\lambda, \lambda}^{-1}\mathbf{B}_{\lambda, \psi} (\hat{\psi} - \bar{\psi}) \end{aligned} \nonumber\] <p>This shows that that there exists a point $\bar{\lambda}$ satisfying the form specified in the Lemma. Now we want to show that this corresponds to the minimum over $\psi \in \mathcal{P}$.</p> <p>Since this value of $\lambda$ is optimal for any fixed $\psi$, we can just plug this into $J(\psi : \bar{\lambda})$ and minimize to find a value for $\bar{\psi}$:</p> \[\begin{aligned} J(\psi : \bar{\lambda}) &amp;= \begin{bmatrix} \hat{\psi} - \psi \\ -\mathbf{B}_{\lambda, \lambda}^{-1} \mathbf{B}_{\lambda, \psi} (\hat{\psi} - \bar{\psi}) \end{bmatrix}^\top \begin{bmatrix} \mathbf{B}_{\psi, \psi} &amp; \mathbf{B}_{\psi, \lambda} \\ \mathbf{B}_{\lambda, \psi} &amp; \mathbf{B}_{\lambda, \lambda} \end{bmatrix} \begin{bmatrix} \hat{\psi} - \psi \\ -\mathbf{B}_{\lambda, \lambda}^{-1} \mathbf{B}_{\lambda, \psi} (\hat{\psi} - \bar{\psi}) \end{bmatrix} \\ &amp;= \begin{bmatrix} (\hat{\psi} - \psi)^\top \mathbf{B}_{\psi, \psi} - (\hat{\psi} - \bar{\psi})^\top \mathbf{B}_{\psi, \lambda} \mathbf{B}_{\lambda, \lambda}^{-1} \mathbf{B}_{\lambda, \psi} \\ (\hat{\psi} - \psi)^\top \mathbf{B}_{\psi, \lambda} - (\hat{\psi} - \bar{\psi})^\top \mathbf{B}_{\psi, \lambda} \mathbf{B}_{\lambda, \lambda}^{-1} \mathbf{B}_{\lambda, \lambda} \end{bmatrix}^\top \begin{bmatrix} \hat{\psi} - \psi \\ -\mathbf{B}_{\lambda, \lambda}^{-1} \mathbf{B}_{\lambda, \psi} (\hat{\psi} - \bar{\psi}) \end{bmatrix} \\ &amp;= (\hat{\psi} - \psi)^\top\mathbf{B}_{\psi, \psi}(\hat{\psi} - \psi) \underbrace{ - (\hat{\psi} - \bar{\psi})^\top \mathbf{B}_{\psi, \lambda} \mathbf{B}_{\lambda, \lambda}^{-1} \mathbf{B}_{\lambda, \psi}(\hat{\psi} - \psi) - (\hat{\psi} - \psi)^\top\mathbf{B}_{\psi, \lambda} \mathbf{B}_{\lambda, \lambda}^{-1} \mathbf{B}_{\lambda, \psi} (\hat{\psi} - \bar{\psi}) + (\hat{\psi} - \bar{\psi})^\top \mathbf{B}_{\psi, \lambda} \mathbf{B}_{\lambda, \lambda}^{-1} \mathbf{B}_{\lambda, \psi} (\hat{\psi} - \bar{\psi})}_{(a)}\\ &amp;= (\hat{\psi} - \psi)^\top\mathbf{B}_{\psi, \psi}(\hat{\psi} - \psi) - (\hat{\psi} - \psi)^\top \mathbf{B}_{\psi, \lambda} \mathbf{B}^{-1}_{\lambda, \lambda} \mathbf{B}_{\lambda, \psi}(\hat{\psi} - \psi) + (\bar{\psi} - \psi)^\top \mathbf{B}_{\psi, \lambda} \mathbf{B}^{-1}_{\lambda, \lambda} \mathbf{B}_{\lambda, \psi}(\bar{\psi} - \psi) \\ &amp;= \underbrace{(\hat{\psi} - \psi)^\top\left( \mathbf{B}_{\psi, \psi} - \mathbf{B}_{\psi, \lambda} \mathbf{B}^{-1}_{\lambda, \lambda} \mathbf{B}_{\lambda, \psi} \right) (\hat{\psi} - \psi)}_{=: f(\psi)} + \underbrace{(\bar{\psi} - \psi)^\top \mathbf{B}_{\psi, \lambda} \mathbf{B}^{-1}_{\lambda, \lambda} \mathbf{B}_{\lambda, \psi}(\bar{\psi} - \psi)}_{=: g(\psi)} \end{aligned} \nonumber\] <details> <summary>Details Of $(a)$.</summary> In $(a)$, we do the following simplification. Let $\bar{\mathbf{B}} = \mathbf{B}_{\psi, \lambda} \mathbf{B}^{-1}_{\lambda, \lambda} \mathbf{B}_{\lambda, \psi}$. Then: $$ \begin{aligned} (a) &amp;= -(\hat{\psi} - \bar{\psi})^\top \bar{\mathbf{B}} (\hat{\psi} - \psi) - (\hat{\psi} - \psi)^\top \bar{\mathbf{B}} (\hat{\psi} - \bar{\psi}) + (\hat{\psi} - \bar{\psi})^\top \bar{\mathbf{B}} (\hat{\psi} - \bar{\psi}) \\ &amp;= \left(-\hat{\psi}^\top\bar{\mathbf{B}}\hat{\psi}+ \bar{\psi}^\top \bar{\mathbf{B}} \hat{\psi} + \hat{\psi}^\top \bar{\mathbf{B}} \psi - \bar{\psi}^\top \bar{\mathbf{B}} \psi \right) + \left( - \hat{\psi}^\top \bar{\mathbf{B}} \hat{\psi} + \psi^\top \bar{\mathbf{B}} \hat{\psi} + \hat{\psi}^\top \bar{\mathbf{B}} \bar{\psi} - \psi^\top \bar{\mathbf{B}} \bar{\psi} \right) + \left( \hat{\psi}^\top \bar{\mathbf{B}} \hat{\psi} - \bar{\psi}^\top \bar{\mathbf{B}} \hat{\psi} - \hat{\psi}^\top \bar{\mathbf{B}} \bar{\psi} + \bar{\psi}^\top \bar{\mathbf{B}} \bar{\psi} \right) \\ &amp;= \underbrace{-\hat{\psi}^\top \bar{\mathbf{B}}\hat{\psi} + \hat{\psi}^\top \bar{\mathbf{B}} \psi + \psi^\top \bar{\mathbf{B}} \hat{\psi}}_{(i)} + \underbrace{\bar{\psi}^\top \bar{\mathbf{B}} \bar{\psi} -\bar{\psi}^\top \bar{\mathbf{B}} \psi - \psi^\top \bar{\mathbf{B}} \bar{\psi}}_{(ii)} \\ &amp;= \underbrace{(\bar{\psi} - \psi)^\top \bar{\mathbf{B}} (\bar{\psi} - \psi) - \psi^\top \bar{\mathbf{B}} \psi}_{=(ii)} - \underbrace{\left((\hat{\psi} - \psi)^\top \bar{\mathbf{B}}(\hat{\psi} - \psi) + \psi^\top \bar{\mathbf{B}}\psi\right)}_{=(i)} \\ &amp;= (\bar{\psi} - \psi)^\top \bar{\mathbf{B}} (\bar{\psi} - \psi) - (\hat{\psi} - \psi)^\top \bar{\mathbf{B}}(\hat{\psi} - \psi) \end{aligned} \nonumber $$ </details> <p>We can see that minimizing $J(\psi: \bar{\lambda})$ over values of $\psi$ is equivalent to minimizing $f(\psi) + g(\psi)$, where:</p> \[f(\psi) = (\hat{\psi} - \psi)^\top \mathbf{B}_{\psi, \psi \cdot \lambda} (\hat{\psi} - \psi)\hspace{25mm} g(\psi) = (\bar{\psi} - \psi)^\top \mathbf{B}_{\psi, \lambda} \mathbf{B}^{-1}_{\lambda, \lambda} \mathbf{B}_{\lambda, \psi}(\bar{\psi} - \psi) \nonumber\] <p>We also know that $f(\psi)$ and $g(\psi)$ are strictly convex and convex, respectively.</p> <details> <summary>Proofs Of Convexity.</summary> First, let's look at $f(\psi)$, in which the middle portion is the Schur complement of $\mathbf{B}_{\lambda, \lambda}$. The positive definiteness of $\mathbf{B}$ implies that the Schur complement is also positive definite (since we also assume $\mathbf{B}_{\lambda, \lambda}$ is invertible). Since $f(\psi)$ has a quadratic form associated with a positive definite matrix, it is strictly convex. <br/> <br/> Looking at $g(\psi)$, we see that the middle portion, $\mathbf{B}_{\lambda, \lambda}^{-1}$ is positive definite due to the fact that the principal sub-matrices of a positive definite matrix are also positive definite and the inverse of a positive definite matrix is also positive definite. Secondly, we know that pre- and post-multiplying a positive definite matrix by another matrix will yield a positive semi-definite matrix. Thus, $g(\psi)$ is positive semi-definite, which implies that it is convex since it is a quadratic form associated with a positive semi-definite matrix. </details> <p>Denote the minimizer of $f(\psi)$ with \(\psi^*\) and consider the line segment \([ \bar{\psi}, \psi^* ]\), which connects the minimizing value of $\psi$ for $J(\theta)$ with the minimizing value of $\psi$ for $f(\psi)$. <br/> Supposedly we can show that $\psi^* = \bar{\psi}$, which would complete the proof. However, I am unsure of how to show this. Here is my best attempt so far, which is an adaptation of <a href="https://math.stackexchange.com/questions/3108304/does-this-special-case-of-convex-quadratic-programming-have-a-partially-unique-s ">AlephZero’s proof</a> on Math StackExchange: <br/> Assume \((\psi^*, \bar{\lambda})\) is a solution to the minimization problem (I don’t know how to show this). Suppose \(\psi^* \neq \bar{\psi}\). Since both \(\bar{\psi}\) and \(\psi^*\) are within $\mathcal{P}$, which is closed and convex, the entire line segment is also contained in $\mathcal{P}$ and any convex combination of $\bar{\psi}$ and $\psi^*$ is also a solution. Let \(a := J(\bar{\psi} : \bar{\lambda}) = J(\psi^* : \bar{\lambda})\), and let $t \in (0, 1)$. Let $t(\psi : \lambda)$ denote the scaling of the respecting entries in the concatenated vector of parameter values. We then have:</p> \[\begin{aligned} J(t(\bar{\psi} : \bar{\lambda}) + (1 - t)(\psi^* : \bar{\lambda})) &amp;= J(t\bar{\psi} + (1-t)\psi^* : \bar{\lambda}) \\ &amp;= f(t\bar{\psi} + (1-t)\psi^*) + g(t\bar{\psi} + (1-t) \psi^*) \\ &amp;&lt; t f(\bar{\psi}) + (1-t)f(\psi^*) + tg(\bar{\psi}) + (1-t)g(\psi^*) \hspace{15mm} f \text{ is strictly convex and } g \text{ is convex} \\ &amp;= t J(\bar{\psi} : \bar{\lambda}) + (1 - t) J(\psi^* : \bar{\lambda}) \\ &amp;= a \end{aligned} \nonumber\] <p>This implies any convex combination of $\bar{\psi}$ and $\psi^<em>$ achieves a value smaller than the minimum, which is a contradiction. Thus, $\bar{\psi} = \psi^</em>$.</p> <p style="color:red;">TODO: FINISH PROOF</p> </li> </ul> <p>With the above lemma, we can prove the following theorem.</p> <h4 id="theorem-1-silvapulle-and-silvapulle">Theorem 1 (Silvapulle and Silvapulle)</h4> <ul id="theorem-1" class="tab" data-tab="430d536f-f7cf-4c60-be75-00a6f4df90ec" data-name="theorem-1"> <li class="active" id="theorem-1-statement"> <a href="#">statement </a> </li> <li id="theorem-1-proof"> <a href="#">proof </a> </li> </ul> <ul class="tab-content" id="430d536f-f7cf-4c60-be75-00a6f4df90ec" data-name="theorem-1"> <li class="active"> <p>Define $\mathbf{S}_n(\theta) = \frac{\partial}{\partial \theta} \left[ \ell(\theta) \right]$ as the score function (the derivative of the log-likelihood), and assume that it satisfies Condition \eqref{eq:condition-a1} and \eqref{eq:condition-a2}. Suppose we are testing $H_0: \psi = \mathbf{0}$ against $H_A: \psi \in \mathcal{C}$ for $\mathcal{C}$ as defined above. As $n \rightarrow \infty$, the likelihood ratio test statistic, $LR = -2 \left(\ell(\theta_0) - \ell(\hat{\theta}) \right)$ where $\hat{\theta}$ is the MLE of $\theta$ over the entire parameter space, satisfies:</p> \[LR = T_s + o_p(1) \nonumber\] <p>under the null hypothesis.</p> </li> <li> <p>We’ll assume we’re in the setting where the null hypothesis is true ($\theta_0 = (\lambda : \mathbf{0})$). Since we’ve taken our estimating equation, $\mathbf{S}_n(\theta)$, as the score, we can take $\mathbf{G}(\theta_0)$ to be the Fisher information under the null (WHY?).</p> <p>First, let’s rewrite the log-likelihood, $\ell(\theta)$, with a Taylor expansion about the unrestricted estimator, $\hat{\theta}$:</p> \[\ell(\theta) = \ell(\hat{\theta}) + \frac{n}{2}(\hat{\theta} - \theta)^\top \mathbf{G}(\theta_0) (\hat{\theta} -\theta) + \Delta(\theta) \nonumber\] <p>where, for any $a &gt; 0$, $\underset{\rvert \rvert \theta - \theta_0 \rvert \rvert \leq \frac{a}{\sqrt{n}}}{\sup} \left{ \rvert \Delta(\theta) \rvert \right} = o_p(1)$. This last term is just a bound on all of the terms in the Taylor expansion that are of a higher order than two, written with stochastic order notation. A related proof can be found in my post “A Score Test Primer”. The multiplication by $n$ is just because we assume we have $n$ i.i.d. observations, so we can just multiply the value for a single observation by the number we have.</p> <p>From this expansion, we can rewrite the likelihood ratio test statistic as:</p> \[LR = n\left[ \underset{\psi = \mathbf{0}}{\min} \left\{ (\hat{\theta} - \theta)^\top \mathbf{G}(\theta_0) (\hat{\theta} - \theta)\right\} - \underset{\psi \in \mathcal{C}}{\min}\left\{ (\hat{\theta} - \theta)^\top \mathbf{G}(\theta_0) (\hat{\theta} - \theta) \right\} \right] + o_p(1) \nonumber\] <p>Using Lemma 1, we can see that:</p> \[\begin{aligned} LR &amp;= n\left[ \underset{\psi = \mathbf{0}}{\min} \left\{ (\hat{\psi} - \psi)^\top \mathbf{G}_{\psi, \psi \cdot \lambda}(\theta_0) (\hat{\psi} - \psi) \right\} - \underset{\psi \in \mathcal{C}}{\min}\left\{(\hat{\psi} - \psi)^\top \mathbf{G}_{\psi, \psi \cdot \lambda}(\theta_0) (\hat{\psi} - \psi) \right\} \right] + o_p(1) \\ &amp;= n \left[ \underset{\psi = \mathbf{0}}{\min} \left\{ \hat{\psi}^\top \mathbf{G}_{\psi, \psi \cdot \lambda}(\theta_0) \hat{\psi}\right\} + \underset{\psi \in \mathcal{C}}{\min}\left\{(\hat{\psi} - \psi)^\top \mathbf{G}_{\psi, \psi \cdot \lambda}(\theta_0) (\hat{\psi} - \psi) \right\}\right] + o_p(1) \\ &amp;= n \left[ \hat{\psi}^\top \mathbf{G}_{\psi, \psi \cdot \lambda}(\theta_0) \hat{\psi} + \underset{\psi \in \mathcal{C}}{\inf}\left\{(\hat{\psi} - \psi)^\top \mathbf{G}_{\psi, \psi \cdot \lambda}(\theta_0) (\hat{\psi} - \psi) \right\}\right] + o_p(1) \hspace{15mm} \mathcal{C} \text{ is closed, so } \min \text{ is same as } \inf \\ \end{aligned} \nonumber\] <p>Notice that $\sqrt{n} \mathbf{G}(\theta_0) (\hat{\theta} - \theta_0) = \frac{1}{\sqrt{n}} \mathbf{S}_n(\theta_0) + o_p(1)$ because</p> \[T_s = \mathbf{U}^\top \tilde{\mathbf{A}}_{\psi, \psi}^{-1}\mathbf{U} - \underset{\mathbf{b} \in \mathcal{C}}{\inf} \left\{ (\mathbf{U} - \mathbf{b})^\top \tilde{\mathbf{A}}_{\psi, \psi}^{-1}(\mathbf{U} - \mathbf{b}) \right\}\] <p style="color:red;">TODO: FINISH PROOF</p> </li> </ul>]]></content><author><name></name></author><category term="glmms"/><category term="score"/><category term="likelihood"/><category term="theory"/><category term="testing"/><summary type="html"><![CDATA[For Variance Components]]></summary></entry><entry><title type="html">Clustering</title><link href="https://aerosengart.github.io/blog/2025/impossibility-clustering/" rel="alternate" type="text/html" title="Clustering"/><published>2025-01-13T00:00:00+00:00</published><updated>2025-01-13T00:00:00+00:00</updated><id>https://aerosengart.github.io/blog/2025/impossibility-clustering</id><content type="html" xml:base="https://aerosengart.github.io/blog/2025/impossibility-clustering/"><![CDATA[<p>Though the journey to this point is a bit confusing, I have recently become interesting in clustering metrics and evaluation. In this post, I’ll work through a couple papers on describing how good a clustering function is based upon a set of axioms. These include Jon Kleinberg’s <i>An Impossibility Theorem for Clustering</i><d-cite key="kleinberg_2002"></d-cite> and Margareta Ackerman and Shai Ben-David’s <i>Measures of Clustering Quality: A Working Set of Axioms for Clustering</i><d-cite key="bendavid_2008"></d-cite>.</p> <hr/> <h2 id="background">Background</h2> <p>First, let’s more rigorously define what we mean by <i>clustering</i> and our problem setting as defined by Kleinberg.</p> <div id="clustering"></div> <blockquote> <p><strong>Definition (Clustering).</strong> Suppose we have a set $S$ of $n \geq 2$ observations, which we notate as $S = { 1, 2, \dots, n}$. We’ll define a <i>distance function</i> $d: S \times S \rightarrow \mathbb{R}$ as a function satisfying, for all $i,j,k \in S$:</p> <ul> <li>Non-Negativity: $d(i,j) \geq 0$</li> <li>Symmetry: $d(i,j) = d(j,i)$</li> <li>Identity of Indiscernibles: $d(i,j) = 0$ if, and only if, $i = j$</li> <li>Triangle Inequality: $d(i,k) \leq d(i,j) + d(j,k)$</li> </ul> <p>The last condition (Triangle Inequality) is not necessary for the following discussion, but it is a nice property and requiring it makes a choice of $d$ a metric. A <i>clustering function</i>, $f$, is a function operating on a distance function $d$ on $S$ that outputs a partition $\Gamma$ of $S$. A partition is called <i>trivial</i> if each cluster contains one point or if it has only one cluster.</p> </blockquote> <p>We can evaluate a clustering function with what the author terms a <i>clustering quality metric</i>.</p> <div id="cqm"></div> <blockquote> <p><strong>Definition (Clustering Quality Metric).</strong> A <i>clustering-quality measure (CQM)</i> is a function operating on a partition $\Gamma$ of a set $S$ with respect to distance $d$ that outputs a non-negative real number which represents the “goodness” of the clustering $\Gamma$. That is, a CQM $m$ is the function $m: (\Gamma, S, d) \rightarrow \mathbb{R}^+_0$.</p> </blockquote> <p>Finally, we’ll define some helpful terms and quantities that will be used later. Let $\alpha \cdot d$ denote scaling the distance function $d$ by $\alpha$. That is $\alpha \cdot d$ is the distance function who assigns distance $\alpha d(i,j)$ between points $i$ and $j$. We’ll denote all possible output partitions of $f$ as $\text{Range}(f)$. We denote the fact points $i$ and $j$ are in the same cluster in partition $\Gamma$ with $i \underset{\Gamma}{\sim} j$, and we use the notation $i \underset{\Gamma}{\not \sim} j$ if they are not in the same cluster.</p> <div id="rep-set"></div> <blockquote> <p><strong>Definition (Representative Set).</strong> We call a set $G$ a <i>representative set</i> of $\Gamma$ if it contains a single observation from each cluster in $\Gamma$. That is, for $\Gamma = { \gamma_1, \dots, \gamma_g}$, $G$ is a representative set of $\Gamma$ if $\rvert G \rvert = g$ and $G\cap \gamma_i \neq \emptyset$ \forall i$.</p> </blockquote> <div id="isomorphism"></div> <blockquote> <p><strong>Definition (Isomorphism).</strong> Let $\Gamma$ and $\Gamma’$ be partitions of $S$ with $d$. We call $\Gamma$ and $\Gamma’$ <i>isomorphic</i>, denoted by $\Gamma \underset{d}{\approx} \Gamma’$ if there is a <i>distance-preserving isomorphism</i> $\phi: S \rightarrow S$ such that $\forall i,j \in S$, we have $i \underset{\Gamma}{\sim} j$ if, and only if, $\phi(i) \underset{\Gamma’}{\sim} \phi(j)$.</p> </blockquote> <aside><p>We can think of a distance-preserving isomorphism as a mapping between our source set, $S$, and some target set $\phi(S)$ such that for any $i \in S$, there exists $\phi(i) \in \phi(S)$; for any $i, j \in S$, $d(i, j) = d(\phi(i), \phi(j))$; and there exists an inverse mapping, $\phi'$, such that $\phi'(\phi(i)) = i$. </p></aside> <div id="gamma-transformation"></div> <blockquote> <p><strong>Definition ($\Gamma$-Transformation).</strong> Let $d$ and $d’$ be distance functions on $S$, and let $\Gamma$ be some partition of $S$. We call $d’$ a <i>$\Gamma$-transformation</i> of $d$ if:</p> <ul> <li>$d'(i,j) \leq d(i,j)$ for all $i,j \in S$ such that $i \underset{\Gamma}{\sim} j$</li> <li>$d'(i,j) \geq d(i,j)$ for all $i,j \in S$ such that $i \underset{\Gamma}{\not \sim} j$</li> </ul> <p>In words, a $\Gamma$-transformation of a distance function $d$ will assigns smaller distances to points in the same cluster and larger distances to points in different clusters.</p> </blockquote> <div id="refinement"></div> <blockquote> <p><strong>Definition (Refinement).</strong> Let $\Gamma$ be some partition of $S$. We call another partition $\Gamma’$ of $S$ a <i>refinement</i> of $\Gamma$ if, for every cluster $C’ \in \Gamma’$, there is a cluster $C \in \Gamma$ such that $C’ \subseteq C$. More intuitively, a refinement is just a finer partition, which means each cluster in $\Gamma$ is either also in $\Gamma’$ or is split into multiple smaller clusters in $\Gamma’$. We can define the partial order $\Gamma’ \preccurlyeq \Gamma$ if $\Gamma’$ is a refinement of $\Gamma$.</p> </blockquote> <div id="antichain"></div> <blockquote> <p><strong>Definition (Antichain).</strong> An <i>antichain</i> is a collection $\mathcal{A}$ of partitions of $S$ such that for all $\Gamma, \Gamma’ \in \mathcal{A}$ where neither $\Gamma$ is a refinement of $\Gamma’$ nor is $\Gamma’$ a refinement of $\Gamma$. We also require that $\Gamma$ and $\Gamma’$ be different partitions.</p> </blockquote> <div id="ab-conforming"></div> <blockquote> <p><strong>Definition ($(a,b)$-Conforming).</strong> Let $\Gamma$ be a partition of $S$. A distance function $d$ is said to <i>$(a,b)$-conform</i> to $\Gamma$ if:</p> <ul> <li>$d(i,j) \leq a$ for all $i,j \in S$ such that $i \underset{\Gamma}{\sim} j$</li> <li>$d(i,j) \geq b$ for all $i,j \in S$ such that $i \underset{\Gamma}{\not \sim} j$</li> </ul> </blockquote> <div id="gamma-forcing"></div> <blockquote> <p><strong>Definition ($\Gamma$-Forcing).</strong> For a partition $\Gamma$ of $S$ and clustering function $f$, a pair of real numbers $(a, b)$ is called <i>$\Gamma$-forcing</i> with respect to $f$ if $f(d) = \Gamma$ for all distance functions that are $(a,b)$-conforming to $\Gamma$.</p> </blockquote> <hr/> <h2 id="the-impossibility-theorem">The Impossibility Theorem</h2> <p>Kleinberg’s impossibility theorem is based upon a set of three axioms that characterize a “good” clustering function, $f$.</p> <div id="axioms"></div> <blockquote> <p><strong>Definition (Kleinberg’s Axioms).</strong></p> <ul> <li><i>Scale Invariance:</i> For any distance function $d$ and positive real $\alpha &gt; 0$, $f(d) = f(\alpha \cdot d)$. A scale-invariant clustering function should not change its output when all distances between points are changed by some factor. </li> <li><i>Richness:</i> $\text{Range}(f)$ should be all possible paritions of $S$. Richness implies that our clustering function is flexible enough that any partition of $S$ can be achieved if we find the right distance function.</li> <li><i>Consistency:</i> For any two distance functions $d$ and $d'$ such that $f(d) = \Gamma$ and $d'$ is a $\Gamma$-transformation of $d$, $f(d') = \Gamma$. A consistent clustering function will output the same partition if we make points within the same cluster closer together and make points in different clusters farther apart.</li> </ul> </blockquote> <p>Kleinberg goes on to show that these axioms imply a semi-surprising result.</p> <h3 id="theorem-21-impossibility-theorem">Theorem 2.1 (Impossibility Theorem).</h3> <p>For $n \geq 2$, there does not exists a clustering function $f$ that is scale-invariant, rich, and consistent.</p> <details> <summary>Proof.</summary> The proof of Theorem 2.1 rests upon the claim that a scale-invariant and consistent clustering fucntion $f$ cannot possibly be rich as $\text{Range(f)}$ forms an antichain (Theorem 3.1 in paper). <br/> First, suppose we have a consistent clustering function $f$, and let $\Gamma$ be some partition in $\text{Range}(f)$. Because $\Gamma \in \text{Range}(f)$, $\exists d$ such that $f(d) = \Gamma$ (by definition). Define $a'$ as the minimum distance between any two points in the same cluster over all clusters in $\Gamma$, and define $b'$ as the maximum distance between any two points in different clusters over all clusters in $\Gamma$. <br/> Select positive real numbers $a, b$ such that $a &lt; b$, $a \leq a'$, and $b \geq b'$. Notice that, for any distance function $d'$ that $(a,b)$-conforms to $\Gamma$ is a $\Gamma$-transformation of $d$. This is due to the fact that $d'(i,j) \leq a \leq a' \leq d(i,j)$ for $i,j \in S$ such that $i \underset{\Gamma}{\sim} j$ and $d'(i,j) \geq b \geq b' \geq d(i,j)$ for $i,j \in S$ such that $i \underset{\Gamma}{\not \sim} j$, which is precisely the definition of a $\Gamma$-transformation. <br/> Since $d'$ is a $\Gamma$-transformation of $d$ and $f$ is consistent by assumption, $(a,b)$ is $\Gamma$-forcing. <br/> Now we further assume that $f$ is scale-invariant and that $\exists \Gamma_0, \Gamma_1 \in \text{Range}(f)$ such that $\Gamma_0$ is a refinement of $\Gamma_1$. Define $(a_0, b_0)$ and $(a_1, b_1)$ be pairs of reals that are $\Gamma_0$-forcing and $\Gamma_1$-forcing, respectively, such that $a_0 &lt; b_0$ and $a_1 &lt; b_1$. Such pairs can be found by taking any two partitions in $\text{Range}(f)$ and setting $\Gamma_0$ and $\Gamma_1$ according to the argument above. <br/> Let $a_2$ be some number such that $a_2 \leq a_1$ and set $\epsilon$ such that $0 &lt; \epsilon &lt; \frac{a_0 a_2}{b_0}$. We now construct the distance function $d$ that satisfies the following: <ul> <li><body>$d(i,j) \leq \epsilon$ for $i,j$ such that $\Gamma_0^i = \Gamma_0^j$</body></li> <li>$a_2 \leq d(i,j) \leq a_1$ for $i,j$ such that $\Gamma_1^i = \Gamma_1^j$ and $\Gamma_0^i \neq \Gamma_0^j$</li> <li>$d(i,j) \geq b_1$ for $i,j$ such that $\Gamma_1^i \neq \Gamma_1^j$</li> </ul> According to the above conditions, $d$ $(a_1, b_1)$-conforms to $\Gamma_1$. Since $f$ is still assumed to be consistent, this implies $f(d) = \Gamma_1$. <br/> Let $\alpha = \frac{b_0}{a_2}$ and $d' = \alpha \cdot d$. Since $f$ is scale-invariant, $f(d') = f(d) = \Gamma_1$. However, for $i,j$ such that $\Gamma_0^i = \Gamma_0^j$, $d'(i,j) = \alpha d(i,j) \leq \alpha \epsilon &lt; \frac{a_0 a_2 b_0}{a_2 b_0} = a_0$. In addition, for $i,j$ such that $\Gamma_0^i \neq \Gamma_0^j$, $d'(i,j) = \alpha d(i,j) \geq \alpha a_2 = \frac{b_0 a_2}{a_2} = b_0$. This implies that $d'$ $(a_0, b_0)$-conforms to $\Gamma_0$, which implies that $f(d') = \Gamma_0$. <br/> Since $\Gamma_0$ is a refinement of $\Gamma_1$, they are distinct partitions, so $\Gamma_0 \neq \Gamma_1$, and we arrive at a contradition. </details> <p>Kleinberg proves an additional theorem that describes the partitions achievable by scale-invariant, consistent clustering functions $f$.</p> <div id="theorem3.2"></div> <h3 id="theorem-32-characterization-theorem">Theorem 3.2 (Characterization Theorem).</h3> <p>Let $\mathcal{A}$ be any antichain of partitions. There exists a scale-invariant, consistent clustering function $f$ such that $\text{Range}(f) = \mathcal{A}$.</p> <details> <summary>Proof.</summary> Kleinberg's proof uses the <i>sum-of-pairs</i> clustering method which outputs the partition $\Gamma \in \mathcal{A}$ that minimizes $\Phi_d(\Gamma) = \sum_{(i,j) \sim \Gamma} d(i,j)$ where the notation $(i,j) \sim \Gamma$ indicates $i \underset{\Gamma}{\sim} j$. <br/> Notice that for any $\alpha &gt; 0$, $\Phi_{\alpha \cdot d}(\Gamma) = \sum_{(i,j) \sim \Gamma} \alpha d(i,j) = \alpha \sum_{(i,j) \sim \Gamma} d(i,j) = \alpha \Phi_{d}(\Gamma)$. Since $\alpha$ is positive, the argmin of $\Phi_{\alpha \cdot d}(\Gamma)$ is equivalent to the argmin of $\Phi_d(\Gamma)$, which implies $f(d) = f(\alpha \cdot d)$ so $f$ is scale-invariant. <br/> <br/> Fix some $\Gamma \in \mathcal{A}$. Let $d$ be a distance function satisfying: <ul> <li>$d(i,j) &lt; \frac{1}{n^3}$ for $i,j$ such that $i \underset{\Gamma}{\sim} j$</li> <li>$d(i,j) \geq 1$ for $(i,j)$ such that $i \underset{\Gamma}{\not \sim} j$</li> </ul> Notice that $\Phi_d(\Gamma) = \sum_{(i,j) \sim \Gamma} d(i,j) &lt; 1$ since the summation is only over $i,j$ in the same clusters of which there can be, at most, $n^2$ pairs (if there is only one cluster). <br/> Also notice that $\Phi_d(\Gamma') &lt; 1$ only if $\Gamma'$ is a refinement of $\Gamma$. To see why, consider $\Gamma'$ that is <i>not</i> a refinement of $\Gamma$. This implies that there is some cluster $C' \in \Gamma'$ such that $C' \not\subseteq C$ for all $C \in \Gamma$. This implies that there exist points $i,j$ such that $i \underset{\Gamma'}{\sim} j$, so they are included in the summation in $\Phi_d(\Gamma')$, but $i \underset{\Gamma}{\not \sim} j$, so $d(i,j) &gt; 1$. If $\Gamma'$ is a refinement of $\Gamma$, then $i \underset{\Gamma}{\sim} j$, so $d(i,j) &lt; \frac{1}{n^3}$. <br/> Because $\mathcal{A}$ is an antichain, there is no refinement $\Gamma' \in \mathcal{A}$ of $\Gamma$. Thus, $\Gamma = \underset{\Gamma^* \in \mathcal{A}}{\arg\min} \Phi_d(\Gamma^*)$, which implies $f(d) = \Gamma$. Thus, $f$ is rich, since there exists a $d$ such that $f(d) = \Gamma$ for any $\Gamma \in \mathcal{A}$. <br/> <br/> Let $d$ be a distance function such that $f(d) = \Gamma$, and let $d'$ be a $\Gamma$-transformation of $d$. Furthermore, let $\Gamma'$ be some other partition and define $\Delta(\Gamma') := \Phi_d(\Gamma') - \Phi_{d'}(\Gamma')$. We have that $\Delta(\Gamma) = \sum_{(i,j) \sim \Gamma}d(i,j) - \sum_{(i,j) \sim \Gamma} d'(i,j)$ and $\Delta(\Gamma) = \sum_{(i,j) \sim \Gamma'} d(i,j) - \sum_{(i,j) \sim \Gamma'} d'(i,j)$. Then: $$ \begin{aligned} \Delta(\Gamma) &amp;= \sum_{(i,j) \sim \Gamma} d(i,j) - \sum_{(i,j) \sim \Gamma} d'(i,j) \\ &amp;= \sum_{(i,j) \sim \Gamma} d(i,j) - d'(i,j) \\ &amp;\overset{(i)}{\geq} \sum_{(i,j) \sim \Gamma, (i,j) \sim \Gamma'} d(i,j) - d'(i,j) \\ &amp;\overset{(ii)}{\geq} \sum_{(i,j) \sim \Gamma'} d(i,j) - d'(i,j) \\ &amp;= \Delta(\Gamma') \end{aligned} $$ We know $d'$ is a $\Gamma$-transformation of $d$. Thus, $d'(i,j) \leq d(i,j)$ for $(i,j) \sim \Gamma$, implying that $d(i,j) - d'(i,j) \geq 0$ for all $(i,j) \sim \Gamma$. $(i)$ follows from the fact that $\rvert (i,j) \sim \Gamma \cap (i,j) \sim \Gamma' \rvert \leq \rvert (i,j) \sim \Gamma \rvert$. <br/> Similarly, $(ii)$ is equivalent to the summation in $(i)$ plus $(i,j) \sim \Gamma'$ such that $(i,j) \not\sim \Gamma$. For these pairs, $d'(i,j) \geq d(i,j)$, since $d'$ is a $\Gamma$-transformation of $d$. This means $d(i,j) - d'(i,j) \leq 0$ for these pairs, which implies $(ii)$. <br/> The above argument implies $\Delta(\Gamma) \geq \Delta(\Gamma')$ for any $\Gamma' \in \mathcal{A}$. This implies $\Phi_d(\Gamma) - \Phi_{d'}(\Gamma) \geq \Phi_d(\Gamma') - \Phi_{d'}(\Gamma')$, which implies $\Phi_d(\Gamma) - \Phi_d(\Gamma') \geq \Phi_{d'}(\Gamma) - \Phi_{d'}(\Gamma')$. <br/> Since $\Gamma'$ minimizes $\Phi_{d'}$ and $\Gamma$ minimizs $\Phi_{d}$, we have that $\Phi_d(\Gamma) - \Phi_{d'}(\Gamma) \leq 0$. Chaining this together with the previous yields $0 \geq \Phi_d'(\Gamma) - \Phi_{d'}(\Gamma')$, which implies $\Phi_{d'}(\Gamma') \geq \Phi_{d'}(\Gamma)$. If $\Gamma'$ minimizes, $\Phi_{d'}$, then it must be the case that $\Gamma' = \Gamma$. Thus, $f(d') = \Gamma$, and therefore $f$ is consistent. </details> <hr/> <h2 id="relaxations">Relaxations</h2> <p>Due to the impossibility theorem, it might be worthwhile to look into easing up the conditions in the axioms. Kleinberg provides a few examples.</p> <h4 id="relaxing-richness">Relaxing Richness</h4> <p><a href="#theorem3.2">Theorem 3.2</a> is an example of a relaxation of the richness property. If we were satisfied with a clustering function that is scale-invariant, consistent, but only achieves an antichain as its range, then the sum-of-pairs method will work.</p> <h4 id="relaxing-consistency">Relaxing Consistency</h4> <p>Kleinberg proposes <i>Refinement-Consistency</i>, in which a $\Gamma$-transformation should output a refinement of $\Gamma$. Unfortunately, this is not yet enough; a scale-invariant, rich, and refinement-consistent clustering function does not exist. However, if one also relaxes richness to say that all but one (trivial) partition can be achieved by $f$ — termed <i>Near-Richness</i> —, then Theorem 2.1 does not hold.</p> <p>An alternative is to relax consistency to something I’ll call <i>Weak Consistency</i>, which is where if $d’$ is a $f(d)$-transformation of $d$, then either $f(d’)$ is a refinement of $f(d)$ or $f(d)$ is a refinement of $f(d’)$. There do exist clustering functions that satisfy all three of scale-invariance, richness, and weak consistency.</p> <p>In some ways, this relaxation may be more reasonable. For example, consider some partition that results in four clusters arranged in a square. Now, construct a distance function that just puts more distance between the left clusters and the right clusters. Although this new distance function is a $\Gamma$-transformation, it might be better to combine the left clusters and right clusters to have a partition with two groups. Ackerman and Ben-David provide this example as an illustration in Figure 1 of their paper.</p> <h4 id="relaxing-scale-invariance">Relaxing Scale-Invariance</h4> <p>Kleinberg does not discuss a relaxation of scale-invariance at length. He mentions that <i>single-linkage clustering</i> where we stop combining clusters when their distances exceed some value $r$ satisfies consistency and richness. It satisfies a weaker scale-invariance where we let $f(\alpha \cdot d)$ be a refinement of $f(d)$ when $\alpha &gt; 1$. Besides this, I don’t see why relaxing scale-invariance any more than this would be an attractive property of a clustering function.</p> <aside><p>Single-linkage clustering is an agglomerative method in which we start by letting each observation be its own cluster. Then we iteratively combine clusters based upon distances until some stopping criterion is met.</p></aside> <hr/> <h2 id="the-consistency-theorem">The Consistency Theorem</h2> <p>Kleinberg’s main result is a theorem (and its proof) stating that there does not exist a clustering function that satisfies three simple and desirable properties. This seems quite disappointing as clustering is a popular subtopic in unsupervised learning, and it seems to imply that clustering is at least impossibly difficult to define precisely if not simply impossible to actually do.</p> <p>Ackerman and Ben-David push back against this interpretation and state that the way in which Kleinberg axiomatized clustering was part of the reason for the impossibility result. They provide a slightly different perspective and provide a set of axioms for the function used to assess clustering quality rather than the clustering function itself.</p> <p>Ackerman and Ben-David adjust Steinberg’s work to apply to CQMs rather than clustering functions, which results in a consistent set of axioms. They also add one additional property in order to make their set of axioms satisfy two properties (<i>soundness</i> and <i>completeness</i>, which I won’t go into here) that make it more useful for defining what methods should be used for clustering.</p> <div id="axioms2"></div> <blockquote> <p><strong>Definition (Ackerman and Ben-David Axioms).</strong></p> <ul> <li><i>Scale Invariance:</i>A CQM $m$ is called <i>scale-invariant</i> if for every partition $\Gamma$ of set $S$ with respect to distance $d$ and ever $\alpha &gt; 0$, we have $m(\Gamma, S, d) = m(\Gamma, S, \alpha \cdot d)$. </li> <li><i>Richness:</i>A CQM $m$ is called _rich_ if for each non-trivial partitions $\Gamma$ of $S$, there exists distance function $d$ such that $\Gamma = \underset{\Gamma'}{\arg \max} \left[ m(\Gamma', S, d) \right]$. A CQM will satisfy the richness property if, for each non-trivial $\Gamma$, we have $m(\Gamma, S, d) \geq m(\Gamma', S, d)$ for all possible partitions $\Gamma'$ of $S$ with $d$ (which may be chosen for each $\Gamma$). The $\max$ becomes a $\min$, and the inequality is reversed for CQMs that assign lower values to better clusterings.</li> <li><i>Consistency:</i>A CQM $m$ is called <i>consistent</i> if for every $\Gamma$ of $S$ with $d$, $m(\Gamma, X, d') \geq m(\Gamma, S, d)$ for any $\Gamma$-transformation, $d'$, of $d$. This condition is weaker than the consistency defined by Kleinberg as it does not penalize clustering functions that are only weakly consistent (recall that this means $f(d)$ and $f(d')$ are permitted to be refinements of each other).</li> <li><i>Isomorphism Invariance:</i>A CQM $m$ is called <i>isomorphism-invariant</i> if, for all $\Gamma$, $\Gamma'$ of $S$ with $d$ such that $\Gamma$ and $\Gamma'$ are isomorphic, we have $m(\Gamma, S, d) = m(\Gamma', S, d)$. This condition basically states that if we have two clusterings that would be the same if we swapped the points around (in a special way...), they should have the same score according to the CQM.</li> </ul> </blockquote> <p>Ackerman and Ben-David then prove that consistent CQMs exist.</p> <h3 id="theorem-2--3-ackerman-and-ben-david">Theorem 2 &amp; 3 (Ackerman and Ben-David).</h3> <p>There exists a clustering quality measure that satisfies all four of scale-invariance, richness, consistency. and isomorphism invariance. That is, the four properties comprise a consistent set of axioms.</p> <details> <summary>Proof.</summary> It suffices to construct a CQM and prove it satisfies the three axioms. <br/> First, we define the <i>Relative Point Margin</i>. For distance $d$ and clustering $\Gamma$ containing $k$ clusters, the <i>$G$-Relative Point Margin</i> is defined as $G$-$RM_{S, d}(x) = \frac{d(x, g_x)}{d(x, g'_{x})}$ where $g_x \in G$ is the closest cluster center to $x$, $g'_{x} \in G$ is the second closest cluster center to $x$, and $G \subseteq S$. <br/> Next, define the <i>Relative Margin</i>. Let $\mathcal{G}$ denote the set of all possible representative sets of $\Gamma$. Then the relative margin of $\Gamma$ over $(S, d)$ is $RM_{S, d}(\Gamma) = \underset{G \in \mathcal{G}}{\min} \left\{ \underset{x \in S \setminus G}{\textit{avg}} G\text{-}RM_{S, d}(x) \right\}$. This is the representative set that achieves the minimum average relative point margin where the average is taken over all points not in the representative set. The relative margin assigns lower values to better clusterings (so the inequalities in the richness and consistency definitions will be reversed). <br/> Let $\Gamma$ be an arbitrary clustering of the set $S$ with distance function $d$ in the following. <br/> <br/> <i>Scale-Invariance.</i> Let $d'$ be a distance function satisfying $d'(i,j) = \alpha d(i, j)$ for all $i,j \in S$ and all $\alpha &gt; 0$. For any $i,j,k \in S$, we have: $$ \frac{d'(i,j)}{d'(i,k)} = \frac{\alpha d(i,j)}{\alpha d(i,k)} = \frac{d(i,j)}{d(i,k)} \implies \frac{d'(i, g_i)}{d'(i, g'_i)} = \frac{d(i, g_i)}{d(i, g'_i)} $$ since scaling all distances will result in the same centers. This implies that $RM_{S, d'}(\Gamma) = RM_{S, d}(\Gamma)$. <br/> <br/> <i>Consistency.</i> Let $d'$ be a $\Gamma$-transformation of $d$. We have: $$ \begin{aligned} \begin{cases} d'(i,j) \leq d(i,j) &amp; \text{for } i \underset{\Gamma}{\sim} j \\ d'(i,j) \geq d(i,j) &amp; \text{for } i \underset{\Gamma}{\not \sim} j \end{cases} &amp;\implies \frac{d'(i,j)}{d'(i,k)} \leq \frac{d(i,j)}{d(i,k)} \text{ for } i \underset{\Gamma}{\sim} j; i \underset{\Gamma}{\not \sim} k \\ &amp;\implies G\text{-}RM_{S,d'}(i) \leq G\text{-}RM_{S,d}(i) \text{ for any } G \in \mathcal{G} \end{aligned} $$ The first implication follows from the fact that the closest cluster center to $i$ will be a point in the same cluster, and the second closest cluster center to $i$ will be a point in a different cluster (by the definition of a representative set). <br/> <br/> <i>Richness.</i> Let $\Gamma$ be an arbitrary non-trivial clustering of $S$ with $d$. Define the distance function $d$ as: $$ \begin{cases} d(i,j) = 1 &amp; \text{for } i \underset{\Gamma}{\sim} j \\ d(i,j) = 10 &amp; \text{for } i \underset{\Gamma}{\not \sim} j \end{cases} \nonumber $$ It follows that: $$ \begin{aligned} \underset{\Gamma'}{\arg \min} \left\{ RM_{S, d}(\Gamma') \right\} &amp;= \underset{\Gamma'}{\arg \min} \left\{ \underset{G \in \mathcal{G}}{\min} \left[ \underset{x \in S \setminus G}{\textit{avg}} G\text{-}RM_{S,d}(x) \right] \right\} \\ &amp;= \underset{\Gamma'}{\arg \min} \left\{ \underset{G \in \mathcal{G}}{\min} \left[ \frac{1}{\rvert S \setminus G \rvert} \sum_{x \in S \setminus G} \frac{d(x, g_x)}{d(x, g'_x)} \right] \right\} \\ &amp;= \underset{\Gamma'}{\arg \min} \left\{ \underset{G \in \mathcal{G}}{\min} \left[ \frac{1}{\rvert S \setminus G \rvert} \sum_{x \in S \setminus G} \frac{1}{10} \right] \right\} \\ &amp;= \underset{\Gamma'}{\arg \min} \left\{ \underset{G \in \mathcal{G}}{\min} \left[ \frac{1}{10} \right] \right\} \end{aligned} $$ We arrive at the fact that, for this choice of $d$, the minimum $RM_{S,d}(\Gamma')$ is achieved by every non-trivial partition of $S$. Thus, $\Gamma = \underset{\Gamma'}{\arg \min} \left\{ RM_{S, d}(\Gamma') \right\}$. <br/> <br/> <i>Isomorphism Invariance.</i> Let $\Gamma'$ be a partition such that $\Gamma \underset{d}{\approx} \Gamma'$. Since they are isomorphic, there exists a distance-preserving isomorphism $\phi$. Let $G' := \{ \phi(x): x \in G \}$, and let $\mathcal{G}'$ be the set of all $G'$. Thus: $$ \begin{aligned} \frac{d(i, g_i)}{d(i, g'_i)} = \frac{d(\phi, g_{\phi(i)})}{d(\phi, g'_{\phi(i)})} &amp;\implies G\text{-}RM_{S,d}(i) = G'\text{-}RM_{S,d}(i) \\ &amp;\implies \underset{i \in S \setminus G}{avg} \left\{ G\text{-}RM_{S,d}(i)\right\} = \underset{i \in S \setminus G'}{avg} \left\{ G'\text{-}RM_{S,d}(i) \right\} \\ &amp;\implies \underset{G \in \mathcal{G}}{\min} \left[ \underset{i \in S \setminus G}{avg} \left\{ G\text{-}RM_{S,d}(i)\right\}\right] = \underset{G \in \mathcal{G}}{\min} \left[ \underset{i \in S \setminus G'}{avg} \left\{ G'\text{-}RM_{S,d}(i)\right\}\right] \\ &amp;\implies RM_{S,d}(\Gamma) = RM_{S,d}(\Gamma') \end{aligned} $$ </details> <h2 id="examples-and-extensions">Examples and Extensions</h2> <p>Ackerman and Ben-David provide a few different examples of CQMs that satisfy their axioms.</p> <h4 id="weakest-link">Weakest Link</h4> <p>Suppose we are in a linkage-based regime. Define the <i>Weakest Link Between Points</i> as the following. Let $\gamma_k$ be the $k$-th cluster in $\Gamma$. For $\Gamma$ over $S$ with $d$:</p> \[\Gamma\text{-}WL_{S, d}(i, j) = \underset{x \in \gamma_k \forall k}{\min} \left\{ \max \left[ d(i, x), d(i, j) \right] \right\}\] <p>The <i>Weakest Link</i> of $\Gamma$ over $S$ with $d$ is:</p> \[WL(\Gamma) = \frac{\underset{i \underset{\Gamma}{\sim} j}{\max} \Gamma\text{-}WL_{S,d}(i,j)}{\underset{i \underset{\Gamma}{\not \sim}}{\min} d(i,j)}\] <p>This is calculable in $O(n^3)$ time.</p> <h4 id="additive-margin">Additive Margin</h4> <p>Suppose we are in a center-based clustering (like $k$-means). Define the <i>Additive Point Margin</i> as:</p> \[G\text{-}AM_{S, d}(i) = d(i, \gamma'_i) - d(i, \gamma_i)\] <p>The <i>Additive Margin</i> of $\Gamma$ over $S$ with $d$ is:</p> \[AM_{S,d}(\Gamma) = \underset{G \in \mathcal{G}}{\min} \left\{ \frac{ \frac{1}{\rvert S \rvert} \sum_{i \in S} G\text{-}AM_{S, d}(i) }{ \frac{1}{\rvert \left\{ \{ i, j\} \subseteq S \rvert i \underset{\Gamma}{\sim} j \right\}} \sum_{i \underset{\Gamma}{\sim} j} d(i, j) } \right\}\] <p>This is calculable in $O(n^{k+1})$ time.</p> <h4 id="functions-of-cqms">Functions of CQMs</h4> <p>One can also use functions of clustering-quality measures to create a new one. If one had a CQM defined for partitions of $k$ clusters, one could consider taking the minimum, maximum, or average over all subsets of size $k$ for a clustering of arbitrary size greater than $k$.</p> <h4 id="cluster-number-dependence">Cluster Number Dependence</h4> <p>The above CQMs do not depend on the number of clusters in a partition, which makes it easy to compare clusterings that are of different sizes. Ackerman and Ben-David extend their framework to CQMs that <strong>do</strong> depend on cluster number, such as those that are based upon the objective functions of some clustering methods (such as $k$-means).</p> <p>In order to make these CQMs compliant with the scale-invariance property, the quality scores must be normalized in some way. An example is <i>$\mathcal{L}$-normalization</i>, which scales the loss of a clustering by the loss of a trivial clustering that has a single cluster of all observations.</p> <p>Loss-based clustering functions also tend to either reward or punish more clusters. Ackerman and Ben-David term CQMs based on a loss function that “prefers” more clusters as <em>refinement-preferring</em> and those based on losses that prefer fewer clusters as <i>coarsening-preferring</i>. More explicitly, a refinement-preferring CQM will assign a better quality score to refinements of $\Gamma$ than to $\Gamma$, and a coarsening-preferring CQM will assign better quality scores to $\Gamma$ than to its refinements. These CQMs do not satisfy the richness property.</p>]]></content><author><name></name></author><category term="clustering"/><category term="clustering"/><summary type="html"><![CDATA[An Axiomatic Approach]]></summary></entry></feed>