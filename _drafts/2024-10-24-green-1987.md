---
layout: post
title:  "Penalized Likelihood for General Semi-parametric Regression Models (Green, 1987)"
date: 28 October 2024
categories: posts
use_math: true
---

Recent work has led me to explore some of the older literature on likelihood-based inference.
Though not _the_ text on penalized likelihood, I found that Peter J. Green's "Penalized Likelihood for General Semi-parametric Regression Models" to be a good starting point for my research into variance component testing.


## Problem Set-Up

Suppose we have $n$ observations, the $i$-th of which includes some response, $y_i$. Our setting will be one in which we believe the responses are generated by some parametric model depending on predictor, $\theta_1, \dots, \theta_n$, with some *flavors* added by a not-completely-known relationship with other variables (e.g. time, space, whatever your mind can imagine). 

We'll assume that our model consists of two parts: one systematic and one random. The random portion will be determined by the log-likelihood function $\ell(y; \theta)$. The systematic portion will be defined by an assumed relationship between $\theta$ and the measured covariates:

$$
\theta_i = b(x_i) + \gamma(t_i)
$$

where $x_i$ and $t_i$ are (usually vector-valued) covariates for the $i$-th case. $b(\cdot)$ and $\gamma(\cdot)$ are some functions. For simplicity, we assume that $b(x_i) = x_i^\top \beta$ for unknown parameter vector $\beta$ and that $\gamma$ is a funciton in a linear space $\mathcal{G}$ and is real-valued. Let $\ell(y; \theta(\beta, \gamma))$ denote this _composite likelihood function_ for our entire model.

The main goal in this setting will be in estimation of $\beta$, the parameter vector. Without additional restrictions, $\beta$ is unidentifiable because of the flexibility afforded by the generality of $\gamma$. As a result, we maximize the _penalized likelihood_:

$$
\ell(y; \theta(\beta, \gamma)) - \frac{1}{2}\lambda J(\gamma)
\label{eq:pl}
$$

instead of the proper likelihood in order to impose some restrictions on $\gamma$ and prevent overfitting. In the above, $J$ is a term that penalizes the roughness of $\gamma$, and $\lambda > 0$ is a tuning hyperparameter to control $\gamma$'s smoothness.

To make estimation feasible, we first constrain the relationship betwen $\theta$ and $\gamma$ as:

$$
\theta(\beta, \xi) = \theta\left(\beta, \sum_{j = 1}^q \xi_j \phi_j\right)
$$

which forces $\gamma$ to be a linear combination of $q$ basis functions, $\{ \phi_j \rvert j = 1, 2, \dots, 1 \}$ (i.e. $\gamma \in \mathcal{F}$, linear subspace of $\mathcal{G}$ that is of dimension $q$ or fewer). Furthermore, we enforce that:

$$
J\left(\sum_{j = 1}^q \xi_j \phi_j\right) = \xi^\top K \xi 
$$

for $q \times q$ matrix $K$ that is non-negative and definite.


Note: This restriction may not be so restrictive depending upon the choice of $\phi_1, \dots, \phi_q$ and $q$. $q$ may be _very_ large, and the basis functions may be allowed to be quite complex.

---

## Estimation

Putting together the above, we rewrite Eq. \eqref{eq:pl} and aim to maximize:

$$
\ell(y; \theta(\beta, \xi)) - \frac{1}{2}\lambda \xi^\top K \xi
\label{eq:mpl}
$$

over $\beta \in \mathbb{R}^p$, $\xi \in \mathbb{R}^q$, and $\theta \in \mathbb{R}^{n}$. 

Letting $\beta_0$ and $\xi_0$ denote the true values, we assume that $\ell$ is "nice enough" to use standard optimization procedures (i.e. iteratively reweighted least squares).

We define the following quantities:

$$
u = \frac{\partial \ell}{\partial \theta}^\top \hspace{4mm} A = \mathbb{E}\left[ - \frac{\partial^2 \ell}{\partial \theta \theta^\top} \right] \hspace{4mm} D = \frac{\partial \theta}{\partial \beta} \hspace{4mm} E = \frac{\partial \theta}{\partial \xi}
\label{eq:uade}
$$

$u$ is the $n$-dimensional [score](https://en.wikipedia.org/wiki/Informant_(statistics)) vector, $A$ is the $n \times n$ matrix equal to the variance of the score (see [Fisher information](https://en.wikipedia.org/wiki/Fisher_information#Definition)), and $D$ and $E$ are $n \times p$ and $n \times q$ matrices, respectively.

Differentiating Eq. \eqref{eq:mpl} with respect to $\beta$ and $\xi$ yield:

$$
\begin{aligned}
&\frac{\partial}{\partial \beta} \left[ \ell(y; \theta(\beta, \xi)) - \frac{1}{2}\lambda \xi^\top K \xi \right] = \frac{\partial}{\partial \beta} \ell(y; \theta(\beta, \xi)) = \frac{\partial \theta}{\beta}^\top \frac{\partial \ell}{\partial \theta}^\top = D^\top u \\
\implies &D^\top u = 0
\end{aligned}
\label{eq:leq1}
$$

and

$$
\begin{aligned}
&\frac{\partial}{\partial \xi} \left[ \ell(y; \theta(\beta, \xi)) - \frac{1}{2}\lambda \xi^\top K \xi \right] = \frac{\partial \ell}{\partial \theta} \frac{\partial \theta}{\partial \xi} - \lambda K \xi = E^\top u - \lambda K \xi \\
\implies &E^\top u = \lambda K \xi
\end{aligned}
\label{eq:leq2}
$$

which are our likelihood equations. Solutions to Eqs. \eqref{eq:leq1} and \eqref{eq:leq2} ($\hat{\beta}$ and $\hat{\xi}$) will be our maximum (modified penalized) likelihood estimates.

The following sub-sections pull from Green's earlier paper "Iteratively Reweighted Least Squares for Maximum Likelihood Estimation, and Some Robust and Resistant Alternatives" in which the iteration refers to Newton-Raphson.

---
<details>
  <summary>Quick Recap of Newton-Raphson.</summary>
  <br>
  Newton-Raphson, or Newton's method, is an optimization method for minimization of a function by finding the roots of its derivative. The basic idea is to step towards the solution by moving between minimizers of second-order approximations of our function of interest. 
  <br>
  Let $f: \mathbb{R}^p \rightarrow \mathbb{R}$ be some "nice enough" function, and let $\nabla^2 f(\mathbf{x}) \in \mathbb{R}^{p \times p}$ denote its Hessian, which we assume to be positive definite.
  <br>
  Let $\mathbf{x}_k \in \mathbb{R}^p$ be some starting point, and let $\mathbf{x}_{k+1}$ be the point we are stepping to. Letting $\mathbf{t} = \mathbf{x}_{k+1} - \mathbf{x}_k$, the second-order Taylor approximation of $f(\mathbf{x}_{k+1})$ about $\mathbf{x}_k$ is given by:

  $$
  f(\mathbf{x}_k + t) \approx f(\mathbf{x}_k) + \mathbf{t}^\top \nabla f(\mathbf{x}_k) + \frac{1}{2}\mathbf{t}^\top \nabla^2f(\mathbf{x}_k) \mathbf{t}
  \label{eq:taylor}
  $$

  By our niceness assumptions, the minimum of Equation \eqref{eq:taylor} can be obtained by setting its gradient, with respect to $\mathbf{t}$, equal to $\mathbf{0}$:

  $$
  \begin{aligned}
  \mathbf{0} &= \nabla_\mathbf{t} \left[ f(\mathbf{x}_k) + \mathbf{t}^\top \nabla f(\mathbf{x}_k) + \frac{1}{2}\mathbf{t}^\top \nabla^2f(\mathbf{x}_k) \mathbf{t} \right] \\
  &= \nabla f(\mathbf{x}_k) + \nabla^2f(\mathbf{x}_k) \mathbf{t} \\
  \implies \mathbf{t} &= - \left[ \nabla^2f(\mathbf{x}_k) \right]^{-1} \nabla f(\mathbf{x}_k) \\
  \implies \mathbf{x}_{k+1} &= \mathbf{x}_k - \left[ \nabla^2f(\mathbf{x}_k) \right]^{-1} \nabla f(\mathbf{x}_k)
  \end{aligned}
  $$

  which gives the position at step $k+1$. 
</details>
---

We are aiming to find the solution, $[\beta, \xi]^\top$, to the following system of equations:

$$
\begin{bmatrix}
f_1(\beta, \xi) \\
f_2(\beta, \xi)
\end{bmatrix} :=
\begin{bmatrix}
D^\top u \\
E^\top u - \lambda K \xi
\end{bmatrix} 
= 
\begin{bmatrix}
0 \\
0
\end{bmatrix}
$$

Newton-Raphson provides an iterative solution where updates are given by:

$$
\begin{aligned}
\begin{bmatrix}
\beta_{k+1} \\
\xi_{k+1}
\end{bmatrix} &= 
\begin{bmatrix}
\beta_k \\
\xi_k
\end{bmatrix} -
\mathcal{J}^{-1}\left( \begin{bmatrix}
f_1(\beta, \xi) \\
f_2(\beta, \xi)
\end{bmatrix}\right)
\begin{bmatrix}
f_1(\beta, \xi) \\
f_2(\beta, \xi)
\end{bmatrix} \\
&= 
\begin{bmatrix}
\beta_k \\
\xi_k
\end{bmatrix} -
\begin{bmatrix}
\frac{\partial f_1(\beta, \xi)}{\partial \beta} & \frac{\partial f_1(\beta, \xi)}{\partial \xi} \\
\frac{\partial f_2(\beta, \xi)}{\partial \beta} & \frac{\partial f_2(\beta, \xi)}{\partial \xi}
\end{bmatrix}^{-1}
\begin{bmatrix}
f_1(\beta, \xi) \\
f_2(\beta, \xi)
\end{bmatrix} \\
\implies 
-\begin{bmatrix}
\frac{\partial f_1(\beta, \xi)}{\partial \beta} & \frac{\partial f_1(\beta, \xi)}{\partial \xi} \\
\frac{\partial f_2(\beta, \xi)}{\partial \beta} & \frac{\partial f_2(\beta, \xi)}{\partial \xi}
\end{bmatrix} \begin{bmatrix}
\beta_{k+1} - \beta_k \\
\xi_{k+1} - \xi_l
\end{bmatrix} &=
\begin{bmatrix}
f_1(\beta, \xi) \\
f_2(\beta, \xi)
\end{bmatrix} \\
\end{aligned}
$$

The partial derivatives in the first term are:

$$
\begin{aligned}
\frac{\partial f_1(\beta, \xi)}{\partial \beta} &= \frac{\partial}{\partial \beta} D^\top u 
= \frac{\partial}{\partial \beta} \left[ \left(\frac{\partial \theta}{\partial \beta}\right)^\top \frac{\partial \ell}{\partial \theta} \right]
= \frac{\partial}{\partial \beta} \left[ \left(\frac{\partial \theta}{\partial \beta}\right)^\top \right] \frac{\partial \ell}{\partial \theta} + \left(\frac{\partial \theta}{\partial \beta}\right)^\top \frac{\partial}{\partial \beta} \left[ \frac{\partial \ell}{\partial \theta} \right] 
= \frac{\partial \theta^2}{\partial \beta \beta^\top} \frac{\partial \ell}{\partial \theta} + \left( \frac{\partial \theta}{\partial \beta} \right)^\top \frac{\partial \ell}{\partial \theta} \left( \frac{\partial \ell}{\partial \theta} \right)^\top \frac{\partial \theta}{\partial \beta}
\end{aligned}
$$



---




### Diagonal Terms

Letting $\beta_k$ be our value of $\beta$ at iteration $k$. Using notation in Green, 1984 rather than $\nabla$, the subsequent value, $\beta_{k+1}$, is:

$$ 
\begin{aligned}
&\beta_{k+1} = \beta_k - \left[ \frac{\partial^2 \ell}{\partial \beta \beta^\top}\right]^{-1}\frac{\partial \ell}{\partial \beta} \\
\implies  &\beta_{k+1} - \beta_k = -\left[ \frac{\partial^2 \ell}{\partial \beta \beta^\top}\right]^{-1}\frac{\partial \ell}{\partial \beta} \\
\implies &-\frac{\partial^2 \ell}{\partial \beta \beta^\top}(\beta_{k+1} - \beta_k) = \frac{\partial \ell}{\partial \beta} \\
\implies &-\frac{\partial^2 \ell}{\partial \beta \beta^\top}(\beta_{k+1} - \beta_k) = \frac{\partial \ell}{\partial \theta} \frac{\partial \theta}{\partial \beta} \\
\implies &-\frac{\partial^2 \ell}{\partial \beta \beta^\top}(\beta_{k+1} - \beta_k) = D^\top u
\end{aligned}
\label{eq:beta-sol}
$$

The last line of Eq. \eqref{eq:beta-sol} is Eq. (2) in (Green, 1984). As noted in (Green, 1984) — I believe this is just the chain rule — the following equality holds true:

$$
-\frac{\partial^2 \ell}{\partial \beta \beta^\top} = - \sum_{i = 1}^p \underbrace{\left( \frac{\partial \ell}{\partial \theta_i} \frac{\partial^2 \theta_i}{\partial \beta \beta^\top} \right)}_{(a)} - \underbrace{\left( \frac{\partial \theta}{\partial \beta} \right)^\top \frac{\partial^2 \ell}{\partial \theta \theta^\top} \left( \frac{\partial \theta}{\partial \beta} \right)}_{(b)}
\label{eq:second-deriv}
$$

Using a variant of [Fisher scoring](https://en.wikipedia.org/wiki/Scoring_algorithm), we can approximate the right-hand side of Eq. \eqref{eq:second-deriv} by taking the expectation. 

The first term (a) is one component of the score ($u$; see Eq. \eqref{eq:uade}) times a non-random value. The score function has zero mean, so the first half of the RHS is $0$. 

The second term (b), in expectation, can be rewritten as:

$$
\left( \frac{\partial \theta}{\partial \beta} \right)^\top \mathbb{E}\left[\frac{\partial^2 \ell}{\partial \theta \theta^\top}\right] \left( \frac{\partial \theta}{\partial \beta} \right) = D^\top A D
$$

Using this approximation in Eq. \eqref{eq:beta-sol} yields:

$$
(D^\top A D)(\beta_{k+1} - \beta_k) = D^\top u
$$

---

We do basically the same thing for $\xi$ as we did for $\beta$. Let $\xi_k$ be our value of $\xi$ at iteration $k$. The subsequent value, $\xi_{k+1}$, is:

$$ 
\begin{aligned}
&\xi_{k+1} = \xi_k - \left[ \frac{\partial^2}{\partial \xi \xi^\top} \left( \ell(y; \theta(\beta_k, \xi_k)) - \frac{1}{2}\lambda \xi_k^\top K \xi_k \right) \right]^{-1}\frac{\partial}{\partial \xi}\left( \ell(y; \theta(\beta_k, \xi_k)) - \frac{1}{2}\lambda \xi_k^\top K \xi_k \right) \\
\implies &\xi_{k+1} - \xi_k = -\left[ \frac{\partial^2 \ell}{\partial \xi \xi^\top} - \lambda K \right]^{-1} \left( \frac{\partial \ell}{\partial \xi} - \lambda K \xi_k \right) \\
\implies &-\left(\frac{\partial^2 \ell}{\partial \xi \xi^\top} - \lambda K \right) (\xi_{k+1} - \xi_k) = \frac{\partial \ell}{\partial \xi} - \lambda K \xi_k \\
\implies &-\left(\frac{\partial^2 \ell}{\partial \xi \xi^\top} - \lambda K \right) (\xi_{k+1} - \xi_k) = \frac{\partial \ell}{\partial \theta}\frac{\partial \theta}{\partial \xi} - \lambda K \xi_k \\
\implies &-\left(\frac{\partial^2 \ell}{\partial \xi \xi^\top} - \lambda K \right) (\xi_{k+1} - \xi_k) = E^\top u - \lambda K \xi_k \\
\end{aligned}
\label{eq:xi-step}
$$

Using something similar to Eq. \eqref{eq:second-deriv}, we have:

$$
\begin{aligned}
&-\frac{\partial^2 \ell}{\partial \xi \xi^\top} = - \sum_{i = 1}^p \left( \frac{\partial \ell}{\partial \theta_i} \frac{\partial^2 \theta_i}{\partial \xi \xi^\top} \right) - \left( \frac{\partial \theta}{\partial \xi} \right)^\top \frac{\partial^2 \ell}{\partial \theta \theta^\top} \left( \frac{\partial \theta}{\partial \xi} \right) \\
\implies &-\frac{\partial^2 \ell}{\partial \xi \xi^\top} \approx -\mathbb{E} \left[ \sum_{i = 1}^p \left( \frac{\partial \ell}{\partial \theta_i} \frac{\partial^2 \theta_i}{\partial \xi \xi^\top} \right)\right] - \mathbb{E}\left[ \left( \frac{\partial \theta}{\partial \xi} \right)^\top \frac{\partial^2 \ell}{\partial \theta \theta^\top} \left( \frac{\partial \theta}{\partial \xi} \right) \right] \\
\implies &-\frac{\partial^2 \ell}{\partial \xi \xi^\top} \approx -\sum_{i = 1}^p \left( \mathbb{E} \left[ \frac{\partial \ell}{\partial \theta_i} \right] \frac{\partial^2 \theta_i}{\partial \xi \xi^\top} \right) -  \left( \frac{\partial \theta}{\partial \xi} \right)^\top  \mathbb{E}\left[\frac{\partial^2 \ell}{\partial \theta \theta^\top}  \right] \left( \frac{\partial \theta}{\partial \xi} \right)\\
\implies &-\frac{\partial^2 \ell}{\partial \xi \xi^\top} \approx - \left( \frac{\partial \theta}{\partial \xi} \right)^\top  \mathbb{E}\left[\frac{\partial^2 \ell}{\partial \theta \theta^\top}  \right] \left( \frac{\partial \theta}{\partial \xi} \right) \\
\implies &-\frac{\partial^2 \ell}{\partial \xi \xi^\top} \approx -E^\top A E\\
\end{aligned}
\label{eq:second-deriv-b}
$$

Putting Eq. \eqref{eq:second-deriv-b} together with Eq. \eqref{eq:xi-step} yields:

$$
-\left(E^\top A E - \lambda K \right) (\xi_{k+1} - \xi_k) = E^\top u - \lambda K \xi_k \\
$$


### Finding Off-Diagonal Terms

The above sub-section deals with 



---

## References

Green, Peter J. "Iteratively Reweighted Least Squares for Maximum Likelihood Estimation, and Some Robust and Resistant Alternatives". Journal of the Royal Statistical Society: Series B (Methodological), 46 (1984): 149-170. [doi:10.1111/j.2517-6161.1984.tb01288.x](https://doi.org/10.1111/j.2517-6161.1984.tb01288.x).


Green, Peter J. “Penalized Likelihood for General Semi-Parametric Regression Models.” International Statistical Review / Revue Internationale de Statistique 55, no. 3 (1987): 245–59. [doi:10.2307/1403404](https://doi.org/10.2307/1403404).
