---
layout: post
title:  "Generalized Linear Mixed Models: Maximum Likelihood Estimation & Variance Component Testing"
date: 30 October 2024
categories: posts
use_math: true
---

This post serves as a general summary of work related to testing and estimation for generalized linear mixed models. The notation will follow that of Zhang and Lin (2008). 


## Set-Up - Generalized Linear Mixed Model

We have $n$ observations, and for the $i$-th observation we have a response, $y_i$ (let $y$ denote the full data). We will have $p$ fixed effect covariates arranged in a $p$-dimensional vector, $x_{i}$, and $q$ random effects covariates in a $q$-dimnsional vector, $z_{i}$. 

We assume that we have a $p$-dimensional fixed effect coefficient vector, $\alpha$, and $q$-dimensional random effect coefficient vector, $\beta$. In addition, we assume $\beta \sim \mathcal{F}(\mathbf{0}, D(\theta))$ for a distribution, $F$, and covariance matrix, $D(\theta)$, depending on an $m$-dimensional variance component vector, $\theta$.

Furthermore, we assume that, conditional on the random effects, the responses are independent with conditional means $\mathbb{E}[y_i \rvert \beta] = \mu_i$ and variances $\text{Var}(y_i \rvert \beta) = \phi \omega_i^{-1}v(\mu_i)$ for scale parameter, $\phi > 0$, weight $\omega_i$, and variance function $v(\cdot)$. Our model comes in the form of a specification of the conditional mean. For a monotonic and differentiable link function (e.g. $\log(\cdot)$ or $\text{logit}(\cdot)$), we assume:

$$
g(\mu_i) = x_i^\top \alpha + z_i^\top \beta
\label{eq:glmm}
$$

In matrix notation, where $\mu$ is the $n$-dimensional vector of conditional means, $X$ is the $n \times p$ fixed effects covariate matrix ($x_i$ are its rows), and $Z$ is the $n \times q$ random effects covariate matrix ($z_i$ are its rows), Eq. \eqref{eq:glmm} becomes:

$$
g(\mu) = X \alpha + Z \beta
\label{eq:glmm-mat}
$$

---
<details>
  <summary>Additional Assumptions.</summary>
  The third moment and higher moments of $\beta$ are of order $o(\rvert \rvert \theta \rvert \rvert)$
  <br>
  WLOG, entries of $D(\theta)$ are linear in $\theta$
  <br>
  WLOG, $D(\theta) = \mathbf{0}$ if $\theta = \mathbf{0}$
</details>
---

Define $\ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \beta) = \int_{y_i}^{\mu_i} \frac{\omega_i(y_i - u)}{\phi v(u)} du$, the log quasi-likelihood of $(\alpha, \theta)$ given $y_i$, conditional on $\beta$. We define the quasi-likelihood of $\alpha$ and $\theta$ given the complete data, $y$, conditional on the random effects, $\beta$, as:

$$
\begin{aligned}
\mathcal{L}_\text{quasi}(\alpha, \theta; y \rvert \beta) &= \prod_{i = 1}^n \mathcal{L}_\text{quasi}(\alpha, \theta; y_i \rvert \beta) \\
&= \prod_{i = 1}^n \exp \left( \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \beta) \right) \\
&= \prod_{i = 1}^n \exp \left( \int_{y_i}^{\mu_i} \frac{\omega_i(y_i - u)}{\phi v(u)} du \right)  \\
&= \exp \left( \sum_{i = 1}^n \int_{y_i}^{\mu_i} \frac{\omega_i(y_i - u)}{\phi v(u)} du  \right)
\end{aligned}
\label{eq:c-q-lik}
$$


---
<details>
  <summary>An Aside On Quasi-Likelihood.</summary>
  In the following, the conditioning on $\beta$ has been suppressed. 
  <br>
  The quasi-likelihood is motivated by the so-called quasi-score, $U(\mu_i; y_i) = \frac{\omega_i(y_i - \mu_i)}{\phi v(\mu_i)}$. 
  <br>
  Notice that the following properties hold for $U$:

  $$
  \begin{aligned}
  \mathbb{E}[U(\mu_i; y_i)] &= \mathbb{E}\left[ \frac{\omega_i(y_i - \mu_i)}{\phi v(\mu_i)} \right] \\
  &= \frac{\omega_i(\mathbb{E}[y_i] - \mu_i)}{\phi v(\mu_i)} \\
  &= \frac{\omega_i(\mu_i - \mu_i)}{\phi v(\mu)} \\
  &= 0
  \end{aligned}
  $$

  $$
  \begin{aligned}
  \text{Var}(U(\mu_i; y_i)) &= \text{Var}\left( \frac{\omega_i(y_i - \mu_i)}{\phi v(\mu_i)} \right) \\
  &= \frac{\omega_i^2}{\phi^2 v^2(\mu_i)} \text{Var}(y_i - \mu_i) \\
  &= \frac{\omega_i^2}{\phi^2 v^2(\mu_i)} \text{Var}(y_i) \\ 
  &= \frac{\omega_i^2}{\phi^2 v^2(\mu_i)} (\phi \omega_i^{-1} v(\mu_i)) \\
  &= \frac{\omega_i}{\phi v(\mu_i)} \\
  \end{aligned}
  $$

  $$
  \begin{aligned}
  - \mathbb{E}\left[ \frac{\partial}{\partial \mu_i} U(\mu_i; y_i) \right] &= -\mathbb{E}\left[ \frac{\partial}{\partial \mu_i} \frac{\omega_i(y_i - \mu_i)}{\phi v(\mu_i)} \right] \\
  &= -\mathbb{E}\left[ \frac{-\omega_i \phi v(\mu_i) - \omega_i(y_i - \mu_i)\phi v'(\mu_i)}{\phi^2 v^2(\mu_i)}  \right] \\
  &= -\frac{-\omega_i \phi v(\mu_i) - \omega_i\mathbb{E}\left[ (y_i - \mu_i) \right]\phi v'(\mu_i)}{\phi^2 v^2(\mu_i)} \\
  &= -\frac{-\omega_i \phi v(\mu_i) - \omega_i (\mu_i - \mu_i) \phi v'(\mu_i)}{\phi^2 v^2(\mu_i)} \\
  &= -\frac{-\omega_i \phi v(\mu_i)}{\phi^2 v^2(\mu_i)} \\
  &= \frac{\omega_i}{\phi v(\mu_i)}
  \end{aligned}
  $$

  The above properties are also had by the score function, which is very useful in situations when a likelihood function can be constructed. The basic idea of quasi-likelihood is to use the quasi-score to get a log quasi-likelihood and use that in cases when a likelihood function cannot be constructed or identified:
  
  $$
  \ell_{\text{quasi}}(\mu_i; y_i) = \int_{y_i}^{\mu_i} \frac{\omega_i(y_i - u)}{\phi v(u)} du
  $$
</details>
---

Let $f$ denote the density associated with $F$, the distribution of the random effects. The integrated quasi-likelihood of $(\alpha, \theta)$ is given by integrating $\beta$ out of the log quasi-likelihood of $(\alpha, \theta, \beta)$, which itself is the product of the conditional quasi-likelihood and the likelihood of $\beta$:

$$
\begin{aligned}
\mathcal{L}_\text{quasi}(\alpha, \theta; y) &= \int \mathcal{L}_\text{quasi}(\alpha, \theta; y \rvert \beta) f(\beta) d\beta \\
&= \int \exp \left( \ell_\text{quasi}(\alpha, \theta; y \rvert \beta) \right) f(\beta) d\beta \\
&= \int \exp \left( \sum_{i = 1}^n \int_{y_i}^{\mu_i} \frac{\omega_i(y_i - u)}{\phi v(u)} du  \right) f(\beta) d\beta
\end{aligned}
\label{eq:comp-q-lik}
$$

---

Though this is helpful, the complexity of Eq. \eqref{eq:comp-q-lik} can lead to difficulty in parameter estimation for Model \eqref{eq:glmm-mat}. Approximate/numerical methods are usually necessary for parameter estimation and inference.


---

## Global Test 

Lin (1997) derives a test for all variance components being zero. Our null hypothesis will be that $H_0: \theta = \mathbf{0}$ where we consider Model \eqref{eq:glmm-mat}. Under the null hypothesis (i.e. $\theta = \mathbf{0}$), $\mathbb{E}[y_i \rvert \beta] = \mathbb{E}[y_i] = \mu_i$ and $g(\mu_i) = x^\top \alpha$ (due to the fact that $\beta$ has mean zero and no variance, so it is a vector of zeros).

First, let's rewrite the conditional log quasi-likelihood of $(\alpha, \theta)$ given $\beta$ using a [Taylor expansion](https://en.wikipedia.org/wiki/Taylor_series). We'll use a second-order expansion since the remainder will be relatively simple due to the assumptions made on the higher order moments of the random effects.

Denote the first- and second-order partial derivatives of $\ell_{\text{quasi}}(\alpha, \theta; y \rvert \beta)$ evaluated at $\beta = \tilde{\beta}$ with $\kappa'(\tilde{\beta})$ and $\kappa''(\tilde{\beta})$, respectively. Since $\beta$ is $q$-dimensional, $\kappa'$ will be a $q$-dimensional vector, and $\kappa''$ will be a $q \times q$-dimensional matrix. They are equivalent to:

$$
\begin{aligned}
\kappa'(\tilde{\beta}) &=  \sum_{i = 1}^n \frac{\partial \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \tilde{\beta})}{\partial \eta_i}  z_i^\top\\
\kappa''(\tilde{\beta}) &= \left( \sum_{i = 1}^n \frac{\partial \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \tilde{\beta})}{\partial \eta_i} z_i \right) \left( \sum_{i = 1}^n \frac{\partial \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \tilde{\beta})}{\partial \eta_i} z_i^\top \right) + \sum_{i = 1}^n \frac{\partial^2 \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \tilde{\beta})}{\partial \eta_i^2} z_i z_i^\top
\end{aligned}
\label{eq:kappas}
$$


---
<details>
  <summary>Derivation of $\kappa'$</summary>
    We use the fact that $g(\mu_i) = \eta_i = x_i^\top \alpha + z_i^\top \beta$. 

    $$
    \begin{aligned}
      \kappa'(\beta) &= \frac{\partial}{\partial \beta} \left[ \ell_{\text{quasi}}(\alpha, \theta; y \rvert \beta) \right] \\
      &= \frac{\partial}{\partial \beta} \left[  \sum_{i = 1}^n \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \beta) \right] \\
      &=  \sum_{i = 1}^n \frac{\partial}{\partial \beta}  \left[  \int_{y_i}^{\mu_i} \frac{\omega_i(y_i - u)}{\phi v(u)} du  \right] \\
      &= \sum_{i = 1}^n \frac{\partial}{\partial \eta_i}  \left[  \int_{y_i}^{\mu_i} \frac{\omega_i(y_i - u)}{\phi v(u)} du  \right] \frac{\partial \eta_i}{\partial \beta} \\
      &= \sum_{i = 1}^n \frac{\partial}{\partial \eta_i}  \left[  \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \beta)  \right] z_i^\top \\
      &= \sum_{i = 1}^n \frac{\partial \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \beta)}{\partial \eta_i}  z_i^\top
    \end{aligned}
    $$
</details>
---
<details>
  <summary>Derivation of $\kappa''$</summary>
  Because of the number of vectors/matrices, I haven't gone through to check the notation of tranposes. The core of the math should still be correct, though. I use $(V)_i$ to denote the $i$-th component of the vector $V$.

  $$
  \begin{aligned}
    \kappa''(\beta) &= \frac{\partial^2}{\partial \beta^\top} \left[ \sum_{i = 1}^n \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \beta)  \right] \\
    &= \frac{\partial}{\partial \beta} \left[ \sum_{i = 1}^n \frac{\partial}{\partial \eta_i}  \left[  \int_{y_i}^{\mu_i} \frac{\omega_i(y_i - u)}{\phi v(u)} du  \right] z_i^\top \right] \\
    &\overset{(i)}{=} 
    \frac{\partial}{\partial \beta} \left[ \sum_{i = 1}^n \left( 
    \frac{\omega_i(y_i - \mu_i)}{\phi v(\mu_i)} \frac{\partial \mu_i}{\partial \eta_i} -
    \frac{\omega_i(y_i - y_i)}{\phi v(y_i)} \frac{\partial y_i}{\partial \eta_i} +
    \int_{y_i}^{\mu_i} \frac{\partial}{\partial \eta_i} \frac{\omega_i(y_i - u)}{\phi v(u)} du \right) z_i^\top \right] \\
    &\overset{(ii)}{=} 
    \frac{\partial}{\partial \beta} \left[ \sum_{i = 1}^n \left( 
    \frac{\omega_i(y_i - \mu_i)}{\phi v(\mu_i)} g^{-1}(\eta_i) \right) z_i^\top \right] \\
    &= \sum_{i = 1}^n \frac{\partial \eta_i}{\partial \beta} \frac{\partial}{\partial \eta_i} \left( 
    \frac{\omega_i(y_i - \mu_i)}{\phi v(\mu_i)} g^{-1}(\eta_i) \right) z_i^\top \\
    \sum_{i = 1}^n \frac{\partial}{\partial \beta} \frac{\partial \eta_i}{\partial \beta} \left( 
    \frac{\omega_i(y_i - \mu_i)}{\phi v(\mu_i)} g^{-1}(\eta_i) \right) z_i^\top \\
   
    &= \left( \sum_{i = 1}^n \frac{\partial \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \beta)}{\partial \eta_i} z_i \right) \left( \sum_{i = 1}^n \frac{\partial \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \beta)}{\partial \eta_i} z_i^\top \right) + 
    \sum_{i = 1}^n \frac{\partial^2 \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \beta)}{\partial \eta_i^2} z_i z_i^\top
  \end{aligned}
  $$
</details>
---


Since $\frac{\partial}{\partial x} \exp(f(x)) = \exp(f(x)) \frac{\partial}{\partial x} f(x)$, we can write the second-order Taylor expansion of $\exp \left( \ell_\text{quasi}(\alpha, \theta; y \rvert \beta) \right)$ at $\tilde{\beta}$ as:

$$
\begin{aligned}
\exp \left( \ell_\text{quasi}(\alpha, \theta; y \rvert \beta \right)
&= \exp \left( \sum_{i = 1}^n \ell_\text{quasi}(\alpha, \theta; y_i \rvert \tilde{\beta} \right) \left[ 1 + (\beta - \tilde{\beta})^\top \kappa'(\tilde{\beta}) + \frac{1}{2}(\beta - \tilde{\beta})^\top \kappa''(\tilde{\beta}) (\beta - \tilde{\beta}) + e \right] \\
&= \exp \left( \sum_{i = 1}^n \ell_\text{quasi}(\alpha, \theta; y_i \rvert \tilde{\beta} \right) \left[ 1 + (\beta - \tilde{\beta})^\top \left( \sum_{i = 1}^n \frac{\partial \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \tilde{\beta})}{\partial \eta_i}  z_i^\top \right) \right] \\
&\hspace{6mm} + \exp \left( \sum_{i = 1}^n \ell_\text{quasi}(\alpha, \theta; y_i \rvert \tilde{\beta} \right) \left[ \frac{1}{2}(\beta - \tilde{\beta})^\top \left(  \left( \sum_{i = 1}^n \frac{\partial \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \tilde{\beta})}{\partial \eta_i} z_i \right) \left( \sum_{i = 1}^n \frac{\partial \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \tilde{\beta})}{\partial \eta_i} z_i^\top \right) + \sum_{i = 1}^n \frac{\partial^2 \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \tilde{\beta})}{\partial \eta_i^2} z_i z_i^\top \right)(\beta - \tilde{\beta}) + e \right]
\end{aligned}
$$

where $e$ is a remainder term containing the higher order partial derivatives.

We'll do the expansion about $\tilde{\beta} = \mathbf{0}$ (the mean of the random effects), which simplifies to:

$$
\begin{aligned}
\exp \left( \ell_\text{quasi}(\alpha, \theta; y \rvert \beta \right)
&= \exp \left( \sum_{i = 1}^n \ell_\text{quasi}(\alpha, \theta; y_i \rvert \mathbf{0}) \right) \left[ 1 + \beta^\top \left( \sum_{i = 1}^n \frac{\partial \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0})}{\partial \eta_i}  z_i^\top \right) \right] \\
&\hspace{6mm} + \exp \left( \sum_{i = 1}^n \ell_\text{quasi}(\alpha, \theta; y_i \rvert \mathbf{0}) \right) \left[ \frac{1}{2} \beta^\top \left(  \left( \sum_{i = 1}^n \frac{\partial \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0})}{\partial \eta_i} z_i \right) \left( \sum_{i = 1}^n \frac{\partial \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0})}{\partial \eta_i} z_i^\top \right) + \sum_{i = 1}^n \frac{\partial^2 \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0})}{\partial \eta_i^2} z_i z_i^\top \right) \beta + e \right]
\end{aligned}
$$

The unconditional quasi-likelihood in Eq. \eqref{eq:comp-q-lik} can be thought of as the expectation of $\exp \left( \sum_{i = 1}^n \ell_\text{quasi}(\alpha, \theta; y_i \rvert \beta \right)$ with respect to the probability measure specified by $F$: 

$$
\begin{aligned}
\mathcal{L}(\alpha, \theta; y \rvert \beta) 
&= \int \exp \left( \sum_{i = 1}^n \ell_\text{quasi}(\alpha, \theta; y_i \rvert \beta) \right) f(\beta) d\beta \\
&= \mathbb{E} \left[ \exp \left( \sum_{i = 1}^n \ell_\text{quasi}(\alpha, \theta; y_i \rvert \beta) \right) \right] \\
&= \mathbb{E}\left[ \exp \left( \sum_{i = 1}^n \ell_\text{quasi}(\alpha, \theta; y_i \rvert \mathbf{0}) \right) \right] \\
&\hspace{6mm} + \mathbb{E}\left[\exp \left( \sum_{i = 1}^n \ell_\text{quasi}(\alpha, \theta; y_i \rvert \mathbf{0}) \right) \beta^\top \left( \sum_{i = 1}^n \frac{\partial \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0})}{\partial \eta_i}  z_i^\top \right)  \right] \\
&\hspace{6mm} + \mathbb{E}\left[\exp \left( \sum_{i = 1}^n \ell_\text{quasi}(\alpha, \theta; y_i \rvert \mathbf{0}) \right)  \frac{1}{2} \beta^\top \left(  \left( \sum_{i = 1}^n \frac{\partial \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0})}{\partial \eta_i} z_i \right) \left( \sum_{i = 1}^n \frac{\partial \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0})}{\partial \eta_i} z_i^\top \right) + \sum_{i = 1}^n \frac{\partial^2 \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0})}{\partial \eta_i^2} z_i z_i^\top \right) \beta \right] \\
&\hspace{6mm} + \mathbb{E}\left[ \exp \left( \sum_{i = 1}^n \ell_\text{quasi}(\alpha, \theta; y_i \rvert \mathbf{0}) \right)  e\right] \\
&= \exp \left( \sum_{i = 1}^n \ell_\text{quasi}(\alpha, \theta; y_i \rvert \mathbf{0}) \right) \\
&\hspace{6mm} + \exp \left( \sum_{i = 1}^n \ell_\text{quasi}(\alpha, \theta; y_i \rvert \mathbf{0}) \right) \mathbb{E}\left[ \beta^\top \right] \left( \sum_{i = 1}^n \frac{\partial \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0})}{\partial \eta_i}  z_i^\top \right) \\
&\hspace{6mm} + \frac{1}{2} \exp \left( \sum_{i = 1}^n \ell_\text{quasi}(\alpha, \theta; y_i \rvert \mathbf{0}) \right) \mathbb{E}\left[ \beta^\top \left(  \left( \sum_{i = 1}^n \frac{\partial \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0})}{\partial \eta_i} z_i \right) \left( \sum_{i = 1}^n \frac{\partial \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0})}{\partial \eta_i} z_i^\top \right) + \sum_{i = 1}^n \frac{\partial^2 \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0})}{\partial \eta_i^2} z_i z_i^\top \right) \beta \right] \\
&\hspace{6mm} + \exp \left( \sum_{i = 1}^n \ell_\text{quasi}(\alpha, \theta; y_i \rvert \mathbf{0}) \right) \mathbb{E}\left[ e\right] \\
&\overset{(i)}{=} \exp \left( \sum_{i = 1}^n \ell_\text{quasi}(\alpha, \theta; y_i \rvert \mathbf{0}) \right) \\
&\hspace{6mm} + \frac{1}{2} \exp \left( \sum_{i = 1}^n \ell_\text{quasi}(\alpha, \theta; y_i \rvert \mathbf{0}) \right) \mathbb{E}\left[ \beta^\top \left(  \left( \sum_{i = 1}^n \frac{\partial \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0})}{\partial \eta_i} z_i \right) \left( \sum_{i = 1}^n \frac{\partial \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0})}{\partial \eta_i} z_i^\top \right) + \sum_{i = 1}^n \frac{\partial^2 \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0})}{\partial \eta_i^2} z_i z_i^\top \right) \beta \right] \\
&\hspace{6mm} + \exp \left( \sum_{i = 1}^n \ell_\text{quasi}(\alpha, \theta; y_i \rvert \mathbf{0}) \right) \mathbb{E}\left[ e\right] \\
&\overset{(ii)}{=} \exp \left( \sum_{i = 1}^n \ell_\text{quasi}(\alpha, \theta; y_i \rvert \mathbf{0}) \right) \\
&\hspace{6mm} + \frac{1}{2} \exp \left( \sum_{i = 1}^n \ell_\text{quasi}(\alpha, \theta; y_i \rvert \mathbf{0}) \right) \text{tr}\left[  \left(  \left( \sum_{i = 1}^n \frac{\partial \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0})}{\partial \eta_i} z_i \right) \left( \sum_{i = 1}^n \frac{\partial \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0})}{\partial \eta_i} z_i^\top \right) + \sum_{i = 1}^n \frac{\partial^2 \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0})}{\partial \eta_i^2} z_i z_i^\top \right) D(\theta) \right] \\
&\hspace{6mm} + \exp \left( \sum_{i = 1}^n \ell_\text{quasi}(\alpha, \theta; y_i \rvert \mathbf{0}) \right) \mathbb{E}\left[ e \right] \\
&\overset{(iii)}{=} \exp \left( \sum_{i = 1}^n \ell_\text{quasi}(\alpha, \theta; y_i \rvert \mathbf{0}) \right) \left(1 + \frac{1}{2} \text{tr}\left[  \left(  \left( \sum_{i = 1}^n \frac{\partial \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0})}{\partial \eta_i} z_i \right) \left( \sum_{i = 1}^n \frac{\partial \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0})}{\partial \eta_i} z_i^\top \right) + \sum_{i = 1}^n \frac{\partial^2 \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0})}{\partial \eta_i^2} z_i z_i^\top \right) D(\theta) \right] + o(\rvert \rvert \theta \rvert \rvert)  \right)
\end{aligned}
\label{eq:trace-term}
$$ 

---
<details>
  <summary>Proof of $(i)$, $(ii)$, $(iii)$.</summary>
  $(i)$ follows from the fact that $\mathbb{E}[\beta] = \mathbf{0}$. This implies that:
  
  $$
  \exp \left( \sum_{i = 1}^n \ell_\text{quasi}(\alpha, \theta; y_i \rvert \mathbf{0}) \right) \mathbb{E}\left[ \beta^\top \right] \left( \sum_{i = 1}^n \frac{\partial \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0})}{\partial \eta_i}  z_i^\top \right) 
  = \exp \left( \sum_{i = 1}^n \ell_\text{quasi}(\alpha, \theta; y_i \rvert \mathbf{0}) \right) \mathbf{0}^\top \left( \sum_{i = 1}^n \frac{\partial \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0})}{\partial \eta_i}  z_i^\top \right) = 0
  $$

  $(ii)$ follows from simple matrix properties and the linearity property of expectation. Let:

  $$
  A = \left( \sum_{i = 1}^n \frac{\partial \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0})}{\partial \eta_i} z_i \right) \left( \sum_{i = 1}^n \frac{\partial \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0})}{\partial \eta_i} z_i^\top \right) + \sum_{i = 1}^n \frac{\partial^2 \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0})}{\partial \eta_i^2} z_i z_i^\top
  $$

  Let $a_{i,j}$ denote the $(i,j)$-th entry in $A$ (which is a $q \times q$ matrix). Then:

  $$
  \begin{aligned}
  \mathbb{E}[\beta^\top A \beta] &= \mathbb{E}\left[ \sum_{i = 1}^q \sum_{j = 1}^q a_{i,j} \beta_i \beta_j \right] \\
  &= \sum_{i = 1}^q \sum_{j = 1}^q a_{i,j} \mathbb{E}\left[ \beta_i \beta_j \right] \\
  &= \sum_{i = 1}^q \sum_{j = 1}^q a_{i,j} \text{Cov}(\beta_i, \beta_j) \\
  &= \sum_{i = 1}^q \sum_{j = 1}^q a_{i,j} D(\theta)_{i,j} \\
  &= \text{tr}\left[ A D(\theta) \right]
  \end{aligned}
  $$

  $(iii)$ is due to the assumption that third order moments and higher of $\beta$ are $o(\rvert \rvert \theta \rvert \rvert)$, and each term in $e$ contains a higher order moment of $\beta$ (i.e. each term involves $\mathbb{E}[\beta^k]$ for $k > 3$).
</details>
---


Let $\frac{\partial \ell_\text{quasi}(\alpha, \theta; y \rvert \beta)}{\partial \eta}$ denote the $n$-dimensional vector first-order partial derivatives. Similarly, let $\frac{\partial^2 \ell_\text{quasi}(\alpha, \theta; y \rvert \beta)}{\partial \eta \partial \eta^\top}$ be the $n \times n$ matrix of second-order partial derivatives. 

We can write our Taylor expansion of the log quasi-likelihood about $\beta = \mathbf{0}$ in matrix notation as:

$$
\ell_\text{quasi}(\alpha, \theta; y) = \sum_{i = 1}^n \ell_\text{quasi}(\alpha, \theta; y_i \rvert \mathbf{0}) + \frac{1}{2}\text{tr}\left[ Z^\top \left( \frac{\partial \ell_\text{quasi}(\alpha, \theta; y \rvert \mathbf{0})}{\partial \eta} \frac{\partial \ell_\text{quasi}(\alpha, \theta; y \rvert \mathbf{0})}{\partial \eta^\top} + \frac{\partial^2 \ell_\text{quasi}(\alpha, \theta; y \rvert \mathbf{0})}{\partial \eta \partial \eta^\top} \right) Z D(\theta) \right] + o(\rvert \rvert \theta \rvert \rvert)
\label{eq:log-quasi-lik-exp}
$$

---

## Global Test

Suppose we are in the situation where we have the standard generalized linear mixed model:

$$
g(\mu) = X \alpha + Z \beta
$$

and we want to test whether $\theta = \mathbf{0}$. 

Define the following $n \times n$ diagonal matrices with entries:

$$
\begin{aligned}
\delta_i &= \frac{1}{g'(\mu_i)}\\
w_i &= \frac{1}{v(\mu_i)(g'(\mu_i))^2} = \frac{\delta_i^2}{v(\mu_i)}
\end{aligned}
$$

and let $\eta_i = \mathbb{E}[y_i \rvert \beta]$. Further define the matrix (under $H_0$):

$$
\begin{aligned}
W_o &= -\frac{\partial^2 \ell_{\text{quasi}}(\alpha, \theta; y \rvert \beta)}{\partial \eta \eta^\top} \\
&= -\frac{\partial^2 \ell_{\text{quasi}}(\alpha, \mathbf{0}; y \rvert \mathbf{0})}{\partial \eta \eta^\top} \\
&= \text{diag}\left( w_i + e_i(y_i - \mu_i) \right)
\end{aligned}
$$

for $e_i = \frac{v'(\mu_i)g'(\mu_i) + v(\mu_i)g''(\mu_i)}{v^2(\mu_i) (g'(\mu_i))^3)}$. 


We can calculate the quasi-score vector for $\beta$ by finding the vector of first-order partial derivatives of $\ell_\text{quasi}(\alpha, \theta; y)$ with respect to the components of $\beta$:

$$
\begin{aligned}
U_{\theta_j}(\hat{\alpha}) &= \frac{\partial \ell_\text{quasi}(\alpha, \theta; y)}{\partial \theta_j} \\
&= 
\end{aligned}
$$





---





<!-- 
$$
\begin{aligned}
\mathcal{L}(\alpha, \theta; y \rvert \beta) &= \exp \left( \sum_{i = 1}^n \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \beta) \right) \\
&= \exp \left( \sum_{i = 1}^n \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0}) \right) + \left( Z\beta - Z\mathbf{0} \right)^\top \nabla_\eta \left[ \exp \left( \sum_{i = 1}^n \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0}) \right) \right] 
+ \frac{1}{2}\left( Z\beta - Z\mathbf{0} \right) ^\top  \nabla^2_\eta \left[ \exp \left( \sum_{i = 1}^n \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0}) \right) \right] \left( Z\beta - Z\mathbf{0} \right)  + \epsilon \\
&= \exp \left( \sum_{i = 1}^n \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0}) \right) \left(1 + (Z\beta)^\top   \nabla_\eta \left[ \sum_{i = 1}^n  \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0}) \right] + \frac{1}{2} (Z \beta)^\top  \nabla^2_\eta \left[ \sum_{i = 1}^n \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0})\right] Z\beta + \epsilon' \right) \\
&= \exp \left( \sum_{i = 1}^n \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0}) \right) \left(1 + (Z\beta)^\top  \left[ \sum_{i = 1}^n   \nabla_\eta \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0}) \right] + \frac{1}{2} (Z \beta)^\top  \left[ \sum_{i = 1}^n \nabla^2_\eta \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0})\right] Z\beta + \epsilon' \right) \\
&= \exp \left( \sum_{i = 1}^n \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0}) \right) \left(1 +  \sum_{i = 1}^n  (Z\beta)^\top  \nabla_\eta \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0}) + \frac{1}{2}  \sum_{i = 1}^n (Z \beta)^\top  \left[ \nabla^2_\eta \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0}) \right] Z\beta + \epsilon' \right) \\
&= \exp \left( \sum_{i = 1}^n \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0}) \right) \left(1 +  \sum_{i = 1}^n  (Z\beta)^\top  \nabla_\eta \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0}) + \frac{1}{2}  \sum_{i = 1}^n (Z \beta)^\top  \left[ \nabla^2_\eta \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0}) \right] Z\beta + \epsilon' \right) \\
&\overset{(i)}{=} 
\end{aligned}
$$ -->

<!-- 
<details>
  <summary>Proof of $(i)$</summary>
  Since each $\ell(\alpha, \theta; y_i \rvert \mathbf{0})$ is a function of only $\eta_i$ (and no other components of $\eta$), the gradient reduces to a vector of $0$s with the $i$-th entry equal to $\frac{\partial \ell(\alpha, \theta; y_i \rvert \beta)}{\partial \eta_i}$, and the second order gradient reduces to a matrix with the $(i,i)$-th entry equal to $\frac{\partial^2 \ell(\alpha, \theta; y_i \rvert \beta)}{\partial \eta_i^2}$. 
  <br>
  Thus, $(i)$ follows from the fact that:

  $$
  \begin{aligned}
  (Z\beta)^\top \nabla_\eta \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0}) 
  &= 
  \begin{bmatrix}
  z_1^\top \beta &
  \dots &
  z_n^\top \beta
  \end{bmatrix}
  \begin{bmatrix}
  0 \\
  \vdots \\
  \frac{\partial \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0})}{\partial \eta_i} \\
  \vdots \\
  0
  \end{bmatrix} \\
  &= \frac{\partial \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0})}{\partial \eta_i} z_i^\top \beta
  \end{aligned}
  $$

  and:

  $$
  \begin{aligned}
  (Z\beta)^\top \nabla^2_\eta \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0})   (Z\beta)
  &= 
  \begin{bmatrix}
  z_1^\top \beta &
  \dots &
  z_n^\top \beta
  \end{bmatrix}
  \begin{bmatrix}
  0 & \dots & 0 & \dots & 0\\
  \vdots & & \ddots & & \vdots \\
  0 & \dots & \frac{\partial^2 \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0})}{\partial \eta_i^2} & \dots & 0 \\
  \vdots & & \ddots & & \vdots \\
  0 & \dots & 0 & \dots & 0
  \end{bmatrix}
  \begin{bmatrix}
  z_1^\top \beta \\
  \dots \\
  z_n^\top \beta
  \end{bmatrix} \\
  &= 
  \begin{bmatrix}
  0 & \dots & \frac{\partial^2 \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0})}{\partial \eta_i^2} z_i^\top \beta & \dots & 0
  \end{bmatrix}
  \begin{bmatrix}
  z_1^\top \beta \\
  \dots \\
  z_n^\top \beta
  \end{bmatrix} \\
  &= \frac{\partial^2 \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \mathbf{0})}{\partial \eta_i^2} z_i^\top \beta
  \end{aligned} 
  $$

  
</details> -->



 <!-- &= \sum_{i = 1}^n  
    \begin{bmatrix}
    \frac{\partial^2 \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \beta)}{\partial \beta_1^2} &
    \dots & 
    \frac{\partial^2 \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \beta)}{\partial \beta_1 \partial \beta_q} \\
    \vdots & \ddots & \vdots \\
    \frac{\partial^2 \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \beta)}{\partial \beta_q \partial \beta_1} &
    \dots & 
    \frac{\partial^2 \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \beta)}{\partial \beta_q^2} \\
    \end{bmatrix} \\
    &= \sum_{i = 1}^n 
    \begin{bmatrix}
    \frac{\partial}{\partial \beta_1} \frac{\partial \eta_i}{\partial \beta_1} \frac{\partial \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \beta)}{\partial \eta_i} &
    \dots & 
    \frac{\partial}{\partial \beta_1} \frac{\partial \eta_i}{\partial \beta_q} \frac{\partial \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \beta)}{\partial \eta_i} \\
    \vdots & \ddots & \vdots \\
    \frac{\partial}{\partial \beta_q} \frac{\partial \eta_i}{\partial \beta_1} \frac{\partial \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \beta)}{\partial \eta_i} &
    \dots & 
    \frac{\partial}{\partial \beta_q} \frac{\partial \eta_i}{\partial \beta_q} \frac{\partial \ell_{\text{quasi}}(\alpha, \theta; y_i \rvert \beta)}{\partial \eta_i} \\
    \end{bmatrix} \\ -->


---
## References


Breslow, N. E., and D. G. Clayton. “Approximate Inference in Generalized Linear Mixed Models.” Journal of the American Statistical Association 88, no. 421 (1993): 9–25. [doi:10.2307/2290687](https://doi.org/10.2307/2290687).


Lin, X. "Variance component testing in generalised linear models with random effects". Biometrika, Volume 84, Issue 2, June 1997. Pages 309–326. [doi:10.1093/biomet/84.2.309](https://doi.org/10.1093/biomet/84.2.309).

Zhang, D., Lin, X. (2008). "Variance Component Testing in Generalized Linear Mixed Models for Longitudinal/Clustered Data and other Related Topics". In: Dunson, D.B. (eds) Random Effect and Latent Variable Model Selection. Lecture Notes in Statistics, vol 192. Springer, New York, NY. [doi:10.1007/978-0-387-76721-5_2](https://doi.org/10.1007/978-0-387-76721-5_2).